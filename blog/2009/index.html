<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8"></meta>
<link rel="alternate" type="application/rss+xml" title="RSS" href="http://www.dwheeler.com/blog/index.rss"></link>
<title>David A. Wheeler's Blog   </title>
<meta name="viewport" content="width=device-width, initial-scale=1.0">
</head>
<body>
<h1 style="margin-left: auto; margin-right: auto; width: 50%;">David A. Wheeler's Blog</h1><p>  </p><p></p><h1>Thu, 31 Dec 2009</h1>
<p><a name="moglen-bilski"><font size="+2"><b>Moglen on Patents and Bilski</b></font></a></p><p></p>
<p>
<a href="http://www.isoc-ny.org/?p=1009">Eben Moglen has a very interesting presentation on patents (including comments on Bilski)</a>
that was originally presented on Nov. 2, 2009.
<a href="http://www.dwheeler.com/essays/software-patents.html">
Software patents and business method patents have been a disaster
for the U.S. and world economy</a>, and he has some interesting things
to say about how we got here (and how it could be fixed).
</p>
<p>
One interesting point he made, which I hadn&#8217;t heard before,
is that there is a fundamental conflict between the patent system and the
<a href="http://en.wikipedia.org/wiki/Administrative_Procedure_Act">Administrative Procedure Act of 1946 (aka the APA)</a>.
Nearly all of the U.S. government must obey the APA before
creating new rules and regulations.
According to the APA, U.S.
agencies must keep the public informed,
provide for public participation in the rulemaking process,
establish uniform standards for rulemaking and adjudication, and
provide for judicial review.
In particular, agencies normally have to perform a cost-benefit analysis.
</p>
<p>
But the patent system pre-existed the APA.
Patents, since they are government-created monopolies, can constrain
people in the same ways that any other rule or regulation can.
However, the government does <i>not</i> follow the APA to determine if
each proposed patent should be granted.
Instead, the old patent process was essentially grandfathered in instead,
as a special exception to the APA.
Because the APA is not considered when examining each patent,
<i>no one</i> in government
asks the normally-required question
&#8220;How will each proposed patent be publicly reviewed
before it is granted?&#8221;.
Patents on ideas that are patently obvious are routinely granted, in part
because there is no public review before they are granted
and because the patent office (by policy) ignores
most information available to the public.
All because the patent-granting process is <i>not</i> required to
enable public participation in the rulemaking process, in this case,
the process for permitting the granting of a patent.
Also, when examining a patent to determine if it should be granted,
no one asks normally-obvious questions like:
<ul><li>&#8220;What is the result of the cost-benefit analysis for this patent?
Is there good evidence that granting this government-created monopoly
would have societal benefits that greatly exceed all
of that monopoly&#8217;s costs to society?&#8221;
</li>
<li>&#8220;If the costs exceed the benefits,
what is the smallest term that could
be granted in which the benefits to society exceed their costs?&#8221;.
</li>
</ul>
</p>
<p>
Because the patent system predates the APA,
all potential harms to society from a patent are completely ignored
during the patent examination process.
If patents were individually considered as new regulations under the APA,
such questions would need to be carefully considered.
That&#8217;s an interesting point Moglen makes.
</p>
<p>
It&#8217;s my hope that the Supreme Court will clearly
<a href="http://www.dwheeler.com/essays/software-patents.html">stop software patents</a>.
We shall see.
</p>
<p>path: <a href="http://www.dwheeler.com/blog/oss">/oss</a> | <a href="http://www.dwheeler.com/blog">Current Weblog</a> | <a href="http://www.dwheeler.com/blog/2009/12/31#moglen-bilski">permanent link to this entry</a></p>
<h1>Sun, 13 Dec 2009</h1>
<p><a name="open-access-2009"><font size="+2"><b>U.S. research should be open access</b></font></a></p><p></p>
<p>
<a href="http://blog.ostp.gov/2009/12/09/ostp-to-launch-public-forum-on-how-best-to-make-federally-funded-research-available-for-free/">
The Office of Science and Technology Policy (OSTP)
has launched a &#8220;public consultation on Public Access Policy&#8221;,
to see if research funded by U.S. grants should be made available
as open access results</a>.
I think this is important &mdash; I believe publicly-funded
unclassified research should actually be made available to the public.
</p>
<p>
Historically, the U.S. pays a fortune for research, the
results are written up as papers for journals, and then various publishers
acquire total rights to these papers and charge exorbitant
monopoly fees for them.
The result:
Most U.S. citizens cannot afford to see the research their taxes pay for.
</p>
<p>
The basic question here is really straightforward:
Should publicly-funded research results
be made available directly to the public instead?
Or, should private companies continue to gain ownership over
publicly-funded results, for either nothing or a tiny fraction
of the public&#8217;s costs?
</p>
<p>
A small number of journal publishers and societies strongly
want to keep things the way they are, of course.
It makes sense from their point of view;
everybody likes free (or nearly free) money!
Historically, this arrangement was created because
it can be expensive to publish and manage paper.
However, that rationale has become completely obsolete.
Few people want the paper any more &mdash; they want the research,
on-line, without a paywall.
And don&#8217;t give me nonsense about the &#8220;costs&#8221; of peer review.
Many journals don&#8217;t pay their reviewers (the reviewers do it gratis),
and even if they did, the total control they gain is still unjustified;
the U.S. government spends far more per paper than they do for review.
</p>
<p>
The current sequestering of research is not good for science or the country.
I&#8217;m currently reading the interesting book &#8220;Are We Rome?&#8221;
by Cullen Murphy, and I can&#8217;t help but see some parallels.
Chapter 3 is all about &#8220;when public good meets private opportunity&#8221;.
Private organizations may pay for private research,
and then keep their results private.
But when the public pays for research, it should be <i>shocking</i>
if it does <i>not</i> get released back to the public.
And by &#8220;released back&#8221;, I mean released back at no fee at all.
</p>
<p>
So who will pay for the printing,
complex peer review, storage, and fancy indexing
of these research results?
I think the very question shows a failure to understand current technology,
but let&#8217;s answer it anyway.
Most peer review isn&#8217;t paid-for anyway, and if it is, it&#8217;s a tiny
cost compared to the research itself.
Storage? Don&#8217;t make me laugh; for $100 I can buy storage for the
all of the U.S. research papers for a year.
Indexing?
The government shouldn&#8217;t be doing serious indexing at all!!
Just put it on a government site with a basic form filled out
(title, authors, date, keywords, abstract, and a link to the actual
paper on the government site).
If it&#8217;s not behind a paywall, the many commercial search systems
will index it for you.
</p>
<p>
I <i>do</i> think there should be a centralized government repository
of such papers.
If it&#8217;s distributed, then papers could be lost without anyone knowing it.
I think they should be freely redistributable, so others can copy what they
want, but a centralized repository would make sure that we keep all of them
available forever.
Also, bandwidth costs can be reduced by scale.
There&#8217;s a risk that they all get lost at once, but it&#8217;s easier to copy
everything if there&#8217;s one place to start from.
If it&#8217;s a complicated site, then they&#8217;ve done it wrong&#8230;. for
each paper there should be a simple &#8220;summary&#8221; page with title, authors,
etc., and the actual paper itself.
</p>
<p>
OSTP cites the experience of NIH; NIH did wonderful work for releasing
as open access, and in my mind the real problems are that they
didn&#8217;t go far enough.
First, NIH has a one-year embargo&#8230; if I already paid for it (and I did),
why should wealthy people and organizations get the results first?
Second, NIH only considers the actual papers, not the data and
software programs that support the works&#8230; yet often those are
<i>more</i> important.
If they were funded by the public, then the public should get them
(unless they&#8217;re classified, of course, but then they shouldn&#8217;t be
released at all).
I&#8217;m sure there are complications and exceptions, but a
&#8220;default open access&#8221; policy would go a long way.
</p>
<p>
So please, tell the OSTP that the U.S.
<i>should</i> release government-funded research as open access publications,
available to anyone on the Internet without a paywall.
In short, if &#8220;we the people&#8221; paid for it,
then &#8220;we the people&#8221; should get it.
For more information, see this
<a href="http://www.ostp.gov/galleries/default-file/RFI%20Final%20for%20FR.pdf">
Request for Information (RFI)</a>
</p>
<p>path: <a href="http://www.dwheeler.com/blog/oss">/oss</a> | <a href="http://www.dwheeler.com/blog">Current Weblog</a> | <a href="http://www.dwheeler.com/blog/2009/12/13#open-access-2009">permanent link to this entry</a></p>
<h1>Sun, 29 Nov 2009</h1>
<p><a name="trusting-trust-success"><font size="+2"><b>Success on Fully Countering Trusting Trust through Diverse Double-Compiling</b></font></a></p><p></p>
<p>
My November 23 public defense of
<b><a href="http://www.dwheeler.com/trusting-trust/">Fully Countering Trusting Trust through Diverse Double-Compiling</a></b>
went well.
This was my 2009 PhD dissertation that expands on how to counter the
&#8220;trusting trust&#8221; attack by
using the &#8220;Diverse Double-Compiling&#8221; (DDC) technique.
</p>

<p>
Most importantly (to me), my PhD committee agreed that I successfully
defended my dissertation.
Whew!
As a result, I&#8217;m essentially done with my PhD.
</p>

<p>
I learned a lot about creating formal proofs using computers by
doing this dissertation.
I wanted to give the strongest possible evidence that DDC counters
the trusting trust attack, and formal proofs are the strongest
form of proof that I know of&#8230; which is why I created them.
Frankly, creating proofs was kind of fun once I knew what I was doing, but
getting there was more painful than it needed to be.
Many books are on the underlying mathematics (e.g., giving you extreme
detail about various logic systems)&#8230; which is great if you&#8217;re a
mathematician, but not so helpful if you are simply trying to
<i>use</i> the mathematics.
Some books explain how to do things by hand, but that is an
unnecessary amount of pain; one of my proofs is 30 steps long, and I
sure wouldn&#8217;t have wanted to create that by hand.
Some books seemed to assume that you <i>already</i> knew everything
the book covered, which is an odd assumption to me :-).
</p>

<p>
Here&#8217;s a trivial example: Most logic systems can prove <i>anything</i>
if you give them inconsistent assumptions.
That&#8217;s bad!
You can get rid of that problem by sending the assumptions to a
model-builder like mace4&#8230; if
it can create a model, then the assumptions are consistent.
So, make sure you send your assumptions
through a model-builder to see if your assumptions are consistent.
</p>

<p>
I&#8217;ve posted
<a href="http://www.dwheeler.com/trusting-trust/dissertation/">
detailed data from my dissertation</a>
so that people can reproduce my results.
I think it&#8217;s really important
that results be reduceable, otherwise, it&#8217;s not science.
As part of that data, I&#8217;ve included a few files
that may help potential proof tool users get started.
In particular, I&#8217;ve posted
<a href="http://www.dwheeler.com/trusting-trust/dissertation/mortal.in">prover9 input to prove that Socrates is mortal</a>, a
<a href="http://www.dwheeler.com/trusting-trust/dissertation/sqrt2.in">prover9 input to prove that the
square root of 2 is irrational</a>,
and
<a href="http://www.dwheeler.com/trusting-trust/dissertation/distinct.in">prover9 input showing
how to easily declare that terms in a list are distinct</a>.
</p>


<p>
The &#8220;trusting trust&#8221; attack
has historically been considered the &#8220;uncounterable&#8221; attack.
Now the attack can be effectively detected &mdash; and thus countered.
</p>
<p>path: <a href="http://www.dwheeler.com/blog/security">/security</a> | <a href="http://www.dwheeler.com/blog">Current Weblog</a> | <a href="http://www.dwheeler.com/blog/2009/11/29#trusting-trust-success">permanent link to this entry</a></p>
<h1>Fri, 20 Nov 2009</h1>
<p><a name="trusting-trust-lastcall"><font size="+2"><b>Fully Countering Trusting Trust through Diverse Double-Compiling</b></font></a></p><p></p>
<p>
A last-minute reminder &mdash; my public defense of
<b><a href="http://www.dwheeler.com/trusting-trust/">Fully Countering Trusting Trust through Diverse Double-Compiling</a></b>
is coming up on November 23, 1-3pm.
This is my 2009 PhD dissertation that expands on how to counter the
&#8220;trusting trust&#8221; attack by using the &#8220;Diverse Double-Compiling&#8221; (DDC)
technique.
</p>

<p>
It will be at
<a href="http://www.gmu.edu">George Mason University</a>,
Fairfax, Virginia,
<a href="http://itu.gmu.edu/innovationhall/aboutih.html">Innovation Hall</a>,
room 105.
[<a href="http://itu.gmu.edu/innovationhall/images/academiciv.gif">campus
location</a>] [<a
href="http://maps.google.com/maps?f=q&amp;source=s_q&amp;hl=en&amp;geocode=&amp;q=38.828240,+-77.307578&amp;sll=38.828258,-77.307615&amp;sspn=0.011417,0.01929&amp;ie=UTF8&amp;ll=38.827539,-77.307336&amp;spn=0.011417,0.01929&amp;z=16">Google
map</a>]
Anyone is welcome!
</p>

<p>
I&#8217;ve made a few small tweaks over the last few weeks.
I modified proof #2 to reduce its requirements even further
(making it even easier to do); I had mentioned in text that this was possible,
but now the formal proof shows it.
I also used mace4 to show that the assumptions of each proof are
consistent.
Formal proofs aren&#8217;t easy to create, or trivial to read, but the reason
I went to that trouble is to show that it&#8217;s not just my opinion
that I&#8217;ve countered the trusting trust attack&#8230; I want to show,
conclusively, that the trusting trust attack has been countered.
I know of no stronger method to show that than a formal proof.
</p>

<p>
The &#8220;trusting trust&#8221; attack
has historically been considered the &#8220;uncounterable&#8221; attack.
Nuts to that.
Now the attack can be effectively detected &mdash; and thus countered.
</p>
<p>path: <a href="http://www.dwheeler.com/blog/security">/security</a> | <a href="http://www.dwheeler.com/blog">Current Weblog</a> | <a href="http://www.dwheeler.com/blog/2009/11/20#trusting-trust-lastcall">permanent link to this entry</a></p>
<h1>Fri, 13 Nov 2009</h1>
<p><a name="trusting-trust-ddc-oss"><font size="+2"><b>Trusting Trust, DDC, and Free-Libre/Open Source Software (FLOSS)</b></font></a></p><p></p>
<p>
<a href="http://www.dwheeler.com/blog/2009/11/02/#trusting-trust-dissertation">
As I noted in my blog</a>,
I&#8217;ve just released my dissertation
<a href="http://www.dwheeler.com/trusting-trust">
&#8220;Fully Countering Trusting Trust through Diverse Double-Compiling</a> (DDC).
But what does that mean for Free-Libre/Open Source Software (FLOSS)?
In short, it&#8217;s <i>fantastic</i> news for FLOSS, but to explain why that&#8217;s so,
I need to backtrack first.
</p>
<p>
The &#8220;trusting trust&#8221; attack is a nasty computer attack that involves
creating a subverted compiler in such a way that it even subverts compilers.
It was originally reported in a 1974 security evaluation of Multics,
but most people heard about it from
Ken Thompson&#8217;s 1984 Turing Award presentation
(Ken Thompson is a creator of Unix).
This attack is incredibly nasty, and what&#8217;s worse,
until now there&#8217;s been no effective countermeasure to it.
Indeed, some have claimed that it could not ever be countered, making
the whole idea of &#8220;computer security&#8221; a non-starter.
</p>
<p>
The &#8220;trusting trust&#8221; attack <i>appears</i> to be especially
devastating to FLOSS.
The problem is that with the trusting trust attack, the source code that
people review does <i>not</i> correspond to the executable that&#8217;s
actually running, and that seems to
completely torpedo the &#8220;many eyes&#8221; review that FLOSS makes possible.
The whole world could carefully review a program&#8217;s source code,
but it wouldn&#8217;t matter if the compiler turns it undetectably
into something malicious.
</p>
<p>
Thankfully, there <i>is</i> an effective countermeasure,
which I&#8217;ve named
<a href="http://www.dwheeler.com/trusting-trust">Diverse
Double-Compiling (DDC)</a>.
You can see my dissertation which explains what it is, proves that
it works, and even demonstrates it with several compilers
including GCC.
(I will be giving a
<a href="http://www.dwheeler.com/trusting-trust">
public defense of it on November 23, 2009</a>, if
you&#8217;d like to come.)
This means that source code review, such as mass review of FLOSS code,
can now actually work.
</p>
<p>
But there&#8217;s more, because there&#8217;s an interesting catch with DDC.
DDC counters the trusting trust attack, but it&#8217;s only useful for
people who have access to the compiler source code.
Fundamentally, DDC is a technique for determining if a
compiler executable corresponds with its source code,
but only people who <i>have</i> the source code can apply DDC to
see if that&#8217;s true.
What&#8217;s more, only people who have access to the source code
will find the statement &#8220;the source and executable correspond&#8221;
particularly useful.
(You could use trusted intermediaries, but this requires total trust in
those intermediaries, making such claims
far weaker than claims that <i>anyone</i> can check.)
What&#8217;s more,
DDC is actually useful beyond what we <i>normally</i> think of
as compilers, because you can redefine
&#8220;compiler&#8221; as including other parts (such as the operating system).
In that case, you can even show that the system&#8217;s executables
all correspond to their source code.
But you can <i>only</i> use DDC to counter the trusting trust attack
if you have access to the source code.
</p>
<p>
So we now have a radical change.
Now that DDC has been shown to work,
we can see that
software with available source code (including FLOSS)
has a <i>fundamental security advantage</i> over other software.
That doesn&#8217;t mean that <i>all</i> FLOSS is more secure than
<i>all</i> proprietary software, of course.
But
<a href="http://www.dwheeler.com/essays/securing-oss.pdf">FLOSS
already had a general security advantage</a> because it better
meets Saltzer &amp; Schroeder&#8217;s &#8220;Open design principle&#8221;
(as explained in their 1974-1975 papers).
Now we have an attack &mdash; the trusting trust attack &mdash;
for which <i>FLOSS has a fundamental security advantage</i>.
The time of ignoring FLOSS options, because of misplaced notions
that FLOSS cannot be as secure as proprietary software,
needs to come to an end.
</p>
<!-- Reviewed by Clyde Roby on 2009-11-13 -->
<p>path: <a href="http://www.dwheeler.com/blog/oss">/oss</a> | <a href="http://www.dwheeler.com/blog">Current Weblog</a> | <a href="http://www.dwheeler.com/blog/2009/11/13#trusting-trust-ddc-oss">permanent link to this entry</a></p>
<h1>Mon, 02 Nov 2009</h1>
<p><a name="trusting-trust-dissertation"><font size="+2"><b>New PhD Dissertation: Fully Countering Trusting Trust through Diverse Double-Compiling</b></font></a></p><p></p>
<p>
An Air Force evaluation of Multics, and Ken Thompson&#8217;s Turing award
lecture (&#8220;Reflections on Trusting Trust&#8221;), showed that compilers can
be subverted to insert malicious Trojan horses into critical software,
<i>including themselves</i>.  If this &#8220;trusting trust&#8221; attack goes undetected,
even complete analysis of a system&#8217;s source code will not find the
malicious code that is running.  Previously-known countermeasures have
been grossly inadequate.  If this attack cannot be countered, attackers
can quietly subvert entire classes of computer systems, gaining complete
control over financial, infrastructure, military, and/or business
system infrastructures worldwide.
</p>
<p>
Thankfully, there <i>is</i> a countermeasure to the
&#8220;trusting trust&#8221; attack.
In 2005
<a href="http://www.dwheeler.com/trusting-trust">I wrote a paper on Diverse Double-Compiling (DDC), published by ACSAC</a>, where I explained DDC and
why it is an effective countermeasure.
But some people still raised concerns.
Would DDC <i>really</i> counter the attack?
Would DDC scale up to real-world compilers?
Also, the ACSAC paper required &#8220;self-parenting&#8221; compilers &mdash;
can DDC handle compilers that are <i>not</i> self-parenting?
</p>
<p>
I&#8217;m now releasing
<b><a href="http://www.dwheeler.com/trusting-trust/">Fully Countering Trusting Trust through Diverse Double-Compiling</a></b>,
my 2009 PhD dissertation that expands on how to counter the
&#8220;trusting trust&#8221; attack by using the &#8220;Diverse Double-Compiling&#8221; (DDC)
technique.
This dissertation was accepted by my PhD committee on October 26, 2009.
</p>
<p>
On <b>November 23, 2009, 1-3pm</b>, I will be giving a
<b>public defense</b> of this dissertation.
<b>If you&#8217;re interested, please come!</b>
It will be at
<a href="http://www.gmu.edu">George Mason University</a>,
Fairfax, Virginia,
<a href="http://itu.gmu.edu/innovationhall/aboutih.html">Innovation Hall</a>,
room 105.
[<a href="http://itu.gmu.edu/innovationhall/images/academiciv.gif">campus
location</a>] [<a
href="http://maps.google.com/maps?f=q&amp;source=s_q&amp;hl=en&amp;geocode=&amp;q=38.828240,+-77.307578&amp;sll=38.828258,-77.307615&amp;sspn=0.011417,0.01929&amp;ie=UTF8&amp;ll=38.827539,-77.307336&amp;spn=0.011417,0.01929&amp;z=16">Google
map</a>]
</p>
<p>
This dissertation&#8217;s thesis is that
the trusting trust attack can be detected and effectively countered
using the &#8220;Diverse Double-Compiling&#8221; (DDC) technique, as demonstrated by
(1) a formal proof that DDC can determine if source code and generated
executable code correspond, (2) a demonstration of DDC with four compilers
(a small C compiler, a small Lisp compiler, a small maliciously corrupted
Lisp compiler, and a large industrial-strength C compiler, GCC), and
(3) a description of approaches for applying DDC in various real-world
scenarios.  In the DDC technique, source code is compiled twice: once
with a second (trusted) compiler (using the source code of the compiler&#8217;s
parent), and then the compiler source code is compiled using the result
of the first compilation.  If the result is bit-for-bit identical with
the untrusted executable, then the source code accurately represents
the executable.
</p>
<p>
Many people commented on my previous 2005 ACSAC paper on the topic.
<a href="http://www.schneier.com/blog/archives/2006/01/countering_trus.html">
Bruce Schneier wrote an article on &#8216;Countering &#8220;Trusting Trust&#8221;&#8217;</a>,
which I think is one of the best independent articles describing my work
on DDC.
</p>
<p>
This 2009 dissertation significantly extends my previous 2005 ACSAC paper.
For example, I now have a formal proof that DDC is effective
(the ACSAC paper only had an informal justification).
I also have additional demonstrations, including one with GCC
(to show that it scales up) and one with a maliciously corrupted compiler
(to show that it really does detect them in the real world).
The dissertation is also more general; the ACSAC paper only considered
the special case of a &#8220;self-parenting&#8221; compiler,
while the dissertation eliminates that assumption.
</p>
<p>
So if you&#8217;re interested in countering the &#8220;trusting trust&#8221; attack,
please take a look at my
<a href="http://www.dwheeler.com/trusting-trust/">work on countering trusting trust through diverse double-compiling (DDC)</a>.
</p>
<p>path: <a href="http://www.dwheeler.com/blog/security">/security</a> | <a href="http://www.dwheeler.com/blog">Current Weblog</a> | <a href="http://www.dwheeler.com/blog/2009/11/02#trusting-trust-dissertation">permanent link to this entry</a></p>
<h1>Wed, 28 Oct 2009</h1>
<p><a name="dod-oss-2009-notes"><font size="+2"><b>Notes about the DoD and OSS memo</b></font></a></p><p></p>
<p>
Yesterday I posted about the
<a href="http://www.dwheeler.com/blog/2009/10/27/#dod-oss-2009">
new 2009 DoD memo about open source software</a>.
I&#8217;m delighted to see that the word is getting out.
<a href="http://linux.slashdot.org/story/09/10/27/2115243/New-DoD-Memo-On-Open-Source-Software?art_pos=16">
Slashdot</a>,
<a href="http://lwn.net/Articles/358971/">Linux Weekly News</a>, and
<a href="http://lxer.com/module/newswire/view/127313/index.html">
LXer.com</a> all mentioned the new memo and even pointed to my post.
Others are noting the new memo too, including
<a href="http://news.cnet.com/8301-13505_3-10384067-16.html">CNet&#8217;s Matt Asay</a>,
<a href="http://www.informationweek.com/news/government/enterprise-apps/showArticle.jhtml?articleID=221100039&subSection=All+Stories">InformationWeek&#8217;s
J. Nicholas Hoover</a>,
<a href="http://www.informationweek.com/blog/main/archives/2009/10/dod_says_yes_to.html">
InformationWeek&#8217;s Serdar Yegulalp</a>,
<a href="http://www.networkworld.com/news/2009/102709-dod-opensource.html?hpg1=bn">NetworkWorld</a>, and
<a href="http://www.h-online.com/open/news/item/US-Department-of-Defense-memo-opens-door-to-open-source-844137.html">The H</a>.
<a href="http://linux.slashdot.org/comments.pl?sid=1420791&cid=29892865">
Dan Risacher has posted on Slashdot some background and history
for this new 2009 DoD memo</a>.
He notes, for example, that &#8220;The lawyers were by far the biggest delay&#8221;
in getting this memo released.
</p>
<p>
There&#8217;s some supporting information for this memo at the
<a href="http://www.defenselink.mil/cio-nii/sites/oss">
DoD Free Open Source Software (FOSS) Communities of Interest (COI) site</a>,
which posts the memo itself and a supporting
<a href="http://www.defenselink.mil/cio-nii/sites/oss/Open_Source_Software_%28OSS%29_FAQ.htm">DoD Open Source Software Frequently Asked Questions (FAQ)</a> document.
</p>
<p>
To help potential users, I&#8217;ve updated my presentation
<a href="http://www.dwheeler.com/essays/dod-oss.pdf">
Open Source Software (OSS) and the U.S. Department of Defense (DoD)</a>,
which I hope will clarify some things.
I should also remind people about the
<a href="http://terrybollinger.com/">2003 MITRE study
&#8220;Use of Free and Open Source Software (FOSS)
in the U.S. Department of Defense&#8221;</a>,
which showed that in 2003 Free/libre/open source software (FLOSS, FOSS, or OSS)
was already widely used in the DoD.
</p>
<p>path: <a href="http://www.dwheeler.com/blog/oss">/oss</a> | <a href="http://www.dwheeler.com/blog">Current Weblog</a> | <a href="http://www.dwheeler.com/blog/2009/10/28#dod-oss-2009-notes">permanent link to this entry</a></p>
<h1>Tue, 27 Oct 2009</h1>
<p><a name="dod-oss-2009"><font size="+2"><b>New DoD memo on Open Source Software</b></font></a></p><p></p>
<p>
The U.S. Department of Defense (DoD) has just released
<a href="http://www.dwheeler.com/misc/DoD-OSS-memo-2009.pdf">
Clarifying Guidance Regarding Open Source Software (OSS)</a>,
a new official memo about
<a href="http://www.dwheeler.com/oss_fs_why.html">open source software (OSS)</a>.
This 2009 memo should soon be posted on the
<a href="http://www.defenselink.mil/cio-nii/policy/memorandums.shtml">list of ASD(NII)/DoD CIO memorandums</a>.
This 2009 memo is important for anyone who works with the DoD
(including contractors) on software and systems that include software&#8230;
and I suspect it will influence many other organizations as well.
Let me explain why this new memo exists, and what it says.
</p>
<p>
Back in 2003 the DoD released a formal memo titled
<a href="http://iase.disa.mil/policy-guidance/oss-in-dodmemo.pdf">
Open Source Software (OSS) in the Department of Defense</a>.
This older memo was supposed to make it clear that it was fine to
use and develop OSS in the DoD.
Unfortunately, as the new 2009 memo states,
&#8220;there have been misconceptions and misinterpretations of
the existing laws, policies and regulations that deal with
software and apply to OSS that have hampered effective DoD use and
development of OSS&#8221;.
</p>
<p>
This new 2009 memo simply explains
&#8220;the implications and meaning of existing laws,
policies and regulations&#8221;, hopefully eliminating
many of those misconceptions and misinterpretations.
A lot of the &#8220;meat&#8221; is in the Attachment 2, section 2 (guidance),
so let&#8217;s walk through that:
</p>
<p>
<ul>
<li>
Point (a) says that
&#8220;In almost all cases, OSS meets the definition of
&#8216;commercial computer software&#8217;
and shall be given appropriate statutory preference&#8230;&#8221;
This confirms what I&#8217;ve been saying for years (see
<a href="http://www.dwheeler.com/essays/commercial-floss.html">
FLOSS is commercial software</a>).
This is important in the U.S. government contracting world, because
government contracts are required to prefer commercial computer software.
If OSS is commercial software, then it needs to be preferred in the same
way that proprietary software is preferred (compared to rebuilding
everything from scratch).
It also makes many decisions easy; there are lots of rules for
&#8220;commercial software&#8221;, and since OSS is &#8220;commercial software&#8221;, then
contractors <i>already</i> know what the rules are.
</li>
<li>
Point (b) says that the DoD is &#8220;required to conduct market research&#8230;
[and should] include OSS [in the research] when it may meet mission needs&#8221;.
That&#8217;s a sea change all by itself &mdash; that means that people will
typically need to justify <i>not</i> considering OSS options.
What follows this statement is perhaps even more important&#8230;
here we have an official government document acknowledging
specific advantages OSS can have.
The memo says that &#8220;There are positive aspects of OSS that
should be considered&#8221;, noting 
&#8220;the continuous and broad peer-review enabled by publicly available
source code&#8221;,
&#8220;the unrestricted ability to modify software source code enables the
Department to respond more rapidly to changing situations, missions, and future threats&#8221;, the reduced risk of lock-in, the possibility of lower costs,
and so on.
Let&#8217;s be clear: this is <i>not</i> a statement that the DoD
will only use OSS, indeed, it clearly states that
&#8220;the software that best meets the needs
and mission of the Department should be used, regardless of whether the
software is open source&#8221;.
But it does tell people that there may be some very pointed questions
if the OSS options aren&#8217;t considered, and that&#8217;s good for both the
military and the taxpayer.
</li>
<li>
Point (c) counters a weird problem specific to the DoD.
DoD Instruction 8500.2 has a control called
&#8220;DCPD-1 Public Domain Software Controls&#8221;.
This control says that if you use a binary program,
you must either have a warranty or the source code for a program.
That makes sense;
if there&#8217;s a defect in the program, the government wants to
know that it will be fixed by someone else <i>or</i> have the rights
to fix it themselves.
Problem is, people decided to ignore the second part, and re-interpreted
this control as &#8220;cannot use open source software&#8221;, which is
<i>not</i> what it says.
Hopefully this terrible misconception will start going away soon.
</li>
<li>
Point (d) says,
&#8220;The use of any software without appropriate maintenance and support
presents an information assurance risk. Before approving the use of
software (including OSS), system/program managers, and ultimately
Designated Approving Authorities (DAAs), must ensure that the plan for
software support (e.g., commercial or Government program office support)
is adequate for mission need&#8221;.
This actually hits at a number of issues.
Some people say nonsense like &#8220;OSS has no support&#8221;, which is silly;
<a href="http://press.redhat.com/2009/07/27/red-hat-included-in-sampp-500-index/">
Red Hat just entered the S&amp;P 500</a>, primarily through paid support.
But it <i>is</i> true that when you use software &mdash; any software &mdash;
for a critically important function, you better have a plan for
maintenance and support.
This isn&#8217;t unique to OSS; it&#8217;s simply a true statement in general.
OSS does add additional options for <i>how</i> you do support, including
self-support and fully competed support, but you still need to plan for it.
</li>
<li>
Point (e) notes that there &#8220;is a misconception that
the Government is always obligated to distribute the
source code of any modified OSS to the public&#8221;, and then
explains the real situation.
</li>
<li>
Point (f) says that
&#8220;software source code and associated design documents are
&#8216;data&#8217; as defined by DoD Directive 8320.02,
and therefore shall be shared across the DoD as
widely as possible to support mission needs&#8230;&#8221;
This strikes a big blow at contractors who try to &#8220;hoard&#8221;
software that the government paid to develop.
In short,
DoD directives already require that such software be shared with others.
</li>
<li>
Point (g) says,
&#8220;Software items, including code fixes and enhancements, developed for
the Government <i>should</i> be released to the public
(such as under an open source license) when
all of the following conditions are met&#8230;&#8221;.
This certainly doesn&#8217;t say that all DoD software will be released as OSS
(nor would you expect that), but now the DoD is clarifying the issues
to be considered before releasing software as OSS.
It&#8217;s actually a simple 3-part test:
<ol>
<li>&#8220;The project manager, program manager, or other comparable official
determines that it is in the Governmentâ€™s interest to do so,
such as through the expectation of future enhancements by others.&#8221;
</li>
<li>&#8220;The Government has the rights to reproduce and release the item, and to authorize others to do so&#8230;.&#8221;</li>
<li>&#8220;The public release of the item is not restricted by other
law or regulation&#8230;&#8221;
</li>
</ol>
</li>
</ul>
</p>
<p>
But perhaps most important is this memo&#8217;s opening statement:
&#8220;To effectively achieve its missions, the Department of Defense
must develop and update its software-based capabilities faster than
ever, to anticipate new threats and respond to continuously changing
requirements. The use of Open Source Software (OSS) can provide advantages
in this regard&#8230;&#8221;.
As with the later part (b),
here we have an official government document acknowledging
that OSS can have a significant advantage.
What&#8217;s more, these potential advantages aren&#8217;t necessarily just minor
cost savings; OSS can in some cases provide a military advantage.
Which is a more-than-adequate justification for considering OSS,
as I have been advocating for years.
</p>
<p>
I&#8217;m really delighted that this memo has finally been released.
I participated in the original brainstorming meeting to create this memo
(as did <a href="http://powdermonkey.blogs.com/">John Scott</a>),
and I reviewed many versions of it,
but many, many other hands have stirred this pot since it began.
It took over 18 months to create it and get it out; getting this
coordinated was a very long and drawn-out process.
My thanks to everyone who worked to help make this happen.
In particular, congrats go to Dan Risacher, who led this project
to its successful completion.
</p>
<p>
By the way, if you&#8217;re interested in the issue of open source software in the
U.S. military/national defense, you probably should look at
<a href="http://www.mil-oss.org/">Mil-OSS</a> (at least, join their
mailing list, and consider going to their upcoming conference;
I was a speaker at their last one).
If you&#8217;re interested in the connection between open source software and
the U.S. government (including the military), you might also be
interested in the upcoming
<a href="http://goscon.org/">GOSCON</a> conference on November 5, 2009
(I&#8217;m one of the speakers there too).
</p>
<p>path: <a href="http://www.dwheeler.com/blog/oss">/oss</a> | <a href="http://www.dwheeler.com/blog">Current Weblog</a> | <a href="http://www.dwheeler.com/blog/2009/10/27#dod-oss-2009">permanent link to this entry</a></p>
<h1>Sat, 17 Oct 2009</h1>
<p><a name="cvc3-license"><font size="+2"><b>CVC3 License Changed to BSD</b></font></a></p><p></p>
<p>
<a href="http://www.cs.nyu.edu/acsys/cvc3/">CVC3</a>
is one of the better automated theorem provers.
Given certain mathematical assertions, it can in many cases prove
that certain claims follow from them.
Some tools that can prove properties about programs
use CVC3 (and/or similar programs).
For example, the
<a href="http://frama-c.cea.fr/jessie.html">Frama-C Jessie plug-in for C</a>
and
<a href="http://why.lri.fr/">Krakatoa for Java</a>
use
<a href="http://why.lri.fr/">Why</a>, which can build on one of several
programs including CVC3.
</p>
<p>
Problem is, CVC&#8217;s license has historically been a problem.
I understand that its authors intended for CVC3 to be
Free/Libre/Open Source Software (FLOSS),
but unfortunately, it was released with additional license clauses
that resulted in yet another non-standard license.
This was an unfortunate mistake; as I note in my
<a href="http://www.dwheeler.com/essays/gpl-compatible.html">
essay on GPL-compatible licenses</a>,
it is absolutely critical to choose a standard FLOSS license
when releasing FLOSS.
In this case, the big problem
was the addition of an &#8220;indemnification&#8221; clause
that was really scary; to some, at least, it seemed to imply that
if the CVC3 authors were sued,
anyone who used or copied the program was obligated to pay their
legal bills.
Interpreted that way, no one wanted to touch the program&#8230; how could
any user possibly know their risks?
Fedora eventually ruled that this license was non-free (aka not FLOSS),
and thus could not be included in Fedora.
There was a less-serious problem that if you made a change to the program,
you had to change the name&#8230; since the program couldn&#8217;t even
<i>compile</i> without a change (at the time), this meant that you <i>had</i>
to change the name almost instantly.
There is a <i>reason</i> that people have converged on standard
FLOSS licenses; if your lawyer says you need to add non-standard clauses,
be wary, because the result may be that few people can use your program.
</p>
<p>
I&#8217;m delighted to report that this has a happy ending.
<a href="http://www.cs.nyu.edu/acsys/cvc3/doc/LICENSE.html">
CVC3&#8217;s license has just been changed to a straight BSD license</a> -
a well-known license that is universally acknowledged as being FLOSS.
This means that there are no licensing problems for Linux distributions.
Only about a day after he found this out,
<a href="https://bugzilla.redhat.com/show_bug.cgi?id=529404">
Jerry James has submitted a CVC3 package to Fedora</a>.
So, I expect that in a relatively short time we&#8217;ll see CVC3
available directly in common Linux distribution repositories.
</p>
<p>
I think this is a helpful step towards
<a href="http://www.openproofs.org">open proofs</a>, which are cases
where an implementation, its proofs, and the necessary tools are all FLOSS.
Having a good tool like CVC3 to build on makes it easier to develop
<i>useful</i> tools.
My hope is to mature formal methods tools so that they can be more
scaleable, applicable, and effective than they are today.
It&#8217;s clear that a single little tool cannot possibly do the job;
we need suites of tools that can work together.
And this is a promising step in that direction.
</p>
<p>path: <a href="http://www.dwheeler.com/blog/oss">/oss</a> | <a href="http://www.dwheeler.com/blog">Current Weblog</a> | <a href="http://www.dwheeler.com/blog/2009/10/17#cvc3-license">permanent link to this entry</a></p>
<h1>Wed, 12 Aug 2009</h1>
<p><a name="auto-destdir"><font size="+2"><b>Auto-DESTDIR released!</b></font></a></p><p></p>
<p>
I&#8217;ve just released
<a href="http://www.dwheeler.com/auto-destdir">Auto-DESTDIR</a>,
a software package
which helps automate program installation on
POSIX/Unix/Linux systems from source code.
If you have the problem it solves &mdash; automatic support for
DESTDIR &mdash; you want this!
</p>

<p>
A little background:
Many programs for Unix/Linux are provided as source code.
Such programs must be
configured, built, and installed, and that last step is normally performed
by typing &#8220;make install&#8221;.
The &#8220;make install&#8221; step normally writes directly
to privileged directories like &#8220;/usr/bin&#8221;
to perform the installation.
Unfortunately,
most modern packaging systems (such as those for .rpm and .deb files)
require that files be written to some intermediate directory instead,
even though when run they will be in a different filesystem location
(because of security issues).
This redirection is easy to do if the installation script supports the
&#8220;DESTDIR convention&#8221;;
simply set DESTDIR to the intermediate directory&#8217;s
value and run &#8220;make install&#8221;.
Supporting DESTDIR is a best practice when
<a href="http://www.dwheeler.com/essays/releasing-floss-software.html">
releasing software</a>.
Unfortunately, many source packages don&#8217;t
support the DESTDIR convention.
Auto-DESTDIR causes &#8220;make install&#8221; to support DESTDIR,
<i>even if</i> the provided &#8220;makefile&#8221; doesn&#8217;t support
the DESTDIR convention.
Auto-DESTDIR is released under the
&#8220;MIT&#8221; license, so it is
Free-libre/open source software (FLOSS).
</p>

<p>
Auto-DESTDIR is implemented using a set of bash shell scripts that
wrap typical install commands (such as install, cp, ln, and mkdir),
These wrappers are placed in a special directory.
The run-redir command modifies the PATH so that the directory with these
scripts is listed first, and then runs the given command.
The make-redir command invokes &#8220;make&#8221; using run-redir, along with
some extra settings to simplify things.
For more information on this approach,
and why this is a good way to automate DESTDIR, see the paper
<a href="http://www.dwheeler.com/essays/automating-destdir.html">
Automating DESTDIR</a>, especially its
<a href="http://www.dwheeler.com/essays/automating-destdir.html#wrappers">
section on wrappers</a>.
</p>

<p>
So please take a look at the
<a href="http://www.dwheeler.com/auto-destdir">Auto-DESTDIR</a>
software package, if you have the problem it solves.
</p>
<p>path: <a href="http://www.dwheeler.com/blog/oss">/oss</a> | <a href="http://www.dwheeler.com/blog">Current Weblog</a> | <a href="http://www.dwheeler.com/blog/2009/08/12#auto-destdir">permanent link to this entry</a></p>
<h1>Sun, 26 Jul 2009</h1>
<p><a name="limiting-filenames-simplifies-things"><font size="+2"><b>Limiting Unix/Linux/POSIX filenames simplifies things: Lowercasing filenames</b></font></a></p><p></p>
<p>
My essay
<a href="http://www.dwheeler.com/essays/fixing-unix-linux-filenames.html">
<i>Fixing Unix/Linux/POSIX Filenames:
Control Characters (such as Newline), Leading Dashes, and Other Problems</i></a>
argues that adding some limitations on legal Unix/Linux/POSIX filenames
would be an improvement.
In particular, a few minor limitations (which most people assume anyway)
would eliminate certain kinds of bugs, some of which end up
being security vulnerabilities.
Forbidding crazy things (like control characters in filenames)
simplifies creating programs that work all the time.
</p>
<p>
Here&#8217;s a little example of this.
I wanted to convert all the filenames inside a directory tree
to all lowercase letters.
I didn&#8217;t want to lose any files without checking on them first,
so I wanted it to ask before doing a rename in a way that would
eliminate a file (i.e., I wanted to use <tt>mv&nbsp;-i</tt>).
I didn&#8217;t find such a program built into my distro, so I wrote
a short script to do it
(which is just as well, because it makes a nice simple example).
I wanted it to be portable, since I might need it again later.
</p>
<p>
So how do we write this?
A simple glob like &#8220;*&#8221; won&#8217;t work, because it needs to
recursively descend through a tree of directories, and simple globs
will skip hidden filesystem objects too (and I want to include them).
I could write a more complex glob that included hidden files and directories,
and recursed down through subdirectories, but the naive way of recursing
down subdirectories can have many problems (e.g., it could get stuck
in endless loops created by symbolic links).
If we need to handle a tree recursively, there&#8217;s a better
tool designed for the purpose &mdash; <tt>find</tt>.
</p>
<p>
Unfortunately, an ordinary
<tt>find&nbsp;.</tt>
has an interesting problem &mdash;
it will pick the upper-level names first,
and if we rename the upper-level names first, find will fail when
it tries to enter them (since they will no longer exist).
No problem &mdash; if we are manipulating the tree structure
(including renames), we can use the <tt>-depth</tt> option of find, which
will process each directory&#8217;s contents
before the directory itself.
We can then rename just the basename of what find returns, so
we won&#8217;t change anything before find descends into it.
</p>
<p>
Now, if we could assume that newlines and tabs cannot be in filenames, 
as recommended in
<a href="http://www.dwheeler.com/essays/fixing-unix-linux-filenames.html">
<i>Fixing Unix/Linux/POSIX Filenames&#8230;</i></a>, then
we can do a simple <tt>for</tt> loop around the results of find.
My
<a href="http://www.dwheeler.com/misc/mklowercase">shell script
<tt>mklowercase</tt> renames filenames to lowercase letters recursively</a>.
Here is its essence:
</p>
<pre>
  #!/bin/sh
  # mklowercase - change all filenames to lowercase recursively from "." down.
  # Will prompt if there's an existing file of that name (mv -i)
  # Presumes that filenames don't include newline or tab.

  set -eu
  IFS=`printf '\n\t'`
  
  for file in `find . -depth` ; do
    [ "." = "$file" ] &amp;&amp; continue                  # Skip "." entry.
    dir=`dirname "$file"`
    base=`basename "$file"`
    oldname="$dir/$base"
    newbase=`printf "%s" "$base" | tr A-Z a-z`
    newname="$dir/$newbase"
    if [ "$oldname" != "$newname" ] ; then
      mv -i "$file" "$newname"
    fi
  done
</pre>
<p>
This script skips &#8220;.&#8221;, which is not strictly necessary,
but I thought it would be a good idea to point out that you may need to skip
&#8220;.&#8221; sometimes.
</p>
<p>
Yes, this <i>could</i> be modified to handle literally all
possible Unix/Linux/POSIX filenames, but those modifications make
it more complicated and uglier.
One approach would be to use one program to use <tt>find&#8230;-exec</tt>, which
then invokes another script to do the renaming.
But then you have to maintain two scripts, and keep them in sync.
You could embed the command into find, but then the find command
becomes hideously complicated.
</p>
<p>
Another solution to handling all filenames would be to change the loop to:
</p>
<pre>
  find . -depth -print0 |
  while IFS="" read -r -d '' file ; do ...
</pre>
<p>
However, this requires non-standard GNU extensions to find (-print0)
and bash (read -d),
as well as being uglier and more complicated.  Also, if &#8220;mv&#8221; is implemented
as required by the Single Unix Standard, then the &#8220;mv -i&#8221;
will fail badly if it tries to rename a file into an existing name.
That&#8217;s because when it tries to get an answer, it will send a prompt to
stderr, but it will expect a RESPONSE from stdin&#8230; and yet, stdin
is where it gets the list of filenames!!
</p>
<p>
And it&#8217;s all silly anyway.
If you put newlines in filenames, <i>lots</i> of scripts fail.
It&#8217;s simply too much of a pain to deal with them &#8220;correctly&#8221;.
Which is the point of
<a href="http://www.dwheeler.com/essays/fixing-unix-linux-filenames.html">
<i>Fixing Unix/Linux/POSIX Filenames</i></a> &mdash;
adding some limitations on legal Unix/Linux/POSIX filenames
would be an improvement.
At the least, by default let&#8217;s forbid control characters
(so simple &#8220;find&#8221; and filename display is safe),
forbid leading dash characters (so simple globbing is safe),
require that all filenames be UTF-8
(so displaying filenames always works), and perhaps
forbid trailing spaces
(since these are dangerously misleading to end-users).
I would like to see kernels build in the mechanisms to
forbid certain kinds of filenames, so that administrators can
then specify the specific &#8220;bad filename&#8221; policy they
would like to use.
</p>
<p>
So please take a look at:
<a href="http://www.dwheeler.com/essays/fixing-unix-linux-filenames.html">
<i>Fixing Unix/Linux/POSIX Filenames:
Control Characters (such as Newline), Leading Dashes,
and Other Problems</i></a>.
I&#8217;ve made a few recent additions, thanks to some interesting
comments people have sent, but the basic message is the same.
</p>
<p>path: <a href="http://www.dwheeler.com/blog/security">/security</a> | <a href="http://www.dwheeler.com/blog">Current Weblog</a> | <a href="http://www.dwheeler.com/blog/2009/07/26#limiting-filenames-simplifies-things">permanent link to this entry</a></p>
<h1>Fri, 05 Jun 2009</h1>
<p><a name="spark-open-source"><font size="+2"><b>SPARK released as FLOSS (Free/ Libre / Open Source Software)!</b></font></a></p><p></p>
<p>
The <a href="http://libre.adacore.com/libre/tools/spark-gpl-edition/">
SPARK toolsuite has just been released as 
FLOSS (Free/ Libre / Open Source Software)</a> by Praxis (its creator).
This is great news for those who want to make
software safer, more reliable, and more secure.
In particular, this means that
<b><a href="http://www.openproofs.org/wiki/Tokeneer">Tokeneer</a> is now an
<a href="http://www.openproofs.org/">open proof</a></b>.
If you haven&#8217;t been following this, here&#8217;s some background.
</p>

<p>
Software is now a part of really critical systems
(ones that need &#8220;high assurance&#8221;),
yet often that software is not as safe, reliable, or secure as it needs to be.
I believe that in the long term,
<a href="http://www.dwheeler.com/essays/high-assurance-floss.html">
we will need to start
<i>proving</i> that our very important programs are correct</a>.
Testing by itself isn&#8217;t enough; completely testing the trivial
&#8220;add three 64-bit integers&#8221; program would take
far longer than the age of the universe
(it would take about 2x10^39 years).
<!--
2^(3*64) tests / 1 billion (per second) / 100 test machines
/ (365.25*24*60*60)
=  1.9890935100852669e+39 years
-->
The basic idea of using mathematics to <i>prove</i> that programs
are correct &mdash; aka &#8220;formal methods&#8221; &mdash;
has been around for decades.
There are a number of cases where formal methods have been applied
successfully, and I&#8217;m glad about that.
And yet, applying formal methods is still relatively rare.
There are many reasons for this, such as inadequate maturation and
capabilities of many formal methods tools, and the fact that
relatively few people know how to
apply formal methods when developing real programs.
But what, in turn, is causing those problems?
It&#8217;s true that applying formal methods is
a hard problem that hasn&#8217;t received the level of
funding it needs, but still, it&#8217;s been decades!
</p>

<p>
I believe one problem hindering the maturation and spread of
formal methods is a &#8220;culture of secrecy&#8221;.
Details of formal method use are often unpublished (e.g., because
the implementations are proprietary or classified).
Similarly, details about formal methods tools
are often unshared and lost (or have to constantly re-invented).
<a href="http://fmv.jku.at/papers/Biere-ETH-TR-444-2004.pdf">
Biere&#8217;s &#8220;The Evolution from LIMMAT to NANOSAT&#8221; (Apr
2004)</a>
gives an example:
&#8220;From the publications alone, without access to the source code,
various details were still unclear&#8230; Only [when CHAFF&#8217;s source code
became available did] our unfortunate design decision became
clear&#8230; The lesson learned is, that important details are often omitted
in publications and can only be extracted from source code. It can be
argued, that making source code of SAT solvers available is as important
to the advancement of the field as publications&#8221;
</p>

<p>
This &#8220;culture of secrecy&#8221;
means that researchers/toolmakers often don&#8217;t receive adequate feedback,
researchers/toolmakers waste time and money rebuilding tools,
educators have difficulty explaining formal methods
(they have no examples to show!),
developers don&#8217;t understand how to apply it
(and it has an uncertain value to them), and
evaluators/end-users don&#8217;t know what to look for.
</p>

<p>
I believe that a way to break through this
&#8220;culture of secrecy&#8221; is to develop
&#8220;open proofs&#8221;.
But what are they?
An
<a href="http://www.openproofs.org/">&#8220;open proof&#8221;</a>
is software or a system where <i>all</i> of the following are
free-libre / open source software (FLOSS):
<ul>
<li>the entire implementation</li>
<li>automatically-verifiable proof(s) of at least one key property, and</li>
<li>required tools (for use and modification)</li>
</ul>
Something is FLOSS if it gives anyone the freedom to use, study, modify, and redistribute modified and unmodified versions of it, meeting the
<a href="http://www.gnu.org/philosophy/free-sw.html">Free software definition</a> and the
<a href="http://www.opensource.org/docs/definition.php">open source definition</a>.
</p>

<p>
Imagine if we had a number of open proofs available.
There could be small open proofs that could be used for learning
(e.g., as examples and use in class exercises).
There could be proofs of various useful functions and small applications,
so developers could see how to scale up these techniques, directly reuse them
as components, or use them as starting points but add additional
(proven) capabilities to them.
When problems come up (and they will!), toolmakers and developers could
work together to find ways to mature the tools and technology
so that they&#8217;d be easier to use (e.g., so more could be automated).
In short, imagine there was a working ecosystem where
researchers/toolmakers/educators, developers of implementations to be proved,
and evaluators/end-users could work together by sharing information.
I believe that would greatly speed up the maturing of
formal methods, resulting in more reliable and secure software.
</p>

<p>
In this context, Praxis has just released the
<a href="http://libre.adacore.com/libre/tools/spark-gpl-edition/">
SPARK GPL Edition</a>.
This is their SPARK toolsuite (a formal methods tool)
released under the GNU General Public License aka GPL
(the most common FLOSS license).
So, what&#8217;s that?
</p>

<p>
SPARK is a variant of the Ada programming language, designed to
enable proofs about programs (by adding and removing some features of Ada).
The additions are in special comments, so SPARK programs can be compiled
by a normal Ada compiler like GNAT (which is part of gcc).
The
<a href="http://www.openproofs.org/wiki/SPARK">Open Proofs page on SPARK</a>
has some information on SPARK.
The page
<a href="http://www.adacore.com/home/products/sparkpro/tokeneer/discovery/lesson_contracts/">What is Special About SPARK Contracts?</a>
gives a nice quick introduction to SPARK, which I will quote here.
It points out that the Ada line:
<pre>
        procedure Inc (X : in out Integer);
</pre>
just says there is some procedure &#8220;Inc&#8221; that may read a value X, and
may write it out, but that&#8217;s it.
In SPARK, you can add much more precise information, and the SPARK tools
can then check to see if they are true.
For example, if you say this using SPARK:
<pre>
        procedure Inc (X : in out Integer);
        --# global in out CallCount;
        --# pre  X &lt; Integer'Last and
        --#      CallCount &lt; Integer'Last;
        --# post X = X~ + 1 and 
        --#      CallCount = CallCount~ + 1;
</pre>
then the SPARK tools will ensure at compile-time (not run-time) that:
<ul>
<li>X and global variable CallCount must be read by at least
one path and must be updated by at least one path through the procedure
</li>
<li>
Inc can only called when both X and CallCount are less than Integer&#8217;Last.
The &#8220;pre&#8221; means &#8220;precondition&#8221;.
</li>
<li>
After Inc runs, both X and CallCount will always be incremented by one
(X~ refers to the initial value of X).
The &#8220;post&#8221; means &#8220;postcondition&#8221;.
</li>
</ul>
</p>

<p>
You can learn more about SPARK from the book
<a href="http://www.praxis-his.com/sparkada/sparkbook.asp">
High Integrity Software: The SPARK Approach to Safety and Security&#8221; by John
Barnes</a>.
<a href="http://www.praxis-his.com/sparkada/pdfs/sampler_final.pdf">
Sample text of Barnes&#8217; book is available online.</a>
The <a href="http://www.openproofs.org/wiki/SPARK">open proofs page
on SPARK</a>
has more information.
</p>

<p>
This means that the
<a href="http://www.openproofs.org/wiki/Tokeneer">&#8220;Tokeneer&#8221;</a>
program is now an open proof.
Remember, to be an open proof, a program&#8217;s implementation, proofs,
and required tools have to be open source software.
Tokeneer was a sample program written to show how to apply these
kinds of techniques to actual systems
(instead of trivial 5-line programs).
The Tokeneer program itself, and its proofs, have already been
released as open source software.
Many of the tools it required are already FLOSS
(e.g., fuzz and LaTeX for its formal
specifications, and an Ada compiler to compile it).
Now that SPARK has been released as FLOSS, people can examine
this entire stack of software to make improvements in all the
technologies, as well as learn from them and create improved
implementations.
No, this doesn&#8217;t suddenly make it trivial to
make proofs about complex programs, but it&#8217;s a step forward.
</p>

<p>
If you are interested in making future software better,
<a href="http://www.openproofs.org/wiki/What_can_I_do_to_help">
please help the open proofs project</a>.
You don&#8217;t need to be a math whiz.
For example, if you know how to do shell scripting,
<a href="http://www.openproofs.org/wiki/Packaging_status">
please help us package some promising formal methods tools
(like SPARK) so they are easy to install</a>.
It&#8217;s hard to get people to try out these tools (and give feedback) if
they&#8217;re too hard to install.
If you know of formal methods software that is rotting in some warehouse,
try to get it released as FLOSS.
<a href="http://www.dwheeler.com/blog/2009/05/22/#default-release-oss">
I think all government-funded unclassified research software
should be released as FLOSS by default, since &#8220;we the people&#8221;
paid for it</a>!
If you&#8217;re interested in the latest software technology,
try out a few of these formal methods tools, and release as FLOSS
any small programs and proofs you develop with them.
Send the toolmakers feedback, or write down their strengths and weaknesses
to help others understand them.
SPARK is a tool that <i>can</i> be used, right now, in certain circumstances.
I have no illusions that today&#8217;s formal methods tools are
ready for arbitrary 20 million line programs.
But if we want future software to be better than today, we need to
figure out how to mature formal methods technology and make it
better-understood so that it <i>can</i> mature and scale.
I think making top-to-bottom worked examples and starting points
can help us get there.
</p>
<p>path: <a href="http://www.dwheeler.com/blog/oss">/oss</a> | <a href="http://www.dwheeler.com/blog">Current Weblog</a> | <a href="http://www.dwheeler.com/blog/2009/06/05#spark-open-source">permanent link to this entry</a></p>
<h1>Thu, 28 May 2009</h1>
<p><a name="parchment"><font size="+2"><b>Parchment: Running the Z-machine</b></font></a></p><p></p>
<p>
I just learned of fun web application called
<a href="http://code.google.com/p/parchment/">Parchment</a>.
Parchment lets you play interactive fiction (I.F., aka &#8220;text adventure games&#8221;)
using just your web browser.
It only works with I.F. in &#8220;Z-machine&#8221; format, but that&#8217;s
a very common format.
</p>
<p>
So
<a href="http://parchment.toolness.com/">
go to the parchment site and try out something from their
long list of interactive fiction</a>&#8230; now you don&#8217;t need to install
anything!
That includes
<a href="http://parchment.googlecode.com/svn/trunk/parchment.html?story=http://parchment.toolness.com/if-archive/games/zcode/Accuse.zblorb.js">
my small replayable puzzle &#8220;Accuse&#8221;</a>
(my <a href="http://www.dwheeler.com/accuse/">Accuse source code
is already available</a>).
</p>
<p>
If you want more information about it,
<a href="http://www.toolness.com/wp/?p=49">
here&#8217;s a brief post about Parchment by its author, Atul Varma</a>.
Atul built this based on an existing program,
Thomas Thurman&#8217;s Gnusto.
Both are open source software (using the GPLv2 license).
Once again, this demonstrates the neat thing about community-developed
software; one person developed a program for one circumstance,
and another extended it for a different circumstance.
</p>
<p>
There are several tools available for <i>creating</i> interactive fiction.
I&#8217;ve been watching
<a href="http://inform7.com/">Inform 7</a> for a while, with interest,
because it takes a radically different approach to writing code.
Inform 7 is a natural-language programming language that tries to
actively exploit features of natural language to make developing these
kinds of things easier.
<a href="http://www.brasslantern.org/writers/howto/i7tutorial.html">
You can see a brief Inform 7 tutorial if you&#8217;re curious</a>, as well as
the full <a href="http://www.inform7.com/learn/man/index.html">
Writing with Inform</a> documentation.
Inform 7 isn&#8217;t itself OSS, though significant portions are;
<a href="http://inform7.com/sources/i6n/">inform 6 (a key substrate)</a>
and
<a href="http://inform7.com/sources/webs/">
many other portions including the Inform 7 standard rules</a>
are released under the Artistic License 2.0.
The
<a href="http://inform7.com/write/extensions/">extensions</a>
are released under the &#8220;Attribution Creative Commons licence&#8221;;
that&#8217;s not normally a license used for software, but I think it&#8217;d meet
the criteria for OSS, and
<a href="https://fedoraproject.org/wiki/Licensing">Fedora approves of
this license for content</a>.
I hope that someday the rest will be released as OSS as well.
The logic behind Inform 7 is described in
<a href="http://www.inform7.com/learn/documents/WhitePaper.pdf">
&#8220;Natural Language, Semantic Analysis and Interactive Fiction&#8221;
by Graham Nelson</a>.
If you&#8217;re interested in some of the technical stuff behind it,
the text of the
<a href="http://www.inform7.com/sources/src/stdrules/Woven/index.html">
Standard Rules</a>,
the text of the
<a href="http://inform7.com/write/extensions/">extensions</a>,
<a href="http://www.ifwiki.org/index.php/Inform_7_for_Programmers">
Inform 7 for programmers</a>, and the
<a href="http://inform7.com/learn/documents/Rules%20Chart.pdf">Chart of Rules</a>
can tell you more.
</p>
<p>path: <a href="http://www.dwheeler.com/blog/oss">/oss</a> | <a href="http://www.dwheeler.com/blog">Current Weblog</a> | <a href="http://www.dwheeler.com/blog/2009/05/28#parchment">permanent link to this entry</a></p>
<h1>Sat, 23 May 2009</h1>
<p><a name="wikipedia-license-change"><font size="+2"><b>Wikipedia changes its license</b></font></a></p><p></p>
<p>
<a href="http://meta.wikimedia.org/wiki/Licensing_update/Result">
The Wikimedia Foundation (WMF) will change the licensing terms on all
its materials &mdash; including Wikipedia</a>.
Now, all of its existing material will be released under the
Creative Commons Attribution-ShareAlike (CC-BY-SA) license
in addition to the current GNU Free Documentation License (GFDL).
The 
WMF says &#8220;This change is meant to advance the WMF&#8217;s mission by
increasing the compatibility and availability of free content.&#8221;
This means that
Wikipedia material can now be combined with the vast amount
of CC-BY-SA licensed material, and Wikipedia can now include the volumes
of CC-BY-SA material (that material will just be CC-BY-SA).
It also makes it easier to use Wikipedia material (and other material
from the Wikimedia Foundation).
</p>
<p>
I think this is a good thing overall.
Incompatible licenses are a real scourge on community-developed works.
Past experience shows that
license incompatibility can be a real problem for
<a href="http://www.dwheeler.com/oss_fs_why.html">free-libre/
open source software (FLOSS or OSS)</a>, in particular.
<a href="http://oreilly.com/catalog/opensources/book/perens.html">
Bruce Perens warned about FLOSS license incompatibility back in 1999!</a>
As I argue in
<i><a href="http://www.dwheeler.com/essays/gpl-compatible.html">
Make Your Open Source Software GPL-Compatible. Or Else</a></i>,
you should release free-libre/ open source software (FLOSS) using a
GPL-compatible license.
You don&#8217;t need to <i>use</i> the GPL, but using
a GPL-compatible license (like the MIT, BSD-new, LGPL, or GPL)
so means that people can combine your software with other software
to create larger works.
I show how this works in
<i><a href="http://www.dwheeler.com/essays/floss-license-slide.html">
The Free-Libre / Open Source Software (FLOSS) License Slide</a></i>,
which has a simple graph showing how common FLOSS licenses can work together.
Wikipedia articles aren&#8217;t software, but the principles still apply -
licenses need to <i>enable</i> community-developed works, not disable them.
</p>
<p>
Now, nothing is perfect.
One nice benefit of the
<a href="http://www.gnu.org/copyleft/fdl.html">
GNU Free Documentation License (GFDL)</a>
is that it requires that readers be able to get editable versions
whose format specification is available to the public
(for details, see its text on &#8220;transparent&#8221; copies).
This is a really nice feature of the GFDL; it counters some of the problems
of proprietary formats.
</p>
<p>
The GFDL has many problems, though, when used for short works like
Wikipedia articles or images.
Most obviously, it requires that you include the entire text of the
license with each work (see GFDL 1.3 section 2).
That&#8217;s no problem for large manuals,
which is what the GFDL was designed for,
but it&#8217;s a big problem for short works.
Nobody likes having a license longer than the article it&#8217;s attached to!
This is one reason why CC-BY-SA is so widely used for short works -
and since Wikipedia is primarily a large set of short works, it makes sense.
Which is why I (and many others) voted to approve this change.
</p>
<p>
Now it&#8217;s certainly true that people also complain that the GFDL
allows the addition of unmodifiable sections.
But many GFDL items don&#8217;t have them, and
<a href="http://www.debian.org/vote/2006/vote_001">
Debian determined through a formal vote that &#8220;GFDL-licensed works
without unmodifiable sections are free [as in freedom]&#8221;</a>.
</p>
<p>
I should also give credit to the
Wikimedia Foundation (WMF), Richard Stallman of the FSF, and
Lawrence Lessig, who worked together to make this possible.
</p>
<p>
For more on the Wikimedia license modification, you can see
<a href="http://meta.wikimedia.org/wiki/Licensing_update/Questions_and_Answers">Wikimedia license FAQ</a>,
<a href="http://lessig.org/blog/2008/11/enormously_important_news_from.html">Lawrence Lessig&#8217;s post on GFDL 1.3</a>,
<a href="http://lwn.net/Articles/305892/">GFDL 1.3: Wikipedia&#8217;s exit
permit</a>,
<a href="http://www.gnu.org/licenses/fdl-1.3-faq.html">FDL 1.3 FAQ</a>,
and
<a href="http://www.fsf.org/blogs/licensing/2008-12-fdl-open-letter">
An open response to Chris Frey regarding GFDL 1.3</a>.
</p>
<p>path: <a href="http://www.dwheeler.com/blog/oss">/oss</a> | <a href="http://www.dwheeler.com/blog">Current Weblog</a> | <a href="http://www.dwheeler.com/blog/2009/05/23#wikipedia-license-change">permanent link to this entry</a></p>
<h1>Fri, 22 May 2009</h1>
<p><a name="default-release-oss"><font size="+2"><b>Government-developed Unclassified Software: Default release as Open Source Software</b></font></a></p><p></p>
<p>
I&#8217;d like to see this idea seriously considered and discussed:
By default, unclassified software which the government paid to develop
should be released to the public as
<a href="http://www.dwheeler.com/oss_fs_why.html">open source software</a>
(unless there&#8217;s a good reason not to).
</p>
<p>
Why? Well,
If &#8220;we the people&#8221; paid to develop it, then
&#8220;we the people&#8221; should get it!
I think this idea fits into the good government ideal of data transparency;
after all, software is data.
Currently, we have a lot of waste and unnecessary
costs due to loss, re-development, and/or government-created monopolies.
The government is not a venture capitalist (VC); people who need a VC
should go to a VC.
</p>
<p>
Let me focus specifically on the United States.
I think this idea easily fits into the broader ideas of
<a href="http://www.whitehouse.gov/blog/09/05/21/Opening/">transparency and open
government</a>, including the
<a href="http://www.whitehouse.gov/the_press_office/Transparency_and_Open_Government/">Memorandum on Transparency and Open Government</a>.
Look at all the excitement over <a href="http://www.data.gov/">data.gov</a>,
indeed,
<a href="http://sunlightlabs.com/contests/appsforamerica2/">Apps for America</a> having a contest to develop software to use data from data.gov.
</p>
<p>
Indeed, there&#8217;s a long history of U.S. laws specifically set up to
make data available.
Most obviously,
Freedom on information act (FOIA) requests make it possible to extract
information from the U.S. government.
<a href="http://www.law.cornell.edu/uscode/17/105.html">17 USC 105</a>
and
<a href="http://www.law.cornell.edu/uscode/17/usc_sec_17_00000101----000-.html">17 USC 101</a>
prevents the U.S. government from claiming U.S. copyright on a work
&#8220;prepared by an officer or employee of the United States Government
as part of that personâ€™s official duties&#8221;.
So this idea would be an extension of what&#8217;s already gone on.
</p>
<p>
Let me focus on research, and how this idea could help advance technology.
Think of all the advantages if software developed by U.S.-funded research
could be reused by other research projects and commercial firms.
For example, imagine if other researchers could simply
extend previous work by modifying previously-developed software,
instead of re-building yet another version from scratch.
Anyone could take commercialize the research making it more likely
that it would be commercialized instead of being lost in the archives shown
at the end of <i>Raiders of the Lost Ark</i>.
Some argue that giving sole rights is the only way to commercialization,
but that&#8217;s just not true;
<a href="http://www.dwheeler.com/essays/commercial-floss.html">
open source software <b>is</b> commercial software</a>, so this is
simply a different and fairer path to commercialization.
In contrast, the current system inhibits all kinds of technical progress;
<a href="http://fmv.jku.at/papers/Biere-ETH-TR-444-2004.pdf">
Biere&#8217;s &#8220;The Evolution from LIMMAT to NANOSAT&#8221; (Apr 2004)</a>
found that
&#8220;important details are often omitted in [research]
publications and can only be extracted from source code&#8230;
[Making source code available]
is as important to the advancement of the field as publications&#8221;.
Originally I thought of this idea for research software, and it&#8217;s
not hard to see why.
But when I starting thinking about the reasons for doing this &mdash;
especially &#8220;if &#8216;we the people&#8217; paid to develop it, then
&#8216;we the people&#8217; should get it&#8221; &mdash; then
I realized that this principle applies much more broadly.
</p>
<p>
An
<a href="http://transition2008.wordpress.com/2009/05/21/open-government-directive-not-yet/">open government directive</a> isn&#8217;t out yet, but they&#8217;re
clearly working on it.
Please submit this - and other ideas like it - to them.
I think there&#8217;s a lot of promise, but they can only enact and refine
ideas that they&#8217;ve heard of.
If you like this idea, <a href="http://opengov.ideascale.com/akira/dtd/2778-4049">please vote for it</a>.
</p>
<p>
If this happened, I envision a two-stage process:
(1) release of the software as an archive (so it can be downloaded), and
(2) some of it will get picked up and used to start an active OSS project.
The second stage might not happen for many years after the first, and
that&#8217;s okay.
Some will ask &#8220;how will people find it&#8221;, but I think that&#8217;s the wrong
question.
There are many commercial search engines that can find code, but they
can only find stuff that&#8217;s web-accessible; let&#8217;s give them
something to find.
</p>
<p>
Perhaps this should be done in stages.
For example, perhaps it&#8217;d be best to start with software developed by
research.
Researchers are supposed to share their results anyway (under most cases),
and the lack of software release often inhibits research
(e.g., it&#8217;s harder to check or repeat results).
You could then broaden this to other types of software.
</p>
<p>
I&#8217;m sure there will need to be exceptions.
There would need to be some sort of guidelines to figure out
when to grant those exceptions, and those guidelines should be
developed though lively discussion.
Most obviously,
if it&#8217;s a special ingredient necessary for national security, then
it should be classified and not revealed in <i>any</i> form.
I would not expect weapon systems or intelligence software
to be released (though sometimes
generic functions developed in them could be released).
Export controls would still apply.
But the exceptions should be that: Exceptions.
<!-- My thanks to Reg Meeson who looked this crazy idea over, and
agreed it wasn't crazy. -->
</p>
<p>path: <a href="http://www.dwheeler.com/blog/oss">/oss</a> | <a href="http://www.dwheeler.com/blog">Current Weblog</a> | <a href="http://www.dwheeler.com/blog/2009/05/22#default-release-oss">permanent link to this entry</a></p>
<h1>Mon, 11 May 2009</h1>
<p><a name="wikipedia-for-schools"><font size="+2"><b>Wikipedia for childrens&#8217; schools</b></font></a></p><p></p>
<p>
<a href="http://en.wikipedia.org/">Wikipedia</a> is a cool project.
But if you want to hand an encyclopedia to younger children or to schools,
Wikipedia is not a great choice.
Wikipedia is not &#8220;child-safe&#8221;, nor is intended to be; it includes
a lot of &#8220;adult&#8221; content.
Also, Wikipedia constantly suffers vandalism;
the vandalism is often repaired quickly,
but that&#8217;s little comfort to parents and teachers.
There&#8217;s also the problem of Internet access; schools typically employ
blocking software, and blocking software is fundamentally not smart.
Since Wikipedia mixes material that&#8217;s okay for children with stuff that
is not, Wikipedia often gets blocked by schools for children.
Some schools for children just don&#8217;t have Internet access at all,
for a variety of reasons.
All of this makes it hard for such schools to directly use Wikipedia.
</p>
<p>
<a href="http://schools-wikipedia.org/">Wikipedia for schools</a>
is a cool project that compensates for this.
It&#8217;s a free, hand-checked, non-commercial selection from Wikipedia,
targeted around the UK National Curriculum and useful for much of the
English speaking world. The current version has about 5500 articles
(as much as can be fit on a DVD with good size images)
and is &#8220;about the size of a twenty
volume encyclopaedia (34,000 images and 20 million words)&#8221;.
It was developed by carefuly selecting for content, then checking for
vandalism and suitability by &#8220;SOS Children volunteers&#8221;.
You can download it for free from the website, or as a free 3.5GB DVD.
<p>
I also see this as a future model for Wikipedia &mdash; allow people
to edit, but have a separate vetting process that identifies
particular versions of an article as vetted.
Then, people can choose if they want to
see the latest version or the most recent vetted version.
To some, this is very controversial, but I don&#8217;t see it that way.
A vetting process doesn&#8217;t prevent future edits,
and it creates a way for people to get what they want&#8230;
material that they can have increased confidence in.
The trick is to develop a good-enough vetting process (or perhaps
multiple vetting/rating processes for different purposes).
This didn&#8217;t make sense back when Wikipedia was first starting
(the problem was to get articles written at all!), but
now that Wikipedia is more mature, it shouldn&#8217;t be surprising
that there&#8217;s a new need to identify vetted articles.
Yes, you have to worry about countries to whom &#8220;democracy&#8221; is
a dirty word, but I think such problems can be resolved.
This is hardly a new idea; see
<a href="http://meta.wikimedia.org/wiki/Article_validation">Wikimedia&#8217;s
article on article validation</a>,
<a href="http://en.wikipedia.org/wiki/Wikipedia:Pushing_to_1.0">Wikipedia&#8217;s
pushing to 1.0</a>,
<a href="http://meta.wikimedia.org/wiki/User:Eloquence/WikiQA">WikiQA by
Eloquence</a>, and
<a href="http://www.mediawiki.org/wiki/Extension:FlaggedRevs">FlaggedRevs</a>.
I am sure that a vetting/validation process will take time to develop, and
it <i>will</i> be imperfect&#8230; but that doesn&#8217;t make it a bad idea.
</p>
<p>
So anyway, if you know or have younger kids, check out
<a href="http://schools-wikipedia.org/">Wikipedia for schools</a>.
This is a project that more people should know about.
</p>
<p>path: <a href="http://www.dwheeler.com/blog/oss">/oss</a> | <a href="http://www.dwheeler.com/blog">Current Weblog</a> | <a href="http://www.dwheeler.com/blog/2009/05/11#wikipedia-for-schools">permanent link to this entry</a></p>
<h1>Thu, 07 May 2009</h1>
<p><a name="floss-doubles-14-months"><font size="+2"><b>FLOSS doubles every 14 months!</b></font></a></p><p></p>
<p>
I just took a look at
<a href="http://www.redhat.com/f/pdf/written-statement-in-case-g.pdf">
Red Hat&#8217;s 2009 brief to the European Patent Office on why software patents
should not be allowed</a>.
It&#8217;s a nice brief, noting that software patents hinder software innovation,
and that there is a sound legal basis not to expand availability
of such patents in Europe.
(Here&#8217;s <a href="http://press.redhat.com/2009/04/30/old-world-and-new-world-software-patent-problems/">Red Hat&#8217;s press release</a>, and
<a href="http://www.computerworlduk.com/community/blogs/index.cfm?entryid=2162&amp;blogid=14">
Glyn Moody&#8217;s comments (ComputerWorld UK) on it</a>).
</p>

<p>
Their brief points to another paper with very interesting results:
<a href="http://dirkriehle.com/publications/2008/the-total-growth-of-open-source/">
&#8220;The Total Growth of Open Source&#8221;
by Amit Deshpande and Dirk Riehle
(Proceedings of the Fourth Conference on Open Source Systems (OSS 2008).
Springer Verlag, 2008. Page 197-209)</a>.
In this paper, they analyze the growth of more than 5000 open source software
projects, and show that
&#8220;the total amount of source code as well as the total number
of open source projects is growing at an exponential rate.&#8221;
In their conclusion they state that the
&#8220;total amount of source code and the total number of projects
double about every 14 months.&#8221;
</p>

<p>
That is an <i>extraordinary</i> rate of growth.
Exponential growth can start small, but when it continues it will
completely flatten anything not growing exponentially (or growing as fast).
This result is consistent with my earlier work,
<a href="http://www.dwheeler.com/sloc/"><i>More than a Gigabuck: Estimating GNU/Linux&#8217;s Size</i></a>, which also found very rapid growth in
<a href="http://www.dwheeler.com/oss_fs_why.html">free/libre/open source software (FLOSS)</a>.
</p>

<p>
So if you&#8217;re interested in software trends, take a look at
<a href="http://dirkriehle.com/publications/2008/the-total-growth-of-open-source/">&#8220;The Total Growth of Open Source&#8221;</a> and
<a href="http://www.redhat.com/f/pdf/written-statement-in-case-g.pdf">
Red Hat&#8217;s brief to the EPO on software patents</a>.
I think they&#8217;re both worth reading.
</p>
<p>path: <a href="http://www.dwheeler.com/blog/oss">/oss</a> | <a href="http://www.dwheeler.com/blog">Current Weblog</a> | <a href="http://www.dwheeler.com/blog/2009/05/07#floss-doubles-14-months">permanent link to this entry</a></p>
<h1>Sat, 02 May 2009</h1>
<p><a name="own-your-own-site"><font size="+2"><b>Own your own site!</b></font></a></p><p></p>
<p>
<a href="http://en.wikipedia.org/wiki/GeoCities">Geocities</a>,
a web hosting site sponsored by Yahoo, is shutting down.
Which means that, barring lots of work by others, all of its information
will be disappearing forever.
<a href="http://ascii.textfiles.com/archives/1961">
Jason Scott is trying to coordinate efforts to
archive GeoCities&#8217; information</a>,
but it&#8217;s not easy.
He estimates they&#8217;re archiving about 2 Gigabytes/hour,
pulling in about 5 Geocities sites per second&#8230; and they
don&#8217;t know if it&#8217;ll be enough.
What&#8217;s more, the group has yet to figure out how to serve it:
&#8220;It is more important to me to grab the data than to figure out how to
serve it later&#8230;.
I don&#8217;t see how the final collection wonâ€™t end up online, but how
is elusive&#8230;&#8221;
</p>

<p>
This sort of thing happens all the time, sadly.
Some company provides a free service for your site / blog / whatever&#8230;
and so you take advantage of it.
That&#8217;s fine, but if you care about your site,
make sure you own your data sufficiently so that you can
<i>move somewhere else</i>&#8230; because you may have to.
Yahoo is a big, well-known company, who paid $3.5 billion for
Geocities&#8230; and now it&#8217;s going away.
</p>

<p>
Please
<i>own your own site</i> &mdash; both its domain name and its content &mdash;
if it&#8217;s important to you.
I&#8217;ve seen way too many people have trouble with their sites because they
didn&#8217;t really own them.
Too many scams are based on folks who &#8220;register&#8221; your domain for you,
but actually register it in their own names&#8230; and then hold your
site as a hostage.
Similarly, many organizations provide wonderful software that is unique
to their site for managing your data&#8230; but then you either can&#8217;t get your
own data, or you can&#8217;t <i>use</i> your data because you can&#8217;t separately
get and re-install the software to use it.
Using open standards and/or open source software
can help reduce vendor lock-in &mdash;
that way, if the software vendor/website
disappears or stops supporting the product/service, you can still
use the software or a replacement for it.
And of course, continuously
back up your data offsite, so if the hosting service
disappears without notice, you still have your data and you can get back on.
</p>

<p>
I practice what I preach.
My personal site, <a href="http://www.dwheeler.com">www.dwheeler.com</a>,
has moved several times, without problems.
I needed to switch my web hosting service (again) earlier in 2009,
and it was essentially no problem.
I just used &#8220;rsync&#8221; to copy the files to my new
hosting service, change the domain information so people would use the
new hosting service instead, and I was up and running.
I&#8217;ve switched web servers several times, but since I emphasize using
ordinary standards like HTTP, HTML, and so on, I haven&#8217;t had any trouble.
The key is to (1) own the domain name, and (2)
make sure that you have your data (via backups) in
a format that lets you switch to another provider or vendor.
Do that, and you&#8217;ll save yourself a lot of agony later.
</p>
<p>path: <a href="http://www.dwheeler.com/blog/misc">/misc</a> | <a href="http://www.dwheeler.com/blog">Current Weblog</a> | <a href="http://www.dwheeler.com/blog/2009/05/02#own-your-own-site">permanent link to this entry</a></p>
<h1>Wed, 22 Apr 2009</h1>
<p><a name="damage-limits-dont-harm-floss"><font size="+2"><b>Why copyright damage limits don&#8217;t hurt FLOSS</b></font></a></p><p></p>
<p>
There&#8217;s a move afoot to argue that copyright infringement penalties should
bear a rational relationship to the value of what was infringed.
You <i>might</i> think that this could harm Free/Libre/Open Source Software
(FLOSS), but I don&#8217;t think so.  
Here&#8217;s why.
</p>
<p>
First: This is all being brought to a head by the
current file-sharing lawsuit against Boston University graduate student
Joel Tenenbaum, which raises a number of interesting questions.
One issue that I find particularly interesting is the issue of
statutory damages:
Are fines from $750 to $150,000 per song (worth at most $1),
non-commercially shared without permission, even legal under the
US Constitution?
Or, are these fines so excessive that they are unconstitutional?
<a href="http://arstechnica.com/tech-policy/news/2009/03/obama-admin-nothing-wrong-with-statutory-damages.ars">
Ars Technical gives a brief summary of the case</a>, if you haven&#8217;t
been following it.
The
<a href="http://beckermanlegal.com/pdf/?file=/Lawyer_Copyright_Internet_Law/sony_tenenbaum_090320FSFAmicusBrief.pdf">
Free Software Foundation (FSF)&#8217;s
Amicus Brief in Connection with defendant&#8217;s motion to dismiss on grounds of unconstitutionality
of copyright act statutory damages as applied
to infringement of single MP3 files</a>
argues that these penalties grossly exceed the crime;
the FSF argues that the
&#8220;State Farm/Gore due process test applicable to punitive damage awards
is likewise applicable to statutory damages, and in particular bars
the suggestion that each infringement of an MP3 file having a retail value
of 99 cents or less may be punishable by statutory damages of
from $750 to $150,000 &#8212; or from 2,100 to 425,000 times the actual damages&#8221;.
</p>
<p>
Frankly, I think the FSF and Tenenbaum have a reasonable argument on this point.
People who shoplift a CD from a store would definitely pay penalties when
caught, but those penalties would bear some relationship to the value of the
property stolen, and would be <i>far</i> smaller than a file-sharer.
This notion that the &#8220;punishment should fit the crime&#8221; is certainly not new;
Proverbs 6:30-31 talks about thieves paying sevenfold if they are caught.
That doesn&#8217;t make such actions <i>right</i> - but unjust 
penalties aren&#8217;t right either.
I think a lot of the problem is that copyright laws were originally
written when only rich people with printing presses could really
make and distribute many copies of material.
Today,
8-year-olds can distribute as much information as the New York Times, and
the law hasn&#8217;t caught up.
</p>
<p>
But does the FSF risk subverting Free/Libre/Open Source Software (FLOSS)
by making this argument?
After all, FLOSS developers also depend on copyright law to enforce
certain conditions, and often charge $0 for copies of their software.
If the penalties would be limited to &#8220;7 times the original cost&#8221;,
would that make FLOSS development impossible?
</p>
<p>
I don&#8217;t think there&#8217;s any problem, but for some people that may not be obvious.
The difference is that in a typical music copyright infringement case,
the filesharer <i>could</i> purchase the right to do what they&#8217;re doing for
a relatively low price, something typically <i>not</i> true for FLOSS.
For example, under normal circumstances
it&#8217;s perfectly legal to buy a song for $1, and then transfer that
song to someone else (as long as you destroy your own copies),
so sharing that song with 10 people is legal after paying $10.
</p>
<p>
In contrast, violations of FLOSS licenses often can&#8217;t be made
legal by simply buying the rights.
If you violate the revised BSD license by removing all credits to the
original author, there&#8217;s typically no &#8220;alternative&#8221; legal
version available for sale without the author credits.
(Indeed, under legal systems with strict &#8220;moral rights&#8221; it may not even be
possible.)
Similarly, if you violate the GPL by releasing binary software yet
refusing to release its source code,
there&#8217;s often no way to pay additional money
to the original authors for that privilege.
In <i>some</i> cases, GPL&#8217;ed software is released via a dual-use license
(e.g., &#8220;GPL or proprietary&#8221;), with the proprietary version costing additional
money; in <i>those</i> cases you <i>do</i> have a
value that you can compare against.
In cases where there <i>is</i> a value you can compare against, then
you should use that value to help determine the penalty.
Otherwise, a much stiffer penalty is justified, because there is no
method for the infringer to &#8220;buy&#8221; his or her way out, and their
actions risk making functional products (not just entertainment) unsupportable.
As noted in the
<a href="http://www.cafc.uscourts.gov/opinions/08-1001.pdf">
United States Court of Appeals for the Federal Circuit case 2008-1001,
JACOBSEN v. KATZER</a>,
the court essentially
found that failing to obey the conditions of an open source software
license led to copyright infringement.
(For more on this particular case,
see <a href="http://lawandlifesiliconvalley.blogspot.com/2007/08/new-open-source-legal-decision-jacobsen.html">New Open Source Legal Decision: Jacobsen &amp; Katzer and How Model Train Software Will Have an Important Effect on Open Source Licensing</a>.)
</p>
<p>
So I think that it <i>does</i> make sense to limit copyright
penalties based on the value of the original infringed item&#8230; but
that doing so does not (necessarily) put FLOSS development processes
at risk.
</p>
<p>path: <a href="http://www.dwheeler.com/blog/oss">/oss</a> | <a href="http://www.dwheeler.com/blog">Current Weblog</a> | <a href="http://www.dwheeler.com/blog/2009/04/22#damage-limits-dont-harm-floss">permanent link to this entry</a></p>
<h1>Mon, 20 Apr 2009</h1>
<p><a name="microsoft-encarta-wikipedia"><font size="+2"><b>Microsoft loses to Open Source Approaches (Encarta vs. Wikipedia)</b></font></a></p><p></p>
<p>
The competition is over.
On one side, we have Microsoft, a company with a market value of about
$166 billion (<a href="http://quotes.nasdaq.com/asp/SummaryQuote.asp?symbol=MSFT&selected=MSFT">according to a 2009-04-20 NASDAQ quote</a>).
On the other side, we have some volunteers who work together
and share their results on the web using open source approaches.
</p>
<p>
And Microsoft lost.
</p>
<p>
As pointed out by
<a href="http://education.zdnet.com/?p=2328">
Chris Dawson (ZDNet)</a>,
<a href="http://www.pcpro.co.uk/blogs/2009/04/02/why-ill-miss-microsoft-encarta/">Mike Jennings (PC Pro)</a>,
<a href="http://www.guardian.co.uk/technology/2009/apr/07/wikipedia-encarta">
Naomi Alderman (the Guardian)</a>,
<a href="http://bits.blogs.nytimes.com/2009/03/30/microsoft-encarta-dies-after-long-battle-with-wikipedia/">Noam Cohen (NY Times)</a>,
<a href="http://mashable.com/2009/03/30/microsoft-encarta-to-close/">Adam
Ostrow (Mashable)</a>,
and many others,
Microsoft Encarta (Microsoft&#8217;s encyclopedia project) has folded,
having failed to compete with
<a href="http://en.wikipedia.org">Wikipedia</a>.
It&#8217;s not even hard to see why:
<ol>
<li>Wikipedia is cheaper than Encarta (no-cost vs. cost)</li>
<li>Wikipedia is easier to start using. If you have a web browser,
you have Wikipedia.
In contrast, you have to specially install Encarta, and it does not
work on all platforms.</li>
<li>Wikipedia is more up-to-date than Encarta.
It often took years before Encarta entries got updated, even on trivially
obvious issues such as death dates.</li>
<li>Wikipedia has far more material.  Wikipedia has far more articles,
and generally it has far more material in each article.</li>
<li>Wikpedia&#8217;s material has fewer legal restrictions, so
users are allowed to do more with Wikipedia results.
Creating mash-ups and reposting portions is part of today&#8217;s world.</li>
</ol>
</p>
<p>
One lesson to be learned here
is that it sometimes doesn&#8217;t matter how large a company is;
changes in technology may mean that they may abandon something in the future.
Plan that the future will change, even if a company seems invincible.
It&#8217;s easy to pick on Microsoft here, but the same can be said of
IBM, or Oracle, or anyone else.
Tying yourself completely to any one company is, in the long term,
a mistake.
Thus, you need to have a reasonable escape plan
if a company folds or stops supporting a product that you depend on.
</p>
<p>
Another lesson to be learned here is that proprietary approaches
can be beaten by open source approaches.
That doesn&#8217;t mean it must happen every time, of course.
But clearly open source approaches can, at least sometimes,
dominate their proprietary competition.
</p>
<p>
In the long term,
it simply doesn&#8217;t matter if a company has more money
if an open-sourced
competitor can produce a better product, make it available at a lower cost,
and can sustain that process indefinitely.
Given those three factors, proprietary vendors
will lose to an open-sourced competitor
unless there&#8217;s a key differentiator
that is <i>sufficiently valuable to users</i>.
In such cases, <i>having</i> more money is just an opportunity to
<i>lose</i> more money; it gives no benefit of scale.
Microsoft&#8217;s Encarta team tried to compete by adding special materials
(like fancy graphics and sound).
I&#8217;m sure that Encarta managers convinced themselves that because they
were spending money to develop these materials, that users would pay for
Encarta instead.
They were wrong.
In the end, users were more interested in good, timely
information than in fancy graphics, and
Encarta simply didn&#8217;t have a chance.
Open source approaches were simply better at providing the encyclopedia
people wanted than proprietary approaches were.
</p>
<p>
The obvious question to me is, are there any lessons
that apply to software too?
Wikipedia uses free / libre / open source software (FLOSS) principles,
but Wikipedia is an encyclopedia not a FLOSS program.
Indeed, software is different than encyclopedias in many ways, for example,
people can easily switch encyclopedias
(while the lock-in and network effects of software are well-known), and
far more people can participate in encyclopedia development than in
software development.
But I still think there are lessons to be learned here.
This Encarta vs. Wikipedia battle
should make it clear that no proprietary company &mdash; no
matter how well-resourced it is &mdash; is invulnerable to
open source competition.
Developers of products with FLOSS-like licenses give up some privileges
that the law permits them to have, and in return, they can often drastically
reduce their development costs and increase the breadth of
the result (because the development efforts can be
shared among many developers).
At a certain point, FLOSS-like projects can end up like a snowball rolling
down the hill; they gain so much momentum that even large
sums of money &mdash; or being the first &mdash; aren&#8217;t
enough to counter them.
As a result,
even proprietary companies with massive cash resources do not always win.
In summary, it doesn&#8217;t matter if you have lots of money; if your
product costs more and does less (from the user&#8217;s point of view),
you must change that circumstance, eliminate all competition, or suffer
failure of the product.
<!-- My thanks to Ed Schneider, who gave this a look-over. -->
</p>
<p>path: <a href="http://www.dwheeler.com/blog/oss">/oss</a> | <a href="http://www.dwheeler.com/blog">Current Weblog</a> | <a href="http://www.dwheeler.com/blog/2009/04/20#microsoft-encarta-wikipedia">permanent link to this entry</a></p>
<h1>Mon, 13 Apr 2009</h1>
<p><a name="releasing-floss-software"><font size="+2"><b>Releasing FLOSS Software</b></font></a></p><p></p>
<p>
If you&#8217;ve written (or started to write) some
Free/Libre/Open Source Software (FLOSS), <i>please</i> follow
the time-tested community standards for releasing FLOSS software
when you want people to be able to install it from source code.
Unfortunately,
a lot of people don&#8217;t seem to be aware of what these conventions are.
This really hit me in my recent
<a href="http://www.openproofs.org">OpenProofs</a> work;
we&#8217;re trying to make it
easy to install programs by pre-packaging them, and we&#8217;ve found that some
programs are a <i>nightmare</i> to package or install
because their developers did not follow the standard conventions.
</p>

<p>
So I&#8217;ve released a brief article:
<a href="http://www.dwheeler.com/essays/releasing-floss-software.html">
<b><i>Releasing Free/Libre/Open Source Software (FLOSS) for Source Installation</i></b></a>, to help people learn about them.
For the details, I point to the
<a href="http://www.gnu.org/prep/standards/"><i>GNU Coding Standards</i></a>
(especially the
<a href="http://www.gnu.org/prep/standards/html_node/Managing-Releases.html">
release process</a> chapter) and the
<a href="http://en.tldp.org/HOWTO/Software-Release-Practice-HOWTO/"><i>Software Release Practice HOWTO</i></a>.
I also point out some of the most important conventions that will
make building and installing your software <i>much</i> easier for
your users:
<ol>
<li>Pick a good, simple, Google-able name.</li>
<li>Identify the version (using simple version numbers or ISO dates),
and include that in the release filename as NAME-VERSION.FORMAT.</li>
<li>Use a standard, widely-used, GPL-compatible FLOSS license &mdash; and say so.</li>
<li>Follow good distribution-making practice, in particular,
make sure tarballs always unpack into a single new directory named NAME-VERSION.</li>
<li>Use the standard invocation to configure, build, and install it:
<tt>./configure; make; make install</tt>.</li>
<li>Support the standard <tt>./configure</tt> options like &#8212;prefix,
&#8212;exec-prefix, &#8212;bindir, &#8212;libdir, and so on.</li>
<li>Create a makefile that can rebuild everything and
uses makefile variables (including applicable
standard makefile variable names and targets).</li>
<li>Have &#8220;make install&#8221; support DESTDIR.</li>
<li>Document the external tools/libraries needed for building and running,
and make it easy to separate/reuse them.</li>
<li>If you patch an external library/tool, get the patch upstream.</li>
<li>Use standard user interfaces.
For command line tools, use &#8220;-&#8221; single-letter options, &#8220;&#8212;&#8221; long-name
options, and &#8220;&#8212;&#8221; by itself to signal &#8220;no more options&#8221;.
For GUI tools, provide a .desktop file.</li>
</ol>
</p>

<p>
To learn more, see the whole article:
<a href="http://www.dwheeler.com/essays/releasing-floss-software.html"><b><i>Releasing Free/Libre/Open Source Software (FLOSS) for Source Installation</i></b></a>.
</p>
<p>path: <a href="http://www.dwheeler.com/blog/oss">/oss</a> | <a href="http://www.dwheeler.com/blog">Current Weblog</a> | <a href="http://www.dwheeler.com/blog/2009/04/13#releasing-floss-software">permanent link to this entry</a></p>
<h1>Tue, 24 Mar 2009</h1>
<p><a name="fixing-unix-linux-filenames"><font size="+2"><b>Fixing Unix/Linux/POSIX Filenames</b></font></a></p><p></p>
<p>
Traditionally, Unix/Linux/POSIX filenames can be almost
any sequence of bytes, and their meaning is unassigned.
The only real rules are that &#8220;/&#8221; is always the directory separator,
and that filenames can&#8217;t contain byte 0 (because this
is the terminator).
Although this is flexible, this creates many unnecessary problems.
In particular, this lack of limitations
makes it unnecessarily difficult to write correct programs
(<a href="http://www.dwheeler.com/secure-programs/Secure-Programs-HOWTO/file-names.html">enabling many security flaws</a>),
makes it impossible to consistently and accurately display filenames,
and it confuses users.
</p>

<p>
So for those of you who understand Unix/Linux/POSIX,
I&#8217;ve just released a new technical article,
<a href="http://www.dwheeler.com/essays/fixing-unix-linux-filenames.html">
Fixing Unix/Linux/POSIX Filenames</a>.
</p>

<p>
This article will try to convince you that
<a href="http://www.dwheeler.com/essays/fixing-unix-linux-filenames.html#complications">
adding <i>some</i> limitations on legal Unix/Linux/POSIX
filenames would be an improvement</a>.
<a href="http://www.dwheeler.com/essays/fixing-unix-linux-filenames.html#standards">
Many programs <i>already</i> presume these limitations,
the POSIX standard <i>already</i> permits such limitations, and
many Unix/Linux filesystems <i>already</i> embed such limitations</a> -
so it&#8217;d be better to make these (reasonable) assumptions
true in the first place.
The article discusses, in particular, the problems of
<a href="http://www.dwheeler.com/essays/fixing-unix-linux-filenames.html#control">control characters in filenames</a>,
<a href="http://www.dwheeler.com/essays/fixing-unix-linux-filenames.html#dashes">leading dashes in filenames</a>, the
<a href="http://www.dwheeler.com/essays/fixing-unix-linux-filenames.html#utf8">lack of a standard encoding scheme (vs. UTF-8)</a>,
and
<a href="http://www.dwheeler.com/essays/fixing-unix-linux-filenames.html#metacharacters">special metacharacters in filenames</a>.
<a href="http://www.dwheeler.com/essays/fixing-unix-linux-filenames.html#spaces">Spaces in filenames</a> are probably hopeless in general,
but resolving some of the other issues will simplify their handling too.
This article will then
<a href="http://www.dwheeler.com/essays/fixing-unix-linux-filenames.html#todo"> briefly discuss some methods for solving this long-term</a>,
though that&#8217;s not easy - if I&#8217;ve convinced you that this needs improving,
I&#8217;d like your help figuring out how to do it!

<p>
So - take a peek at
<a href="http://www.dwheeler.com/essays/fixing-unix-linux-filenames.html">
Fixing Unix/Linux/POSIX Filenames</a>.
If you have ideas on how to help, I&#8217;d love to know.
</p>
<p>path: <a href="http://www.dwheeler.com/blog/oss">/oss</a> | <a href="http://www.dwheeler.com/blog">Current Weblog</a> | <a href="http://www.dwheeler.com/blog/2009/03/24#fixing-unix-linux-filenames">permanent link to this entry</a></p>
<h1>Thu, 26 Feb 2009</h1>
<p><a name="uk-action-plan-2009"><font size="+2"><b>2009 UK Action Plan for Open Source Software</b></font></a></p><p></p>
<p>
A new report from the UK titled
<a href="http://www.cio.gov.uk/transformational_government/open_source/index.asp">Open Source, Open Standards and Reâ€“Use: Government Action Plan</a> is
in the news; it&#8217;s been reported by the
<a href="http://news.bbc.co.uk/2/hi/technology/7910110.stm">BBC</a>,
<a href="http://technology.timesonline.co.uk/tol/news/tech_and_web/article5804014.ece">Times Online</a>,
and
<a href="http://arstechnica.com/open-source/news/2009/02/uk-government-eyes-open-source-for-flexibility-cost-savings.ars">Ars Technica</a>
(among many others).
</p>
<p>
Here&#8217;s the first paragraph of its foreword:
&#8220;Open Source has been one of the most significant cultural developments in
IT and beyond over the last two decades: it has shown that individuals,
working together over the Internet, can create products that rival
and sometimes beat those of giant corporations; it has shown how giant
corporations themselves, and Governments, can become more innovative,
more agile and more cost-effective by building on the fruits of community
work; and from its IT base the Open Source movement has given leadership
to new thinking about intellectual property rights and the availability
of information for reâ€“use by others.&#8221;
</p>
<p>
In the policy section, it says that (note the last point):
<ul>
<li>
&#8220;The Government will actively and fairly consider open source solutions alongside proprietary ones in making procurement decisions,
</li>
<li>
Procurement decisions will be made on the basis on the best value for money solution to the business requirement, taking account of total lifetime cost of ownership of the solution, including exit and transition costs, after ensuring that solutions fulfil minimum and essential capability, security, scalability, transferability, support and manageability requirements.
</li>
<li>
The Government will expect those putting forward IT solutions to develop where necessary a suitable mix of open source and proprietary products to ensure that the best possible overall solution can be considered.
</li>
<li>
Where there is no significant overall cost difference between open and non-open source products, open source will be selected on the basis of its additional inherent flexibility.&#8221;
</li>
</ul>
</p>
<p>
Remarkable stuff.
</p>
<p>path: <a href="http://www.dwheeler.com/blog/oss">/oss</a> | <a href="http://www.dwheeler.com/blog">Current Weblog</a> | <a href="http://www.dwheeler.com/blog/2009/02/26#uk-action-plan-2009">permanent link to this entry</a></p>
<h1>Wed, 11 Feb 2009</h1>
<p><a name="open-proofs"><font size="+2"><b>Open Proofs: New site and why we need them</b></font></a></p><p></p>
<p>
There&#8217;s a new website in town:
<a href="http://www.openproofs.org">http://www.openproofs.org</a>.
This site exists to define the term &#8220;open proofs&#8221; and
encourage their development.
What <i>are</i> open proofs, you ask?
Well, let&#8217;s back up a little&#8230;
</p>
<p>
The world needs secure, accurate, and reliable software -
but most software isn&#8217;t.
Testing can find some problems, but testing by itself is inadequate.
In fact, it&#8217;s completely impractical to fully test real programs.
For example,
completely testing a trivial program that only add three 64-bit numbers,
using a trillion superfast computers, would take about
49,700,000,000,000,000,000,000,000,000 years!
Real programs, of course, are far more complex.
</p>
<p>
There is actually an old, well-known approach that <i>can</i> give much
more confidence that some software will do what it&#8217;s supposed to do.
These are often called &#8220;formal methods&#8221;,
which apply mathematical proof techniques to software.
These approaches can produce <i>verified</i> software, where you can prove
(given certain assumptions) that the software will (or won&#8217;t) do something.
There&#8217;s been progress made over the last several decades,
but they&#8217;re not widely used, even where it might make sense to use them.
If there&#8217;s a need, and a technology, why hasn&#8217;t it matured faster and
become more common?
</p>
<p>
There are many reasons, but I believe that one key problem is that
there are relatively few fully-public examples of verified software.
Instead,
verified software is often highly classified, sensitive, and/or proprietary.
Many of the other reasons are actually perpetuated by this.
Existing formal methods tools need more maturing, true, but it&#8217;s
rediculously hard for tool developers to mature the tools
when few people can show or share meaningful examples.
Similarly, software developers who have never used them
do not believe such approaches can be used in &#8220;real software development&#8221;
(since there are few examples) and/or can&#8217;t figure out how to apply them.
In addition, they don&#8217;t have existing verified software that they
can build on or modify to fit their needs.
Teachers have difficulty explaining them, and students have
difficulty learning from them.
All of this ends up being self-perpetuating.
</p>
<p>
I believe one way to help the logjam is to encourage
the development of &#8220;open proofs&#8221;.
An &#8220;open proof&#8221; is software or a system where all of the
following are free-libre / open source software (FLOSS):
<ul>
<li>the entire implementation</li>
<li>automatically-verifiable proof(s) of at least one key property, and</li>
<li>required tools (for use and modification)</li>
</ul>
Something is FLOSS if it gives anyone the freedom to use, study, modify,
and redistribute modified and unmodified versions of it, meeting the
Free software definition and the open source definition.
</p>
<p>
Open proofs do not solve every possible problem, of course.
I don&#8217;t expect formal methods techologies to become instantly trivial to
use just because a few open proofs show up.
And formal methods are always subject to limitations, e.g.:
(1) the formal specification might be wrong or incomplete for its purpose;
(2) the tools might be incorrect; (3) one or more assumptions might
be wrong. But they would still be a big improvement from where we are
today. Many formal method approaches have historically not scaled up to
larger programs, but open proofs may help counter that by enabling tool
developers to work with others.
In any case, I believe it&#8217;s worth trying.
</p>
<p>
So please take a look at:
<a href="http://www.openproofs.org">http://www.openproofs.org</a>.
For example, for open proofs to be easily created and maintained, we need
for FLOSS formal methods tools to be packaged up for common systems
so they can be easily installed and used;
<a href="https://www.openproofs.org/wiki/Packaging_status">the web site
has a page on the packaging status of various FLOSS tools</a>.
Please feel welcome to join us.
</p>
<p>path: <a href="http://www.dwheeler.com/blog/oss">/oss</a> | <a href="http://www.dwheeler.com/blog">Current Weblog</a> | <a href="http://www.dwheeler.com/blog/2009/02/11#open-proofs">permanent link to this entry</a></p>
<h1>Thu, 22 Jan 2009</h1>
<p><a name="automic-destdir"><font size="+2"><b>Automating DESTDIR for Packaging</b></font></a></p><p></p>
<p>
Today&#8217;s users of Linux and Unix systems (including emulation systems
like Cygwin) don&#8217;t want to manually
install programs - they want to easily install pre-packaged software.
But that means that someone has to create those packages.
</p>

<p>When you&#8217;re creating packages, an annoying step is handling
&#8220;make install&#8221; if the original software developer doesn&#8217;t support the
<a href="http://www.gnu.org/prep/standards/html_node/DESTDIR.html">
DESTDIR convention</a>.
DESTDIR support is very important, because
two of the most common packaging formats -
<a
 href="http://www.debian.org/doc/maint-guide/ch-modify.en.html#s-destdir">Debian&#8217;s
.deb</a> (used by Debian and Ubuntu) and RPM
(used by Fedora, Red Hat, and SuSE/Novell) - both <i>require</i>
actions (redirection of writes) that DESTDIR enables.
Unfortunately, many software developers don&#8217;t include support for
DESTDIR, and it&#8217;s sometimes a pain to add DESTDIR support.
Indeed, it&#8217;s often trivial to create packages <i>except</i> for having to
make the modifications for DESTDIR support.
Yes, adding DESTDIR support isn&#8217;t hard compared to many other tasks, but
since it applies to <i>every</i> program, why not automate this instead?
</p>
<p>
So, I&#8217;ve written a little essay about
<a href="http://www.dwheeler.com/essays/automating-destdir.html">
Automating DESTDIR for Packaging</a>.
In it, I identify some of the ways I&#8217;ve identified for automating DESTDIR.
If there are more - great! (Please let me know!).
In any case, I&#8217;d love to see more automation, so that software will become
easier to package and install.
</p>
<p>
Here&#8217;s the link, again:
<a href="http://www.dwheeler.com/essays/automating-destdir.html">
Automating DESTDIR for Packaging</a>.
</p>
<p>path: <a href="http://www.dwheeler.com/blog/oss">/oss</a> | <a href="http://www.dwheeler.com/blog">Current Weblog</a> | <a href="http://www.dwheeler.com/blog/2009/01/22#automic-destdir">permanent link to this entry</a></p>
<h1>Mon, 12 Jan 2009</h1>
<p><a name="ask-apple"><font size="+2"><b>Apple Feedback URL</b></font></a></p><p></p>
<p>
Oh, quick update - the URL for feedback to Apple is
<a href="http://www.apple.com/feedback">http://www.apple.com/feedback</a> -
I gave the wrong URL in my last post.
My thanks to Steve Hoelzer, who was the first to send me a correction!
Again - please ask them to support Ogg.
</p>
<p>path: <a href="http://www.dwheeler.com/blog/oss">/oss</a> | <a href="http://www.dwheeler.com/blog">Current Weblog</a> | <a href="http://www.dwheeler.com/blog/2009/01/12#ask-apple">permanent link to this entry</a></p>
<h1>Sat, 10 Jan 2009</h1>
<p><a name="ask-apple-support-ogg-ipod"><font size="+2"><b>Ask Apple to Support Ogg on iPod/iTunes</b></font></a></p><p></p>
<p>
<a href="http://www.apple.com/feedback">
Please ask Apple to support Ogg on their iPods, iPhones, and iTunes!</a>
It wouldn&#8217;t hurt to also
<a href="http://www.petitiononline.com/appl1435/petition.html">sign this
petition</a>
(and maybe <a href="http://jyte.com/cl/ipods-should-play-ogg-vorbis">this
one</a>), though I don&#8217;t know how strongly they&#8217;d influence Apple.
Here&#8217;s why, as good news and bad news.
</p>

<p>
Bad news: Some of the most common formats for audio (like MP3 and AAC) are
patent-encumbered, and thus not
<a href="http://www.digistan.org/">open standards</a>.
Because they&#8217;re patent-encumbered they are
harder and more expensive to support.
Many organizations like <a href="http://en.wikipedia.org">Wikipedia</a>
forbid the use of patent-encumbered standards, and they can&#8217;t be directly
implemented in FLOSS products used in the U.S. and some other countries.
</p>
<p>
Good news:
<a href="http://xiph.org/">Ogg</a> (as maintained by the Xiph.org foundation)
is available!
Ogg is a &#8220;container format&#8221; that can contain audio, video, and
related material using one of several encodings.
Usually audio is encoded with &#8220;Vorbis&#8221; (the combination is &#8220;Ogg Vorbis&#8221;);
perfect sound reproductions can be created with FLAC.
This format is already the required audio format for Wikipedia, and
the <a href="http://www.0xdeadbeef.com/weblog/?p=492">next
version of Mozilla&#8217;s Firefox will include Ogg built in</a>.
<a href="http://discussions.apple.com/thread.jspa?messageID=8543066">
Many people already have huge music collections in Ogg format</a>,
and both
<a
href="http://www.marcozehe.de/2008/10/01/why-i-would-not-buy-an-ipod-just-yet/">
many people report that Ogg is an important requirement for a player</a>.
See my older
<a href="http://www.dwheeler.com/blog/2008/10/02/#play-ogg-vorbis-theora">
blog entry on playing Ogg Vorbis and Theora</a> for more information.
</p>

<p>
Bad news: Apple&#8217;s iPods do not directly support Ogg.
That&#8217;s really unfortunate for iPod users, and it also makes it
harder to release files in Ogg.
So please, ask Apple to add support for Ogg.
<a href="http://george.hotelling.net/90percent/digital_music/howto_how_and_why_you_would_want_to_get_ogg_vorbis_on_itunes.php">People have been asking
for this for some time</a>, so it&#8217;s not true that
&#8220;no one&#8217;s asking for it&#8221;.
Some people have even taken radical efforts and
<a href="http://wiki.xiph.org/index.php/PortablePlayers#Apple.27s_iPod.2A">
rewritten the iPod software</a> - but although that shows there&#8217;s a real
interest, that&#8217;s an extreme measure that normal people shouldn&#8217;t have to do.
There&#8217;s already software available to Apple to implement Ogg
at no charge, and even the
<a
href="http://gizmodo.com/archives/ogg-vorbis-on-ipod-a-rebuttal-015738.php">
original iPods have enough horsepower to implement Ogg</a>.
Thus, it will cost Apple very little to add support for Ogg -
and there <i>are</i> people who want it.
</p>
<p>path: <a href="http://www.dwheeler.com/blog/oss">/oss</a> | <a href="http://www.dwheeler.com/blog">Current Weblog</a> | <a href="http://www.dwheeler.com/blog/2009/01/10#ask-apple-support-ogg-ipod">permanent link to this entry</a></p>
<h1>Thu, 08 Jan 2009</h1>
<p><a name="update-website-with-rsync"><font size="+2"><b>Updating cheap websites with rsync+ssh</b></font></a></p><p></p>
<p>
I&#8217;ve figured out how to run and update cheap, simple websites using
rsync and ssh and Linux.
I thought I&#8217;d share that info here, in case you want to copy my approach.
</p>

<p>
My site (<a href="http://www.dwheeler.com">www.dwheeler.com</a>)
is an intentionally simple website.
It&#8217;s simply a bunch of directories with static files;
those files may contain Javascript and animated GIF, but site visitors
aren&#8217;t supposed to cause them to change.
Programs to manage my site (other than the web server) are run
<i>before</i> the files are sent to the server.
Most of today&#8217;s sites can&#8217;t be run this way&#8230; but when you can do this,
the site is much easier to secure and manage.
It&#8217;s also really efficient (and thus fast).
Even if you can&#8217;t run a <i>whole</i> site this way, if you can run a
big part of it this way, you can save yourself a lot of security,
management, and performance problems.
</p>

<p>
This means that I can make arbitrary changes to a local copy of the website,
and then use rsync+ssh to upload just those changes.
<a href="http://samba.anu.edu.au/rsync/">rsync</a> is a <i>wonderful</i>
program, originally created by Andrew Tridgell, that can copy a
directory tree to and from remote directory trees, but only send the
changes.
The result is that rsync is a great bandwidth-saver.
</p>

<p>
This approach is easy to secure, too.
Rsync uses ssh to create the connection, so
people can&#8217;t normally snoop on the transfer, and redirecting DNS will
be immediately noticed.
If the website is compromised, just reset it and re-send a copy;
as long as you retain a local copy, no data can be permanently lost.
I&#8217;ve been doing this for years, and been happy with this approach.
</p>

<p>
On a full-capability hosting service, using rsync this is easy.  Just
install rsync on the remote system (typically using yum or apt-get), and run:
<pre>
 rsync -a LOCALDIR REMOTENAME@REMOTESITE:REMOTEDIR
</pre>
</p>

<p>
Unfortunately, at least some of the cheap hosting services available today
don&#8217;t make this quite so easy.
The cheapest hosting services are &#8220;shared&#8221; sites that share
resources between many users without using full operating system or
hardware virtualization.
I&#8217;ve been looking at a lot of the cheap Linux web hosting services
like these such as WebhostGIANT, Hostmonster, Hostgator, and Bluehost.
It appears that at least some
of these hosting companies improve their security by greatly limiting
the access granted to you via the ssh/shell interface.
I know that
<a href="http://www.webhostgiant.com/">WebhostGIANT</a> is an example,
but I believe there are many such examples.
So, even if you have ssh access on a Linux system,
you may only get a few commands you can run like &#8220;mv&#8221; and &#8220;cp&#8221;
(and <i>not</i> &#8220;tar&#8221; or &#8220;rsync&#8221;).
You could always ask the hosting company
to install programs, but they&#8217;re often reluctant to add new ones.
But&#8230; it turns out that you <i>can</i> use rsync and other such services,
without asking them to install anything, at least in some cases.
I&#8217;m looking for new hosting providers, and realized
(1) I can still use this approach without asking them to install anything, but
(2) it requires some technical &#8220;magic&#8221; that others might not know.
So, here&#8217;s how to do this, in case this information/example helps others.
</p>

<p>
<b>Warning:</b> Complicated technical info ahead.
</p>

<p>
I needed to install some executables, and rather than recompiling my
own, I grabbed pre-compiled executables.
To do this, I found out the Linux distribution used by the hosting service
(in the case of WebhostGIANT, it&#8217;s CentOS 5, so all my examples
will be RPM-based).
On my local Fedora
Linux machine I downloaded the DVD &#8220;.iso&#8221; image of that distro,
and did a &#8220;loopback mount&#8221; as root so that I could directly view its contents:
<pre>
 cd /var/www     # Or wherever you want to put the ISO.
 wget ...mirror location.../CentOS-5.2-i386-bin-DVD.iso
 mkdir /mnt/centos-5.2
 mount CentOS-5.2-i386-bin-DVD.iso /mnt/centos-5.2 -o loop
 # Get ready to extract some stuff from the ISO.
 cd
 mkdir mytemp
 cd mytemp
</pre>
</p>

<p>
Now let&#8217;s say I want the program &#8220;nice&#8221;.
On a CentOS or Fedora machine you can determine the package that &#8220;nice&#8221; is
in using this command:
<pre>
 rpm -qif `which nice`
</pre>
Which will show that nice is in the &#8220;coreutils&#8221; package.
You can extract &#8220;nice&#8221; from its package by doing this:
<pre>
 rpm2cpio /mnt/centos-5.2/CentOS/coreutils-5.97-14.el5.i386.rpm | \
   cpio --extract --make-directories
</pre>
Now you can copy it to your remote site.  Presuming that you
want the program to go into the remote directory &#8220;/private/&#8221;,
you can do this:
<pre>
 scp -p ./usr/bin/rsync MY_USERID@MY_REMOTE_SITE:/private/
</pre>
</p>

<p>
Now you can run /private/nice, and it works as you&#8217;d expect.
But what about rsync?
Well, when you try to do this with rsync and run it,
it will complain with an error message.
The error message says that rsync can&#8217;t find another library (libpopt
in this case).
The issue is that and cheap web hosting services often don&#8217;t provide a lot of
libraries, and they won&#8217;t let you install new libraries in the &#8220;normal&#8221; places.
Are we out of luck?  Not at all!
We <i>could</i> just recompile the program statically, so that the library
is embedded in the file, but we don&#8217;t even have to do <i>that</i>.
We just need to upload the needed library to a different place,
and tell the remote site where to find the library.
It turns out that the program &#8220;/lib/ld-linux.so&#8221; has an option called
&#8220;&#8212;library-path&#8221; that is specially designed for this purpose.
ld-linux.so is the loader (the &#8220;program for running programs&#8221;),
which you don&#8217;t normally invoke directly, but if you need to
add library paths, it&#8217;s a reasonable way to do it.
(Another way is to use LD_LIBRARY_PATH, but that requires that the
string be interpreted by a shell, which doesn&#8217;t always happen.)
So, here&#8217;s what I did (more or less).
</p>

<p>
First, I extracted the rsync program and necessary
library (popt) on the local system, and
copied them to the remote system (to &#8220;/private&#8221;, again):
<pre>
 rpm2cpio /mnt/centos-5.2/CentOS/rsync-2.6.8-3.1.i386.rpm | \
   cpio --extract --make-directories
 # rsync requires popt:
 rpm2cpio /mnt/centos-5.2/CentOS/popt-1.10.2-48.el5.i386.rpm | \
   cpio --extract --make-directories
 scp -p ./usr/bin/rsync ./usr/lib/libpopt.so.0.0.0 \
        MY_USERID@MY_REMOTE_SITE:/private/
</pre>
Then, I logged into the remote system using ssh, and added
symbolic links as required by the normal Unix/Linux library conventions:
<pre>
 ssh MY_USERID@MY_REMOTE_SITE
 cd /private
 ln -s libpopt.so.0.0.0 libpopt.so 
 ln -s libpopt.so.0.0.0 libpopt.so.0
</pre>
</p>

<p>
Now we&#8217;re ready to use rsync!
The trick is to tell the local rsync where the remote rsync is, using
&#8220;&#8212;rsync-path&#8221;. That option&#8217;s contents must invoke ld-linux.so to tell
the remote system where the additional library path (for libopt) is.
So here&#8217;s an example, which copies files from the directory
LOCAL_HTTPD to the directory REMOTE_HTTPDIR:
<pre>
rsync -a \
 --rsync-path="/lib/ld-linux.so.2 --library-path /private /private/rsync" \
 LOCAL_HTTPDIR REMOTENAME@REMOTESITE:REMOTE_HTTPDIR
</pre>
</p>

<p>
There are a few ways we can make this nicer for everyday production use.
If the remote server is a cheap shared system, we want to be very
kind on its CPU and bandwidth use (or we&#8217;ll get thrown off it!).
The &#8220;nice&#8221; command (installed by the steps above) will reduce CPU use
on the remote web server when running rsync.
There are several rsync options that can help, too.
The &#8220;&#8212;bwlimit=KBPS&#8221; option will limit the bandwidth used.
The &#8220;&#8212;fuzzy&#8221; option will reduce bandwidth use if there&#8217;s a similar
file already on the remote side.
The &#8220;&#8212;delete&#8221; option is probably a good idea; this means that files
deleted locally are also deleted remotely.
I also suggest &#8220;&#8212;update&#8221; (this will avoid updating remote files if they
have a newer timestamp) and &#8220;&#8212;progress&#8221; (so you can see what&#8217;s happening).
Rsync is able to copy hard links (using &#8220;-H&#8221;), but that takes more
CPU power; I suggest using symbolic links and then not invoking that option.
You can enable compression too, but that&#8217;s a trade-off; compression will
decrease bandwidth but increase CPU use.
So our final command looks like this:
<pre>
rsync -a --bwlimit=100 --fuzzy --delete --update --progress \
 --rsync-path="/private/nice /lib/ld-linux.so.2 --library-path /private /private/rsync" \
 LOCAL_HTTPDIR REMOTENAME@REMOTESITE:REMOTE_HTTPDIR
</pre>
</p>

<p>
Voila!
Store that script in some easily-run place.
Now you can easily update your website locally and
push it to the actual webserver, even on a cheap hosting service,
with very little bandwidth and CPU use.
That&#8217;s a win-win for everyone.
</p>
<p>path: <a href="http://www.dwheeler.com/blog/misc">/misc</a> | <a href="http://www.dwheeler.com/blog">Current Weblog</a> | <a href="http://www.dwheeler.com/blog/2009/01/08#update-website-with-rsync">permanent link to this entry</a></p>
<p><a name="moving"><font size="+2"><b>Moving hosting service at end of January 2009</b></font></a></p><p></p>
<p>
I will be moving to a new hosting service at the end of January 2009.
(I haven&#8217;t determined which hosting service yet.)
In theory, there should be very little downtime, but it&#8217;s possible
the site will be off for a little while.
But if that happens, it will be very temporary - I&#8217;ll
get the site back up as soon as I can.
</p>
<p>path: <a href="http://www.dwheeler.com/blog/website">/website</a> | <a href="http://www.dwheeler.com/blog">Current Weblog</a> | <a href="http://www.dwheeler.com/blog/2009/01/08#moving">permanent link to this entry</a></p>
</body></html>