<?xml version="1.0"?>
<!-- name="generator" content="blosxom/2.0" -->

<rss version="2.0">
  <channel>
    <title>David A. Wheeler's Blog   </title>
    <link>http://www.dwheeler.com/blog</link>
    <description>David A. Wheeler's weblog.</description>
    <language>en</language>

  <item>
    <title>Updating cheap websites with rsync+ssh</title>
    <link>http://www.dwheeler.com/blog/2009/01/08#update-website-with-rsync</link>
    <pubDate>Thu, 08 Jan 2009 18:57 GMT</pubDate>
    <!-- date: 2009-01-08 -->
    <description>
&lt;p&gt;
I&amp;#8217;ve figured out how to run and update cheap, simple websites using
rsync and ssh and Linux.
I thought I&amp;#8217;d share that info here, in case you want to copy my approach.
&lt;/p&gt;

&lt;p&gt;
My site (&lt;a href=&quot;http://www.dwheeler.com&quot;&gt;www.dwheeler.com&lt;/a&gt;)
is an intentionally simple website.
It&amp;#8217;s simply a bunch of directories with static files;
those files may contain Javascript and animated GIF, but site visitors
aren&amp;#8217;t supposed to cause them to change.
Programs to manage my site (other than the web server) are run
&lt;i&gt;before&lt;/i&gt; the files are sent to the server.
Most of today&amp;#8217;s sites can&amp;#8217;t be run this way&amp;#8230; but when you can do this,
the site is much easier to secure and manage.
It&amp;#8217;s also really efficient (and thus fast).
Even if you can&amp;#8217;t run a &lt;i&gt;whole&lt;/i&gt; site this way, if you can run a
big part of it this way, you can save yourself a lot of security,
management, and performance problems.
&lt;/p&gt;

&lt;p&gt;
This means that I can make arbitrary changes to a local copy of the website,
and then use rsync+ssh to upload just those changes.
&lt;a href=&quot;http://samba.anu.edu.au/rsync/&quot;&gt;rsync&lt;/a&gt; is a &lt;i&gt;wonderful&lt;/i&gt;
program, originally created by Andrew Tridgell, that can copy a
directory tree to and from remote directory trees, but only send the
changes.
The result is that rsync is a great bandwidth-saver.
&lt;/p&gt;

&lt;p&gt;
This approach is easy to secure, too.
Rsync uses ssh to create the connection, so
people can&amp;#8217;t normally snoop on the transfer, and redirecting DNS will
be immediately noticed.
If the website is compromised, just reset it and re-send a copy;
as long as you retain a local copy, no data can be permanently lost.
I&amp;#8217;ve been doing this for years, and been happy with this approach.
&lt;/p&gt;

&lt;p&gt;
On a full-capability hosting service, using rsync this is easy.  Just
install rsync on the remote system (typically using yum or apt-get), and run:
&lt;pre&gt;
 rsync -a LOCALDIR REMOTENAME@REMOTESITE:REMOTEDIR
&lt;/pre&gt;
&lt;/p&gt;

&lt;p&gt;
Unfortunately, at least some of the cheap hosting services available today
don&amp;#8217;t make this quite so easy.
The cheapest hosting services are &amp;#8220;shared&amp;#8221; sites that share
resources between many users without using full operating system or
hardware virtualization.
I&amp;#8217;ve been looking at a lot of the cheap Linux web hosting services
like these such as WebhostGIANT, Hostmonster, Hostgator, and Bluehost.
It appears that at least some
of these hosting companies improve their security by greatly limiting
the access granted to you via the ssh/shell interface.
I know that
&lt;a href=&quot;http://www.webhostgiant.com/&quot;&gt;WebhostGIANT&lt;/a&gt; is an example,
but I believe there are many such examples.
So, even if you have ssh access on a Linux system,
you may only get a few commands you can run like &amp;#8220;mv&amp;#8221; and &amp;#8220;cp&amp;#8221;
(and &lt;i&gt;not&lt;/i&gt; &amp;#8220;tar&amp;#8221; or &amp;#8220;rsync&amp;#8221;).
You could always ask the hosting company
to install programs, but they&amp;#8217;re often reluctant to add new ones.
But&amp;#8230; it turns out that you &lt;i&gt;can&lt;/i&gt; use rsync and other such services,
without asking them to install anything, at least in some cases.
I&amp;#8217;m looking for new hosting providers, and realized
(1) I can still use this approach without asking them to install anything, but
(2) it requires some technical &amp;#8220;magic&amp;#8221; that others might not know.
So, here&amp;#8217;s how to do this, in case this information/example helps others.
&lt;/p&gt;

&lt;p&gt;
&lt;b&gt;Warning:&lt;/b&gt; Complicated technical info ahead.
&lt;/p&gt;

&lt;p&gt;
I needed to install some executables, and rather than recompiling my
own, I grabbed pre-compiled executables.
To do this, I found out the Linux distribution used by the hosting service
(in the case of WebhostGIANT, it&amp;#8217;s CentOS 5, so all my examples
will be RPM-based).
On my local Fedora
Linux machine I downloaded the DVD &amp;#8220;.iso&amp;#8221; image of that distro,
and did a &amp;#8220;loopback mount&amp;#8221; as root so that I could directly view its contents:
&lt;pre&gt;
 cd /var/www     # Or wherever you want to put the ISO.
 wget ...mirror location.../CentOS-5.2-i386-bin-DVD.iso
 mkdir /mnt/centos-5.2
 mount CentOS-5.2-i386-bin-DVD.iso /mnt/centos-5.2 -o loop
 # Get ready to extract some stuff from the ISO.
 cd
 mkdir mytemp
 cd mytemp
&lt;/pre&gt;
&lt;/p&gt;

&lt;p&gt;
Now let&amp;#8217;s say I want the program &amp;#8220;nice&amp;#8221;.
On a CentOS or Fedora machine you can determine the package that &amp;#8220;nice&amp;#8221; is
in using this command:
&lt;pre&gt;
 rpm -qif `which nice`
&lt;/pre&gt;
Which will show that nice is in the &amp;#8220;coreutils&amp;#8221; package.
You can extract &amp;#8220;nice&amp;#8221; from its package by doing this:
&lt;pre&gt;
 rpm2cpio /mnt/centos-5.2/CentOS/coreutils-5.97-14.el5.i386.rpm | \
   cpio --extract --make-directories
&lt;/pre&gt;
Now you can copy it to your remote site.  Presuming that you
want the program to go into the remote directory &amp;#8220;/private/&amp;#8221;,
you can do this:
&lt;pre&gt;
 scp -p ./usr/bin/rsync MY_USERID@MY_REMOTE_SITE:/private/
&lt;/pre&gt;
&lt;/p&gt;

&lt;p&gt;
Now you can run /private/nice, and it works as you&amp;#8217;d expect.
But what about rsync?
Well, when you try to do this with rsync and run it,
it will complain with an error message.
The error message says that rsync can&amp;#8217;t find another library (libpopt
in this case).
The issue is that and cheap web hosting services often don&amp;#8217;t provide a lot of
libraries, and they won&amp;#8217;t let you install new libraries in the &amp;#8220;normal&amp;#8221; places.
Are we out of luck?  Not at all!
We &lt;i&gt;could&lt;/i&gt; just recompile the program statically, so that the library
is embedded in the file, but we don&amp;#8217;t even have to do &lt;i&gt;that&lt;/i&gt;.
We just need to upload the needed library to a different place,
and tell the remote site where to find the library.
It turns out that the program &amp;#8220;/lib/ld-linux.so&amp;#8221; has an option called
&amp;#8220;&amp;#8212;library-path&amp;#8221; that is specially designed for this purpose.
ld-linux.so is the loader (the &amp;#8220;program for running programs&amp;#8221;),
which you don&amp;#8217;t normally invoke directly, but if you need to
add library paths, it&amp;#8217;s a reasonable way to do it.
(Another way is to use LD_LIBRARY_PATH, but that requires that the
string be interpreted by a shell, which doesn&amp;#8217;t always happen.)
So, here&amp;#8217;s what I did (more or less).
&lt;/p&gt;

&lt;p&gt;
First, I extracted the rsync program and necessary
library (popt) on the local system, and
copied them to the remote system (to &amp;#8220;/private&amp;#8221;, again):
&lt;pre&gt;
 rpm2cpio /mnt/centos-5.2/CentOS/rsync-2.6.8-3.1.i386.rpm | \
   cpio --extract --make-directories
 # rsync requires popt:
 rpm2cpio /mnt/centos-5.2/CentOS/popt-1.10.2-48.el5.i386.rpm | \
   cpio --extract --make-directories
 scp -p ./usr/bin/rsync ./usr/lib/libpopt.so.0.0.0 \
        MY_USERID@MY_REMOTE_SITE:/private/
&lt;/pre&gt;
Then, I logged into the remote system using ssh, and added
symbolic links as required by the normal Unix/Linux library conventions:
&lt;pre&gt;
 ssh MY_USERID@MY_REMOTE_SITE
 cd /private
 ln -s libpopt.so.0.0.0 libpopt.so 
 ln -s libpopt.so.0.0.0 libpopt.so.0
&lt;/pre&gt;
&lt;/p&gt;

&lt;p&gt;
Now we&amp;#8217;re ready to use rsync!
The trick is to tell the local rsync where the remote rsync is, using
&amp;#8220;&amp;#8212;rsync-path&amp;#8221;. That option&amp;#8217;s contents must invoke ld-linux.so to tell
the remote system where the additional library path (for libopt) is.
So here&amp;#8217;s an example, which copies files from the directory
LOCAL_HTTPD to the directory REMOTE_HTTPDIR:
&lt;pre&gt;
rsync -a \
 --rsync-path=&quot;/lib/ld-linux.so.2 --library-path /private /private/rsync&quot; \
 LOCAL_HTTPDIR REMOTENAME@REMOTESITE:REMOTE_HTTPDIR
&lt;/pre&gt;
&lt;/p&gt;

&lt;p&gt;
There are a few ways we can make this nicer for everyday production use.
If the remote server is a cheap shared system, we want to be very
kind on its CPU and bandwidth use (or we&amp;#8217;ll get thrown off it!).
The &amp;#8220;nice&amp;#8221; command (installed by the steps above) will reduce CPU use
on the remote web server when running rsync.
There are several rsync options that can help, too.
The &amp;#8220;&amp;#8212;bwlimit=KBPS&amp;#8221; option will limit the bandwidth used.
The &amp;#8220;&amp;#8212;fuzzy&amp;#8221; option will reduce bandwidth use if there&amp;#8217;s a similar
file already on the remote side.
The &amp;#8220;&amp;#8212;delete&amp;#8221; option is probably a good idea; this means that files
deleted locally are also deleted remotely.
I also suggest &amp;#8220;&amp;#8212;update&amp;#8221; (this will avoid updating remote files if they
have a newer timestamp) and &amp;#8220;&amp;#8212;progress&amp;#8221; (so you can see what&amp;#8217;s happening).
Rsync is able to copy hard links (using &amp;#8220;-H&amp;#8221;), but that takes more
CPU power; I suggest using symbolic links and then not invoking that option.
You can enable compression too, but that&amp;#8217;s a trade-off; compression will
decrease bandwidth but increase CPU use.
So our final command looks like this:
&lt;pre&gt;
rsync -a --bwlimit=100 --fuzzy --delete --update --progress \
 --rsync-path=&quot;/private/nice /lib/ld-linux.so.2 --library-path /private /private/rsync&quot; \
 LOCAL_HTTPDIR REMOTENAME@REMOTESITE:REMOTE_HTTPDIR
&lt;/pre&gt;
&lt;/p&gt;

&lt;p&gt;
Voila!
Store that script in some easily-run place.
Now you can easily update your website locally and
push it to the actual webserver, even on a cheap hosting service,
with very little bandwidth and CPU use.
That&amp;#8217;s a win-win for everyone.
&lt;/p&gt;
</description>
   </item>
  <item>
    <title>Moving hosting service at end of January 2009</title>
    <link>http://www.dwheeler.com/blog/2009/01/08#moving</link>
    <pubDate>Thu, 08 Jan 2009 11:57 GMT</pubDate>
    <!-- date: 2009-01-08 -->
    <description>
&lt;p&gt;
I will be moving to a new hosting service at the end of January 2009.
(I haven&amp;#8217;t determined which hosting service yet.)
In theory, there should be very little downtime, but it&amp;#8217;s possible
the site will be off for a little while.
But if that happens, it will be very temporary - I&amp;#8217;ll
get the site back up as soon as I can.
&lt;/p&gt;
</description>
   </item>
  </channel>
</rss>