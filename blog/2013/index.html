<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8"></meta>
<link rel="alternate" type="application/rss+xml" title="RSS" href="http://www.dwheeler.com/blog/index.rss"></link>
<title>David A. Wheeler's Blog   </title>
<meta name="viewport" content="width=device-width, initial-scale=1.0">
</head>
<body>
<h1 style="margin-left: auto; margin-right: auto; width: 50%;">David A. Wheeler's Blog</h1><p>  </p><p></p><h1>Sun, 01 Dec 2013</h1>
<p><a name="shellcheck"><font size="+2"><b>Shellcheck</b></font></a></p><p></p>
<p>
I just learned about <a href="http://www.shellcheck.net/">shellcheck</a>,
a tool that reports on common mistakes in (Bourne) shell scripts.
If you write shell scripts, you should definitely check out this
static analyzer.
You can try it out by pasting shell scripts into their website.
It is open source software, so you can also download and use it
to your heart&#8217;s content.
</p>
<p>
It even covers some of the issues identified in
<a href="http://www.dwheeler.com/essays/filenames-in-shell.html">Filenames and Pathnames in Shell: How to do it Correctly</a>.
If you are interested in static analyzers for software,
you can also see my
<a href="http://www.dwheeler.com/flawfinder/">Flawfinder home page</a>
which identifies many other static analysis tools.
</p>
<p>path: <a href="http://www.dwheeler.com/blog/oss">/oss</a> | <a href="http://www.dwheeler.com/blog">Current Weblog</a> | <a href="http://www.dwheeler.com/blog/2013/12/01#shellcheck">permanent link to this entry</a></p>
<h1>Sat, 16 Nov 2013</h1>
<p><a name="vulnerability-economics"><font size="+2"><b>Vulnerability bidding wars and vulnerability economics</b></font></a></p><p></p>
<p>
I worry that the economics of software vulnerability reporting
is seriously increasing the risks to society.
The problem is the rising bidding wars for vulnerability information,
leading to a rapidly-growing number of
vulnerabilities known only to attackers.
These kinds of vulnerabilities, when exploited,
are sometimes called &#8220;zero-days&#8221;
because users and suppliers had zero days of warning.
I suspect we should create laws limiting the
sale of vulnerability information,
similar to the limits we place on organ donation,
to change the economics of vulnerability reporting.
To see why, let me go over some background first.
</p>

<p>
A big part of the insecure software problem today is that
relatively few of today&#8217;s software developers know how to
develop software that resists attack (e.g., via the Internet).
Many schools don&#8217;t teach it at all.
I think that&#8217;s ridiculous;
you&#8217;d think people would have heard about the Internet by now.
I do have some hope that this will get better.
I teach a graduate course on how to develop secure software at
George Mason University (GMU), and attendance has increased over time.
But today, most software developers do not know how to create secure software.
</p>

<p>
In contrast,
there is an increasing bidding war for vulnerability information
by organizations who intend to exploit those vulnerabilities.
This incentivizes people to search for vulnerabilities, but
<i>not</i> report them to the suppliers (who could fix them) and
<i>not</i> alert the public.
As
<a href="http://www.schneier.com/blog/archives/2012/06/the_vulnerabili.html">Bruce Schneier reports in
&#8220;The Vulnerabilities Market and the Future of Security&#8221;
(June 1, 2012)</a>,
&#8220;This new market perturbs the economics
of finding security vulnerabilities.
And it does so to the detriment of us all.&#8221;
Forbes ran an article about this in 2012,
<a href="http://www.forbes.com/sites/andygreenberg/2012/03/21/meet-the-hackers-who-sell-spies-the-tools-to-crack-your-pc-and-get-paid-six-figure-fees/">
Meet The Hackers Who Sell Spies The Tools To Crack Your PC (And Get Paid Six-Figure Fees)</a>.
The Forbes article describes what happened when
French security firm Vupen broke the security of the Chrome web browser.
Vupen would not tell Google how they broke in, because the
$60,000 award Google from Google was <i>not enough</i>.
Chaouki Bekrar, Vupen&#8217;s chief executive, said that they
&#8220;wouldn&#8217;t share this [information]
with Google for even $1 million&#8230;
We want to keep this for our customers.&#8221;
These customers do not plan to fix security bugs;
they purchase exploits or techniques with the
&#8220;explicit intention of invading or disrupting&#8221;.
Vupen even
&#8220;hawks each trick to multiple government agencies,
a business model that often plays its customers against one another
as they try to keep up in an espionage arms race.&#8221;
Just
<a href="http://www.networkworld.com/news/2012/060812-price-tag-for-microsoft-piece-260001.html?source=NWWNLE_nlt_daily_pm_2012-06-08">one part of the
Flame espionage software (exploiting Microsoft Update) has been estimated
as being worth $1 million when it was not known</a>.
</p>

<p>
This imbalance in economic incentives creates a
dangerous and growing mercenary subculture.
You now have a growing number of people looking for vulnerabilities,
keeping them
secret, and selling them to the highest bidder&#8230; which will encourage
<i>more</i> to look for, and keep secret, these vulnerabilities.
After all, they are incentivized to do it.
In contrast, the original developer typically does not know 
how to develop secure software,
and there are fewer economic incentives to develop secure software anyway.
This is a volatile combination.
</p>

<p>
Some think the solution is for suppliers to pay people when
they report security vulnerabilities to suppliers
(&#8220;bug bounties&#8221;).
I do not think bug bounty systems (by themselves) will be enough, though
suppliers are trying.
</p>

<p>
There has been a lot of discussion about Yahoo and bug bounties.
On September 30, 2013, the article
<a href="https://www.htbridge.com/news/what_s_your_email_security_worth_12_dollars_and_50_cents_according_to_yahoo.html">What&#8217;s your email security worth? 12 dollars and 50 cents according to Yahoo</a>
reported that Yahoo paid for each vulnerability
only $12.50 USD.
Even worse, this was not actual money, it was
&#8220;a discount code that can only be used in the Yahoo Company Store,
which sell Yahoo&#8217;s corporate t-shirts,
cups, pens and other accessories&#8221;.
Ilia Kolochenko, High-Tech Bridge CEO, says:
&#8220;Paying several dollars per vulnerability is a bad joke and won&#8217;t
motivate people to report security vulnerabilities to them, especially
when such vulnerabilities can be easily sold on the black market for
a much higher price. Nevertheless, money is not the only motivation
of security researchers. This is why companies like Google efficiently
play the ego card in parallel with [much higher] financial rewards and
maintain a &#8216;Hall of Fame&#8217; where all security researchers who have
ever reported security vulnerabilities are publicly listed. If Yahoo
cannot afford to spend money on its corporate security, it should at
least try to attract security researchers by other means. Otherwise,
none of Yahoo&#8217;s customers can ever feel safe.&#8221;
Brian Martin, President of Open Security Foundation, said:
&#8220;Vendor bug bounties are not a new thing. Recently, more vendors have
begun to adopt and appreciate the value it brings their organization,
and more importantly their customers. Even Microsoft, who was the most
notorious hold-out on bug bounty programs realized the value and jumped
ahead of the rest, offering up to $100,000 for exploits that bypass their
security mechanisms. Other companies should follow their example and
realize that a simple &#8220;hall of fame&#8221;,
credit to buy the vendor&#8217;s products,
or a pittance in cash is not conducive to researcher cooperation.
Some of these companies pay their janitors more money to clean their offices,
than they do security researchers finding vulnerabilities that may put
thousands of their customers at risk.&#8221;
<a href="http://www.itnews.com.au/News/359206,yahoo-plots-bug-bounties-up-to-15000.aspx">Yahoo has since decided to establish a bug bounty system with larger rewards</a>.
</p>

<p>
More recently, the
<a href="https://hackerone.com/internet">Internet Bug Bounty Panel</a>
(<a href="http://thenextweb.com/insider/2013/11/06/microsoft-facebook-sponsor-internet-bug-bounty-program-offer-cash-hacking-internet-stack/">founded by Microsoft and Facebook</a>)
will award public research into vulnerabilities with the potential for severe security implications to the public.
It has a minimum bounty of $5,000.
However, it certainly does not cover everything; they only
intend to pay out
widespread vulnerabilities (wide range of products or end users),
and
plan to limit bounties to only severe vulnerabilities that are novel
(new or unusual in an interesting way).
I think this could help, but it is no panacea.
</p>

<p>
Bug bounty systems are typically drastically outbid by attackers,
and I see no reason to believe this will change.
</p>

<p>
Indeed, I do not think we should mandate, or even expect,
that suppliers will pay people when
people report security vulnerabilities to suppliers (aka bug bounties).
Such a mandate or expectation
could kill small businesses and open source software development,
and it would almost certainly chill software development in general.
Such payments would not also deal with what I see as a key problem:
the people who sell vulnerabilities to the highest bidder.
Mandating payment by suppliers would get most people to send them
problem reports&#8230; if the bug bounty payments were required to
be larger than payments to those who would exploit the vulnerability.
<i>That</i> would be absurd, because
given current prices, such a requirement
would almost certainly prevent a lot of software development.
</p>

<p>
I think people who find a vulnerability in software
should normally
be free to tell the software&#8217;s supplier, so that the supplier can
rapidly repair the software (and thus fix it before it is exploited).
Some people call this &#8220;responsible disclosure&#8221;, though
some suppliers misuse this term.
Some suppliers say they want &#8220;responsible disclosure&#8221;,
but they instead appear to irresponsibly abuse the term
to stifle warning those at risk (including customers and the public),
as well as irresponsibly delay the repair of critical vulnerabilities
(if they repair the vulnerabilities at all).
After all, if a supplier convinces the researcher to <i>not</i>
alert users, potential users,
and the public about serious security defects in their product,
then these irresponsible suppliers may
believe they don&#8217;t need to fix it quickly.
People who are suspicious about &#8220;responsible disclosure&#8221;
have, unfortunately, excellent reasons to be suspicious.
Many suppliers have shown themselves untrustworthy, and
even trustworthy suppliers need to have a reason to stay that way.
For that and other reasons,
I also think people should be free to alert the public in detail,
at no charge, about a software vulnerability
(so-called &#8220;full disclosure&#8221;).
Although it&#8217;s not ideal for users, full disclosure is sometimes
necessary; it can be especially justifiable
when a supplier has demonstrated (through past or current actions)
that he will not rapidly fix the problem that he created.
In fact, I think it&#8217;d be an inappropriate constraint
of free speech to prevent people from revealing serious
problems in software products to the public.
</p>

<p>
But if we don&#8217;t want to mandate bug bounties,
or so-called &#8220;responsible disclosure&#8221;,
then where does that leave us?
We need to find some way to change the rules so that
economics works more closely
<i>with</i> and not <i>against</i> computer security.
</p>

<p>
Well, here is an idea&#8230; at least one to start with.
Perhaps we should criminalize <i>selling</i> vulnerability information
to anyone other than the supplier or the reporter&#8217;s government.
Basically, treat vulnerability information like organ donation:
intentionally eliminate economic incentives in a specific area
for a greater social good.
</p>

<p>
That would mean that suppliers can set up bug bounty programs,
and researchers can publish information about vulnerabilities to the public,
but this would sharply limit who else can legally
buy the vulnerability information.
In particular, it would be illegal to sell the information to
organized crime, terrorist groups, and so on.
Yes, governments can do bad things with the information;
this particular proposal does nothing directly to address it.
But I think it&#8217;s impossible to prevent a citizen from
telling his country&#8217;s government about a software vulnerability;
a citizen could easily see it as his duty.
I also think no government would forbid buying such information for itself.
However, by limiting sales to that particular
citizen&#8217;s government, it becomes
harder to create bidding wars between
governments and other groups for vulnerability information.
Without the bidding wars, there&#8217;s less incentive for others to
find the information and sell it to them.
Without the incentives, there would be fewer people working to
find vulnerabilities that they would intentionally hide from 
suppliers and the public.
</p>

<p>
I believe this would not impinge on freedom of speech.
You can tell no one, everyone, or anyone you want about
the vulnerability.
What you cannot do is receive <i>financial benefit</i> from
selling vulnerability information to anyone other than
the supplier (who can then fix it) or your own government
(and that at least reduces bidding wars).
</p>

<p>
Of course, you always have to worry about
unexpected consequences or easy workarounds for any new proposed law.
An organization could set itself up specifically to find vulnerabilities
and then exploit them itself&#8230; but that&#8217;s already illegal,
so I don&#8217;t see a problem there.
A trickier problem is that
a malicious organization (say, the mob) could create a
&#8220;supplier&#8221; (e.g., a reseller of proprietary software, or a
downstream open source software package) that vulnerability
researchers could sell their information to, working around the law.
This could probably be handled by requiring, in law,
that suppliers report (in a timely manner) any
vulnerability information they receive to their relevant suppliers.
</p>

<p>
Obviously there are some people will do illegal things,
but some people will avoid doing illegal things in principle, and
others will avoid illegal activities because they fear getting caught.
You don&#8217;t need to stop all possible cases, just enough to
change the economics.
</p>

<p>
I fear that the current &#8220;vulnerability bidding wars&#8221; -
left unchecked -
will create an overwhelming tsunami of zero-days
available to a wide variety of malicious actors.
The current situation might impede the peer review of
open source software (OSS), since currently people can make more money
selling an exploit than in helping the OSS project fix the problem.
Thankfully, OSS projects are still widely viewed as public goods,
so there are still many people who are willing to take the pay cut
and help OSS projects find and fix vulnerabilities.
I think proprietary and custom software are actually in much
more danger than OSS;
in those cases it&#8217;s a lot easier for people to think
&#8220;well, they wrote this code for their financial gain,
so I may as well sell my vulnerability information for my financial gain&#8221;.
The problem for society is that this attitude completely ignores the users and
those impacted by the software, who can get hurt by the later
exploitation of the vulnerability.
</p>

<p>
Maybe there&#8217;s a better way.
If so, great&#8230; please propose it!
My concern is that
economics currently makes it hard - not easy - to have computer security.
We need to figure out ways to get
Adam Smith&#8217;s invisible hand to work <i>for</i> us,
not <i>against</i> us.
</p>

<p>
Standard disclaimer:
As always, these are
<a href="http://www.dwheeler.com/aboutsite.html">my personal opinions</a>,
not those of employer, government, or
<a href="http://www.dwheeler.com/wiggles.html">(deceased) guinea pig</a>.
</p>
<p>path: <a href="http://www.dwheeler.com/blog/security">/security</a> | <a href="http://www.dwheeler.com/blog">Current Weblog</a> | <a href="http://www.dwheeler.com/blog/2013/11/16#vulnerability-economics">permanent link to this entry</a></p>
<h1>Mon, 14 Oct 2013</h1>
<p><a name="readable-one-zero-zero"><font size="+2"><b>Readable Lisp version 1.0.0 released!</b></font></a></p><p></p>
<p>
Lisp-based languages have been around a long time.
They have some interesting properties, especially when you
want to write programs that analyze or manipulate programs.
The problem with Lisp is that the traditional Lisp notation -
<i>s-expressions</i> - is notoriously hard to read.
</p>

<p>
I think I have a solution to the problem.
I looked at past (failed) solutions and found that they generally
failed to be <i>general</i> or <i>homoiconic</i>.
I then worked to find notations with these key properties.
My solution is a set of notation tiers that
make Lisp-based languages much more pleasant to work with.
I&#8217;ve been working with many others to turn this idea of readable
notations into a reality.
If you&#8217;re interested, you can
<a href="http://readable.sourceforge.net/">watch a short video</a> or
<a href="http://sourceforge.net/p/readable/wiki/Solution/">read
our proposed solution</a>.
</p>

<p>
The big news is that we have
<a href="https://sourceforge.net/projects/readable/files/">reached
version 1.0.0 in the readable project</a>.
We now have an open source software (MIT license) implementation for
both (guile) Scheme and Common Lisp, as well as a variety of
support tools.
The Scheme portion implements the
<a href="http://srfi.schemers.org/srfi-105/">SRFI-105</a> and
<a href="http://srfi.schemers.org/srfi-110/">SRFI-110</a> specs,
which we wrote.
One of the tools, unsweeten, makes it possible to process files in
other Lisps as well.
</p>

<p>
So what do these tools do?
Fundamentally, they implement the 3 notation tiers we&#8217;ve created:
curly-infix-expressions, neoteric-expressions, and sweet-expressions.
Sweet-expressions have the full set of capabilities.
</p>

<p>
Here&#8217;s an example of (awkward) traditional s-expression format:
<pre>
(define (factorial n)
  (if (&lt;= n 1)
    1
    (* n (factorial (- n 1)))))
</pre>
</p>

<p>
Here&#8217;s the same thing, expressed using sweet-expressions:
<pre>
define factorial(n)
  if {n &lt;= 1}
    1
    {n * factorial{n - 1}}
</pre>
</p>

<p>
I even briefly mentioned sweet-expressions in my
<a href="http://www.dwheeler.com/trusting-trust/">PhD dissertation
&#8220;Fully Countering Trusting Trust through Diverse Double-Compiling&#8221;</a>
(see section A.3).
</p>

<p>
So if you are interested in how to make Lisp-based languages
easier to read,
<a href="http://readable.sourceforge.net/">watch our short video about the readable notations</a>
or 
<a href="https://sourceforge.net/projects/readable/files/">download
the current version of the readable project</a>.
We hope you enjoy them.
</p>
<p>path: <a href="http://www.dwheeler.com/blog/misc">/misc</a> | <a href="http://www.dwheeler.com/blog">Current Weblog</a> | <a href="http://www.dwheeler.com/blog/2013/10/14#readable-one-zero-zero">permanent link to this entry</a></p>
<h1>Thu, 26 Sep 2013</h1>
<p><a name="ddc-interest"><font size="+2"><b>Welcome, those interested in Diverse Double-Compiling (DDC)!</b></font></a></p><p></p>
<p>
A number of people have recently been discussing or referring to
my PhD work,
<a href="http://www.dwheeler.com/trusting-trust/">&#8220;Fully Countering Trusting Trust through Diverse Double-Compiling (DDC)&#8221;</a>,
which
<a href="http://www.dwheeler.com/trusting-trust/">counters Trojan Horse attacks on compilers</a>.
<a href="http://www.reddit.com/r/programming/comments/1m4mwn/a_simple_way_of_defeating_the_compiler_backdoor/">Last week&#8217;s discussion on reddit</a>
based on a short
<a href="http://imgur.com/a/BWbnU#0">short slide show</a>
discussed it directly, for example.
There have also been related discussions such as
<a href="https://blog.torproject.org/blog/deterministic-builds-part-one-cyberwar-and-global-compromise">Tor&#8217;s work on creating deterministic builds</a>.
</p>
<p>
For everyone who&#8217;s interested in DDC&#8230; welcome!
I intentionally posted my dissertation, and a video about it,
directly on the Internet with no paywall.
That way, anyone who wants the information can immediately get it.
<a href="http://www.dwheeler.com/trusting-trust/">Enjoy!</a>
</p>
<p>
I even include enough background material
so other people
can independently repeat my experiments and verify my claims.
I believe that
if you cannot reproduce the results, it is not science&#8230;
and a lot of computational research has stopped being a science.
This is not a new observation;
<a href="http://academiccommons.columbia.edu/catalog/ac:140124">&#8220;Reproducible Research: Addressing the Need for Data and Code Sharing in Computational Science&#8221; by Victoria C. Stodden (Computing in Science &amp; Engineering, 2010)</a> summarizes a roundtable on
this very problem.
The roadtable found that
&#8220;Progress in computational science
is often hampered by researchers&#8217;
inability to independently
reproduce or verify published results&#8221;
and, along with a number of specific steps,
&#8220;reproducibility must be embraced
at the cultural level within the
computational science community.&#8221;
<a href="http://www.isgtw.org/feature/does-computation-threaten-scientific-method">&#8220;Does computation threaten the scientific method
(by Leslie Hatton and Adrian Giordani)</a>
and
<a href="http://www.nature.com/nature/journal/v482/n7386/full/nature10836.html">
&#8220;The case for open computer programs&#8221; in <i>Nature</i>
(by Darrel C. Ince, Leslie Hatton, and John Graham-Cumming)</a>
make similar points.
For one of many examples, the paper
<a href="http://fmv.jku.at/papers/Biere-ETH-TR-444-2004.pdf">&#8220;The Evolution from LIMMAT to NANOSAT&#8221; by Armin Biere (Technical Report #444, 15 April 2004)</a>
reported that they could not reproduce results because
&#8220;From the publications
alone, without access to the source code,
various details were still unclear.&#8221;
In the end they realized that
&#8220;making source code&#8230; available is as important
to the advancement of the field as publications&#8221;.
I think we should not pay researchers, or their institutions,
if they fail to provide the materials necessary to reproduce the work.
</p>
<p>
I do have a request, though.
There is no patent on DDC,
nor is there a legal requirement to report using it.
Still, if you apply my approach, please
<a href="http://www.dwheeler.com/contactme.html">let me know</a>;
I&#8217;d like to hear about it.
Alternatively, if you are <i>seriously</i> trying to use DDC
but are having some problems,
<a href="http://www.dwheeler.com/contactme.html">let me know</a>.
</p>
<p>
Again -
<a href="http://www.dwheeler.com/trusting-trust/">enjoy!</a>
</p>
<p>path: <a href="http://www.dwheeler.com/blog/security">/security</a> | <a href="http://www.dwheeler.com/blog">Current Weblog</a> | <a href="http://www.dwheeler.com/blog/2013/09/26#ddc-interest">permanent link to this entry</a></p>
<h1>Wed, 21 Aug 2013</h1>
<p><a name="open-security"><font size="+2"><b>Open security</b></font></a></p><p></p>
<p>
Modern society depends on computer systems.
Yet computer security problems let attackers subvert the very systems
that society depends on.
This is a serious problem.
</p>
<p>
I think one approach that could help is
<a href="http://www.dwheeler.com/essays/open-security-definition.html">
&#8220;open security&#8221;</a> -
applying open source software (OSS)
approaches to help solve computer security problems.
To see why, let&#8217;s look at some background.
</p>
<p>
Back in the 1970s people collaboratively developed software
that today we would call open source software or free-libre software.
At the time many assumed these approaches could not scale up
to big systems&#8230; but they were wrong.
<a href="http://www.dwheeler.com/sloc">Software systems
that would cost over a billion U.S. dollars to redevelop</a>
have been developed as open source software, and
Wikipedia has used similar approaches to collaboratively develop
the world&#8217;s largest encyclopedia.
</p>
<p>
So&#8230; if we can collaboratively develop multi-billion software systems,
and large encyclopedias, can we use the same kinds of collaborative
approaches to improve computer security?
I believe we can&#8230; but if we are going to do this, we need to
define a term for this (so that we can agree on what we are doing!).
</p>

<p>
I propose that
<i>open security is the application of open source software (OSS)
approaches to help solve cyber security problems</i>.
OSS approaches
collaboratively develop and maintain intellectual works (including
software and documentation) by enabling users to use them for any
purpose, as well as study, create, change, and redistribute them (in
whole or in part).
Cyber security problems are a lack of security
(confidentiality, integrity, and/or availability), or potential lack of
security (a vulnerability), in computer systems and/or the networks they
are a part of.
In short, open security improves <i>security</i> through <i>collaboration.</i>
</p>

<p>
You can see more details in my paper
<a href="http://www.dwheeler.com/essays/open-security-definition.html">What is open security?</a>
<a href="http://www.dwheeler.com/essays/open-security-definition.pdf">[PDF]</a>
<a href="http://www.dwheeler.com/essays/open-security-definition.doc">[DOC]</a>.
I intentionally built on previous work such as the
<a href="http://www.gnu.org/philosophy/free-sw.html"><i>Free Software Definition</i> by the Free Software Foundation (FSF)</a>,
the <a href="http://opensource.org/osd-annotated"><i>Open Source Definition (Annotated)</i> by the Open Source Initiative (OSI)</a>,
the <a href="http://creativecommons.org/licenses/">Creative Commons license work</a>, and the
<a href="http://freedomdefined.org/Definition"><i>Definition of Free Cultural Works</i> by Freedom Defined</a>
(the last one is, for example, the basis of the
<a href="http://wikimediafoundation.org/wiki/Resolution:Licensing_policy">Wikimedia/Wikipedia licensing policy</a>).
</p>

<p>
The <a href="http://open-sec.org/">Open security site</a>
has been recently set up so that you and others can join and get involved.
So please - get involved!
We are only just starting, and the direction we go depends
on the feedback we get.
</p>

<p>
Further reading:
</p>
<ul>
<li><a href="http://open-sec.org/">Open security site</a></li>
<li><a href="http://www.dwheeler.com/essays/open-security-definition.html">What is open security?</a></li>
</ul>
<p>path: <a href="http://www.dwheeler.com/blog/oss">/oss</a> | <a href="http://www.dwheeler.com/blog">Current Weblog</a> | <a href="http://www.dwheeler.com/blog/2013/08/21#open-security">permanent link to this entry</a></p>
<h1>Tue, 06 Aug 2013</h1>
<p><a name="anthropomorphism"><font size="+2"><b>Don&#8217;t anthropomorphize computers, they hate that</b></font></a></p><p></p>
<p>
A lot of people who program computers or live in the computing world &dash;
including me &dash;
talk about computer hardware and software as if they are people.
Why is that?
This is not as obvious as you&#8217;d think.
</p>

<p>
After all, if you read the literature about learning how to program,
you&#8217;d think that programmers
would <i>never</i> use anthropomorphic language.
<a href="http://www.codinghorror.com/blog/2006/07/separating-programming-sheep-from-non-programming-goats.html">
&#8220;Separating Programming Sheep from Non-Programming Goats&#8221;
by Jeff Atwood</a>
discusses teaching programming and points to the
intriguing paper
<a href="http://www.eis.mdx.ac.uk/research/PhDArea/saeed/paper1.pdf">
&#8220;The camel has two humps&#8221; by
Saeed Dehnadi and Richard Bornat</a>.
This paper reported experimental evidence on why
some people can learn to program, while others struggle.
Basically, to learn to program you must fully understand
that computers mindlessly follow rules, and that
computers just don&#8217;t act like humans.
As their paper said,
&#8220;Programs&#8230; are utterly meaningless.
To write a computer program you have to come to terms with this,
to accept that whatever you might want the program to mean,
the machine will blindly follow its meaningless rules and come to
some meaningless conclusion&#8230;
the consistent group [of people] showed a pre-acceptance of this fact:
they are capable of seeing mathematical calculation problems
in terms of rules, and can follow those rules wheresoever they may lead.
The inconsistent group, on the other hand, looks for meaning
where it is not.
The blank group knows that it is looking at meaninglessness,
and refuses to deal with it.
[The experimental results suggest] that it is extremely difficult
to teach programming to the inconsistent and blank groups.&#8221;
<a href="http://www.eis.mdx.ac.uk/research/PhDArea/saeed/">
Later work by Saeed Dehnadi and sometimes others</a>
expands on this earlier work.
<!-- "Testing Programming Aptitude" -->
The intermediate paper
&#8220;Mental models, Consistency and Programming Aptitude&#8221; (2008)
seemed to have refuted the idea that consistency
(and ignoring meaning) was critical to programming, but the later
<a href="http://www.eis.mdx.ac.uk/research/PhDArea/saeed/SD_PPIG_2009.pdf">
&#8220;Meta-analysis of the effect of consistency on success
in early learning of programming&#8221; (2009)</a>
added additional refinements and then re-confirmed this hypothesis.
The reconfirmation involved a meta-analysis of
six replications of an improved version of Dehnadi&#8217;s
original experiment, and again showed that understanding that computers were
mindlessly consistent was key in successfully learning to program.
</p>

<p>
So the good programmers know darn well that computers mindlessly
follow rules.
But many use anthropomorphic language anyway.
Huh?
Why is that?
</p>

<p>
Some do object to anthropomorphism, of course.
<a href="http://lambda-the-ultimate.org/node/264">Edjar Dijkstra
certainly railed against anthropomorphizing computers</a>.
For example,
in <a href="http://www.cs.utexas.edu/users/EWD/ewd08xx/EWD854.PDF">EWD854</a>
(1983) he said,
&#8220;I think anthropomorphism is the worst of all [analogies].
I have now seen programs &#8216;trying to do things&#8217;, &#8216;wanting to do things&#8217;,
&#8216;believing things to be true&#8217;, &#8216;knowing things&#8217; etc.
Don&#8217;t be so naive as to believe that this use of language is harmless.&#8221;
He believed that analogies (like these) led to a host of misunderstandings,
and that those misunderstandings led to repeated
multi-million-dollar failures.
It is certainly true that misunderstandings can lead to catastrophe.
But I think one reason Dijkstra railed particularly against anthropomorphism
was (in part) because it is a widespread practice, even among those
who <i>do</i> understand things &dash;
and I see no evidence that anthropomorphism is going away.
</p>

<p>
The
<a href="http://www.catb.org/jargon/html/anthropomorphization.html">
Jargon file specifically discusses anthropomorphization</a>:
&#8220;one rich source of jargon constructions is the hackish tendency
to anthropomorphize hardware and software. English purists and
academic computer scientists frequently look down on others for
anthropomorphizing hardware and software, considering this sort of
behavior to be characteristic of naive misunderstanding.
But most hackers anthropomorphize freely,
frequently describing program behavior in terms of wants and desires.
Thus it is common to hear hardware or software talked about as though
it has homunculi talking to each other inside it, with intentions and
desires&#8230;
As hackers are among the people
who know best how these phenomena work, it seems odd that they would
use language that seems to ascribe consciousness to them. The mind-set
behind this tendency thus demands examination.
The key to understanding this kind of usage is that it isn&#8217;t done in
a naive way; hackers don&#8217;t personalize their stuff in the sense of
feeling empathy with it, nor do they mystically believe that the things
they work on every day are &#8216;alive&#8217;.&#8221;
</p>

<p>
Okay, so others have noticed this too.
The
<a href="http://www.catb.org/jargon/html/anthropomorphization.html">
Jargon file even proposes some possible reasons for anthropomorphizing
computer hardware and software</a>:
</p>

<ol>
<li>It reflects a &#8220;mechanistic view of human behavior.&#8221;
&#8220;In this view, people are biological
machines - consciousness is an interesting and valuable epiphenomenon,
but mind is implemented in machinery which is not fundamentally different
in information-processing capacity from computers&#8230;
Because hackers accept that a human machine can have intentions, it
is therefore easy for them to ascribe consciousness and intention to
other complex patterned systems such as computers.&#8221;
But while the materialistic view of humans has respectible company,
this &#8220;explanation&#8221; fails to explain
why humans would use anthropomorphic
terms about computer hardware and software,
since they are manifestly not human.
Indeed, as the Jargon file acknowledges,
even hackers who have contrary religious views will use
anthropological terminology.
</li>
<li>
It reflects
&#8220;a blurring of the boundary between the programmer and
his artifacts - the human qualities belong to the programmer and the
code merely expresses these qualities as his/her proxy. On this view,
a hacker saying a piece of code &#8216;got confused&#8217; is really saying that
he (or she) was confused about exactly what he wanted the computer to do,
the code naturally incorporated this confusion, and the code expressed the
programmer&#8217;s confusion when executed by crashing or otherwise misbehaving.
Note that by displacing from &#8220;I got confused&#8221; to
&#8220;It got confused&#8221;, the programmer is not avoiding responsibility,
but rather getting some analytical distance in order
to be able to consider the bug dispassionately.&#8221;
</li>
<li>
&#8220;It has also been suggested that anthropomorphizing complex systems is
actually an expression of humility, a way of acknowleging that simple
rules we do understand (or that we invented) can lead to emergent
behavioral complexities that we don&#8217;t completely understand.&#8221;
</li>
</ol>

<p>
The Jargon file claims that
&#8220;All three explanations accurately model hacker psychology, and should
be considered complementary rather than competing.&#8221;
I think the first &#8220;explanation&#8221; is completely unjustified.
The second and third explanations do have some merit.
However, I think there&#8217;s a simpler and more important reason: Language.
</p>

<p>
When we communicate with a human, we must use some language that will be
more-or-less understood by the other human.
Over the years people have developed a variety of human languages
that do this pretty well (again, more-or-less).
Human languages were not particularly designed to deal with computers,
but languages <i>have</i> been honed over long periods of time
to discuss human behaviors and their mental states
(thoughts, beliefs, goals, and so on).
The sentence
&#8220;Sally says that Linda likes Tom,
but Tom won&#8217;t talk to Linda&#8221;
would be understood by any normal seven-year-old girl
(well, assuming she speaks English).
</p>

<p>
I think a primary reason people anthropomorphic terminology
is because it&#8217;s much easier to communicate that way
when discussing computer hardware and software using existing languages.
Compare &#8220;the program got confused&#8221; with
the overly long
&#8220;the program executed a different path than the one expected by the
program&#8217;s programmer&#8221;.
Human languages have been honed to discuss human behaviors and
mental states, so it is much easier to use languages this way.
As long as both the sender and receiver of the message understand the
message, the fact that the terminology is anthropomorphic is
not a problem.
</p>

<p>
It&#8217;s true that anthropomorphic language can confuse some people.
But the primary reason it confuses some people is that they
still have trouble understanding that computers are mindless &dash;
that computers simply do whatever their instructions tell them.
Perhaps this is an innate weakness in some people,
but I think that addressing this weakness head-on
can help counter it.
This is probably a good reason for ensuring that people learn
a little programming as kids &dash; not because they will
necessarily do it later, but because computers are so central
to the modern world that people should have a basic understanding of them.
</p>
<p>path: <a href="http://www.dwheeler.com/blog/misc">/misc</a> | <a href="http://www.dwheeler.com/blog">Current Weblog</a> | <a href="http://www.dwheeler.com/blog/2013/08/06#anthropomorphism">permanent link to this entry</a></p>
<h1>Thu, 20 Jun 2013</h1>
<p><a name="misunderstanding-https-caching"><font size="+2"><b>Industry-wide Misunderstandings of HTTPS (SSL/TLS)</b></font></a></p><p></p>
<p>
<a href="http://securityevaluators.com/content/case-studies/caching/">
Industry-wide Misunderstandings of HTTPS</a>
describes a nasty security problem involving HTTP (SSL/TLS)
and caching.
The basic problem is that developers of web applications
do not know or understand web standards.
The result: 70% of sites tested expose private data on users&#8217; machines
by recording data that is supposed to be destroyed.
</p>
<p>
Here&#8217;s the abstract:
&#8220;Most web browsers, historically, were cautious about caching content
delivered over an HTTPS connection to disk -
to a greater degree than required by the HTTP standard.
In recent years, in response to the
increased use of HTTPS for non-sensitive data, and the proliferation
of bandwidth-hungry AJAX and Web 2.0 sites, some browsers have been
changed to strictly follow the standard, and cache HTTPS content far more
aggressively than before.
HTTPS web servers must explicitly include a
response header to block standards-compliant browsers from caching the
response to disk -
and not all web developers have caught up to the new browser behavior.
ISE identified 21 (70% of sites tested) financial,
healthcare, insurance and utility account sites that failed to forbid
browsers from storing cached content on disk, and as a result, after
visiting these sites, unencrypted sensitive content is left behind on
end-users&#8217; machines.&#8221;
</p>
<p>
This vulnerability isn&#8217;t as easy to exploit as some other problems;
it just means that data that <i>should</i> have been destroyed is
hanging around.
But it does set up serious problems, because that information
<i>should</i> have been destroyed.
</p>
<p>
This is really just yet another example of the security problems
that can happen when people assume, &#8220;the only web browser is
Internet Explorer 6&#8221;.
That was <i>never</i> true, and by ignoring standards,
they set themselves up for disaster.
This isn&#8217;t even a new standard; HTTP version 1.1 was released in 1999,
so there&#8217;s been plenty of time to fix things.
Today, many modern systems use AJAX, and SSL/TLS encryption is
far more widely used as well, and given these changing conditions,
web browsers are changing in standards-compliant ways.
Web application developers who followed the standard are doing
just fine.
The web application developers who ignored the standards are,
once again, putting their users at risk.
</p>
<p>path: <a href="http://www.dwheeler.com/blog/security">/security</a> | <a href="http://www.dwheeler.com/blog">Current Weblog</a> | <a href="http://www.dwheeler.com/blog/2013/06/20#misunderstanding-https-caching">permanent link to this entry</a></p>
<h1>Tue, 30 Apr 2013</h1>
<p><a name="oss-license-clinic-2013"><font size="+2"><b>OSS License Clinic</b></font></a></p><p></p>
<p>
If you&#8217;re interested in understanding the
legal, contract, or government
acquisition issues in applying free / libre / open source software (FLOSS),
come to the
<a href="http://opensourcecommunitysummit.org">
&#8220;Open Source License Clinic&#8221; on May 9, 2013, 9am-noon (EDT), in
Washington, DC</a>.
This clinic will be hosted by the non-profit
Open Source Initiative (OSI), and is
&#8220;designed as a cross-industry, cross-community workshop
for legal, contract, acquisition and program professionals who wish to
deepen their understanding of open source software licenses, and raise
their proficiency to better serve their organizations objectives as well
as identify problems which may be unique to government. Discussion of
licenses and issues in straight-forward terms make the clinic of value
to anyone involved in the lifecycle of a technology decision/acquisition
or strategy for internal software development.&#8221;
</p>
<p>
I&#8217;m one of the speakers, along with:
</p>
<ul>
<li>Ms. Vicki Allums, General Counsel, Defense Information Systems Agency, Department of Defense</li>
<li>Mr. Jim Jagielski, OSI board director and President, Apache Foundation</li>
<li>Mr. Mike Milinkovich, OSI board director and Executive Director, Eclipse Foundation</li>
<li>Mr. Luis Villa, OSI board director and Deputy General Counsel, WIkimedia Foundation</li>
</ul>

<p>
The location for the license clinic will be:
</p>
<pre>
101 Independence Ave SE
Madison Building, 6th Floor, Dining Room A
Washington, DC 20540
</pre>

<p>
You might also be interested in the
<a href="http://www.opensourcecommunitysummit.org/">Open Source Community Summit on May 10 (the following day)</a>
in Washington, DC.
</p>
<p>path: <a href="http://www.dwheeler.com/blog/oss">/oss</a> | <a href="http://www.dwheeler.com/blog">Current Weblog</a> | <a href="http://www.dwheeler.com/blog/2013/04/30#oss-license-clinic-2013">permanent link to this entry</a></p>
<h1>Thu, 21 Mar 2013</h1>
<p><a name="french-government-oss"><font size="+2"><b>French government OSS policy</b></font></a></p><p></p>
<p>
Free/libre/open source software (FLOSS) continues to grow around the
world, and governments around the world are trying to establish
policies about it.
Yet in the U.S. we often don&#8217;t hear about them.
I just posted about a UK policy;
<a href="http://www.april.org/en/french-prime-minister-instructions-usage-free-software-french-administration">here&#8217;s a recent French policy, translated
into English</a>.
</p>
<p>
The French administration, in September 2012, established a set of
guidelines and recommendations on the proper use of Free Software
(aka open source software) in the French government.
This is called the
&#8220;Ayrault Memorandum&#8221; (circulaire Ayrault, in French) and was
signed in September 2012 by the French Prime Minister.
The document was mainly produced by the DISIC (the
Department of Interministerial Systems Information and Communication)
and the CIOs of some departments.
The DISIC is in charge of coordinating the administration actions
on information systems.
</p>
<p>path: <a href="http://www.dwheeler.com/blog/oss">/oss</a> | <a href="http://www.dwheeler.com/blog">Current Weblog</a> | <a href="http://www.dwheeler.com/blog/2013/03/21#french-government-oss">permanent link to this entry</a></p>
<h1>Mon, 18 Mar 2013</h1>
<p><a name="uk-government-prefers-oss"><font size="+2"><b>UK Government prefers OSS</b></font></a></p><p></p>
<p>
The
<a href="http://www.computerweekly.com/news/2240179643/Government-mandates-preference-for-open-source">UK government is mandating a
&#8220;preference&#8221; for open source software</a> in its
<a href="https://www.gov.uk/service-manual">Government Service
Design Manual</a>
<a href="https://www.gov.uk/service-manual/making-software/open-source.html">Open Source section</a>,
to be effective April 2013.
The draft manual says,
&#8220;Use open source software in preference to proprietary or closed source
alternatives, in particular for operating systems, networking software,
web servers, databases and programming languages.&#8221;
</p>
<p>path: <a href="http://www.dwheeler.com/blog/oss">/oss</a> | <a href="http://www.dwheeler.com/blog">Current Weblog</a> | <a href="http://www.dwheeler.com/blog/2013/03/18#uk-government-prefers-oss">permanent link to this entry</a></p>
<h1>Sun, 10 Mar 2013</h1>
<p><a name="sweet-expressions-srfi-start"><font size="+2"><b>Readable Lisp: Sweet-expressions</b></font></a></p><p></p>
<p>
I&#8217;ve used Lisp-based programming languages
for decades, but while they have some nice properties,
their traditional s-expression notation is not very readable.
Even the original creator of Lisp
did not particularly like its notation!
However, this problem turns out to be surprisingly hard to solve.
</p>
<p>
After reviewing the many past failed efforts,
I think I have figured out why they failed.
Past solutions typically did not work because they failed to be
general (the notation is independent from any underlying semantic)
or homoiconic (the underlying data structure is clear from the syntax).
Once I realized that, I devised (with a lot of help from others!)
a new notation, called sweet-expressions (t-expressions),
that <i>is</i> general and homoiconic.
I think this creates a real solution for an old problem.
</p>
<p>
You can download and try out
sweet-expressions as released by the
<a href="http://readable.sourceforge.net/">Readable Lisp S-expressions Project</a>
by downloading our new
<a href="https://sourceforge.net/projects/readable/files/?source=navbar">version 0.7.0 release</a>.
</p>
<p>
If you&#8217;re interested, please participate!
In particular, please participate in the
<a href="http://srfi.schemers.org/srfi-110/">SRFI-110 sweet-expressions (t-expressions)</a> mailing list.
SRFIs let people write specifications for extensions to the Scheme
programming language (a Lisp), and this SRFI lets
people in the Scheme community discuss it.
</p>
<p>
The following table shows what an example of traditional (ugly)
Lisp s-expressions, the same thing in sweet-expressions,
and a short explanation.
</p>

<table border="1" cellpadding="4">
<tr>
<th align="center">s-expressions</th>
<th align="center">Sweet-expressions (t-expressions)</th>
<th align="center">Explanation</th>
</tr>
<tr>
<td align="left" valign="top">
<pre>
(define (fibfast n)
  (if (&lt; n 2)
    n
    (fibup n 2 1 0)))
</pre>
</td>
<td align="left" valign="top">
<pre>
define fibfast(n)
  if {n &lt; 2}
    n
    fibup n 2 1 0
</pre>
</td>
<td align="left" valign="top">
<pre>
Typical function notation
Indentation, infix {...}
Single expr = no new list
Simple function calls
</pre>
</td>
</tr>
</table>
<p>path: <a href="http://www.dwheeler.com/blog/misc">/misc</a> | <a href="http://www.dwheeler.com/blog">Current Weblog</a> | <a href="http://www.dwheeler.com/blog/2013/03/10#sweet-expressions-srfi-start">permanent link to this entry</a></p>
<h1>Tue, 22 Jan 2013</h1>
<p><a name="acm-presentation"><font size="+2"><b>Speaking at ACM DC Chapter</b></font></a></p><p></p>

<p>
FYI, on 2013-03-04 I plan to speak about
<a href="http://meetup.dcacm.org/events/100444092/?eventId=100444092&action=detail">&#8220;Open Source Software, Government, and Cyber Security&#8221;</a>
at the Association for Computing Machinery (ACM), Washington, DC Chapter.
It will be at 1203 19th St, 3rd Floor, Washington, DC.
See the link for more information.
</p>
<p>path: <a href="http://www.dwheeler.com/blog/oss">/oss</a> | <a href="http://www.dwheeler.com/blog">Current Weblog</a> | <a href="http://www.dwheeler.com/blog/2013/01/22#acm-presentation">permanent link to this entry</a></p>
<p><a name="owf-released"><font size="+2"><b>Ozone Widget Framework (OWF) released as OSS!</b></font></a></p><p></p>
<p>
The
<a href="http://owfgoss.org/">Ozone Widget Framework (OWF)</a>
has recently been released as open source software (OSS)
by the U.S. government.
OWF is useful but a little tricky to explain; as their website explains,
OWF is a web application that &#8220;allows users to easily access
all their online tools from one location&#8230; [users can]
access websites and applications with widgets [and]
group them and configure &#8230; applications to interact
with each other via widget intents&#8221;.
Go see their website to learn more about it; here, I&#8217;ll
talk about the wider implications of OWF.
</p>
<p>
To me, OWF is interesting on several fronts.
</p>
<p>
From potential user&#8217;s point of view, this is great news.
If you want something like this, well, now you can easily get it.
If you&#8217;re outside the U.S. government, you&#8217;ve never had this
program at all before.
But even for those inside the U.S. government, this release
makes OWF far easier to get, use, and improve if necessary.
</p>
<p>
But from the point-of-view of collaborative software development,
this is a much bigger deal.
The government all too often pays money to develop software on
one project, and then re-pays to develop that software again on any
other project that needs it.
In the rare cases where reuse happens at all, the government makes it
hard for others in the government to improve it as needed.
The government often talks about &#8220;public/private partnerships&#8221;,
and such partnerships are a good idea&#8230; but all too often
this doesn&#8217;t happen in software development.
</p>
<p>
Here we have an awesome change.
Per their original plans and a Congressional mandate,
OWF is now released to the public.
This means that instead of the government having to re-develop the
code for every use, and for the public to have to re-develop it as well,
&#8220;we the people&#8221; who paid to develop the software
can actually get it.
</p>
<p>
What&#8217;s more, OWF has avoided some of the terrible mistakes
that have hurt some past efforts:
</p>
<ol>
<li>Sometimes software developed via government funding gets
&#8220;captured&#8221; by one vendor, so that even though
the government paid to have it developed,
essentially no one else has the right or ability to maintain it.
Once it&#8217;s captured, the cost of maintaining the software skyrockets.
By releasing the software as OSS, the OWF project has avoided that problem.
Instead, the OWF project can get wide use and improvements from
around the world.
</li>
<li>OWF has wisely released the software under an industry-standard
OSS license (in this case, Apache 2.0), instead of writing some
government-unique non-standard license.
Nearly all OSS is licensed under a few licenses (GPL, LGPL, BSD-new, MIT,
Apache 2.0); using nonstandard or incompatible licenses greatly
impedes any possibility of collaboration.
</li>
<li>Second, OWF has wisely chosen to use a widely-used repository and
development infrastructure (in this case, GitHub), instead of
unnecessarily developing and maintaining its own.
</li>
</ol>
<p>
The U.S. federal government was formed by &#8220;we the people&#8221;.
It&#8217;s great to see the government releasing software back to the people;
in the end, we&#8217;re the ones who paid to develop it.
I wish the OWF project the best of success, and I hope that there
will be many similar OSS projects to come.
</p>
<p>path: <a href="http://www.dwheeler.com/blog/oss">/oss</a> | <a href="http://www.dwheeler.com/blog">Current Weblog</a> | <a href="http://www.dwheeler.com/blog/2013/01/22#owf-released">permanent link to this entry</a></p>
</body></html>