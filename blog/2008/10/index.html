<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8"></meta>
<link rel="alternate" type="application/rss+xml" title="RSS" href="http://www.dwheeler.com/blog/index.rss"></link>
<title>David A. Wheeler's Blog   </title>
<meta name="viewport" content="width=device-width, initial-scale=1.0">
</head>
<body>
<h1 style="margin-left: auto; margin-right: auto; width: 50%;">David A. Wheeler's Blog</h1><p>  </p><p></p><h1>Wed, 29 Oct 2008</h1>
<p><a name="internet-2008"><font size="+2"><b>Internet Wishlist</b></font></a></p><p></p>
<p>
It&#8217;s election season in the United States, a fact that&#8217;s rather
hard to miss in Northern Virginia (where I live).
<a href="http://www.popsci.com/scitech/article/2008-10/dear-mr-president">
Popular Science is running a letter by Daniel Engber (of Slate Magazine)</a>
in which he offers the US Presidential nominees advice on using
the full potential of the Internet upon their election into office.
<a href="http://politics.slashdot.org/article.pl?sid=08/10/28/2325222">
This letter is being discussed in Slashdot</a>.
<a href="http://www.internetevolution.com/author.asp?doc_id=166888&f_src=securitysentinel">Terry Sweeney believes that issues related to the
Internet Won&#8217;t Matter</a>
in this election, and unfortunately, I think he&#8217;s right.
But still, we can hope, can&#8217;t we?
</p>
<p>
In any case, election season is a good excuse to think of helpful things
that the U.S. government could do
relating to the Internet and related IT technology.
Engber&#8217;s letter certainly got me thinking that direction.
I think it&#8217;s useful to try to think of such things, because by examining
and discussing them, some of them might come to pass.
So in that spirit, here&#8217;s my candidate list:
<ol>
<li><b>Make spam illegal</b>. Make sending unsolicited bulk email (spam) illegal, and in particular, require that people OPT-IN to receive messages sent in bulk. The current &#8216;opt-out&#8217; system in the U.S. is silly, and always was. <a href="http://www.uic.edu/depts/accc/newsletter/adn29/spam.html">As essentially all information about spam notes</a>, &#8220;Never Reply To Spam&#8221;.  &#8220;Don&#8217;t [reply] to the spam message or [try] to send email to an email address given in the body of the spam and asking to be removed from the mailing list&#8230;  spammers are much too sophisticated now for replies to affect them at all. And the From: addresses in spam messages are usually faked anyway.&#8221; Responding &#8220;just identifies you as a real person who read their message&#8221;. Europeans have the more sensible opt-in system. Laws do make a differenace; far more spam is U.S. than European in origin, due to the U.S.&#8217;s lax laws. It&#8217;s not that spam hard to define; if more than 1000 people (say) receive it, and they didn&#8217;t sign up for it (e.g., by signing up for a mailing list), it&#8217;s spam. A law will <i>not</i> solve everything, but it would help; technical measures can only go so far, and need laws to help make them work.  The U.S. currently protects fax machines from spam, and that <i>has</i> worked!  The current <a href="http://www.cybertelecom.org/spam/">CAN-SPAM</a> law legalizes spam - and thus is a sick joke. It&#8217;s time to make it illegal, to protect all of our inboxes.</li>
<li><b>Require public access (free via web) to federally-funded research</b>. Put <i>all</i> federally-funded unclassified research papers on the web, with no fees or sign-ins, so that a Google search can find it. NIH is already doing this; see the <a href="http://publicaccess.nih.gov/">NIH public access policy</a>.  NIH isn&#8217;t perfect; their &#8220;12 month&#8221; period is silly (the web publication should occur immediately).  Still, it&#8217;s an improvement, and it&#8217;s absurd that this is limited to NIH; federally-funded research should be published government-wide, no matter what arm it came from.  Why should the public pay for research, then pay again to read it? Just imagine how much faster research could go if <i>anyone</i> could quickly click and review the latest research.  Just imagine how much better the public could be informed if they could easily read U.S. research on a topic&#8230; instead of only having the flim-flam artists.  I think I could make a good case that in academic research, the word &#8220;published&#8221; is increasingly meaning &#8220;accessible via Google&#8221;; anything Google can&#8217;t find doesn&#8217;t exist to many people.  It&#8217;s shameful how certain publishers effectively steal U.S. research for private gain through monopolistic publishing contracts - they do not pay for the research, and typically they don&#8217;t even pay the researchers or reviewers! If you want exclusive rights to publish research, then you should pay all the costs of performing the research. I can see a case where the publisher footed 50% of the research bill (not just the paper-writing costs) and got a one-year publication delay, but the &#8220;owning&#8221; of research papers is indefensible.  If you accept government money - and the government is of the people, by the people, and for the people - then the people should be receiving the research results. Let&#8217;s get rid of the unnecessary intermediaries and &#8220;poll taxes&#8221; on U.S. funded research.</li>
<li><b>Federally-developed unclassified software: Open source software by default</b>. By default, if the government funds unclassified software development (e.g., via research), that software should be released as open source software (under some common license). That way, anyone can use it, modify, and redistribute it (in modified or unmodified form).  Again, why should the public pay for software, then pay again to use it? Currently, if researcher B wants to continue work of researcher A, both of which were paid via government funds, researcher B typically has to re-implement what researcher A did - and that can stop the research before it begins.  This even applies to the government itself; often the government pays for re-development of the same software, because there&#8217;s no public information on software the government has already paid to develop.  If the funds are mixed, try to break it down into pieces; if that won&#8217;t work, release the mixed-funding software after some fixed time (the U.S. DoD has a 5-year clock, starting at contract signing, for when the DoD <i>could</i> release some mixed-funding software as open source).  If you are starting a proprietary software company, and want exclusive rights to developed software, then go to the bank or a venture capitalist (VC). The government is <i>not</i> a VC, so don&#8217;t expect it to be one.  Exceptions will be needed&#8230; but they should be exceptions, not the rule.</li>
<li><b>Increase funding on computer security</b>. Some is done now, of course, but it pales compared to the problem. I guess this could be construed as being self-serving; after all, I try to improve computer security as a living.  But the reason I do it is because I believe in it.  There are many tools that enhance our muscles (cars, jackhammers, etc.), but essentially only one tool that enhances our mind: Computers.  Which is one reason why computers are everywhere. Yet their very ubiquity is a problem, because they were generally not designed to be secure against determined attackers.  I believe governments should not try to do all things; there are a lot of things government just isn&#8217;t good at.  But defense is an area that is hard to do on an individual or business-by-business basis, yet we need it collectively - and it&#8217;s those kinds of problems that governments can help with.</li>
<li><b>Increase formal methods research</b>.  The world is globalizing, and we increasingly depend on software.  Testing is not a good way to make (or verify) high quality software; you can&#8217;t even fully test the trivial program &#8220;add 3 64-bit numbers&#8221; in less time than the age of the universe.  In the long run, if we want really high levels of quality for software, we need better approaches, and there&#8217;s one obvious one: Formal methods.  Formal methods apply mathematical approaches to software development. There are a lot of reasons people don&#8217;t use them today in typical software development projects, though.  We need research to help turn those reasons into the past tense for most projects.</li>
<li><b>Drop the DMCA&#8217;s anti-circumvention measures</b>. The anti-circumvention stuff is just nonsense; they don&#8217;t fight piracy, but they do try to inhibit legal activities - and thus encourage lawlessness.  <a href="http://xkcd.com/488/">XKCD&#8217;s &#8220;Steal this comic&#8221;</a> shows the nonsense that Digital Restrictions Management (DRM) schemes bring, ones that the DMCA is absurdly trying to prop up.  As far as I can tell, people are still making music and movies, even though the DRM schemes (and the anti-circumvention measures that prop them up) are a failure. Anti-circumvention measures make obviously lawful uses illegal (e.g., viewing DVDs on a Linux machine or putting your DVDs on your hard drive) - encouraging everyone to break the law.</li>
<li><b>Drop software patents</b>.  Software patents have been a massive unjustified government intervention in the market.  There is still no evidence that they are an improvement, and a <i>lot</i> of evidence that they are causing serious market failures.  Save massive amounts of government money by getting rid of the whole useless bureaucracy.</li>
<li><b>Fix copyright laws so that they make sense to normal people</b>.  I believe that the current copyright laws were written under the assumption that only large publishers, with reams of lawyers, needed to understand them.  Now 9-year-olds need to understand them&#8230; except that they&#8217;re completely nonsensical.  &#8220;Normal&#8221; people expect that short extractions aren&#8217;t copyright infringements, yet current U.S. law and court cases endorse such nonsensical interpretations (e.g., Bridgeport Music Inc. v. Dimension Films, 410 F.3d 792 (6th Cir. 2005) seems to say that even 3 notes can be an infringment).  Strictly speaking, many Youtube videos break the law, even when a normal person would expect that the use would be okay.  The term lengths of copyright far exceed the minimum necessary to obtain such works (which should be the criteria), and &#8220;fair use&#8221; needs to be clearer and more expansive. The penalties are also absurd; I disapprove of illegal copying, but the current penalties ($750 for a $1 song??) are so disproportionate that they probably violate the U.S. Constitution&#8217;s 8th amendment (&#8220;Excessive bail shall not be required, nor excessive fines imposed, nor cruel and unusual punishments inflicted.&#8221;).  I believe that copyright law is in principle a good idea, but it sure isn&#8217;t working in practice like it&#8217;s supposed to.  See <a href="http://www.law.duke.edu/cspd/comics/">Tales from the Public Domain: Bound by Law</a> for an interesting perspective on this. For a specific example, I think that anything <i>not</i> marked by its author as copyrighted should be in the public domain; currently every jot and tiddle on the Internet is &#8220;copyrighted&#8221; by someone, making it nigh-impossible to keep track of all the claims over rights.  It used to be that way - there&#8217;s no reason it couldn&#8217;t be again.  A much shorter copyright term would be helpful, too - something <i>within</i> people&#8217;s lifetimes.  In the past, publishers got disproportionate control over the process of modifying the copyright laws. We need to fix these laws so that they <i>balance</i> the needs of creators, publishers/distributors, and recipients. They need to be very simple, clear, and fair, because with the Internet, 9-year-olds <i>can and do</i> become publishers.</li> 
</ol>
</p>
<p>
So, there&#8217;s my Christmas list.
Some of them don&#8217;t even cost money; they simply remove bad laws, and
actually <i>save</i> money.
This is my <i>personal</i> list, not influenced by my employer, my pets,
and so on.
Perhaps this list (and others like it) will start the ball rolling.
</p>
<p>path: <a href="http://www.dwheeler.com/blog/security">/security</a> | <a href="http://www.dwheeler.com/blog">Current Weblog</a> | <a href="http://www.dwheeler.com/blog/2008/10/29#internet-2008">permanent link to this entry</a></p>
<h1>Thu, 23 Oct 2008</h1>
<p><a name="why-is-esc-big"><font size="+2"><b>Solved: Why is ESC so big?</b></font></a></p><p></p>
<p>
In my post
<a href="http://www.dwheeler.com/blog/2008/10/22/#development-cost-linux-2008">
Estimating the Total Development Cost of a Linux Distribution</a>,
I noted that one of Fedora 9&#8217;s largest components was
Enterprise Security Client (ESC), and wondered why
ESC would be so big.
After all, a security client should be small - not large.
</p>
<p>
I just got the answer from Rahul Sundaram of the Fedora project,
who asked internally.
It turns out that ESC currently includes its own copy of
<a href="https://developer.mozilla.org/en/XULRunner">XULRunner</a>.
XULRunner essentially provides a library and infrastructure for running
&#8220;XUL+XPCOM&#8221; applications such as Firefox, Thunderbird, and ESC.
You can
<a href="http://www.redhat.com/docs/manuals/cert-system/7.3/html/Enterprise_Security_Client_Guide/Enterprise_Security_Client_Configuration-Enterprise_Security_Client_File_Locations.html">confirm this using the on-line ESC documentation</a>.
This is clearly not optimal; as I noted in a previous blog entry,
<a href="http://www.dwheeler.com/blog/2008/09/19/#use-system-libraries">
developers should use system libraries, and not create their own local
copies</a>.
Rahul says that the
&#8220;the developers are currently working on making it use the system
copy[,] which should drop down the size considerably&#8221;.
</p>
<p>
So ESC isn&#8217;t really that big - it&#8217;s just that ESC creates its own
local copy of a massive infrastructure.
This is obviously not great for security,
since there&#8217;s a higher risk that bugs fixed in the real XULRunner
would not be fixed in ESC&#8217;s local copy.
But this appears to be a temporary issue; once Fedora&#8217;s version of
ESC switches to the system XULRunner, the problem will disappear.
</p>
<p>
By the way, if you&#8217;re interested in the whole &#8220;measuring Linux&#8217;s size&#8221; thing,
you should definitely take a look at the past measurements of Debian.
<a href="http://www.dwheeler.com/sloc/">My page on counting Source
Lines of Code (SLOC)</a> includes links and summaries of that work.
It&#8217;s neat stuff!
My thanks to
Jesús M. González-Barahona, Miguel A. Ortuño Pérez, Pedro de las Heras Quirós,
José Centeno González, Vicente Matellán Olivera,
Juan-José Amor-Iglesias, Gregorio Robles-Martínez, and Israel Herráiz-Tabernero
for doing that.
</p>
<p>path: <a href="http://www.dwheeler.com/blog/oss">/oss</a> | <a href="http://www.dwheeler.com/blog">Current Weblog</a> | <a href="http://www.dwheeler.com/blog/2008/10/23#why-is-esc-big">permanent link to this entry</a></p>
<h1>Wed, 22 Oct 2008</h1>
<p><a name="development-cost-linux-2008"><font size="+2"><b>Estimating the Total Development Cost of a Linux Distribution</b></font></a></p><p></p>
<p>
There&#8217;s a new and interesting paper from the Linux Foundation that
estimates the total development cost of a Linux distro.
Before looking at it, some background would help&#8230;
</p>

<p>
In 2000 and 2001 I published the first estimates of a GNU/Linux distribution&#8217;s
development costs.
The second study (released in 2001, lightly revised in 2002) was titled
<a href="http://www.dwheeler.com/sloc/redhat71-v1/redhat71sloc.html">More than a Gigabuck</a>.
That study
analyzed Red Hat Linux 7.1 as a representative GNU/Linux distribution,
and found that it would cost over $1 billion (over a Gigabuck)
to develop this GNU/Linux distribution by conventional proprietary means
in the U.S. (in year 2000 U.S. dollars).
It included over 30 million physical source lines of code (SLOC),
and had it been developed using conventional proprietary means, it would
have taken 8,000 person-years of development time to create.
My later paper
<a href="http://www.dwheeler.com/essays/linux-kernel-cost.html">
Linux Kernel 2.6: It&#8217;s Worth More!</a> focused on how to estimate the
development costs for just the Linux kernel
(this was picked up by <a href="http://www.groklaw.net/article.php?story=20041012233246869">Groklaw</a>).
</p>

<p>
The Linux Foundation has just re-performed this analysis with Fedora 9,
and released it as
<a href="http://www.linuxfoundation.org/publications/estimatinglinux.php">
&#8220;Estimating the Total Development Cost of a Linux Distribution&#8221;</a>.
<a href="http://linux-foundation.org/weblogs/press/2008/10/21/linux-foundation-publishes-study-estimating-the-value-of-linux/">
Here&#8217;s their press release</a>.
I&#8217;d like to thank the authors (Amanda McPherson, Brian Proffitt,
and Ron Hale-Evans), because they&#8217;ve reported a lot of interesting
information.
</p>

<p>
For example, they found that it would take approximately 
$10.8 billion to rebuild the Fedora 9 distribution in today&#8217;s dollars;
it would take $1.4 billion to develop just the Linux kernel alone.
This isn&#8217;t the <i>value</i> of the distribution;
typically people won&#8217;t write software unless the software had more
value to them than what it cost them (in time and effort) to write it.
They state that quite clearly in the paper;
they note that these numbers estimate
&#8220;how much it would cost to develop the software in a Linux distribution today,
from scratch. It’s important to note that this estimates the cost but not
the value to the greater ecosystem&#8230;&#8221;.
To emphasize that point, the authors reference a 2008 IDC study
(&#8220;The Role of Linux Commercial Servers and Workloads&#8221;) which claims
that Linux represents a $25 billion ecosystem.
I think IDC&#8217;s figure is (in fact) a gross underestimation of
the ecosystem <i>value</i>, understandably so (ecosystem value
is very hard to measure).
Still, the cost to <i>redevelop</i> a system is a plausible lower bound
for the value of something (as long as people keep using it).
More importantly, it clearly proves that very large and sophisticated systems
can be developed as free-libre / open source software (FLOSS).
</p>

<p>
They make a statement about me that I&#8217;d like to expand on:
&#8220;[Wheeler] concluded—as we did—that Software Lines of Code is
the most practical method to determine open source software value
since it focuses on the end result and not on per-company or
per-developer estimates.&#8221;
That statement is quite true, but please let me explain why.
Directly measuring the amount of time and money spent in development would
be, by far, the best way of finding those numbers.
But few developers would respond to a survey requesting that information,
so direct measurement is completely impractical.
Thus, using well-known industry models is the best <i>practical</i>
approach to doing so, in spite of their limitations.
</p>

<p>
I was delighted with their section on the
&#8220;Limitations and Advantages to this Study&#8217;s Approach&#8221;.
All studies have limitations, and I think it&#8217;s much better to
acknowledge them than hide them.
They note several reasons why this approach grossly
underestimates the <i>real</i> effort in developing a distribution,
and I quite agree with them.
In particular: (1) collaboration often takes additional time
(though it often produces better results because you see all sides);
(2) deletions are work yet they are not counted;
(3) &#8220;bake-offs&#8221; to determine the
best approach (where only the winner is included) produce great results
but the additional efforts for the alternatives aren&#8217;t included in the
estimates.
(I noted the bake-off problem in my
<a href="http://www.dwheeler.com/essays/linux-kernel-cost.html">paper on
the Linux kernel</a>.)
They note that some drivers aren&#8217;t often used, but I don&#8217;t see that as
a problem; after all, it still took effort to develop them, so it&#8217;s valid
to include them in an effort estimate.
Besides, one challenge to creating an operating system is this very issue -
to become useful to many, you must develop a large number of drivers - even
though many of the drivers have a relatively small set of users.
</p>

<p>
This is <i>not</i> a study of &#8220;all FLOSS&#8221;; many
FLOSS programs are not included in Fedora (as they note in their limitations).
Others have examined Debian and the Perl CPAN library using my approach
(see <a href="http://www.dwheeler.com/sloc/">my page on SLOC</a>), and
hopefully someday someone will actually try to measure &#8220;all FLOSS&#8221;
(good luck!!).
However, since the Linux Foundation measured a
descendent of what I used for my original analysis,
it&#8217;s valid to examine what&#8217;s happened to the size
of this single distribution over time.
That&#8217;s really interesting, because that lets us examine overall trends.
So let&#8217;s take advantage of that!
In terms of physical source lines of code (SLOC) we have:
<pre>
Distribution         Year   SLOC(million)
Red Hat Linux 6.2    2001    17
Red Hat Linux 7.1    2002    30
Fedora 9             2008   204
</pre>
If Fedora was growing linearly, the first two points estimate a rate of
13MSLOC/year, and Fedora 9 would have 108 MSLOC (30+6*13).
Fedora 9 is almost twice that size, which shows clearly that there&#8217;s
exponential growth.
Even if you factored in the month of release (which I haven&#8217;t done), I
believe you&#8217;d still have clear evidence of exponential growth.
This observation is consistent with
<a href="http://www.riehle.org/publications/2008/the-total-growth-of-open-source/">&#8220;The Total Growth of Open Source&#8221; by Amit Deshpande and Dirk Riehle</a>
(2008), which found that 
&#8220;both the growth rate as well as the absolute amount of source
code is best explained using an exponential model&#8221;.
</p>

<p>
Another interesting point:
<a href="http://www.informationweek.com/blog/main/archives/2007/10/linux_will_be_w.html">
Charles Babcock predicted, in Oct. 19, 2007, that the Linux kernel
would be worth $1 billion in the first 100 days of 2009</a>.
He correctly predicted that it would pass $1 billion, but it
happened somewhat earlier than he thought: by Oct. 2008 it&#8217;s already happened,
instead of waiting for 2009.
I think the reason it happened slightly earlier is that Charles Babcock&#8217;s
rough estimate was based on a linear approximation
(&#8220;adding 2,000 lines of code a day&#8221;).
But these studies all seem to indicate that mature
FLOSS programs - including the Linux kernel - are currently
growing <i>exponentially</i>, not linearly.
Since the rate is also increasing, the date of arrival at $1 billion
was sooner than Babcock&#8217;s rough estimate.
Babcock&#8217;s fundamental point - that the Linux kernel keeps adding value
at a tremendous pace - is still absolutely correct.
</p>

<p>
I took a look at some of the detailed data, and some very interesting
factors were revealed.
By lines of code, here were the largest programs in Fedora 9
(biggest first):
<pre>
  kernel-2.6.25i686
  OpenOffice.org
  Gcc-4.3.0-2 0080428
  Enterprise Security Client 1.0.1
  eclipse-3.3.2
  Mono-1.9.1
  firefox-3.0
  bigloo3.0b
  gcc-3.4.6-20060404
  ParaView3.2.1
</pre>
</p>
<p>
The Linux kernel is no surprise; as I noted in the past, it&#8217;s <i>full</i>
of drivers, and there&#8217;s a continuous stream of new hardware that
need drivers.
The Linux Foundation decided to count both gcc3 and gcc4; since there
was a radical change in approach between gcc3 and gcc4, I think that&#8217;s fair
in terms of effort estimation.
(My tool ignores duplicate files, which helps counter double-counting
of effort.)
Firefox wasn&#8217;t included by name in the Gigabuck study, but Mozilla was,
and Firefox is essentially its descendent.
It&#8217;s unsurprising that Firefox is big; it does a lot of things, and
trying to make things &#8220;look&#8221; simple often takes more code (and effort).
</p>
<p>
What&#8217;s remarkable is that many of the largest programs in Fedora 9
were not even <i>included</i> in the &#8220;Gigabuck&#8221; study - these are whole
new applications that were added to Fedora since that time.
These largest programs not in the Gigabuck study are:
OpenOffice.org (an office suite, aka word processor,
spreadsheet, presentation, and so on),
Enterprise Security Client, eclipse (a development environment), Mono
(an implementation of the C# programming language and its underlying
&#8220;.NET&#8221; environment), bigloo (an implementation of the Scheme programming
language), and paraview (a data analysis and visualization application for
large datasets).
OpenOffice.org&#8217;s size is no surprise; it does a lot.
I&#8217;m a little concerned that &#8220;Enterprise Security Client&#8221; is so huge -
a security client should be <i>small</i>, not big, so that you can analyze
it thoroughly for trustworthiness.
Perhaps someone will analyze that program further to see why this is so,
and if that&#8217;s a reason to be concerned.
</p>

<p>
Anyway, take a look at
<a href="http://www.linuxfoundation.org/publications/estimatinglinux.php">
&#8220;Estimating the Total Development Cost of a Linux Distribution&#8221;</a>.
It conclusively shows that large and useful systems can be developed as FLOSS.
</p>

<p>
An interesting coincidence: Someone else (Heise) almost simultaneously
released a study of just the Linux kernel, again using SLOCCount.
<a href="http://www.heise-online.co.uk/open/Kernel-Log-More-than-10-million-lines-of-Linux-source-files--/news/111759">
Kernel Log: More than 10 million lines of Linux source files</a>
notes that the Linux kernel version 2.6.27 has 6,399,191 SLOC.
&#8220;More than half of the lines are part of hardware drivers; the second largest
chunk is the arch/ directory which contains the source code of the various
architectures supported by Linux.&#8221;
In that code,
&#8220;96.4 per cent of the code is written in C and 3.3 percent in Assembler&#8221;.
They didn&#8217;t apply the corrective factors specific to Linux kernels
that I discussed in
<a href="http://www.dwheeler.com/essays/linux-kernel-cost.html">
Linux Kernel 2.6: It&#8217;s Worth More!</a>, but it&#8217;s still interesting to see.
And their conclusion is inarguable:
&#8220;There is no end in sight for kernel growth which has been ongoing in the Linux
2.6 series for several years - with every new version, the kernel hackers
extend the Linux kernel further to include new functions and drivers,
improving the hardware support or making it more flexible, better or faster.&#8221;
</p>
<p>path: <a href="http://www.dwheeler.com/blog/oss">/oss</a> | <a href="http://www.dwheeler.com/blog">Current Weblog</a> | <a href="http://www.dwheeler.com/blog/2008/10/22#development-cost-linux-2008">permanent link to this entry</a></p>
<h1>Thu, 02 Oct 2008</h1>
<p><a name="play-ogg-vorbis-theora"><font size="+2"><b>Play Ogg (Vorbis and Theora)!</b></font></a></p><p></p>
<p>
The good news: There&#8217;s lots of digital audio and video available through
the Internet (some free, some pay-for).
The bad news: Lots of audio and video
is locked up in formats that aren&#8217;t open standards.
This makes it impractical for people to use them on arbitrary devices, shift
the media between devices, and so on.
This hurts product developers too; they&#8217;ve become vulnerable to
massive lawsuits.
Even though the MPEG standards are ratified by ISO and
are often used - MP3 is particularly common for audio -
they are not <a href="http://www.digistan.org/">open standards</a>.
In particular,
they are subject to a raft of patents, which prevent arbitrary use
(e.g., by free-libre / open source software).
Things are even worse if you use a format with DRM (aka
&#8220;Digital Restrictions Management&#8221;).
DRM tries to arbitrarily restrict how you can use the media you&#8217;ve paid for;
when the company decides to abandon support for that DRM format,
you&#8217;ve effectively lost all the money you spent on the audio and video media
(<a href="http://en.wikipedia.org/wiki/Digital_rights_management#Obsolescence">examples of DRM abandonment</a> include Microsoft&#8217;s MSN Music,
Microsoft&#8217;s PlaysForSure which is not supported by Microsoft Zune,
Yahoo! Music Store, and Walmart&#8217;s DRM-encumbered music).
<p>
Thankfully, there&#8217;s a solution, and that&#8217;s
<a href="http://xiph.org/">Ogg</a> (as maintained by the Xiph.org foundation).
Ogg is a &#8220;container format&#8221; that can contain audio, video, and
related material.
Audio and video can be encoded inside Ogg using one of several encodings,
but usually audio is encoded with &#8220;Vorbis&#8221; and video is encoded with &#8220;Theora&#8221;.
For perfect sound reproduction, you can use &#8220;FLAC&#8221; instead of Vorbis
(but for most circumstances, Vorbis is the better choice).
</p>
<p>
I encourage you to use Ogg, and I&#8217;m not the only one.
<a href="http://en.wikipedia.org/wiki/Wikipedia:Creation_and_usage_of_media_files">Wikipedia <b>requires</b> that audio and video be in Ogg Vorbis and Ogg Theora format (respectively)</a>;
according to Alexa,
<a href="http://www.alexa.com/site/ds/top_sites?cc=US&ts_mode=country&lang=none">Wikipedia is the 8th most popular website in the U.S.</a>
(as of Oct 2, 2008).
The <a href="http://www.fsf.org/resources/formats/playogg">
Free Software Foundation (FSF)&#8217;s &#8220;Play Ogg&#8221; campaign</a> is
encouraging the use of Ogg, too.
<a href="http://www.xiph.org/press/2007/w3c/#5">Xiph.org&#8217;s 2007 press release</a>
and
<a href="http://www.xiph.org/about/">about Xiph</a>
explain some of the reasons for preferring Ogg.
</p>
<p>
So, please seek out and create Ogg files!
Their file extensions are easily recognized:
<a href="http://wiki.xiph.org/index.php/MIME_Types_and_File_Extensions">
&#8220;.ogg&#8221; (Ogg Vorbis sound), &#8220;.oga&#8221; (Ogg audio using other codecs like
FLAC), and &#8220;.ogv&#8221; (Ogg video, typically Theora plus Vorbis)</a>.
If you need to download software to play Ogg files,
<a href="http://www.fsf.org/resources/formats/playogg/how">FSF Ogg&#8217;s &#8220;how&#8221; page</a>
or
<a href="http://xiph.org">Xiph.org&#8217;s home page</a> will explain how
to download and install software to play Ogg files (they&#8217;re free,
in all senses!).
Many video players can play Ogg already; among them,
<a href="http://www.videolan.org/">VLC (from VideoLAN)</a> is
often recommended as a player.
</p>
<p>
Probably the big news is that the
<a href="http://www.0xdeadbeef.com/weblog/?p=492">next
version of Mozilla&#8217;s Firefox will include Ogg - built in</a>!
So soon, you can just install Firefox, and you&#8217;ll have Ogg support.
That should encourage even more use of Ogg, because there will be
so many more people who have Ogg (or can get install it easily), as well
as lots of reasons to install such software.
</p>
<p>
If you want more technical details, you can see the
<a href="http://en.wikipedia.org/wiki/Ogg">Wikipedia article on Ogg</a>.
You can also see
<a href="http://www.ietf.org/rfc/rfc5334.txt">Internet standard
RFC 5334</a>, which discusses the basic file extensions
and MIME types, as well as pointing to other technical documents.
</p>
<p>
Currently there is a babel of formats out there, and most of the more
common ones are not open standards.
I have no illusions that this babel will instantly disappear, with
everyone using Ogg by tomorrow.
Getting a new audio or video format used is a
difficult chicken-and-egg problem: People don&#8217;t want to release
audio or video until everyone can play them, and people don&#8217;t want to
install format players until there&#8217;s something to play.
</p>
<p>
But with Wikipedia, Firefox, and many others all working to encourage the Ogg
format, I think the chicken-and-egg problem has been overcome.
I&#8217;m now discovering all sorts of organizations support Ogg, such as
<a href="http://metavid.org/wiki/Help:FAQ">Metavid,
(who provide video footage from the U.S. Congress in Ogg Theora format)</a>.
<a href="http://www.groklaw.net/article.php?story=20080821100600851">Groklaw
interviewed Richard Hulse of Radio New Zealand, who explained why
they recently added support for Ogg Vorbis</a>.
Many other radio stations support Ogg; I&#8217;ve confirmed support by the
<a href="http://www.cbc.ca/listen/index.html">Canadian Broadcasting
Corporation (CBC) (Radio feeds 1 and 2)</a>,
<a href="http://theclassicalstation.org/">WPCE</a>, and
<a href="http://www.wbur.org/">WBUR</a>
(<a href="http://wiki.xiph.org/index.php/VorbisStreams">Xiph.org has
a much longer list of stations supporting Ogg</a>).
Ogg is widely used in games;
there&#8217;s Ogg support in the engines for
Doom 3, Unreal Tournament 2004, Halo: Combat Evolved, Myst IV:
Revelation, Serious Sam: The Second Encounter, Lineage 2, Vendetta Online, and
the Grand Theft Auto engines
(<a href="http://wiki.xiph.org/index.php/Games_that_use_Vorbis">Xiph.org
has a longer list of games</a>).
In short,
there are now enough Ogg players, and Ogg media, to get the ball rolling.
</p>
<p>
In particular: Don&#8217;t buy a portable audio (music) player
unless it can play Ogg Vorbis.
<a href="http://wiki.xiph.org/index.php/PortablePlayers">Xiph has
a list of audio players that support Ogg Vorbis</a> (read the details
for the player you&#8217;re considering!).
If a manufacturer doesn&#8217;t support Ogg,
complain to them until they fix the problem.
</p>
<p>path: <a href="http://www.dwheeler.com/blog/oss">/oss</a> | <a href="http://www.dwheeler.com/blog">Current Weblog</a> | <a href="http://www.dwheeler.com/blog/2008/10/02#play-ogg-vorbis-theora">permanent link to this entry</a></p>
</body></html>