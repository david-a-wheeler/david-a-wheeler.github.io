<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8"></meta>
<link rel="alternate" type="application/rss+xml" title="RSS" href="http://www.dwheeler.com/blog/index.rss"></link>
<title>David A. Wheeler's Blog   </title>
<meta name="viewport" content="width=device-width, initial-scale=1.0">
</head>
<body>
<h1 style="margin-left: auto; margin-right: auto; width: 50%;">David A. Wheeler's Blog</h1><p>  </p><p></p><h1>Sat, 16 Nov 2013</h1>
<p><a name="vulnerability-economics"><font size="+2"><b>Vulnerability bidding wars and vulnerability economics</b></font></a></p><p></p>
<p>
I worry that the economics of software vulnerability reporting
is seriously increasing the risks to society.
The problem is the rising bidding wars for vulnerability information,
leading to a rapidly-growing number of
vulnerabilities known only to attackers.
These kinds of vulnerabilities, when exploited,
are sometimes called &#8220;zero-days&#8221;
because users and suppliers had zero days of warning.
I suspect we should create laws limiting the
sale of vulnerability information,
similar to the limits we place on organ donation,
to change the economics of vulnerability reporting.
To see why, let me go over some background first.
</p>

<p>
A big part of the insecure software problem today is that
relatively few of today&#8217;s software developers know how to
develop software that resists attack (e.g., via the Internet).
Many schools don&#8217;t teach it at all.
I think that&#8217;s ridiculous;
you&#8217;d think people would have heard about the Internet by now.
I do have some hope that this will get better.
I teach a graduate course on how to develop secure software at
George Mason University (GMU), and attendance has increased over time.
But today, most software developers do not know how to create secure software.
</p>

<p>
In contrast,
there is an increasing bidding war for vulnerability information
by organizations who intend to exploit those vulnerabilities.
This incentivizes people to search for vulnerabilities, but
<i>not</i> report them to the suppliers (who could fix them) and
<i>not</i> alert the public.
As
<a href="http://www.schneier.com/blog/archives/2012/06/the_vulnerabili.html">Bruce Schneier reports in
&#8220;The Vulnerabilities Market and the Future of Security&#8221;
(June 1, 2012)</a>,
&#8220;This new market perturbs the economics
of finding security vulnerabilities.
And it does so to the detriment of us all.&#8221;
Forbes ran an article about this in 2012,
<a href="http://www.forbes.com/sites/andygreenberg/2012/03/21/meet-the-hackers-who-sell-spies-the-tools-to-crack-your-pc-and-get-paid-six-figure-fees/">
Meet The Hackers Who Sell Spies The Tools To Crack Your PC (And Get Paid Six-Figure Fees)</a>.
The Forbes article describes what happened when
French security firm Vupen broke the security of the Chrome web browser.
Vupen would not tell Google how they broke in, because the
$60,000 award Google from Google was <i>not enough</i>.
Chaouki Bekrar, Vupen&#8217;s chief executive, said that they
&#8220;wouldn&#8217;t share this [information]
with Google for even $1 million&#8230;
We want to keep this for our customers.&#8221;
These customers do not plan to fix security bugs;
they purchase exploits or techniques with the
&#8220;explicit intention of invading or disrupting&#8221;.
Vupen even
&#8220;hawks each trick to multiple government agencies,
a business model that often plays its customers against one another
as they try to keep up in an espionage arms race.&#8221;
Just
<a href="http://www.networkworld.com/news/2012/060812-price-tag-for-microsoft-piece-260001.html?source=NWWNLE_nlt_daily_pm_2012-06-08">one part of the
Flame espionage software (exploiting Microsoft Update) has been estimated
as being worth $1 million when it was not known</a>.
</p>

<p>
This imbalance in economic incentives creates a
dangerous and growing mercenary subculture.
You now have a growing number of people looking for vulnerabilities,
keeping them
secret, and selling them to the highest bidder&#8230; which will encourage
<i>more</i> to look for, and keep secret, these vulnerabilities.
After all, they are incentivized to do it.
In contrast, the original developer typically does not know 
how to develop secure software,
and there are fewer economic incentives to develop secure software anyway.
This is a volatile combination.
</p>

<p>
Some think the solution is for suppliers to pay people when
they report security vulnerabilities to suppliers
(&#8220;bug bounties&#8221;).
I do not think bug bounty systems (by themselves) will be enough, though
suppliers are trying.
</p>

<p>
There has been a lot of discussion about Yahoo and bug bounties.
On September 30, 2013, the article
<a href="https://www.htbridge.com/news/what_s_your_email_security_worth_12_dollars_and_50_cents_according_to_yahoo.html">What&#8217;s your email security worth? 12 dollars and 50 cents according to Yahoo</a>
reported that Yahoo paid for each vulnerability
only $12.50 USD.
Even worse, this was not actual money, it was
&#8220;a discount code that can only be used in the Yahoo Company Store,
which sell Yahoo&#8217;s corporate t-shirts,
cups, pens and other accessories&#8221;.
Ilia Kolochenko, High-Tech Bridge CEO, says:
&#8220;Paying several dollars per vulnerability is a bad joke and won&#8217;t
motivate people to report security vulnerabilities to them, especially
when such vulnerabilities can be easily sold on the black market for
a much higher price. Nevertheless, money is not the only motivation
of security researchers. This is why companies like Google efficiently
play the ego card in parallel with [much higher] financial rewards and
maintain a &#8216;Hall of Fame&#8217; where all security researchers who have
ever reported security vulnerabilities are publicly listed. If Yahoo
cannot afford to spend money on its corporate security, it should at
least try to attract security researchers by other means. Otherwise,
none of Yahoo&#8217;s customers can ever feel safe.&#8221;
Brian Martin, President of Open Security Foundation, said:
&#8220;Vendor bug bounties are not a new thing. Recently, more vendors have
begun to adopt and appreciate the value it brings their organization,
and more importantly their customers. Even Microsoft, who was the most
notorious hold-out on bug bounty programs realized the value and jumped
ahead of the rest, offering up to $100,000 for exploits that bypass their
security mechanisms. Other companies should follow their example and
realize that a simple &#8220;hall of fame&#8221;,
credit to buy the vendor&#8217;s products,
or a pittance in cash is not conducive to researcher cooperation.
Some of these companies pay their janitors more money to clean their offices,
than they do security researchers finding vulnerabilities that may put
thousands of their customers at risk.&#8221;
<a href="http://www.itnews.com.au/News/359206,yahoo-plots-bug-bounties-up-to-15000.aspx">Yahoo has since decided to establish a bug bounty system with larger rewards</a>.
</p>

<p>
More recently, the
<a href="https://hackerone.com/internet">Internet Bug Bounty Panel</a>
(<a href="http://thenextweb.com/insider/2013/11/06/microsoft-facebook-sponsor-internet-bug-bounty-program-offer-cash-hacking-internet-stack/">founded by Microsoft and Facebook</a>)
will award public research into vulnerabilities with the potential for severe security implications to the public.
It has a minimum bounty of $5,000.
However, it certainly does not cover everything; they only
intend to pay out
widespread vulnerabilities (wide range of products or end users),
and
plan to limit bounties to only severe vulnerabilities that are novel
(new or unusual in an interesting way).
I think this could help, but it is no panacea.
</p>

<p>
Bug bounty systems are typically drastically outbid by attackers,
and I see no reason to believe this will change.
</p>

<p>
Indeed, I do not think we should mandate, or even expect,
that suppliers will pay people when
people report security vulnerabilities to suppliers (aka bug bounties).
Such a mandate or expectation
could kill small businesses and open source software development,
and it would almost certainly chill software development in general.
Such payments would not also deal with what I see as a key problem:
the people who sell vulnerabilities to the highest bidder.
Mandating payment by suppliers would get most people to send them
problem reports&#8230; if the bug bounty payments were required to
be larger than payments to those who would exploit the vulnerability.
<i>That</i> would be absurd, because
given current prices, such a requirement
would almost certainly prevent a lot of software development.
</p>

<p>
I think people who find a vulnerability in software
should normally
be free to tell the software&#8217;s supplier, so that the supplier can
rapidly repair the software (and thus fix it before it is exploited).
Some people call this &#8220;responsible disclosure&#8221;, though
some suppliers misuse this term.
Some suppliers say they want &#8220;responsible disclosure&#8221;,
but they instead appear to irresponsibly abuse the term
to stifle warning those at risk (including customers and the public),
as well as irresponsibly delay the repair of critical vulnerabilities
(if they repair the vulnerabilities at all).
After all, if a supplier convinces the researcher to <i>not</i>
alert users, potential users,
and the public about serious security defects in their product,
then these irresponsible suppliers may
believe they don&#8217;t need to fix it quickly.
People who are suspicious about &#8220;responsible disclosure&#8221;
have, unfortunately, excellent reasons to be suspicious.
Many suppliers have shown themselves untrustworthy, and
even trustworthy suppliers need to have a reason to stay that way.
For that and other reasons,
I also think people should be free to alert the public in detail,
at no charge, about a software vulnerability
(so-called &#8220;full disclosure&#8221;).
Although it&#8217;s not ideal for users, full disclosure is sometimes
necessary; it can be especially justifiable
when a supplier has demonstrated (through past or current actions)
that he will not rapidly fix the problem that he created.
In fact, I think it&#8217;d be an inappropriate constraint
of free speech to prevent people from revealing serious
problems in software products to the public.
</p>

<p>
But if we don&#8217;t want to mandate bug bounties,
or so-called &#8220;responsible disclosure&#8221;,
then where does that leave us?
We need to find some way to change the rules so that
economics works more closely
<i>with</i> and not <i>against</i> computer security.
</p>

<p>
Well, here is an idea&#8230; at least one to start with.
Perhaps we should criminalize <i>selling</i> vulnerability information
to anyone other than the supplier or the reporter&#8217;s government.
Basically, treat vulnerability information like organ donation:
intentionally eliminate economic incentives in a specific area
for a greater social good.
</p>

<p>
That would mean that suppliers can set up bug bounty programs,
and researchers can publish information about vulnerabilities to the public,
but this would sharply limit who else can legally
buy the vulnerability information.
In particular, it would be illegal to sell the information to
organized crime, terrorist groups, and so on.
Yes, governments can do bad things with the information;
this particular proposal does nothing directly to address it.
But I think it&#8217;s impossible to prevent a citizen from
telling his country&#8217;s government about a software vulnerability;
a citizen could easily see it as his duty.
I also think no government would forbid buying such information for itself.
However, by limiting sales to that particular
citizen&#8217;s government, it becomes
harder to create bidding wars between
governments and other groups for vulnerability information.
Without the bidding wars, there&#8217;s less incentive for others to
find the information and sell it to them.
Without the incentives, there would be fewer people working to
find vulnerabilities that they would intentionally hide from 
suppliers and the public.
</p>

<p>
I believe this would not impinge on freedom of speech.
You can tell no one, everyone, or anyone you want about
the vulnerability.
What you cannot do is receive <i>financial benefit</i> from
selling vulnerability information to anyone other than
the supplier (who can then fix it) or your own government
(and that at least reduces bidding wars).
</p>

<p>
Of course, you always have to worry about
unexpected consequences or easy workarounds for any new proposed law.
An organization could set itself up specifically to find vulnerabilities
and then exploit them itself&#8230; but that&#8217;s already illegal,
so I don&#8217;t see a problem there.
A trickier problem is that
a malicious organization (say, the mob) could create a
&#8220;supplier&#8221; (e.g., a reseller of proprietary software, or a
downstream open source software package) that vulnerability
researchers could sell their information to, working around the law.
This could probably be handled by requiring, in law,
that suppliers report (in a timely manner) any
vulnerability information they receive to their relevant suppliers.
</p>

<p>
Obviously there are some people will do illegal things,
but some people will avoid doing illegal things in principle, and
others will avoid illegal activities because they fear getting caught.
You don&#8217;t need to stop all possible cases, just enough to
change the economics.
</p>

<p>
I fear that the current &#8220;vulnerability bidding wars&#8221; -
left unchecked -
will create an overwhelming tsunami of zero-days
available to a wide variety of malicious actors.
The current situation might impede the peer review of
open source software (OSS), since currently people can make more money
selling an exploit than in helping the OSS project fix the problem.
Thankfully, OSS projects are still widely viewed as public goods,
so there are still many people who are willing to take the pay cut
and help OSS projects find and fix vulnerabilities.
I think proprietary and custom software are actually in much
more danger than OSS;
in those cases it&#8217;s a lot easier for people to think
&#8220;well, they wrote this code for their financial gain,
so I may as well sell my vulnerability information for my financial gain&#8221;.
The problem for society is that this attitude completely ignores the users and
those impacted by the software, who can get hurt by the later
exploitation of the vulnerability.
</p>

<p>
Maybe there&#8217;s a better way.
If so, great&#8230; please propose it!
My concern is that
economics currently makes it hard - not easy - to have computer security.
We need to figure out ways to get
Adam Smith&#8217;s invisible hand to work <i>for</i> us,
not <i>against</i> us.
</p>

<p>
Standard disclaimer:
As always, these are
<a href="http://www.dwheeler.com/aboutsite.html">my personal opinions</a>,
not those of employer, government, or
<a href="http://www.dwheeler.com/wiggles.html">(deceased) guinea pig</a>.
</p>
<p>path: <a href="http://www.dwheeler.com/blog/security">/security</a> | <a href="http://www.dwheeler.com/blog">Current Weblog</a> | <a href="http://www.dwheeler.com/blog/2013/11/16#vulnerability-economics">permanent link to this entry</a></p>
</body></html>