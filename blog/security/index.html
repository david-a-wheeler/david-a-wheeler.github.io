<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8"></meta>
<link rel="alternate" type="application/rss+xml" title="RSS" href="https://www.dwheeler.com/blog/index.rss"></link>
<title>David A. Wheeler's Blog   </title>
<meta name="viewport" content="width=device-width, initial-scale=1.0"></meta>
</head>
<body>
<h1 style="margin-left: auto; margin-right: auto; width: 50%;">David A. Wheeler's Blog</h1><p>  </p><p></p><h1>Sun, 13 Dec 2020</h1>
<p><a name="floss-weekly-609"><font size="+2"><b>FLOSS Weekly #609!</b></font></a></p><p></p>
<p>
I&#8217;m currently scheduled to be a guest on
<a href="https://twit.tv/shows/floss-weekly/"
>FLOSS Weekly</a> on 
Wednesday, 2020-12-16, at 12:30pm Eastern Time (9:30am Pacific, 17:30 UTC).
The general topic will be about Linux Foundation work on
improving Open Source Software security.
</p>
<p>
Please join the live audience or listen later.
I expect it will be interesting.
I expect that we&#8217;ll discuss the
<a href="https://openssf.org/">Open Source Security Foundation (OpenSSF)</a>,
the
<a href="https://www.linuxfoundation.org/blog/2020/12/download-the-report-on-the-2020-foss-contributor-survey/"
>Report on the 2020 FOSS Contributor Survey</a>,
the free edX trio of courses on
<a href="https://www.edx.org/professional-certificate/linuxfoundationx-secure-software-development-fundamentals"
>Secure Software Development Fundamentals</a>,
and the
<a href="https://bestpractices.coreinfrastructure.org/"
>CII Best Practices Badge</a> program.
</p>
<p>path: <a href="https://www.dwheeler.com/blog/security">/security</a> | <a href="https://www.dwheeler.com/blog">Current Weblog</a> | <a href="https://www.dwheeler.com/blog/2020/12/13#floss-weekly-609">permanent link to this entry</a></p>
<p><a name="2020-foss-contributor-report"><font size="+2"><b>Report on the 2020 FOSS Contributor Survey</b></font></a></p><p></p>
<p>
It&#8217;s here!
You can now see the
<a href="https://www.linuxfoundation.org/blog/2020/12/download-the-report-on-the-2020-foss-contributor-survey/"
>Report on the 2020 Free and Open Source Software (FOSS) Contributor Survey</a>!
This work was done by the Linux Foundation
under the Core Infrastructure Initiative (CII) and later the
<a href="https://openssf.org/"
>Open Source Software Foundation (OpenSSF)</a>, along with
Harvard University.
</p>
<p>path: <a href="https://www.dwheeler.com/blog/security">/security</a> | <a href="https://www.dwheeler.com/blog">Current Weblog</a> | <a href="https://www.dwheeler.com/blog/2020/12/13#2020-foss-contributor-report">permanent link to this entry</a></p>
<p><a name="secure-software-development-fundamentals"><font size="+2"><b>Secure Software Development Fundamentals</b></font></a></p><p></p>
<p>
If you develop software, please consider taking the <i>free</i>
trio of courses
<a href="https://www.edx.org/professional-certificate/linuxfoundationx-secure-software-development-fundamentals"
>Secure Software Development Fundamentals</a>
on edX that I recently created for the Linux Foundation&#8217;s
<a href="https://openssf.org/"
>Open Source Security Foundation (OpenSSF)</a>.
The trio of courses is free; if you want to get a certificate to prove
you learned it, you can pay to take some tests to earn the certificate
(this is how many edX courses work).
</p>
<p>
Here&#8217;s a brief summary:
<blockquote>
Almost all software is under attack today, and many organizations are unprepared in their defense. This professional certificate program, developed by the Open Source Security Foundation (OpenSSF), a project of the Linux Foundation, is geared towards software developers, DevOps professionals, software engineers, web application developers, and others interested in learning how to develop secure software, focusing on practical steps that can be taken, even with limited resources to improve information security. The program enables software developers to create and maintain systems that are much harder to successfully attack, reduce the damage when attacks are successful, and speed the response so that any latent vulnerabilities can be rapidly repaired. The best practices covered in the course apply to all software developers, and it includes information especially useful to those who use or develop open source software.
<br><br>
The program discusses risks and requirements, design principles, and evaluating code (such as packages) for reuse. It then focuses on key implementation issues: input validation (such as why allowlists and not denylists should be used), processing data securely, calling out to other programs, sending output, cryptography, error handling, and incident response. This is followed by a discussion on various kinds of verification issues, including tests, including security testing and penetration testing, and security tools. It ends with a discussion on deployment and handling vulnerability reports.
<br><br>
The training courses included in this program focus on practical steps that you (as a developer) can take to counter most common kinds of attacks. It does not focus on how to attack systems, how attacks work, or longer-term research.
<br><br>
Modern software development depends on open source software, with open source now being pervasive in data centers, consumer devices, and services. It is important that those responsible for cybersecurity are able to understand and verify the security of the open source chain of contributors and dependencies. Thanks to the involvement of OpenSFF, a cross-industry collaboration that brings together leaders to improve the security of open source software by building a broader community, targeted initiatives, and best practices, this program provides specific tips on how to use and develop open source secur
</blockquote>
</p>
<p>
I also teach a graduate course on how to the design and implementation
of secure software.
As you might expect, a graduate course isn&#8217;t the same thing.
But please, if you&#8217;re a software developer, take the free edX, my class,
or in some other way learn about how to develop secure software.
The software that society depends on needs to be more secure
than it is today.
Having software developers know how develop secure software
is a necesary step towards creating that secure software we all need.
</p>
<p>path: <a href="https://www.dwheeler.com/blog/security">/security</a> | <a href="https://www.dwheeler.com/blog">Current Weblog</a> | <a href="https://www.dwheeler.com/blog/2020/12/13#secure-software-development-fundamentals">permanent link to this entry</a></p>
<h1>Sat, 23 May 2020</h1>
<p><a name="verizon-rpki"><font size="+2"><b>Verizon still failing to support RPKI</b></font></a></p><p></p>
<p>
On 2019-06-24 parts of the Internet became inaccessible because
Verizon failed to implement a key security measure called
<a href="https://blog.cloudflare.com/rpki/"
>Resource Public Key Infrastructure (RPKI)</a>.
<a href="https://blog.cloudflare.com/how-verizon-and-a-bgp-optimizer-knocked-large-parts-of-the-internet-offline-today/"
>Here&#8217;s a brief story about the 2019 failure by Verizon</a>, with
<a href="https://blog.cloudflare.com/the-deep-dive-into-how-verizon-and-a-bgp-optimizer-knocked-large-parts-of-the-internet-offline-monday/"
>follow-on details</a>.
</p>
<p>
What&#8217;s shocking is that Verizon is <i>still</i> failing to implement RPKI.
Verizon&#8217;s continuing failure continues to make it trivial
for both accidents and
malicious actors (including governments) to shut down large swathes of
the Internet, including networks around the US capital.
That&#8217;s especially absurd because during the
COVID-19 pandemic we have become <i>more</i> dependent on the Internet.
There have been many routine failures by accident or on purpose;
it&#8217;s past time to deploy the basic countermeasure (RPKI) to deal with it.
Verizon needs to implement RPKI, as many other operators already have.
</p>
<p>
The fundamental problem is that the Internet depends on a routing system
called Border Gateway Protocol (BGP), which never included a
(useful) security mechanism.
<a href="https://blog.cloudflare.com/rpki/">Resource Public Key (RPKI)</a>
provides an important security mechanism to counter certain kinds of
BGP problems (either by accident or on purpose).
<a href="https://www.youtube.com/watch?v=Y9vbbxr-GbI"
>&#8220;Why it&#8217;s time to deploy RPKI&#8221; (RIPE NCC, 2019-05-17)</a>
is a short 2-minute video that explains why it&#8217;s past time to deploy RPKI.
</p>
<p>
Verizon already knows that they&#8217;re failing to support RPKI; here&#8217;s a
<a href="https://forums.verizon.com/t5/Fios-Internet/Verizon-FIOS-does-NOT-provide-important-network-security-known/td-p/895575">complaint
posted on 2020-04-19 7:16AM that Verizon wasn&#8217;t supporting RPKI</a>.
It&#8217;s clear RPKI is useful;
<a href="https://blog.thousandeyes.com/visualizing-the-benefits-of-rpki/"
>&#8220;Visualizing the Benefits of RPKI&#8221; by Kemal Sanjta (2019-07-19)</a>
shows how RPKI really does help.
</p>
<p>
If you&#8217;re a Verizon customer, you can easily verify Verizon&#8217;s status via
<a href="https://isbgpsafeyet.com/"
>Is BGP safe yet?</a>.
The answer for Verizon users is &#8220;no&#8221;.
</p>
<p>
If your Internet Service Provider (ISP) doesn&#8217;t support RPKI, please
nag them to do so.
If you&#8217;re a government, and your ISPs won&#8217;t yet support RPKI,
ask when they&#8217;re going secure their network with this basic security measure.
It will take work, and it won&#8217;t solve all problems in the universe,
but those are merely excuses for failure; those statements describe
all things that should be done.
RPKI is an important minimum part of securing the Internet, and it&#8217;s time
to ensure that every Internet Service Provider (ISP) supports it.
</p>
<p>path: <a href="https://www.dwheeler.com/blog/security">/security</a> | <a href="https://www.dwheeler.com/blog">Current Weblog</a> | <a href="https://www.dwheeler.com/blog/2020/05/23#verizon-rpki">permanent link to this entry</a></p>
<h1>Tue, 19 May 2020</h1>
<p><a name="ntia-sbom"><font size="+2"><b>Software Bill of Materials (SBOM) work at NTIA</b></font></a></p><p></p>
<p>
Modern software systems contain many components, which themselves
contain components, which themselves contain components.
Which raises some important questions, for example,
when a vulnerability is publicly identified, how do you know if
your system is affected?
Another issue involves licensing - how can you be confident that
you are meeting all your legal obligations?
This is getting harder to do as systems get bigger, and also because
software development is a global activity.
</p>

<p>
On July 19, 2018, the US National Telecommunications and Information
Administration (NTIA) &#8220;convened a meeting of stakeholders from across
multiple sectors to begin a discussion about software transparency and
the proposal being considered for a common structure for describing the
software components in a product containing software.&#8221;
[<i>Framing Software Component Transparency: Establishing a Common Software
Bill of Material (SBOM)</i>]
</p>

<p>
A key part of this is to make it much easier to define and exchange a
&#8220;Software Bill of Materials&#8221; (SBOM).
You can see a lot of their information at the
<a href="https://www.ntia.gov/sbom">Community-Drafted
Documents on Software Bill of Materials</a>.
If you&#8217;re interested in this topic, that&#8217;s a decent place to start.
</p>
<p>path: <a href="https://www.dwheeler.com/blog/security">/security</a> | <a href="https://www.dwheeler.com/blog">Current Weblog</a> | <a href="https://www.dwheeler.com/blog/2020/05/19#ntia-sbom">permanent link to this entry</a></p>
<h1>Fri, 15 May 2020</h1>
<p><a name="underhanded-code"><font size="+2"><b>Initial Analysis of Underhanded Source Code</b></font></a></p><p></p>
<p>
Announcing - a newly-available security paper I wrote! It&#8217;s titled
<a href="https://www.ida.org/-/media/feature/publications/i/in/initial-analysis-of-underhanded-source-code/d-13166.ashx"
>&#8220;Initial Analysis of Underhanded Source Code&#8221;
(by David A. Wheeler, IDA Document D-13166, April 2020)</a>.
Here&#8217;s what it&#8217;s about, from its executive summary:
</p>
<blockquote>
&#8220;It is possible to develop software source code that appears benign
to human review but is actually malicious. In various competitions,
such as the Obfuscated V Contest and Underhanded C Contest,
software developers have demonstrated that it is possible to solve
a data processing problem “with covert malicious behavior [in the]
source code [that] easily passes visual inspection.”
This is not merely an academic concern; in 2003, an attacker attempted to
subvert the widely used Linux kernel by inserting underhanded software
(this attack inserted code that used = instead of ==, an easily missed,
one-character difference).
This paper provides a brief initial look at
underhanded source code, with the intent to eventually help develop
countermeasures against it. &#8230;
<br><br>
This initial work suggests that countering underhanded code is not
an impossible task; it appears that a relatively small set of simple
countermeasures can significantly reduce the risk from underhanded
code. I recommend examining more samples, identifying a recommended
set of underhanded code countermeasures, and applying countermeasures
in situations where countering underhanded code is important and the
benefits exceed their costs.&#8221;
</blockquote>

<p>
In my experience there are usually ways to reduce security risks,
once you know about them.
This is another case in point; once you know that this is a potential
attack, there are a variety of ways to reduce their effectiveness.
I don&#8217;t think this is the last word at all on this topic, but
I hope it can be immediately applied and that others can build on it.
</p>

<p>
This was the last paper I wrote when I worked at IDA
(I now work at the Linux Foundation).
My thanks to IDA for releasing it!
My special thanks go to Margaret Myers, Torrance Gloss, and
Reginald N. Meeson, Jr., who all worked to make this paper possible.
</p>
<p>
So if you&#8217;re interested in the topic, you can view the
<a href="https://www.ida.org/research-and-publications/publications/all/i/in/initial-analysis-of-underhanded-source-code">Landing page for
IDA Document D-13166</a> or go directly to the
<a href="https://www.ida.org/-/media/feature/publications/i/in/initial-analysis-of-underhanded-source-code/d-13166.ashx"
>PDF for IDA DOcument D-13166,
&#8220;Initial Analysis of Underhanded Source Code&#8221;</a>.
(If that doesn&#8217;t work, use this
<a href="https://perma.cc/FVQ8-EKWA">Perma.cc link to paper D-13166</a>.)
Enjoy!
</p>
<p>path: <a href="https://www.dwheeler.com/blog/security">/security</a> | <a href="https://www.dwheeler.com/blog">Current Weblog</a> | <a href="https://www.dwheeler.com/blog/2020/05/15#underhanded-code">permanent link to this entry</a></p>
<h1>Tue, 07 Apr 2020</h1>
<p><a name="covid-19-attacks"><font size="+2"><b>COVID-19/Coronavirus and Computer Attacks</b></font></a></p><p></p>
<p>
Sadly, attackers have been exploiting the
COVID-19 pandemic (caused by Coronavirus SARS-CoV-2) to
cause problems via computers around the world.
<a href="https://www.modernhealthcare.com/cybersecurity/hackers-taking-advantage-covid-19-spread-malware">Modern Healthcare</a> notes that
hospitals are seeing active attacks,
emails where a sender (pretending to be from the
Centers for Disease Control and Prevention) asks the receiver
to open a link (which is actually malware),
other scams claim to track COVID-19 cases but actually steals personal
information.
<a href="https://threatpost.com/official-government-covid-19-apps-threats/154512/"
>Many official government COVID-19 mobile applications have
threats</a> (ranging from malware to incredibly basic security
problems).
For example, in Columbia the government released
a mobile app called CoronApp-Colombia to help people track potential
COVID-19 symptoms; the intention is great, but as of March 25 it failed
to use HTTPS (secure communication), and instead used HTTP
(insecure) to relay personal data (including health data).
</p>

<p>
In the long term, the solution is for software developers and operators
to do a much better job in creating and deploying secure applications.
In the short term,
<a href="https://hbr.org/2020/03/will-coronavirus-lead-to-more-cyber-attacks"
>we need to take extra care about our computer security</a>.
</p>
<p>path: <a href="https://www.dwheeler.com/blog/security">/security</a> | <a href="https://www.dwheeler.com/blog">Current Weblog</a> | <a href="https://www.dwheeler.com/blog/2020/04/07#covid-19-attacks">permanent link to this entry</a></p>
<h1>Tue, 17 Sep 2019</h1>
<p><a name="cwe-top-25-2019"><font size="+2"><b>CWE Top 25 for 2019</b></font></a></p><p></p>
<p>
In case you weren’t aware of it,
there is now a
<a href="https://cwe.mitre.org/top25/archive/2019/2019_cwe_top25.html"
>2019 version of the CWE Top 25 list</a>.
This list attempts to rank what are the most important kinds
of software vulnerabilities
(what they call &#8220;weaknesses&#8221;).
</p>

<p>
Their new approach is to directly use
the National Vulnerability Database (NVD) to score
various kinds of vulnerabilities.
There are a number of limitations with this approach,
and they discuss many of them in the cited page.
</p>

<p>
Their approach does have some oddnesses, for example,
their #1 worst problem
(CWE-119, Improper restriction of operations within the bounds of
a memory buffer) is itself the parent of items #5 (CWE-125, out-of-bounds read)
and #12 (CWE-787, out-of-bounds write).
</p>

<p>
Another oddity: they rank Cross-Site Request Forgery (CSRF) quite high (#9).
CSRF doesn&#8217;t even appear in the
<a href="https://www.owasp.org/index.php/Category:OWASP_Top_Ten_Project"
>2017 (latest) OWASP Top 10 list</a>, even though the
OWASP top 10 list focuses on websites (where CSRF can occur).
I think this happens because the CWE folks are
using a large dataset from 2017-2018, where there are still a large
number of CSRF vulnerabilities.
But the impact of those remaining vulnerabilities has been going down,
due to changes to frameworks, standards, and web browsers.
Most sites use a pre-existing frameworks, and frameworks have been
increasingly adding on-by-default CSRF countermeasures.
The &#8220;SameSite&#8221; cookie attribute that provides an easy countermeasure
against CSRF was implemented in most browsers around 2016-2018
(depending on the browser),
but having it take effect required that websites make changes,
and during that 2017-2018 timeframe websites
were only starting to deploy those changes.
As of late 2019 several browsers are in the
process of switching their SameSite defaults so that they
counter CSRF by default, <i>without</i> requiring sites to do anything.
(In particular, see the announcement for
<a href="https://www.chromestatus.com/feature/5088147346030592"
>Chrome</a>
and the change log for
<a href="https://bugzilla.mozilla.org/show_bug.cgi?id=1551798"
>Mozilla Firefox</a>.)
These changes to the SameSite defaults
implement the security improvements proposed in
<a href="https://tools.ietf.org/html/draft-west-cookie-incrementalism-00"
>Incrementally Better Cookies</a> by M. West in May 2019.
This change in the security default could not have been realistically done
before 2019 because of a
<a href="https://bugs.webkit.org/show_bug.cgi?id=198181">bug
in the Apple Safari browser that was only fixed in 2019</a>.
As more browsers self-protect against CSRF by default,
without requiring sites or developers to do anything,
CSRF vulnerabilities will become dramatically less likely.
This once again shows the power of defaults;
systems should be designed to be secure by default whenever possible,
because normally people simply accept the defaults.
</p>

<p>
That said, having a top 25 list based on quantitative analysis is probably
for the best long-term, and the results appear to be (mostly)
very reasonable.
I&#8217;m glad to see it!
</p>
<p>path: <a href="https://www.dwheeler.com/blog/security">/security</a> | <a href="https://www.dwheeler.com/blog">Current Weblog</a> | <a href="https://www.dwheeler.com/blog/2019/09/17#cwe-top-25-2019">permanent link to this entry</a></p>
<h1>Tue, 26 Mar 2019</h1>
<p><a name="assurance-case"><font size="+2"><b>Assurance cases</b></font></a></p><p></p>
<p>
No one thing creates secure software, so you
need to do a set of things to make adequately secure software.
But no one has infinite resources; how can you have confidence
that you are doing the right set?
Many experts (including me) have recommended creating an
<i>assurance case</i> to connect the various approaches together
to an efficient, cohesive whole.
It can be hard to start an assurance case, though, because
there are few public examples.
</p>

<p>
So I am pleased to report that you can now freely get my paper
<a href="https://www.ida.org/idamedia/Corporate/Files/Publications/IDA_Documents/ITSD/2019/P-9278.pdf"><i>A Sample Security Assurance Case Pattern</i> by David A. Wheeler, December 2018</a>.
This paper discusses how to create secure software by applying an
assurance case, and uses the
<a href="https://github.com/coreinfrastructure/best-practices-badge/blob/master/doc/security.md">Badge Application&#8217;s assurance case</a> as
an example.
If you are trying to create a secure application, I hope you
will find it useful.
</p>
<p>path: <a href="https://www.dwheeler.com/blog/security">/security</a> | <a href="https://www.dwheeler.com/blog">Current Weblog</a> | <a href="https://www.dwheeler.com/blog/2019/03/26#assurance-case">permanent link to this entry</a></p>
<h1>Thu, 16 Aug 2018</h1>
<p><a name="verified-voting"><font size="+2"><b>Verified voting still necessary, paperless voting still untrustworthy</b></font></a></p><p></p>
<p>
In
<a href="https://www.dwheeler.com/blog/2006/10/14/">2006 I wrote
&#8220;Direct Recording Electronic (DRE) Voting: Why Your Vote Doesn&#8217;t Matter&#8221;</a>.
Over a decade later, voting systems are <i>still</i> being used that
are fundamentally insecure - though things are <i>better</i> in some places.
</p>

<p>
First, the basics.
If a voting system uses anything other than voter-verified paper
to vote, then that voting system is
<b>not secure</b>.  Paper does not automatically make a voting system secure,
but a system that does not use voter-verified paper <i>cannot</i> be secure.
Verified voting using paper ballots is a <i>minimum</i>
requirement for a trustworthy voting system.
Direct recording equipment (DRE) and mobile phone voting systems
<i>cannot</i> be adequately secure for elections of government positions.
These insecure systems are simply invitations for vote tampering.
</p>

<p>
The article <a href="https://www.theguardian.com/us-news/2018/aug/13/us-election-cybersecurity-hacking-voting">&#8220;Why US elections remain &#8216;dangerously vulnerable&#8217; to cyber-attacks&#8221;</a>
discusses some of the reasons why many of the US voting systems are
fundamentally untrustworthy.
One quote:
&#8220;Georgia&#8217;s election officials continue to defend the state&#8217;s
electronic voting system that is demonstrably unreliable and insecure,
and have repeatedly refused to take administrative, regulatory or
legislative action to address the election security failures.&#8221;
Another quote:
&#8220;there is little mystery about the safest available voting technology -
optically scanned paper ballots, now used by about 80% of US voters.
Some of the states that don&#8217;t have this technology, like Louisiana,
would like it but don&#8217;t have the funds to switch. Others, like
Georgia and South Carolina, simply aren&#8217;t interested in ditching their
all-electronic systems despite the compelling reasons to do so.&#8221;
</p>

<p>
<a href="https://money.cnn.com/2018/08/06/technology/mobile-voting-west-virginia-voatz/index.html">&#8220;West Virginia to introduce mobile phone voting for midterm elections&#8221; by Donie O&#8217;Sullivan</a>
discusses West Virginia&#8217;s introduction of mobile phone voting.
Does this require a paper ballot?
No.
Therefore, West Virginia&#8217;s proposed
voting system is horrifically insecure,
and its results will be completely untrustworthy if implemented.
</p>

<p>
<a href="https://xkcd.com/2030/">XKCD&#8217;s &#8220;Voting Software&#8221;</a>
is a funny summary.
In short: experts on computer security agree that computers must not
be directly used for voting when there are important stakes
(such as a vote for a political office).
When experts say &#8220;you cannot adequately trust the systems we build&#8221;
you should believe the experts.
</p>

<p>
<a href="https://www.dwheeler.com/blog/2006/10/14/">As I noted earlier</a>,
&#8220;I used to do magic tricks, and all magic tricks work the same way -
misdirect the viewer, so that what they think they see is not the same
as reality. Many magic tricks depend on rigged props, where what you
see is NOT the whole story. DREs are the ultimate illusion - the naive
think they know what&#8217;s happening, but in fact they have no way to know
what&#8217;s really going on.&#8221;
</p>

<p>
I am sure that some election officials will bristle when told that
we cannot trust the legitimacy of their results.
Too bad.
If your election system uses
technology that is widely known to be easily subverted,
such as voting machines that do not use voter-verified paper ballots,
then your results <i>should</i> be viewed with deep suspicion.
Without voter-verified paper ballots
there is no way to independently verify vote counts, so there
is no <i>reason</i> to trust the results.
This is old information; those who have not replaced insecure systems
are those who have failed to act.
Some states certify or approve the use of voting machines without
voter-verified paper ballots, but that just shows that their certification
or approval processes fail to provide even a minimum level of security.
</p>

<p>
There is more to protecting the legitimacy of votes, of course.
For example, it is critical to ensure
that only eligible voters can vote, that voters can vote at most once,
and that paper votes cannot be added or removed.
But currently many districts are not doing the minimum necessary to have
trustworthy election results, and we need to get systems up to
minimal standards.
</p>

<p>
There is an old phrase:
“It’s not the people who vote that count.
It’s the people who count the votes.”
<a href="https://www.snopes.com/fact-check/stalin-vote-count-quote/">Stalin
did not say that exactly, but he did say something like it</a>.
The point is that if we do not adequately protect the
process of counting votes,
then the vote counts are vulnerable to manipulation.
</p>

<p>
The
<a href="https://www.verifiedvoting.org/voting-system-principles/">Voting
system principles from Verified Voting</a> provides a useful
starting list of requirements; there are other guides too.
Voting systems that fail to meet those principles are untrustworthy toys
that should not be used for real elections.
It is fine to use direct recording equipment, mobile phone voting,
or other insecure systems when you are voting for homecoming queen
or deciding where to go to lunch.
But it is time to stop using fundamentally flawed voting systems like these
for elections that matter.
</p>
<p>path: <a href="https://www.dwheeler.com/blog/security">/security</a> | <a href="https://www.dwheeler.com/blog">Current Weblog</a> | <a href="https://www.dwheeler.com/blog/2018/08/16#verified-voting">permanent link to this entry</a></p>
<h1>Tue, 24 Jul 2018</h1>
<p><a name="email-encryption"><font size="+2"><b>Email encryption is here! Use STARTTLS everywhere!</b></font></a></p><p></p>
<p>
Historically most email has been unencrypted, and that has a serious flaw:
unencrypted email can be read and modified by anyone between
the sender and final receiver.
Tools to do &#8220;end-to-end&#8221; encryption of email
(to prevent reading and/or modifying it)
have been available for decades,
but they are often hard to use by &#8220;normal&#8221; users.
</p>

<p>
Thankfully, there&#8217;s been work to significantly improve email security.
In particular, STARTTLS email encryption is now widely supported, and
<a href="https://www.starttls-everywhere.org/">the
Electronic Frontier Foundation &#8220;STARTTLS Everywhere&#8221; initiative</a>
is working to get <i>everyone</i> to support STARTTLS in their
email systems. Therefore:
</p>

<ul>
<li><b>Organizations: support STARTTLS if you don&#8217;t already!</b>
If you support STARTTLS, then
<a href="https://www.starttls-everywhere.org">check your STARTTLS configuration,
join the &#8220;STARTTLS Policy List&#8221;, and check that policy list before
sending an email</a>.
<li><b>Individuals:
<a href="https://www.starttls-everywhere.org">see if your organization
properly supports STARTTLS</a></b>. If they do not,
bug your organization to get that fixed.
</ul>

<p>
STARTTLS is not perfect, as I&#8217;ll discuss below.
My point is that it&#8217;s <i>way</i> more secure
than most email without it, because it improves security without
requiring end-users to do anything.
Below is additional information that I think you&#8217;ll find interesting.
</p>

<p>
First, here&#8217;s how STARTTLS works.
Email is transmitted by a series of &#8220;hops&#8221;;
if the hop recipient supports STARTTLS, email is
<i>automatically</i> encrypted on that hop as it
goes through the infrastructure, <i>without</i> requiring
email users to do anything special.
That ease-of-use is a <i>big deal</i> - users normally do whatever is the
default, so if the default is secure, then users will normally do
the secure thing.
</p>

<p>
<i>Lots</i> of organizations now support STARTTLS.
Google reports that by 2018-07-24
90% of its incoming email, and 90% of outgoing email, was
encrypted using STARTTLS
(&#8220;<a href="https://transparencyreport.google.com/safer-email/overview">Email encryption in transit</a>&#8221;).
Many email services support STARTTLS, including
<a href="https://www.starttls-everywhere.org/results/?gmail.com">Gmail</a>,
<a href="https://www.starttls-everywhere.org/results/?yahoo.com">Yahoo.com</a>,
<a href="https://www.starttls-everywhere.org/results/?outlook.com">Outlook.com</a>,
and
<a href="https://www.starttls-everywhere.org/results/?runbox.com">runbox.com</a>.
(This includes the
<a href="https://morningconsult.com/2017/06/21/poll-gmail-dominates-email-use/">top email services</a>.)
Many other organizations support STARTTLS, including
<a href="https://www.starttls-everywhere.org/results/?google.com">Google</a>,
<a href="https://www.starttls-everywhere.org/results/?microsoft.com">Microsoft</a>,
<a href="https://www.starttls-everywhere.org/results/?bankofamerica.com">Bank of America</a>,
<a href="https://www.starttls-everywhere.org/results/?redcross.org">The American Red Cross</a>,
<a href="https://www.starttls-everywhere.org/results/?salvationarmy.org">The Salvation Army</a>,
<a href="https://www.starttls-everywhere.org/results/?sei.cmu.edu">The Software Engineering Institute (SEI)</a>,
<a href="https://www.starttls-everywhere.org/results/?cmu.edu">Carnegie Mellon University (CMU)</a>,
and
<a href="https://starttls-everywhere.org/results/?berkeley.edu">University of California, Berkeley</a>.
I give this list to show that there are many different kinds of organizations
that support STARTTLS.
The <a href="https://starttls-everywhere.org/policy-list/">STARTTLS
Policy List</a> has an incomplete list of organizations
known to be supporting STARTTLS.
</p>

<p>
<a href="https://www.starttls-everywhere.org/">The
Electronic Frontier Foundation &#8220;STARTTLS Everywhere&#8221; initiative</a>
is an effort to get lagging organizations to support STARTTLS.
As I noted earlier,
you should use their tools to see if your organization properly supports
STARTTLS on its incoming emails, and if not, complain to get that fixed.
</p>

<p>
There are some historical problems that
the STARTTLS Everywhere project is working to fix:
</p>

<ul>
<li>Some people allow the absurdly old vulnerable protocols SSLv2 and SSLv3
when using STARTTLS, instead of using the newer TLS protocols.
TLS version 1.0 came out in January 1999, so there&#8217;s been plenty of
time to transition away from SSLv2 and SSLv3.
The STARTTLS Everywhere project expressly warns about that, and
won&#8217;t give credit unless you&#8217;re using TLS.
<li>Some sites with STARTTLS use invalid certificates.
STARTTLS Everywhere tells you about that, and also explains some solutions
so you can use valid certificates instead.
<li>Historically active attackers can downgrade email encryption by
stripping away the STARTTLS request, or can interpose nonsense certificates
so that they receive the encrypted email instead.
The STARTTLS Everywhere project
has created a &#8220;STARTTLS Policy List&#8221; where supporting organizations
can assert that they always use STARTTLS using a valid certificate.
Organizations should join that list and check that list before they send email.
Once they do, email transmissions cannot be downgraded and certificates
cannot replaced, because senders will know to only use STARTTLS with valid
certificates for those organizations.
</ul>

<p>
STARTTLS is <i>not</i> an end-to-end encryption system.
STARTTLS only encrypts while the email is being sent between systems (&#8220;hops&#8221;).
That&#8217;s not all bad.
For example, it means that receiving organizations can continue to
examine the emails to check for viruses/malware, counter spam, and so on.
But of course, there are downsides.
</p>

<p>
STARTTLS is, in general, <i>not</i> as strong as an end-to-end
encryption system (from the point-of-view of providing confidentiality
and integrity).
For example, receiving organizations (and anyone who subverts their
email system) can see and modify the email.
Users who do not trust their email service providers should <i>not</i>
depend on STARTTLS; they <i>must</i> use end-to-end encryption.
In general, end-to-end encryption is stronger,
so we should still work to make end-to-end email encryption easier
to use and deploy.
But for various reasons it&#8217;s hard to deploy end-to-end email encryption,
and we&#8217;ve spent decades trying.
Also, STARTTLS works just fine <i>with</i> end-to-end encryption.
</p>

<p>
Please indulge me: I think a small rant is appropriate here.
There are some security specialists who think that only the perfect is
acceptable.
Nonsense! Requiring perfection is crazy.
I think it is important, when creating and maintaining systems, to have
an <i>engineering</i> mindset.
In particular, you must always remember that
that choices have trade-offs.
It is not possible to have no risk; an asteroid might land
on your head tomorrow.
It is not reasonable to demand that systems be used regardless of their
difficulty or expense; we all have limited time and money.
Security issues are real, and we <i>do</i> need to address them,
but time, money, and ease-of-use also matter greatly.
</p>

<p>
Unlike most other systems,
STARTTLS is completely automatic (end-users don&#8217;t have to do anything)
once it is set up, it is not hard to set up,
and it counters a large class of attacks.
For almost all users, email encryption with STARTTLS
is a major improvement over what they had before.
Let&#8217;s keep working to deploy even better systems,
but let&#8217;s take partial victories where we can get them.
</p>
<p>path: <a href="https://www.dwheeler.com/blog/security">/security</a> | <a href="https://www.dwheeler.com/blog">Current Weblog</a> | <a href="https://www.dwheeler.com/blog/2018/07/24#email-encryption">permanent link to this entry</a></p>
<h1>Sat, 23 Sep 2017</h1>
<p><a name="who-decides-update"><font size="+2"><b>Who decides when you need to update vulnerable software? (Equifax)</b></font></a></p><p></p>
<p>
I have a trick question:
Who decides when you <i>need</i> to update vulnerable software
(presuming that if it&#8217;s unpatched it might lead to bad consequences)?
In a company, is that the information technology (IT) department?
The chief information officer (CIO)?
A policy?
The user of the computer?
At home, is it the user of the computer?
Perhaps the family&#8217;s &#8220;tech support&#8221; person?
</p>
<p>
Remember, it&#8217;s a trick question.  What&#8217;s the answer?
The answer is&#8230;
</p>
<p>
<b>The attacker decides.</b>
</p>
<p>
The <i>attacker</i> is the person who decides when you get attacked, and how.
Not the computer user.  Not a document.  Not support. Not an executive.
<i>The attacker decides.</i>
And that means <i>the attacker decides when you need to update
your vulnerable software</i>.
If that statement makes you uncomfortable,
then you need to change your thinking.
This is reality.
</p>
<p>
So let&#8217;s look at Equifax, and see what we can learn from it.
</p>
<p>
Let&#8217;s start with the first revelation in 2017:
<a href="https://arstechnica.com/information-technology/2017/09/massive-equifax-breach-caused-by-failure-to-patch-two-month-old-bug/">A security vulnerability
in Apache Struts (a widely-used software component)
was fixed in March 2017, but Equifax failed to update it for two whole months,
leading to the loss of
sensitive information on about 143 million US consumers</a>.
The update was available for free, for two months, and it was well-known
that attackers were <i>exploiting</i> this vulnerability in other
organizations.
Can we excuse Equifax?
Is it &#8220;too hard&#8221; to update vulnerable software (aka &#8220;patch&#8221;) in a timely way?
Is it acceptable that organizations fail to update vulnerable components
when those vulnerabilities allow unauthorized
access to lots of sensitive high-value data?
</p>
<p>
<i>Nonsense.</i>
Equifax may <i>choose</i> to fail to update known vulnerable components.
Clearly it did so!
But Equifax <i>needed</i> to update rapidly, because <i>the need to update
was decided by the attackers</i>, not by Equifax.
In fact, two months is an absurdly long time, because again, the timeframe
is determined by the <i>attacker</i>.
</p>
<p>
Now it&#8217;s true that if you don&#8217;t plan to rapidly update, it&#8217;s
hard to update.
Too bad.  Nobody cares.
Vulnerabilities are routinely found in software components, and have been
for decades.
Since it is 100% predictable that there will be vulnerabilities found in
the software you use (including third-party software components you reuse),
you need to <i>plan ahead</i>.
I don&#8217;t know when it will rain, but I know it will, so
I plan ahead by paying for a roof and buying umbrellas.
When something is certain to happen, you need to plan for it.
For example, make sure you rapidly learn about vulnerabilities in
third party software you depend on,
and that you have a process in place (with tools and
automated testing) so that you can update and ship in minutes, not months.
Days, not decades.
</p>
<p>
The
<a href="https://blogs.apache.org/foundation/entry/apache-struts-statement-on-equifax">Apache Struts Statement on Equifax Security Breach</a>
has some great points about how to properly handle
reused software components (no matter where it&#8217;s from).
The Apache Struts team notes that you should
(1) understand the software you use,
(2) establish a rapid update process,
(3) remember that all complex software has flaws,
(4) establish security layers, and
(5) establish monitoring.
Their statement has more details, in particular for #2 they say,
&#8220;establish a process to quickly roll out a security fix release&#8230;
[when reused software] needs to be updated for security reasons. Best
is to think in terms of hours or a few days, not weeks or months.&#8221;
</p>
<p>
Many militaries refer to the
<a href="https://en.wikipedia.org/wiki/OODA_loop">&#8220;OODA loop&#8221;, which is the
decision cycle of observe, orient, decide, and act</a>.
The idea was developed by military strategist and
United States Air Force Colonel John Boyd.
Boyd noted that, &#8220;In order to win, we should operate at a faster tempo
or rhythm than our adversaries&#8230;&#8221;.
Of course, if you want to lose, then you simply need to operate more
slowly than your adversary.
You need to get comfortable with this adversarial terminology, because
if you&#8217;re running a computer system today, you <i>are</i> in an
adversarial situation, and the attackers are your adversaries.
</p>
<p>
In short, you <i>must</i> update your software when vulnerabilities
are found <i>before</i> attackers can exploit them (if they can be exploited).
If you can&#8217;t do that, then you need to change how you manage your software
so <i>can</i> do that.
Again, <b>the attacker decides how fast you need to react</b>.
</p>
<p>
We&#8217;re only beginnning to learn about the Equifax disaster of 2017, but
it&#8217;s clear that Equifax &#8220;security&#8221; is just one failure after another.
The more we learn, the worse it gets.
Here are some of the information we have so far.
Equifax used the rediculous pair
<a href="https://www.cnbc.com/2017/09/14/equifax-used-admin-for-the-login-and-password-of-a-non-us-database.html">Username &#8220;admin&#8221;, password
&#8220;admin&#8221;</a> for a database with personal employee information.
<a href="https://www.grc.com/sn/SN-628-Notes.pdf">Security Now! #628</a>
showed that Equifax recommended using Netscape Navigator in their
website discussion on security, a rediculously obsolete suggestion
(Netscape shut down in 2003, 14 years ago).
Equifax provided customers with PINs that were simply the date and time,
making the PINs predictable and thus insecure.
Equifax set up a &#8220;checker&#8221; site
<a href="https://techcrunch.com/2017/09/08/psa-no-matter-what-you-write-equifax-may-tell-you-youve-been-impacted-by-the-hack/">which makes false statements</a>:
&#8220;In what is an unconscionable move by the credit report company, the
checker site, hosted by Equifax product TrustID, seems to be telling
people at random they may have been affected by the data breach&#8230;
It&#8217;s clear Equifax&#8217;s goal isn&#8217;t to protect the consumer or
bring them vital information. It&#8217;s to get you to sign up for its
revenue-generating product TrustID&#8230;  [and] TrustID&#8217;s Terms of Service [say]
that anyone signing up for the product is barred from suing the company after.&#8221;
<a href="http://www.zdnet.com/article/equifax-freeze-your-account-site-is-also-vulnerable-to-hacking/">Equifax&#8217;s credit report monitoring site was
found to be vulnerable to hacking</a>
(specifically, an XSS vulnerability that was quickly found by others).
Equifax failed to use its own domain name for all its sites (as is standard),
making it easy for others to spoof them.
Indeed, NPR reported that that
<a href="http://www.npr.org/sections/thetwo-way/2017/09/21/552681357/after-massive-data-breach-equifax-directed-customers-to-fake-site">&#8221;After Massive Data Breach, Equifax Directed Customers To Fake Site&#8221;</a>.
There are now suggestions that there were
<a href="https://arstechnica.com/information-technology/2017/09/massive-equifax-hack-reportedly-started-4-months-before-it-was-detected/">break-ins even
earlier</a> which Equifax never detected.
In short: The more we learn, the worse it gets.
</p>

<p>
Most obviously, Equifax failed
to responsibly update a known vulnerable component
in a timely way.
Updating software doesn&#8217;t matter when there&#8217;s no valuable
information, but in this case
extremely sensitive personal data was involved.
This was especially sensitive data, Equifax was using a component version
with a publicly-known vulnerability, and it was known that attackers were
exploiting that vulnerability.
It was completely foreseeable that attackers would use this vulnerable
component to extract sensitive data.
In short, Equifax had a duty of care
that they failed to perform.
Sometimes attackers perform an unprecedented kind of sneaky attack,
and get around a host of prudent defenses;
that would be different.
But there is no excuse for failing to promptly respond
when you <i>know</i> that a component is vulnerable.
That is negligence.
</p>
<p>
But how can you quickly update software components?
Does this require magic?  Not at all, it just requires accepting that
this <i>will</i> happen and so you <i>must</i> be ready.
This is not an unpredictable event; I may not know exactly <i>when</i>
it will happen, but I can be certain that it <i>will</i> happen.
Once you accept that it will happen, you can easily get ready for it.
There are tools that can help you monitor when your components publicly
report a vulnerability or security update, so that you quickly find out
when you have a problem.
Package managers let you rapidly download, review, and update a component.
You need to have an automated checking system that
uses a variety of static tools, automated test suites,
and other dynamic tools so that you can be confident that the system
(with updated component) works correctly.
You need to be confident that you can ship to production immediately
with acceptable risk after
you&#8217;ve updated your component and run your automated checking system.
If you&#8217;re not confident, then your checking system is unacceptable and
needs to be fixed.
You also need to quickly ship that to production (and this must be
automated), because again,
you have to address vulnerabilities <i>faster</i> than the attacker.
</p>
<p>
Of course, your risks go down much further if you think about security
the whole time you&#8217;re developing software.
For example, you can design your system so that a defect is
(1) less likely to lead to a system vulnerability or
(2) has less of an impact.
When you do that, then a component vulnerability will often not
lead to a system vulnerability anyway.
A single vulnerability in a front-end component should not
have allowed such a disastrous outcome in the first place, since this
was especially sensitive data, so the Equifax
design also appears to have been negligent.
They also failed to detect the problem for a long time; you should be
monitoring high-value systems, to help reduce the impact of a vulnerability.
The failure to notice this is also hard to justify.
Developing secure software is quite possible, and you don&#8217;t need to
break the bank to do it.
It&#8217;s impossible in the real world to be perfect,
but it&#8217;s very possible to be <i>adequately</i> secure.
</p>
<p>
Sadly, very few software developers know how to develop secure software.
So I&#8217;ve created a video that&#8217;s on YouTube that should help:
<a href="https://www.youtube.com/watch?v=5a5D4d6hcEY">&#8220;How to Develop Secure Applications: The BadgeApp Example&#8221; (by David A. Wheeler)</a>.
This walks through a real-world program (BadgeApp)
as an example, to show approaches
for developing far more secure software.
If you&#8217;re involved in software development in any way, I encourage you
to take a look at that video.
Your software will almost certainly look different, but if you think
about security throughout development, the results will almost certainly
be much better.
Perfection is impossible, but you can <i>manage</i> your risks, that is,
reduce the probability and impact of attacks.
There are a wide variety of countermeasures that can often prevent attacks,
and they work well when combined with monitoring and response mechanisms
for the relatively few attacks that get through.
</p>
<p>
The contrast between Equifax and BadgeApp is stark.
Full disclosure: I am the technical lead of the BadgeApp project&#8230; but
it is clear we did a better job than Equifax.
Earlier this week a vulnerability was announced
in one of the components (nokogiri) that is used by the BadgeApp.
This vulnerability was announced on
<a href="https://github.com/rubysec/ruby-advisory-db">ruby-advisory-db</a>,
a database of vulnerable Ruby gems (software library components)
used to report to users about component vulnerabilities.
Within two hours of that announcement the BadgeApp
project had downloaded the security update, run the BadgeApp
application through a variety of tools and its automated test suite
(with 100% statement coverage) to make sure everything was okay,
and pushed the fixed version to the production site.
The BadgeApp application is a simpler program, sure, but it also manages much
less sensitive data than Equifax&#8217;s systems.
We should expect Equifax to do <i>at least</i> as well, because they
handle much more sensitive data.
Instead,
Equifax failed to update reused components with known vulnerabilities in
a timely fashion.
</p>
<p>
Remember, <b>the attacker decides</b>.
</p>
<p>
The attacker decides how fast you need to react, what you need to defend
against, and what you need to counter.
More generally, the attacker decides how much you need to do to
counter attacks.
You do not get to decide what the attacker will choose to do.
But you <i>can</i> plan ahead to make your software secure.
</p>
<p>path: <a href="https://www.dwheeler.com/blog/security">/security</a> | <a href="https://www.dwheeler.com/blog">Current Weblog</a> | <a href="https://www.dwheeler.com/blog/2017/09/23#who-decides-update">permanent link to this entry</a></p>
<h1>Tue, 25 Oct 2016</h1>
<p><a name="law-security"><font size="+2"><b>Creating Laws for Computer Security</b></font></a></p><p></p>
<p>
In 2016 the website <a href="https://krebsonsecurity.com">KrebsonSecurity</a>
was taken down by a large
distributed denial-of-service (DDoS) attack.
More recently, many large sites became inaccessible
due to a massive DDoS attack
(see, e.g., <a href="http://www.nytimes.com/2016/10/22/business/internet-problems-attack.html">&#8220;Hackers Used New Weapons to Disrupt Major Websites Across U.S.&#8221; by Nicole Perlroth, Oct. 21, 2016, NY Times</a>).
</p>

<p>
Sadly,
<a href="https://twitter.com/drdavidawheeler/status/705438314162094080">the &#8220;Internet of Things&#8221; is really the
&#8220;Internet of painfully insecure things&#8221;</a>.
This is fundamentally an externalities problem (the buyers and sellers are not
actually bearing the full cost of the exchange), and in these cases
mechanisms like law and regulation are often used.
</p>

<p>
So, what laws or regulations should be created to improve computer security?
Are there any?
Obviously there are risks to creating laws and regulations.
These need to be targeted at countering widespread problems, without
interfering with experimentation, without hindering free expression or
the development of open source software, and so on.
It&#8217;s easy to create bad laws and regulations - but
I believe it is <i>possible</i> to create good laws and regulations
that will help.
</p>

<p>
My article
<a href="http://www.dwheeler.com/essays/law-security.html">Creating Laws for Computer Security</a>
lists some potential items that could be turned into laws that
I think could help computer security.
No doubt some could be improved, and there are probably things I&#8217;ve
missed.
But I think it&#8217;s important that people start discussing how to
create narrowly-tailored laws that counter the more serious problems
without causing too many negative side-effects.
Enjoy!
</p>
<p>path: <a href="https://www.dwheeler.com/blog/security">/security</a> | <a href="https://www.dwheeler.com/blog">Current Weblog</a> | <a href="https://www.dwheeler.com/blog/2016/10/25#law-security">permanent link to this entry</a></p>
<h1>Mon, 01 Feb 2016</h1>
<p><a name="address-sanitizer-gentoo"><font size="+2"><b>Address Sanitizer on an entire Linux distribution!</b></font></a></p><p></p>
<p>
Big news in computer security: Hanno Boeck has recently managed to get
<a href="https://blog.hboeck.de/archives/879-Safer-use-of-C-code-running-Gentoo-with-Address-Sanitizer.html">Address Sanitizer running on an
entire Linux distribution (Gentoo)</a> as an experimental edition.
For those who don&#8217;t know,
<a href="https://en.wikipedia.org/wiki/AddressSanitizer">Address Sanitizer</a>
is an amazing compile-time option that detects a huge range of memory errors
in memory-unsafe languages (in particular C and C++).
These kinds of errors often lead to disastrous security vulnerabilities,
such as
<a href="http://www.dwheeler.com/essays/heartbleed">Heartbleed</a>.
</p>
<p>
This kind of distribution option is absolutely not for everyone.
Address Sanitizer on average increases processing time by about 73%,
and memory usage by 340%.
What&#8217;s more, this work is currently very experimental,
and you have to disable some other security mechanisms to make it work.
That said, this effort has already borne a lot of valuable fruit.
Turning on these mechanisms across
an entire Linux distribution has revealed a large number of memory errors
that are getting fixed.
I can easily imagine this being directly useful in the future, too.
Computers are very fast and have lots of memory, even when compared
to computers of just a few years earlier.
There are definitely situations where it&#8217;s okay to effectively
halve performance and reduce <i>useful</i> memory, and in exchange,
significantly increase the system&#8217;s resistance to novel attack.
My congrats!!
</p>
<p>path: <a href="https://www.dwheeler.com/blog/security">/security</a> | <a href="https://www.dwheeler.com/blog">Current Weblog</a> | <a href="https://www.dwheeler.com/blog/2016/02/01#address-sanitizer-gentoo">permanent link to this entry</a></p>
<h1>Mon, 23 Nov 2015</h1>
<p><a name="ransomware-medical"><font size="+2"><b>Ransomware coming to medical devices?</b></font></a></p><p></p>
<p>
<a href="http://motherboard.vice.com/read/ransomware-is-coming-to-medical-devices">Forrester Research has an interesting
cybersecurity prediction for 2016: We’ll see ransomware for a medical device or wearable</a>.
</p>

<p>
This is, unfortunately, plausible.
I don&#8217;t know if it will happen in 2016, but it&#8217;s pretty reasonable.
Indeed, I can see threats.. even if we can&#8217;t be sure that
the ransomware is even installed.
</p>

<p>
After all,
<a href="http://www.popsci.com/article/gadgets/how-dick-cheney-took-his-heart-offline-thwart-hackers">Dick Cheney had his pacemaker’s Wifi disabled because of this concern</a>
(see also
<a href="http://www.ibtimes.com/dick-cheney-deactivates-pacemaker-wi-fi-former-vp-felt-threatened-hackers-wanted-avoid-homeland">here</a>).
<a href="http://www.telegraph.co.uk/news/science/science-news/11212777/Terrorists-could-hack-pacemakers-like-in-Homeland-say-security-experts.html">People have already noted that terrorists might use this, since medical devices are often poorly secured</a>.
The additional observation is that may be a better way to (criminally)
make money.  We already have ransomware, including organizations who are
getting better at extorting with it. Traditional ransomware is foiled
by good backups; in this case backups won’t help, and victims will
(understandably) be willing to pay much, much more.  And I think that
medical devices are actually a softer target.
</p>

<p>
With luck, this won&#8217;t come true in 2016.
The question is, is that because it doesn&#8217;t show up until 2017 or 2018&#8230;
or because the first ones were in 2015?
<a href="https://gcn.com/articles/2015/11/18/medical-device-cybersecurity.aspx">DHS is funding work in this area</a>, and that&#8217;s good&#8230;
but while research can help, the real problem is that we have
too many software developers who do not have a clue how to develop
secure software&#8230; and too many people (software developers or not)
who think that&#8217;s acceptable.
</p>

<p>
In short,
we still have way too many people building safety-critical devices who
don&#8217;t understand that security is necessary for safety.
I hope that this changes - and quickly.
</p>
<p>path: <a href="https://www.dwheeler.com/blog/security">/security</a> | <a href="https://www.dwheeler.com/blog">Current Weblog</a> | <a href="https://www.dwheeler.com/blog/2015/11/23#ransomware-medical">permanent link to this entry</a></p>
<h1>Tue, 07 Apr 2015</h1>
<p><a name="heartbleed-afl-asan"><font size="+2"><b>Heartbleed found with american fuzzy lop (afl) and Address Sanitizer (ASan)</b></font></a></p><p></p>
<p>
Big news in security vulnerability research:
<a href="https://blog.hboeck.de/archives/868-How-Heartbleed-couldve-been-found.html">Hanno Böck found Heartbleed using american fuzzy lop (afl)
and Address Sanitizer (ASan)</a> - and in only 6 hours of execution time.
</p>
<p>
This means that software developers should seriously consider using
a more-advanced fuzzer, such as
<a href="http://lcamtuf.coredump.cx/afl/">american fuzzy lop (afl)</a>,
along with
<a href="https://code.google.com/p/address-sanitizer/">Address Sanitizer (ASan)</a>
(an option in both the LLVM/clang and gcc compilers),
whenever you write in C, C++, Objective-C, or in other
circumstances that are not memory-safe.
In particular, seriously consider doing
this if your program is exposed to the internet or
it processes data sent via the internet
(practically all programs meet this criteria nowadays).
I had speculated that this combination could have found Heartbleed in
<a href="/essays/heartbleed.html">my essay on Heartbleed</a>,
but this confirmation is really important.
Here I will summarize what&#8217;s going on
(using the capitalization conventions
of the various tool developers).
</p>
<p>
The
<a href="http://lcamtuf.coredump.cx/afl/">american fuzzy lop (afl)</a>
program created by Michal Zalewski is a surprisingly effective fuzzer.
A fuzzer is simply a tool that sends lots of semi-random inputs into a program
and to detect gross problems (typically a crash).
Fuzzers do not know what the exact correct answers are,
but because they do not, they can try out more inputs than systems that
know the exact correct answers.
But afl is smarter than most fuzzers; instead of just sending random inputs,
afl tracks which branches are taken in a program.
Even more interestingly, afl even tracks how often different branches
are taken when running a program (that is <i>especially</i> unusual).
Then, when afl creates new inputs, it prefers to create them
based on inputs that have produced different counts on at least some branches.
This evolutionary approach, using both branch coverage and the
<i>number of times</i> a branch is used, is remarkably effective.
Simple dumb random fuzzers can only perform relatively shallow tests;
getting any depth has required more complex approaches such as
detailed descriptions of the
required format (the approach used by
so-called &#8220;smart&#8221; fuzzers) and/or
white-box constraint solving (such as
<a href="http://esec-lab.sogeti.com/pages/Fuzzgrind">fuzzgrind</a>
or Microsoft&#8217;s SAGE).
It&#8217;s not at all clear that afl eliminates the value of these other
fuzzing approaches; I can see combining their approaches.
However, afl is clearly getting far better results than
simple dumb fuzzers that just send random values.
Indeed, the afl of today is getting remarkably deep coverage for a fuzzer.
For example, the post
<a href="http://lcamtuf.blogspot.com/2014/11/pulling-jpegs-out-of-thin-air.html">Pulling JPEGs out of thin air</a> shows how afl was able to
start with only the text &#8220;hello&#8221; (a hideously bad starting point)
and still automatically figure out how to create valid JPEG files.
</p>
<p>
However, while afl is really good at creating inputs, it can only
detect problems if they lead to a crash; vulnerabilities like
Heartbleed do not normally cause a crash.
That&#8217;s where Address Sanitizer (ASan) comes in.
Address Sanitizer turns many memory access errors, including nearly
all out-of-bounds accesses, double-free, and use-after-free, into
a crash.
ASan was originally created by
Konstantin Serebryany, Derek Bruening, Alexander Potapenko, and Dmitry Vyukov.
ASan is amazing all by itself, and the combination is even better.
The fuzzer afl is good at creating inputs, and ASan is good
at turning problems into something that afl can detect.
Both are available at no cost as
Free/ libre/ open source software (FLOSS),
so anyone can try them out, see how they work, and even make improvements.
</p>
<p>
Normally afl can only fuzz file inputs, but Heartbleed could only be
triggered by network access.
This is no big deal; Hanno describes in his article how to wrap up
network programs so they can be fuzzed by file fuzzers.
</p>
<p>
Sometimes afl and ASan do not work well together today on 64-bit systems.
This has to do with some technical limitations involving memory use;
on 64-bit systems ASan reserves (but does not use) a lot of memory.
This is not necessarily a killer;
in many cases you can use them together anyway (as Hanno did).
More importantly, this problem is about to go away.
Recently I co-authored (along with Sam Hakim) a tool we call
afl-limit-memory; it uses Linux cgroups to eliminate the problem so
that you can always combine afl and ASan (at least on Linux).
We have already submitted the code to the afl project leader,
and we hope it will become part of afl soon.
So this is already a disappearing problem.
</p>
<p>
There are lots of interesting related resources.
If you want to learn about fuzzing more generally,
some books
you might want to read are
<a href="http://www.amazon.com/gp/product/0321446119/ref=as_li_tl?ie=UTF8&camp=1789&creative=9325&creativeASIN=0321446119&linkCode=as2&tag=davawhesperho-20&linkId=FXMNBVAG5BVBKXQF"><i>Fuzzing: Brute Force Vulnerability Discovery</i> by Sutton, Greene, and Amini</a><img src="http://ir-na.amazon-adsystem.com/e/ir?t=davawhesperho-20&l=as2&o=1&a=0321446119" width="1" height="1" border="0" alt="" style="border:none !important; margin:0px !important;" />
and
<a href="http://www.amazon.com/gp/product/1596932147/ref=as_li_tl?ie=UTF8&camp=1789&creative=9325&creativeASIN=1596932147&linkCode=as2&tag=davawhesperho-20&linkId=ZWWDLP2TYGCZUDLU"><i>Fuzzing for Software Security Testing and Quality Assurance</i> (Artech House Information Security and Privacy) by Takanen, DeMott, and Miller</a><img src="http://ir-na.amazon-adsystem.com/e/ir?t=davawhesperho-20&l=as2&o=1&a=1596932147" width="1" height="1" border="0" alt="" style="border:none !important; margin:0px !important;" />.
My
<a href="http://www.dwheeler.com/secure-class/">class materials for
secure software design and programming, #9 (analysis tools)</a>,
also cover fuzzing (and are freely available).
<a href="https://fuzzing-project.org/">The Fuzzing Project</a>
led by Hanno is an effort to encourate the use of fuzzing to
improving the state of free software security, and includes some tutorials
on how to do it.
The paper
<a href="http://research.google.com/pubs/pub37752.html">AddressSanitizer: A Fast Address Sanity Checker</a>
is an excellent explanation of how ASan works.
My essay
<a href="/essays/heartbleed.html">How to Prevent the next Heartbleed</a>
discusses many different approaches that would, or would not,
have detected Heartbleed.
</p>
<p>
I do not think that fuzzers (or any dynamic technique) completely
replace static analysis approaches such as source code weakness analyzers.
Various tools, including dynamic tools like fuzzers and static tools
like source code weakness analyzers,
are valuable complements for finding vulnerabilities
before the attackers do.
</p>
<p>path: <a href="https://www.dwheeler.com/blog/security">/security</a> | <a href="https://www.dwheeler.com/blog">Current Weblog</a> | <a href="https://www.dwheeler.com/blog/2015/04/07#heartbleed-afl-asan">permanent link to this entry</a></p>
<h1>Sat, 04 Apr 2015</h1>
<p><a name="security-presentations-updates"><font size="+2"><b>Security presentation updates</b></font></a></p><p></p>
<p>
I&#8217;ve updated my
<a href="/secure-class/">presentations on how to design and
implement secure software</a>.
In particular, I&#8217;ve added much about analysis tools and
formal methods.
There is a lot going on in those fields, and no matter what I do
I am only scratching the surface.
On the other hand, if you have not been following these closely,
then there&#8217;s a lot you probably don&#8217;t know about.
Enjoy!
</p>
<p>path: <a href="https://www.dwheeler.com/blog/security">/security</a> | <a href="https://www.dwheeler.com/blog">Current Weblog</a> | <a href="https://www.dwheeler.com/blog/2015/04/04#security-presentations-updates">permanent link to this entry</a></p>
<h1>Sat, 14 Feb 2015</h1>
<p><a name="learning-from-disaster"><font size="+2"><b>Learning from Disaster</b></font></a></p><p></p>
<p>
<a href="http://www.dwheeler.com/essays/learning-from-disaster.html">Learning from Disaster</a> is a collection of essays
that examines computer security disasters,
and what we can learn from those disasters.
This includes
<a href="http://www.dwheeler.com/essays/heartbleed.html">Heartbleed</a>,
<a href="http://www.dwheeler.com/essays/shellshock.html">Shellshock</a>,
<a href="http://www.dwheeler.com/essays/poodle-sslv3.html">POODLE</a>,
the <a href="http://www.dwheeler.com/essays/apple-goto-fail.html">Apple goto fail</a>,
and <a href="http://www.dwheeler.com/essays/sony-lax.html">Sony Pictures</a>.
If you&#8217;re interested in computer security I think you&#8217;ll
find this collection interesting.
</p>
<p>
So: please enjoy
<a href="http://www.dwheeler.com/essays/learning-from-disaster.html">Learning from Disaster</a>.
</p>
<p>path: <a href="https://www.dwheeler.com/blog/security">/security</a> | <a href="https://www.dwheeler.com/blog">Current Weblog</a> | <a href="https://www.dwheeler.com/blog/2015/02/14#learning-from-disaster">permanent link to this entry</a></p>
<h1>Tue, 06 Jan 2015</h1>
<p><a name="cloud-security"><font size="+2"><b>Cloud security</b></font></a></p><p></p>
<p>
There seems to be a lot of confusion about security fundamentals
of cloud computing (and other utility-based approaches).
For example, many people erroneously think hardware virtualization is required
for clouds (it is not), or that hardware virtualization and containerization
are the same (they are not).
</p>
<p>
My essay
<a href="/essays/cloud-security-virtualization-containers.html">Cloud Security: Virtualization, Containers, and Related Issues</a>
is my effort to counteract some of this confusion.
It has a quick introduction to clouds,
a contrast of various security isolation mechanisms
used to implement them, and a discussion of some related issues.
</p>
<p>
So please check out (and enjoy)
<a href="/essays/cloud-security-virtualization-containers.html">Cloud Security: Virtualization, Containers, and Related Issues</a>.
</p>
<p>path: <a href="https://www.dwheeler.com/blog/security">/security</a> | <a href="https://www.dwheeler.com/blog">Current Weblog</a> | <a href="https://www.dwheeler.com/blog/2015/01/06#cloud-security">permanent link to this entry</a></p>
<h1>Wed, 31 Dec 2014</h1>
<p><a name="learn-2014"><font size="+2"><b>I hope we learn from the computer security problems of 2014</b></font></a></p><p></p>
<p>
As 2014 draws to a close, I hope anyone involved with computers
will resolve to <i>learn</i> from the legion of security
problems of 2014.
</p>
<p>
We had way too many serious vulnerabilities in widely-used software
revealed in 2014.
In each case, there are lessons that people <i>could</i> learn from them.
Please take a look at the lessons that can be learned from
<a href="/essays/heartbleed.html">Heartbleed</a>,
<a href="/essays/shellshock.html">Shellshock</a>,
the
<a href="/essays/poodle-sslv3.html">POODLE attack on SSLv3</a>, and the
<a href="/essays/apple-goto-fail.html">Apple goto fail vulnerability</a>.
More generally,
<a href="http://www.dwheeler.com/secure-programs/">a lot of information
is available on how develop secure software</a> -
even though most software developers still do not know how to develop
secure software.
Simiarly, there are a host of lessons that organizations could learn from
<a href="/essays/sony-lax.html">Sony Pictures</a>.
</p>
<p>
<i>Will</i> people actually learn anything?
Georg Wilhelm Friedrich Hegel reportedly said that,
&#8220;We learn from history that we do not learn from history&#8221;.
</p>
<p>
Yet I think there are reasons to hope.
There are a lot of efforts to improve the security of
Free/Libre/Open Source Software (FLOSS) that are important yet
inadequately secure.
The
<a href="http://www.linuxfoundation.org/programs/core-infrastructure-initiative">Linux Foundation (LF) Core Infrastructure Initiative (CII)</a>
was established to &#8220;fund open source projects that are in the critical
path for core computing functions&#8221; to improve their security.
<a href="https://fsfe.org/news/2014/news-20141219-01.en.html">most recent European Union (EU) budget includes €1 million for auditing free-software programs</a> to identify and fix vulnerabilities.
The <a href="https://host-project.org/">US DHS HOST project</a> is also
working to improve security using open source software (OSS).
The
<a href="https://www.google.com/about/appsecurity/patch-rewards/">Google Application Security Patch Reward Program</a> is also working to improve security.
And to be fair, these problems were found by people who were examining
the software or protocols so that the problems could be fixed - exactly
what you <i>want</i> to happen.
At an organizational level, I think Sony was unusually lax in its
security posture.
I am already seeing evidence that other organizations have suddenly
become much more serious about security, now that they see what has been
done to Sony Pictures.
In short, they are finally starting to see that
security problems are not theoretical; they are real.
</p>
<p>
Here&#8217;s hoping that 2015 will be known as the year where people took
computer security more seriously, and as a result, software and our
computer systems became much harder to attack.
If that happens, that would make 2015 an awesome year.
</p>
<p>path: <a href="https://www.dwheeler.com/blog/security">/security</a> | <a href="https://www.dwheeler.com/blog">Current Weblog</a> | <a href="https://www.dwheeler.com/blog/2014/12/31#learn-2014">permanent link to this entry</a></p>
</body></html>