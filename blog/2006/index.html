<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8"></meta>
<link rel="alternate" type="application/rss+xml" title="RSS" href="http://www.dwheeler.com/blog/index.rss"></link>
<title>David A. Wheeler's Blog   </title>
<meta name="viewport" content="width=device-width, initial-scale=1.0">
</head>
<body>
<h1 style="margin-left: auto; margin-right: auto; width: 50%;">David A. Wheeler's Blog</h1><p>  </p><p></p><h1>Wed, 27 Dec 2006</h1>
<p><a name="commercial-floss"><font size="+2"><b>&#8220;Commercial&#8221; is not the opposite of Free-Libre / Open Source Software (FLOSS)</b></font></a></p><p></p>

<p>
Announcing
<a href="http://www.dwheeler.com/essays/commercial-floss.html">
&#8220;Commercial&#8221; is not the opposite of Free-Libre / Open Source Software (FLOSS)
</a> &#8212; a new and hopefully useful essay!
</p>

<p>
Why this new essay?
When I talk with with other people about
<a href="http://www.dwheeler.com/oss_fs_why.html">
Commercial Free-Libre / Open Source Software (FLOSS)</a>,
I still hear a lot of people mistakenly use the term
&#8220;commercial software&#8221; as if it had the opposite meaning of
&#8220;open source software&#8221;, Free-Libre Software, OSS/FS, or FLOSS.
That&#8217;s in spite of (1) the rise in commercial support for FLOSS,
(2) official definitions of &#8220;commercial item&#8221; that <i>include</i> FLOSS, and
(3) FLOSS licenses and projects clearly approving commercial support.
</p>

<p>
This confusion &#8212; that FLOSS and commercial software are opposites &#8212;
is a dreadful mistake.
Speakers who differentiate between FLOSS and commercial products,
as if they were opposites, are simply unable to
understand what is happening in the software industry.
And if you cannot understand something, you cannot make good decisions
about it.
</p>

<p>
If you wish to understand the 21st century (and beyond),
you need to understand the basics
of what controls software&#8230; because software controls everything else.
</p>

<p>
So, this essay
<a href="http://www.dwheeler.com/essays/commercial-floss.html">
&#8220;Commercial&#8221; is not the opposite of Free-Libre / Open Source Software (FLOSS)
</a>
explains why it&#8217;s so important to understand that the word
&#8220;commercial&#8221; is not the opposite of FLOSS,
and then gives examples to justify the claim.
</p>

<p>
Enjoy!
</p>
<p>path: <a href="http://www.dwheeler.com/blog/oss">/oss</a> | <a href="http://www.dwheeler.com/blog">Current Weblog</a> | <a href="http://www.dwheeler.com/blog/2006/12/27#commercial-floss">permanent link to this entry</a></p>
<h1>Sat, 14 Oct 2006</h1>
<p><a name="voting-subverted"><font size="+2"><b>Direct Recording Electronic (DRE) Voting: Why Your Vote Doesn&#8217;t Matter</b></font></a></p><p></p>
<p>
Direct Recording Electronic (DRE) voting machines have been installed
in many locations across the United States.
In a DRE machine, votes are only recorded electronically &#8212;
there&#8217;s no paper that voters check.
DREs can be rigged to forge any election, fairly easily, so
DREs are completely inappropriate for use in any serious election.
In fact, I suspect that vote-rigging has <i>already</i> occurred with DREs,
and there is <i>no way to prove otherwise</i>.
</p>
<p>
In September 2006,
<a href="http://itpolicy.princeton.edu/voting/">
Feldman, Halderman, and Felten posted
&#8220;Security Analysis of the Diebold AccuVote-TS Voting Machine&#8221;</a>,
showing how trivial it was to completely control a common DRE voting machine.
It turned to be trivial to write programs to do vote-stealing.
The manufacturer&#8217;s reply didn&#8217;t really address the issue at all.
<a
href="http://www.wijvertrouwenstemcomputersniet.nl/images/9/91/Es3b-en.pdf">
A report on the Nedap/Groenendaal ES3B voting computer</a> found that
anyone given brief access to a different voting machine can gain
complete and virtually undetectable control over
election results - and how radio emanations from
an unmodified ES3B can tell who voted what from
several meters away.
</p>
<p>
On the Secure Coding mailing list (SC-L), Jeremy Epstein noted that
election officials&#8217; responses was &#8220;amusing and scary&#8221;;
when shown that DREs could be trivially subverted, instead of forbidding
the use of DREs, they ignored the problem and
asked why the researchers didn&#8217;t attack real systems.
That&#8217;s a foolish question - anyone who <i>really</i> wanted to control
an election would just do it and <i>not</i> tell anyone.
The manufacturer of that system claims that all the problems reported
by the researchers have been &#8216;fixed&#8217;.
I&#8217;m willing to believe that some <i>elections</i> were fixed, but
if there&#8217;s no voter-verifiable paper trail, the machines are not
appropriate for real elections.
Since they lack a voter-verifiable paper trail,
<i>no DRE can be trusted</i>. Period.
</p>
<p>
I used to do magic tricks, and all magic tricks work the same way - misdirect
the viewer, so that what they think they see is not the same as reality.
Many magic tricks depend on rigged props, where what you see is NOT the
whole story. DREs are the ultimate illusion - the naive
<i>think</i> they know what&#8217;s happening, but in fact they have no
way to know what&#8217;s really going on. There&#8217;s no way to even <i>see</i> the
trap door under the box, as it were&#8230; DREs are a great prop for the
illusion. Printing &#8220;zero&#8221; totals and other stuff looks just like
a magic show to me - it has lots of pizazz, and it distracts the
viewer from the fact that they have <i>no</i> idea what&#8217;s really going on.
</p>
<p>
I&#8217;m of the opinion that elections using DREs have
<i>already</i> been manipulated.
No, I can&#8217;t prove that an election <i>has</i> been manipulated, and I certainly
can&#8217;t point to a specific manufacturer or election. And I sincerely
hope that no elections have been manipulated. But there&#8217;s a <i>lot</i> of money
riding on big elections, and a small fraction of that would be enough
to tempt someone to do it. And many people <i>strongly</i> believe in their
cause/party, and might manipulate an election on the grounds that it&#8217;s
for the &#8220;greater good&#8221; - it need not be about money at all.
</p>
<p>
It&#8217;s crazy to assume that absolutely no one&#8217;s subverted a DRE
in an election, when it&#8217;s so easy
and the systems are <i>known</i> to be weak. The whole problem
is that DRE designs make it essentially impossible to detect
massive fraud, almost impossible to find the perpetrator even if
you detected it, and allow a single person to control an entire election
(so there&#8217;s little risk of a &#8220;squeeler&#8221; as there is with other
techiques to subvert elections).
And if an unethical person knows they won&#8217;t be caught,
it <i>increases</i> the probability of them doing it.
Anyone who thinks that all candidates and parties are too honest to
do this needs to discover the newspaper and history books. Ballot-stuffing
is at least as ancient as ancient Greece, and as modern as Right Now.
</p>
<p>
These voting systems and their surrounding processes
would not meet the criteria for an electronic one-armed bandit
in Las Vegas. Yet there&#8217;s more at stake.
Many people have motives for subverting elections - DREs provide
the method and opportunity.
The state commissions cannot provide any justifiable evidence
that votes are protected from compromise if they use DREs.
And that is their job.
</p>
<p>
For more information about the problems with DREs, see
<a href="http://www.verifiedvoting.org/article.php?id=5018">
Frequently Asked Questions about DRE Voting Systems</a>.
Another interesting article is
<a href="http://www.schneier.com/blog/archives/2004/11/the_problem_wit.html">
Bruce Schneier&#8217;s &#8220;The Problem with Electronic Voting Machines&#8221;</a>
</p>

<p>
There&#8217;s a solution, and that&#8217;s verified voting - see the
<a href="http://www.verifiedvoting.org/">verified voting site</a>.
The Verified Voting Foundation advocates
the use of voter-verified paper ballots (VVPBs) for all elections
(so voters can inspect individual permanent records of their ballots
before they are cast and so meaningful recounts may be conducted),
insists that electronic voting equipment and software be open
to public scrutiny, and that random, surprise recounts
be conducted on a regular basis to audit election equipment.
I would add at least three things:
(1) there must be separate voting stations and ballot readers,
where the ballot reader totals are the only official votes
(this prevents a collusion by the voting station), and
(2) there should a standard paper ballot format; this makes it possible
to have independent recounts using equipment from different manufacturers, as
well as making it possible to mix-and-match vendor equipment
(lowering costs for everyone);
(3) there should a standard electronic formats for
defining elections and producing results, again to make it
possible to dramatically reduce costs by enabling mixing and
matching of equipment.
I also think having 100% of the source code of these systems publicly
available for inspection is important - the public must depend on these
systems, so the public should be able to know what they are depending on.
<a href="http://www.openvotingconsortium.org/">The Open Voting Consortium</a>
(OVC) is a non-profit organization dedicated to the development, maintenance,
and delivery of open voting systems for use in public elections.
OVC is developing a reference version of free voting software
to run on very inexpensive PC hardware, which produces
voter-verifiable paper ballots.
</p>

<p>
I hope that election officials will see the light, and quickly
replace DREs with voting systems that could actually be trusted.
If not, I think we&#8217;re headed for election disputes that will make the
year 2000 disputes look like like a picnic.
If election officials don&#8217;t get rid of DREs, sooner or later we will
have an election where one candidate wins even though all the polls
will say he/she lost&#8230; and then the courts will find out that
they&#8217;re untrustworthy and do not permit any kind of real audit or recount.
</p>

<p>
DREs are unfit for use in any elections that matter. They should
be decommissioned with prejudice, and frankly,
I&#8217;d like to see laws requiring vendors to take them back and give
their purchasers a refund, or add voter-verified paper systems
acceptable to the customer at no charge.
(As I noted earlier, the paper needs to meet some standard too,
so that you can use counting
machines from different manufacturers to prevent collusion.)
At no time was this DRE technology appropriate
for use in voting, and the companies selling them would have known better
had they done any examination of their real requirements.
The voters were given a lemon, and they should have the right to get
their money back.
</p>
<p>path: <a href="http://www.dwheeler.com/blog/security">/security</a> | <a href="http://www.dwheeler.com/blog">Current Weblog</a> | <a href="http://www.dwheeler.com/blog/2006/10/14#voting-subverted">permanent link to this entry</a></p>
<h1>Fri, 01 Sep 2006</h1>
<p><a name="planet-problems"><font size="+2"><b>Planet definition problems, and Pluto too</b></font></a></p><p></p>
<p>
Well, at the last minute
International Astronomical Union (IAU) changed their
proposed definition of the term &#8220;planet&#8221;, and voted on it.
Pluto is no longer a planet!
</p>

<p>
Well, maybe.
</p>

<p>
Usually definitions like this go through a lot of analysis;
I think this one was rushed through at the last minute.
I see three problems: It was an irregular vote, it&#8217;s vague as written,
and it doesn&#8217;t handle faraway planets well.
Let&#8217;s look at each issue in turn.
</p>

<p>
This was a pretty irregular vote, I think.
As I noted, at the last minute the proposal changed, with no time
for deeply examining it.
There were
<a href="http://news.bbc.co.uk/2/hi/science/nature/5283956.stm">2,700
attendees, but only 424 astronomers (about 10%)
voted on the proposal that &#8220;demoted&#8221; Pluto</a>.
And only a few days after the vote,
<a href="http://www.livescience.com/blogs/2006/08/31/300-astronomers-wont-use-new-planet-definition/">300 astronomers have signed a petition saying
they do not accept this IAU definition</a> - almost as many as
voted in the first place.
That doesn&#8217;t sound like consensus to me.
</p>

<p>
More importantly, it&#8217;s too vague.
That&#8217;s not just my opinion;
<a href="http://www.space.com/scienceastronomy/060831_planet_definition.html">
Space.com notes that there&#8217;s a lot of uncertainty about it</a>.
Now a planet has to control its zone&#8230; but Earth doesn&#8217;t, there
are <i>lots</i> of objects that cross Earth&#8217;s orbit.
Does this mean that Earth is not a planet?
I haven&#8217;t seen any published commentary on it, but I think there&#8217;s even
a more obvious problem - Neptune is clearly <i>not</i> a planet, because
it hasn&#8217;t cleared out Pluto and Charon.
A definition which is that vague is not an improvement.
</p>


<p>
But in my mind, the worst problem with this definition is a practical one:
it doesn&#8217;t handle other planets around other stars well.
We are too far away to observe small objects around other stars, and I think
we will always be able to detect larger objects but not smaller ones
in many faraway orbits.
So when we detect an object in another galaxy with the mass of Jupiter,
and it&#8217;s orbiting a star, is it a planet?
Well, under this current definition we don&#8217;t know if it&#8217;s a planet
or not.
Why? Because we may not be able to know what else is there in orbit.
And <i>that</i> is a real problem.
I think it&#8217;s clear that we will always be able to observe some larger objects
without being able to detect the presence of smaller ones.
If we can&#8217;t use the obvious word, then the definition is useless - so
we need a better definition instead.
</p>

<p>
I thought the previous proposal (orbits a star, enough mass to become round)
was a good one, as I noted earlier in
<a href="http://www.dwheeler.com/blog/2006/08/21/#planet">
What&#8217;s a planet? Why I&#8217;m glad there&#8217;s an argument</a>.
I think they should return to that previous definition,
<i>or</i> find some other definition
that is (1) much more precise and (2) lets us use the term &#8220;planet&#8221;
in a sensible way to discuss large non-stars that are
orbiting faraway stars.
Whether Pluto is in, or not.
Of course, none of this affects reality; this is merely a definition war.
But clear terminology is important in any science.
</p>

<p>
I still think that what&#8217;s great
about this debate is that it has caused many people to discuss and think about
what&#8217;s happening in the larger universe, instead of focusing on the transient.
That is probably the most positive result of all.
</p>
<p>path: <a href="http://www.dwheeler.com/blog/misc">/misc</a> | <a href="http://www.dwheeler.com/blog">Current Weblog</a> | <a href="http://www.dwheeler.com/blog/2006/09/01#planet-problems">permanent link to this entry</a></p>
<p><a name="gpl-bsd"><font size="+2"><b>GPL, BSD, and NetBSD - why the GPL rocketed Linux to success</b></font></a></p><p></p>
<p>
Charles M. Hannum (one of the 4 originators of NetBSD)
has posted a sad article about
serious problems in the NetBSD project, saying
&#8220;the NetBSD Project has stagnated to the point of irrelevance.&#8221;
You can see the
<a href="http://article.gmane.org/gmane.os.netbsd.general/17246">article</a>
or an
<a href="http://lwn.net/Articles/197748/">LWN article about it</a>.
</p>

<p>
There are still active FreeBSD and OpenBSD communities, and there&#8217;s
much positive to say about FreeBSD and OpenBSD.
I use them occasionally, and
I always welcome a chance to talk to their developers - they&#8217;re sharp folks.
Perhaps NetBSD will partly revive.
But systems based on the Linux kernel (&#8220;Linux&#8221;)
absolutely stomp the *BSDs (FreeBSD, OpenBSD, and NetBSD) in market share.
And Linux-based systems will continue to stomp on the *BSDs
into the foreseeable future.
</p>

<p>
I think there is one primary reason Linux-based systems
completely dominate the *BSDs&#8217; market share - Linux uses the protective
GPL license, and the *BSDs use the permissive (&#8220;BSD-style&#8221;) licenses.
The BSD license has been a lot of trouble for all the *BSDs, even though
they keep protesting that it&#8217;s good for them.
But look what happens.
Every few years, for many years, someone has said,
&#8220;Let&#8217;s start a company based on this BSD code!&#8221;
BSD/OS in particular comes to mind, but Sun (SunOS)
and others have done the same.
They pull the *BSD code in, and some of the best BSD developers,
and write a proprietary derivative. But as a proprietary vendor, their
fork becomes expensive to self-maintain, and eventually the company founders
or loses interest in that codebase
(BSD/OS is gone; Sun switched to Solaris).
All that company work is then lost forever, and good developers
were sucked away during that period. Repeat, repeat, repeat.
That&#8217;s enough by itself to explain why the BSDs
don&#8217;t maintain the pace of Linux kernel development.
But wait - it gets worse.
</p>

<p>
In contrast, the GPL has enforced a consortia-like arrangement
on any major commercial companies that want to use it.
Red Hat, Novell, IBM, and many others are all contributing as a result, and
they feel safe in doing so because the others are legally required
to do the same.
Just look at the domain names on the Linux kernel mailing list - big companies,
actively paying for people to contribute.
<a href="http://gcn.com/vol1_no1/daily-updates/26641-1.html">
In July 2004, Andrew Morton addressed a forum held by U.S. Senators,</a>
and reported that most Linux kernel code was generated by
corporate programmers (37,000 of the last 38,000 changes were
contributed by those paid by companies to do so;
see <a href="http://www.dwheeler.com/oss_fs_why.html#starving_programmers">
my report on OSS/FS numbers</a> for more information).
BSD license advocates claim that the BSD is more &#8220;business friendly&#8221;, but
if you look at actual practice, that argument doesn&#8217;t wash.
The GPL has created a &#8220;safe&#8221; zone of cooperation among companies,
without anyone
having to sign complicated legal documents. A company can&#8217;t feel safe
contributing code to the BSDs, because its competitors might simply copy
the code without reciprocating. There&#8217;s much more corporate cooperation in the
GPL&#8217;ed kernel code than with the BSD&#8217;d kernel code. Which means that in
practice, it&#8217;s actually been the GPL that&#8217;s most &#8220;business-friendly&#8221;.
</p>

<p>
So while the BSDs have lost energy every time a company gets involved,
the GPL&#8217;ed programs gain every time a company gets involved.
And that explains it all.
</p>

<p>
That&#8217;s not the only issue, of course. Linus Torvalds makes mistakes,
but in general he&#8217;s a good leader; leadership
issues are clearly an issue for some of the BSDs.
And Linux&#8217;s ability early on
to support dual-boot computers turned out to be critical years ago. Some people
worried about the legal threats that the BSDs were under early on,
though I don&#8217;t think it had that strong an effect.
But the early Linux kernel had a number of problems
(nonstandard threads, its early network stack was terrible, etc.), which
makes it harder to argue that it was &#8220;better&#8221; at first. And the
Linux kernel came AFTER the *BSDs - the BSDs had a head start,
and a lot of really smart people.
Yet the Linux kernel, and operating systems based on it,
jumped quickly past all of them. I believe that&#8217;s in large
part because Linux didn&#8217;t suffer the endless draining of people and effort
caused by the BSD license.
</p>

<p>
Clearly, some really excellent projects can work well on BSD-style
licenses; witness Apache, for example. It would be a mistake to think
that BSD licenses are &#8220;bad&#8221; licenses, or that the GPL is always the
&#8220;best&#8221; license. But others, like Linux, gcc, etc., have done better
with copylefting / &#8220;protective&#8221; licenses. And some projects, like Wine,
have switched to a protective (copylefting)
license to stem the tide of loss from the project.
Again, it&#8217;s not as simple as &#8220;BSD license bad&#8221; - I don&#8217;t think
we fully understand exactly when each license&#8217;s effects truly have the
most effect. But clearly the license matters; this as close to an experiment
in competing licenses as you&#8217;re likely to get.
</p>

<p>
Obviously, a license choice should depend on your goals. But let&#8217;s look
more carefully at that statement, maybe we can see what type of
license tends to be better for different purposes.
</p>

<p>
If your goal is to get an idea or approach widely used to the largest
possible extent, a permissive license like the BSD (or MIT) license has
much to offer. Anyone can quickly snap up the code and use it. Much of
the TCP/IP code (at least for tools) in Windows was originally from BSD,
I believe; there are even some copyright statements still in it. BSD
code <i>is</i> widely used, and even when it isn&#8217;t used (the Linux
kernel developers wrote their own TCP/IP code) it is certainly studied.
But don&#8217;t expect the public BSD-licensed code to be maintained by
those with a commercial interest in it.
I haven&#8217;t noticed a large number of Microsoft developers
being paid to improve any of the *BSDs, even though they share the same
code ancestries in some cases.
</p>

<p>
If your goal is to have a useful program that stays useful long-term,
then a protective (&#8220;copylefting&#8221;)
license like the LGPL or GPL licenses has much to offer.
Protective licenses force the cooperation that is good for everyone in the long
term, if a long-term useful project is the goal. For example, I&#8217;ve noticed
that GPL projects are far less likely to fork than BSD-licensed projects;
the GPL completely eliminates any financial advantage to forking.
The power of the GPL license
is so strong that even if you choose to not use a copylefting license,
<a href="http://www.dwheeler.com/essays/gpl-compatible.html">
it is critically important that an open source software project use
a GPL-compatible license</a>.
</p>

<p>
Yes, companies could voluntarily cooperate without a license forcing them to.
The *BSDs try to depend on this. But it today&#8217;s cutthroat market,
that&#8217;s more like the &#8220;Prisoner&#8217;s Dilemma&#8221;. In the dilemma, it&#8217;s better
to cooperate; but since the other guy might choose to not cooperate, and
exploit your naivete, you may choose to not cooperate.
A way out of this dilemma is to create a situation where you <i>must</i>
cooperate, and the GPL does that.
</p>

<p>
Again, I don&#8217;t think license selection is all that simple when developing
a free-libre/open source software (FLOSS) program. Obviously the Apache
web server does well with its BSD-ish license. But packages like Linux,
gcc, Samba, and so on all show that the GPL does work.
And more interestingly, they show that a lot of
competing companies can cooperate, when the license requires them to.
</p>
<p>path: <a href="http://www.dwheeler.com/blog/oss">/oss</a> | <a href="http://www.dwheeler.com/blog">Current Weblog</a> | <a href="http://www.dwheeler.com/blog/2006/09/01#gpl-bsd">permanent link to this entry</a></p>
<h1>Mon, 21 Aug 2006</h1>
<p><a name="planet"><font size="+2"><b>What&#8217;s a planet? Why I&#8217;m glad there&#8217;s an argument</b></font></a></p><p></p>
<p>
The International Astronomical Union (IAU) has a proposed definition
of the term &#8220;planet&#8221; that is currently being vigorously debated.
I like the definition, and I&#8217;m even more delighted that there&#8217;s a
vigorous discussion about the proposed definition.
</p>

<p>
We&#8217;ve used the term &#8220;planet&#8221; for thousands of years, without trouble.
But now that we&#8217;re learning
more about the heavens, we now know about many objects orbiting other stars,
and about many more objects orbiting our own Sun.
As a result, our simple intuitions aren&#8217;t enough.
Obviously objects will orbit other objects no matter what we call them,
but since we want humans to be able to communicate with each other, it&#8217;s
very important that we have definitions
that help rather than hinder communication.
</p>

<p>
The latest proposed definition is actually very sensible.
Basically (and I&#8217;m paraphrasing here),
it defines a planet as an object that (1) orbits a star, and
(2) has enough mass that its gravity can make itself &#8220;round&#8221;.
The real definition has some clever nuances that make this workable.
For example, Saturn rotates so fast that it bulges, but since it has enough
mass to make it round, it&#8217;s clearly a planet by this definition.
Also, if objects orbit each other and their center of gravity isn&#8217;t inside
any of them, then they&#8217;re all planets - so both Pluto and Charon
become planets under this definition.
The object &#8220;2003 UB313&#8221; (unofficially called Xena) would
be recognized as a planet as well, as would Ceres.
</p>

<p>
I think this definition is very sensible, because it&#8217;s based on observable
basic physical properties, which are at least somewhat less arbitrary.
One previous proposal was &#8220;orbits a star and is at least as big as Mercury&#8221; -
which makes the &#8220;Pluto isn&#8217;t a planet&#8221; group happy, but is incredibly
arbitrary.
Another approach, which I happened to prefer before this proposal surfaced,
is &#8220;orbits a star and is at least as big as Pluto&#8221; -
which makes the &#8220;Pluto is a planet&#8221; group (like me) happy,
and causes fewer changes to textbooks, but it&#8217;s still really arbitrary.
All such definitions are a little arbitrary, but this new proposed
definition is at least somewhat <i>less</i> arbitrary - this definition
emphasizes a fundamental physical characteristic of the object.
Namely, it has enough gravity to force a change in its own shape.
</p>

<p>
Some astronomers have complained that this proposed
definition doesn&#8217;t account
for &#8220;how the planets were formed&#8221;; I think this is <i>nonsense</i>.
Humans weren&#8217;t around to watch the planets form; our current theories
about planet formation may be grossly mistaken.
In fact, I think it&#8217;s almost certain we&#8217;re wrong, because
we can&#8217;t observe much about planets of other stars to
verify or debunk our current theories.
And what&#8217;s worse, since we can&#8217;t really observe much about objects orbiting
other stars, we can&#8217;t really know if they&#8217;re planets based on some
&#8220;formation&#8221; definition - because we can&#8217;t get enough data to figure out
their ancient history.
And here&#8217;s a funny thought experiment - someday we may be able to
create planets ourselves.  Yes, that&#8217;s not exactly likely to be soon,
but it&#8217;s a great thought experiment.
If <i>we</i> create a planet, is it a planet? It should be.
Any definition like &#8220;planet&#8221; should be based on its obvious observable
properties, not on best guesses (likely wrong!)
about formation events of long ago.
A definition that uses only unimpeachable data is far more useful,
and when observing farway objects we get very little information.
</p>

<p>
No one wants to claim that every pebble orbiting a star is a planet &#8212;
our intuition says that there&#8217;s something fundamentally &#8220;big&#8221; about
a planet that makes it a planet.
I asked an 11-year-old what made a planet, a planet.
Her first answer, before hearing about this argument, was &#8220;it&#8217;s round&#8221; (!).
While that&#8217;s not a scientific survey, it does suggest that the creators of
this definition really are on to something - kids can intuitively
understand (and even guess at!) this definition.
</p>

<p>
Are there weird things about this definition?  Sure!
Charon becomes a planet too, as I noted above.
Since our own moon is slowly moving away, in a few billion years
our moon might become a planet (assuming the Earth and Moon don&#8217;t
get destroyed by the Sun first).
I guess you could &#8220;fix&#8221; the definition for those two cases
by saying that the &#8220;most massive&#8221; object of a group
was the planet, but I don&#8217;t see the need for this &#8220;fix&#8221;.
In fact, you get an interesting insight if your definition forces you to
note that their centroid isn&#8217;t inside any of them.
The object Ceres, now considered an unusually large asteriod, becomes a planet.
That&#8217;s okay, Ceres was originally considered a planet when it
was discovered - it even has a planetary symbol.
It&#8217;ll make people rewrite the textbooks, but we&#8217;ve learned so much
recently that they need rewriting anyway.
It&#8217;ll make it easy to see which books are obsolete - they&#8217;re the ones
which say we have only 9 planets.
</p>

<p>
What&#8217;s really great about this debate is that it&#8217;s
making people think about the heavens.
Any definition of &#8220;planet&#8221; is in some ways arbitrary, frankly.
I think this essential arbitrariness
makes such definitions the hardest things to agree on in science, because
there&#8217;s no way that more observations can prove or disprove a theory.
This definition is much less arbitrary than others people
have come up with, because it focuses on an important &#8220;change in state&#8221;
of the object.
And that&#8217;s a pretty good reason to endorse this definition.
</p>

<p>
In any case,
this debate has caused many people to discuss and think about
what&#8217;s happening in the larger universe, instead of focusing on the transient.
And that is probably the most positive result of all.
</p>
<p>path: <a href="http://www.dwheeler.com/blog/misc">/misc</a> | <a href="http://www.dwheeler.com/blog">Current Weblog</a> | <a href="http://www.dwheeler.com/blog/2006/08/21#planet">permanent link to this entry</a></p>
<h1>Tue, 18 Jul 2006</h1>
<p><a name="sloccount-ohloh"><font size="+2"><b>Ohloh, SLOCCount, and evaluating Free-libre / Open Source Software (FLOSS)</b></font></a></p><p></p>
<p>
A new start-up company named <a href="http://www.ohloh.net/">Ohloh</a>
has recently appeared, and is mentioned in many articles such as those from 
<a href="http://arstechnica.com/journals/microsoft.ars/2006/7/14/4655">
Ars Technica</a>,
<a href="http://www.eweek.com/article2/0,1895,1988547,00.asp">
eWeek</a>, and
<a href="http://news.zdnet.com/2100-3513_22-6094497.html">ZDNet</a>.
This start-up will do analysis
of free-libre / open source software (FLOSS) projects for customers,
as well as do analysis of in-house proprietary software.
To do the analysis, they&#8217;ll use a suite of FLOSS tools.
Some articles suggest that they&#8217;ll also help customers determine
which FLOSS programs best fit their needs,
by basing their recommendations on this analysis (at least in part).
<i>Exactly</i> what this start-up will do
is hard to figure out from the news reports.
That&#8217;s understandable&#8230; this is a start-up, and no doubt the
start-up itself will adjust what it does
depending on who shows up as customers.
But the general niche they&#8217;re trying to fill seems clear enough.
</p>

<p>
This seems like a very reasonable business idea.
Indeed, companies like IBM, Sun, and HP have been helping customers
select programs and develop systems for a long time, and they
often recommend and help transition customers to FLOSS programs.
IBM has said they invested over a billion dollars in Linux, and have
already recouped it, so IBM shows that this can be a lucrative business model.
Ohloh will have some stiff competition if they want to muscle into the
job of giving recommendations, but I love to see competition; hooray!
And I have stated for years that I&#8217;d love to see more analysis of FLOSS
programs, so I&#8217;m happy that it&#8217;s happening.
</p>

<p>
I am amused, though.
Some of the articles about this start-up seem to suggest
that this kind of data was impossible to get before.
Yet after a quick web search, you&#8217;ll discover that a lot of what they discuss
is already here on my website, and has been for years.
The article in Ars Technica
says Ohloh intends to &#8220;analyze open-source software projects and provide
customers with detailed information about them, including how
much it would cost to duplicate the project given an
average programmer salary of US$55,000 per year.
The Linux kernel, for example, clocked in at nearly 4.7 million lines
of code, has had 1,434 man-years of coding effort put in so far,
and would have cost approximately US$79 million in salaries.&#8221;
I&#8217;m all for analyzing code, but to do <i>that</i> kind of analysis,
you can just run my program
<a href="http://www.dwheeler.com/sloccount">SLOCCount</a>.
You can even see my papers discussing the results of
<a href="http://www.dwheeler.com/essays/linux-kernel-cost.html">
SLOCCount applied to the Linux kernel</a>, or my
<a href="http://www.dwheeler.com/sloc/">More than a Gigabuck</a> paper
(which analyzed a whole Linux distribution).
In fact, I&#8217;ll bet that they <i>are</i> using SLOCCount as one
of their tools, even though it appears to be uncredited (shame on you!).
After all, they state that they&#8217;re using FLOSS programs
to do their analysis, and some of the reports they&#8217;re generating include
exactly the kind of data that only SLOCCount provides.
This is <i>not</i> a license problem - I&#8217;m glad they&#8217;ve found it useful!
And to be fair, they&#8217;re not just using SLOCCount; they&#8217;re also
gathering other data such as check-in rates, and they&#8217;re grouping
the results as a nice prepackaged web page.
There are other tools that do some similar analysis of project activity,
of course, such as <a href="http://cia.navi.cx/doc">CIA</a>.
Still, I think making information like this
more accessible is really valuable&#8230; so good show!
</p>

<p>
If they plan to go beyond number-generating, and move into the
business of giving specific advice, you could do the same thing yourself.
Just go read my paper,
<a href="http://www.dwheeler.com/oss_fs_eval.html">How to Evaluate
Open Source Software / Free Software Programs</a>.
That paper outlines steps to take, and the kind of information to look for.
Doing things well is much harder than just following some process,
of course, but at least you&#8217;ll have an idea of what should be done.
</p>

<p>
So it turns out that the basic tools, and basic approaches,
for doing this kind of analysis and making recommendations already exist.
Is that a problem for this start-up?  Not really.
Not every company has the skills, knowledge, or time to do these kinds
of analyses.
It&#8217;s quite reasonable to hire someone who specializes in gathering
a particular kind of knowledge or doing a particular kind of analysis&#8230;
people who work in one area tend to get good at it.
I don&#8217;t know how well they&#8217;ll do, and execution <i>always</i> matters,
but their idea seems reasonable enough.
</p>

<p>
Frankly, what&#8217;s more interesting to me is who&#8217;s starting the company -
it&#8217;s basically lots of former Microsoft folks!
It&#8217;s headed by two former Microsoft executives:
Scott Collison (former director of platform strategy at Microsoft)
and Jason Allen (a former development manager for XML Web Services).
Investors include
Paul Maritz (who was a member of the Microsoft executive committee
and manager of the overall Microsoft company from 1986 to 2000)
and Pradeep Singh (who spent nine years at Microsoft in
various management positions).
Years ago,
<a href="http://www.catb.org/~esr/halloween/">Microsoft&#8217;s Halloween
documents</a> revealed their deep concern about FLOSS, and how they
were going to try to bury it.
Hmm, it doesn&#8217;t seem to be very buried.
I&#8217;ve no idea where this will end up, but it sure is interesting.
</p>
<p>path: <a href="http://www.dwheeler.com/blog/oss">/oss</a> | <a href="http://www.dwheeler.com/blog">Current Weblog</a> | <a href="http://www.dwheeler.com/blog/2006/07/18#sloccount-ohloh">permanent link to this entry</a></p>
<h1>Sat, 24 Jun 2006</h1>
<p><a name="wisdom-crowds"><font size="+2"><b>The Wisdom of Crowds and Free-Libre / Open Source Software</b></font></a></p><p></p>
<p>
I just came across an interesting
<a href="http://leshatton.org/A17.html">
short essay by Dr. Les Hatton titled &#8220;Open source inevitably good&#8221;</a>;
it appears it was published in the July 2005 <i>IT week</i>.
He has some intriguing conclusions about free-libre / open source software
(FLOSS).
</p>
<p>
But first, a little about
<a href="http://leshatton.org/">Dr. Hatton</a>, to show that he
is no lightweight.
Dr. Hatton holds the Chair in Forensic Software Engineering at the
University of Kingston, UK,
he is a fellow of the British Computer Society,
and was voted in &#8220;World&#8217;s leading Scholars of
Systems and Software Engineering (1993-2002)&#8221;
by U.S. Journal of Systems and Software.
His
<a href="http://leshatton.org/index_CS.html">
work in computer science</a> has primarily been in the field
of software failure, especially the design and execution of experiments
to determine the cause and reduce the likelihood
of failure in software systems.
He&#8217;s particularly known for his work on
<a href="http://leshatton.org/index_SA.html">safer language subsets</a>,
such as &#8220;Safer C&#8221;.
One paper of his I especially like is
<a href="http://leshatton.org/ISOC_subset1103.html">
&#8220;EC&#8212;, a measurement based safer subset of ISO C suitable
for embedded system development&#8221;</a> - in this, he <i>measures</i>
the common mistakes made in C by professional developers, and then
proposes simple rules to reduce their likelihood (if you write software
in C, it&#8217;s definitely worth reading).
In any case, here is someone who understands software development,
and in particular has carefully studied why software fails
and how to prevent such failures in the future.
</p>
<p>
In his essay
<a href="http://leshatton.org/A17.html">&#8220;Open source inevitably good&#8221;</a>,
Hatton starts by first examining James Surowiecki&#8217;s interesting book
&#8220;The Wisdom of Crowds: Why the Many are smarter than the Few&#8221;.
It turns out that crowds working together regularly beat the experts;
there&#8217;s both good evidence for this (with legions of examples),
and good mathematical underpinnings justifying this too.
For this to happen, two simple conditions must be met:
they must all have <i>some</i> knowledge, and they must
act effectively independently.
</p>
<p>
He notes that while the &#8220;many eyeballs&#8221; theory of Raymond still operates,
this &#8220;wisdom of the crowds&#8221; also has a strong effect.
In short, FLOSS software development often appears chaotic because
much of it uses a &#8220;survival of the fittest&#8221; development approach; several
different ideas are tried, and then the most successful approach is selected
by many others.
When viewed through the lens of the &#8220;wisdom of crowds&#8221;, this is
an entirely sensible thing to do.
He concludes this startling way:
&#8220;High quality open source isn&#8217;t a surprise, it&#8217;s inevitable.&#8221;
</p>
<p>
Obviously, there has to <i>be</i> a crowd for this concept to hold.
But there are many FLOSS projects where it&#8217;s obvious that there is a
crowd, <i>and</i> where the results are really very good.
So take a peek at <a href="http://leshatton.org/A17.html">
Les Hatton&#8217;s &#8220;Open source inevitably good&#8221;</a>.
It&#8217;s an interesting and provocative piece that will make you think.
</p>
<p>path: <a href="http://www.dwheeler.com/blog/oss">/oss</a> | <a href="http://www.dwheeler.com/blog">Current Weblog</a> | <a href="http://www.dwheeler.com/blog/2006/06/24#wisdom-crowds">permanent link to this entry</a></p>
<h1>Sat, 17 Jun 2006</h1>
<p><a name="readable-sweet-lisp"><font size="+2"><b>Readable s-expressions and sweet-expressions: Getting the infix fix and fewer parentheses in Lisp-like languages</b></font></a></p><p></p>
<p>
Lisp-based programming languages
normally represent programs as <i>s-expressions</i>,
where an operation and its parameters are surrounded by parentheses.
The operation to be performed is identified first,
and each parameter afterwards is
separated by whitespace.  So the traditional &#8220;2+3&#8221; is written as
&#8220;(+&nbsp;2&nbsp;3)&#8221; instead.
This is regular, but most people find this hard to read.
Here&#8217;s a longer example of an s-expression - notice the many parentheses
and the lack of infix operations:
<pre>
 (defun factorial (n)
   (if (&lt;= n 1)
       1
       (* n (factorial (- n 1)))))
</pre>
</p>

<p>
I think there&#8217;s a small resurging interest in Lisp-based systems, because Lisp
is still very good at &#8220;programs that manipulate programs&#8221;.
The major branches of Lisp (Common Lisp, Scheme, and Emacs Lisp) have
not disappeared, after all.
And I recently encountered a very cool and very new language in development,
<a href="http://www.coyotos.org/docs/bitc/spec.html">BitC</a>.
This language was created to write low-level programs
(e.g., operating system kernels and real-time programs)
that are easy to mathematically <i>prove</i> correct.
I learned about this very cool idea while writing my paper
<a href="http://www.dwheeler.com/essays/high-assurance-floss.html">
High Assurance (for Security or Safety) and
Free-Libre / Open Source Software (FLOSS)&#8230; with Lots on Formal Methods</a>.
BitC combines ideas from Scheme, ML, and C, but it&#8217;s represented using
s-expressions because it&#8217;s easy to manipulate program fragments that way.
I don&#8217;t know how well it&#8217;ll succeed, but it has a good chance;
if nothing else, I don&#8217;t know of <i>anyone</i> who&#8217;s tried this particular
approach.
The program-prover
<a href="http://www.cs.utexas.edu/users/moore/acl2/">ACL2</a> uses
Common Lisp as a basis, for the same reason: program-manipulating programs
are easy.
The FSF backs guile (a Scheme dialect) as their recommended
tool for scripting; guile gives lots of power in a small package.
</p>

<p>
But many software developers avoid Lisp-based languages,
even in cases where they would be a good tool to use, because
most software developers find s-expressions really hard to read.
S-expressions are very regular&#8230; but so is a Turing machine.
They don&#8217;t call it
&#8216;Lots of Irritating Superfluous Parentheses&#8217; for nothing.
Even if you can read it, most developers have to work with others.
Some people like s-expressions as they are - and if so, fine!
But many others are not satisfied with the status quo.
Lots of people have tried to create easier-to-read versions, but
they generally tend to lose the advantages of s-expressions
(such as powerful macro and quoting capabilities).
Can something be done to make it easy to create easier-to-read
code for Lisp-like languages - without spoiling their advantages?
</p>

<p>
I think something can be done, and
I hope to spur a discussion about various options.
To get that started, I&#8217;ve developed my own approach, &#8220;sweet-expressions&#8221;,
which I think is actually a plausible solution.
</p>

<p>
A sweet-expression reader will accept the traditional s-expressions
(except for some pathological cases),
but it also supports various extensions that make it easier to read.
Sweet-expressions are automatically translated into s-expressions, so
they lose no power.
Here&#8217;s how that same program above could be written using sweet-expressions:
<pre>
 defun factorial (n)         ; Parameters can be indented, but need not be
   if (n &lt;= 1)               ; Supports infix, prefix, &amp; function &lt;=(n 1)
       1                     ; This has no parameters, so it's an atom.
       n * factorial(n - 1)  ; Function(...) notation supported
</pre>
</p>

<p>
Sweet-expressions add the following abilities:
<ol>
<li><b>Indentation</b>. Indentation may be used instead
of parentheses to start and end
expressions: any indented line is a parameter of its parent,
later terms on a line are parameters of the first term,
lists of lists are marked with GROUP, and
a function call with 0 parameters is surrounded or followed by a pair of
parentheses [e.g., (pi) and pi()].
A &#8220;(&#8221; disables indentation until its matching &#8220;)&#8221;.
Blank lines at the beginning of a new expression are ignored.
A term that begins at the left edge and is immediately followed by newline
is immediately executed, to make interactive use pleasant.
<li><b>Name-ending</b>. Terms of the form &#8216;NAME(x y&#8230;)&#8217;, with no whitespace before
&#8216;(&#8217;, are interpreted as &#8216;(NAME x y&#8230;)&#8217;;.
Parameters are space-separated inside.
If its content is an infix expression, it&#8217;s considered one parameter instead
(so f(2 + 3) passes the its parameter, 5, to f).
<li><b>Infix</b>.  Optionally,
expressions are automatically interpreted as infix
if their second parameter is an infix operator
(by matching an &#8220;infix operator&#8221; pattern of symbols),
the first parameter is not an infix operator,
and it has at least three parameters.
Otherwise, expressions are interpreted as
normal &#8220;function first&#8221; prefix notation.
To disable infix interpretation, surround the second parameter with as(&#8230;).
Infix expressions must have an odd number of parameters with the
even ones being binary infix operators.
You must separate each infix operator with whitespace on both sides;
precedence is supported.
Use the &#8220;name-ending&#8221; form for unary operations, e.g., -(x) for &#8220;negate x&#8221;.
Thus &#8220;2 + y * -(x)&#8221; is a valid expression, equivalent to (+ 2 (* y (- x))).
Infix operators must match this pattern (and in Scheme cannot be =&gt;):
<pre>
    [+-\*/&lt;&gt;=&amp;\|\p{Sm}]{1-4}|\:
</pre>
</ol>
</p>

<p>
I call this combination &#8220;sweet-expressions&#8221;,
because by adding syntactic sugar (which are essentially abbreviations),
I hope to create a sweeter result.
</p>

<p>
For more information on sweet-expressions or on making
s-expressions more readable in general, see my website page at
<a href="http://www.dwheeler.com/readable">http://www.dwheeler.com/readable</a>.
For example, I provide a sweet-expression reader in Scheme
(under the MIT license), as well as an indenting pretty-printer in
Common Lisp.
In particular, you can
<a href="http://www.dwheeler.com/readable/readable-s-expressions.html">
see my lengthy paper about why sweet-expressions do what they do, and
some plausible alternatives.</a>
You can also download some other implementation code.
</p>

<p>
I&#8217;ve set up a
<a href="http://sourceforge.net/projects/readable">
SourceForge project named &#8220;readable&#8221;</a> to
discuss options in making s-expressions more readable,
and to distribute open source software to implement them
(unimplemented ideas don&#8217;t go far!).
I will probably need to work on other things for a while, but
since I had this idea, I thought it&#8217;d be a good idea to
write the idea and a quick sample demo of it, so that others could
build on top of it.
There hasn&#8217;t a single place for people to discuss how to make
s-expressions more readable.. so now there is one.
There are a lot of smart people out there; giving like-minded parties
a place to discuss them is likely to produce something good.
If you&#8217;re interested in this topic, please visit/join!
</p>
<p>path: <a href="http://www.dwheeler.com/blog/misc">/misc</a> | <a href="http://www.dwheeler.com/blog">Current Weblog</a> | <a href="http://www.dwheeler.com/blog/2006/06/17#readable-sweet-lisp">permanent link to this entry</a></p>
<p><a name="learning-masters"><font size="+2"><b>Learning from the Masters</b></font></a></p><p></p>
<p>
If you want to learn something, study what the masters do.
To me that seems obvious, and yet many don&#8217;t do it.
Perhaps we simply forget.
So let me inspire you with a few examples&#8230;
</p>
<p>
I just got an advance copy of David Shenk&#8217;s
&#8220;The Immortal Game: A history of chess&#8221; - and I&#8217;m referenced in it!
Which is an odd thing; I don&#8217;t normally think of myself
as a chess commentator.
But I do like the game of chess, and one of my key approaches to
getting better is simple: Study the games of good players.
I&#8217;ve even posted a few of the games with my comments on my web site,
including The Game of the Century
(<a href="http://www.dwheeler.com/misc/game_of_the_century.pgn">PGN</a>/<a href="http://www.dwheeler.com/misc/game_of_the_century.txt">Text</a>),
The Immortal Game (<a href="http://www.dwheeler.com/misc/immortal.pgn">PGN</a>/<a href="http://www.dwheeler.com/misc/immortal.txt">Text</a>),
The Evergreen Game (<a href="http://www.dwheeler.com/misc/evergreen.pgn">PGN</a>/<a href="http://www.dwheeler.com/misc/evergreen.txt">Text</a>),
and
Deep Blue - Kasparov, 1996, Game 1 (<a href="http://www.dwheeler.com/misc/deepblue-kasparov.pgn">PGN</a>/<a href="http://www.dwheeler.com/misc/deepblue-kasparov.txt">Text</a>).
It&#8217;s my Byrne/Fischer writeup that was referenced in Shenk&#8217;s book.
But I didn&#8217;t create that stuff for a book, originally.
I can&#8217;t play like these great players can,
but I get better by studying what they do.
In short, I&#8217;ve found that I must study the work of the masters.
</p>
<p>
There are many children&#8217;s educational philosophies that have, at least in part,
the notion of studying good examples as part of education.
Ruth Beechick&#8217;s &#8220;natural method&#8221; for teaching writing emphasizes starting
by copying and studying examples of great writing. She even notes
Jack London and Benjamin Franklin started by studying works they admired.
Learning begins by studying the work of the masters.
</p>
<p>
I often write about
<a href="http://www.dwheeler.com/oss_fs_why.html">
free-libre/open source software (FLOSS)</a>.
In part, I do because it&#8217;s one amazingly interesting development.
But there are other reasons, too.
Some developers of FLOSS programs are the best in the business -
you can learn a lot by seeing what they do.
In short, one important advantage of FLOSS is that it is now possible for
software developers to study the work of the masters.
</p>
<p>
I recently wrote the article
<a href="http://www.dwheeler.com/essays/high-assurance-floss.html">
High Assurance (for Security or Safety) and Free-Libre / Open Source Software (FLOSS)&#8230; with Lots on Formal Methods (aka high confidence or high integrity)</a>
(I gave it the long title to help people find it).
Here, I note the many tools to <i>create</i> high assurance software -
but there are precious few FLOSS examples of high assurance software.
True, there are very few examples of high assurance software, period,
but where are the high assurance software components that people can study
and modify without legal encumberances?
(If you know of more,
<a href="http://www.dwheeler.com/contactme.html">contact me</a>.)
That worries me; how are we supposed to educate people how to create
high assurance software, if students never see it?
People do not wake up one morning and discover that they are an expert.
They must learn, and books about a topic are not enough.
They must study the work of the masters.
</p>
<p>path: <a href="http://www.dwheeler.com/blog/misc">/misc</a> | <a href="http://www.dwheeler.com/blog">Current Weblog</a> | <a href="http://www.dwheeler.com/blog/2006/06/17#learning-masters">permanent link to this entry</a></p>
<h1>Tue, 13 Jun 2006</h1>
<p><a name="microsoft-outlook-tnef"><font size="+2"><b>How to read the mysterious Winmail.dat / Part 1.2 files (TNEF)</b></font></a></p><p></p>
<p>
All too often nowadays people report that they
&#8220;can&#8217;t open the attachment&#8221; of an email, because
they only received a file named (typically) &#8220;Part 1.2&#8221; or &#8220;Winmail.dat&#8221;.
</p>
<p>
The basic problem is that in certain cases Microsoft Outlook
uses a nonstandard extra packaging mechanism called &#8220;ms-tnef&#8221; or &#8220;tnef&#8221;
when it sends email - typically when it sends attachments.
What Outlook is <i>supposed</i> to do is simply use the industry standards
(such as MIME and HTML) directly for attachments, but Outlook fails to do so
and adds this other nonsense instead.
The full name of the format is &#8220;Transport Neutral Encapsulation Format&#8221;,
but that is a misleading name&#8230; it may be neutral on transport, but
it obstructs reception.
</p>
<p>
Almost no other email reader can read this nonstandard format.
Email clients that can&#8217;t (currently) read this format include
Lotus Notes, Thunderbird / Netscape Mail, and Eudora.
In fact, I&#8217;ve been told that
even Microsoft&#8217;s own Outlook Express can&#8217;t read this format!
</p>
<p>
So take a look at my new article,
<a href="http://www.dwheeler.com/essays/microsoft-outlook-tnef.html">Microsoft Outlook MS-TNEF handling (aka Winmail.dat or &#8220;Part 1.2&#8221; problem of unopenable email attachments)</a>.
It gives you a brief explanation of the problem, and what to do about it,
both from the sender view (how can I stop sending unopenable email?)
and the receiver view (how can I read them anyway?).
</p>
<p>path: <a href="http://www.dwheeler.com/blog/misc">/misc</a> | <a href="http://www.dwheeler.com/blog">Current Weblog</a> | <a href="http://www.dwheeler.com/blog/2006/06/13#microsoft-outlook-tnef">permanent link to this entry</a></p>
<h1>Mon, 05 Jun 2006</h1>
<p><a name="playstation3"><font size="+2"><b>Sony Playstation 3: Train wreck in process</b></font></a></p><p></p>
<p>
I try to keep up with the general gaming business.
Many of the best new computer hardware technologies
first show up in the gaming world, for one thing.
And for another, I once in the business; in the mid-1980s
I was lead software developer/maintainer for the first commercial
multiplayer role-playing game in the U.S.,
<a href="http://en.wikipedia.org/wiki/Scepter_of_Goth">Scepter of Goth</a>.
(Full disclosure: I didn&#8217;t write the original, I maintained it after
it had been initially developed. Scepter may have been
the first commercial multiplayer RPG in the world, but I have never gotten
good-enough data to show conclusively if MUD or Scepter were first.
Bartle&#8217;s MUD was clearly first in the UK, and Scepter
was clearly the first in the US, and neither knew of the other for
a long time.)
I also wrote some videogames for the Apple ][, which I sold.
(I still play occasionally, but my hand/eye coordination is awful;
my brother had to playtest them, since I couldn&#8217;t get far in my own games.)
I generally hope for good competition, since that is what keeps the
the innovation flowing and the prices down.
My hopes are getting dashed, because Sony seems to have had
a full lobotomy recently.
</p>
<p>
If Sony is trying to go (mostly) out of business, it&#8217;s
got a great process going.
Recently about half of Sony&#8217;s income has depended on the Playstation 2,
so you&#8217;d think that they would avoid bone-headed decisions that
would doom them in the market as they release their next-generation console.
</p>
<p>
But the
<a href="http://www.philly.com/mld/philly/news/columnists/14733980.htm">
Sony Playstation 3 will come with an outrageous pricetag</a>:
starting at $599 (or $499 for a stripped-down version).
Home video-game consoles have sold for $199 to $299 traditionally, and the
X-Box 360 (its primary competitor) costs much
less than this announced price too.
</p>
<p>
Why so much?
One significant reason is
because Sony is including a Blu-ray reader, a proprietary video format
that they hope will replace DVDs;
this is both raising the price substantially <i>and</i> appears to be
delaying shipment.
Didn&#8217;t Sony learn its lesson from Betamax, their earlier costly blunder
in the videotape format war?
No, it appears that Sony must go out of business to learn.
Betamax was supposed to be better technically (and it was in some ways),
but it cost much more.
In part, the higher cost was due to the lack of competing suppliers; the
competing VHS market was full of competing suppliers who quickly marched
past the proprietary format.
Sony has even lost big money on other proprietary formats, too.
Blu-Ray has all the same earmarks of a failure, in exactly the same way.
The Playstation 3 will have a hopelessly high price tag because of
Blu-ray, and it looks like the Playstation 3 will go down with it.
Since both Blu-Ray and its competitor HD-DVD
have really more egregious digital restrictions management (DRM) mechanisms
built in, I hope both fail - their improvements frankly don&#8217;t
justify abandoning DVDs in my eyes.
</p>
<p>
Ah, but the higher price tag implies better performance, right?  Wrong.
<a href="http://www.theinquirer.net/?article=32171">The Inquirer reports
that there are some serious technical flaws in the Playstation 3</a>,
The Playstation 3 will have half the triangle setup capability compared
to Xbox 360.
What&#8217;s worse, its local cell memory read speed is about 1/1000th of
the speed it <i>should</i> be getting.
In fact, one slide describing the Playstation 3 performance had to say
&#8220;no that isn&#8217;t a typo&#8221;, because the performance figures on this
fundamental subsystem are so horrifically bad.
So people will have the option of spending a lot more money for a
less capable machine that is saddled with
yet another failed proprietary format.
And in addition, Sony is already really late with its next-gen console;
if you&#8217;re not first, you need to be <i>better or cheaper</i>, not
<i>obviously worse and more expensive</i>.
Yes, it&#8217;ll run Linux, but I can run Linux very well
on a general-purpose computer system, for less money and
without the hampering I expect from Sony.
</p>
<p>
Has greed disabled Sony&#8217;s ability to think clearly?
The
<a href="http://www.eff.org/IP/DRM/Sony-BMG/">
Sony-BMG DRM music CD scandal</a>, where Sony subverted a massive number of
computers through a rootkit on its music CDs, just led to a big settlement.
Granted, it could have been worse for Sony; under the laws of most
countries, many Sony executives should probably be in jail.
In the Sony-BMG case,
Sony tried to force a digital restrictions management (DRM)
system on users by breaking into their customers&#8217; operating systems.
The point of DRM systems is to prevent you from using copyrighted products
in ways the company doesn&#8217;t approve of &#8212; even if they are legal (!).
Hrmpf.
The
<a href="http://www.apig.org.uk/index/APIG_DRM_Report-final.pdf">
All Party Parliamentary Internet Group (APIG) in the UK</a>
recommended the publication of
&#8220;guidance to make it clear that companies
distributing Technical Protection Measures systems in the UK would, if they
have features such as those in Sony-BMG’s MediaMax and XCP systems, run a
significant risk of being prosecuted for criminal actions.&#8221;
It&#8217;s fine to want money, but it&#8217;s wise to make money by making
a good product &#8212; one that is cheaper or better in some way.
&#8220;Get rich quick&#8221; schemes, like rootkitting your customers to keep them
from doing stuff you don&#8217;t like, or
trying to establish proprietary format locks so everyone has
to go to you, often backfire.
</p>
<p>
What&#8217;s weird is that this was all unnecessary; it would have
been relatively easy for Sony
to create a platform with modern electronics that had much better performance,
worth paying for, without all this.
It would have been much less risk to Sony if they&#8217;d taken a simpler
route.
What&#8217;s more, their market share is so large that it was theirs to keep;
they just had to be smart about making a good follow-on product.
</p>
<p>
Maybe Sony will pull things through in spite of its problems.
I hope they don&#8217;t just collapse, because competition is a critical force in
keeping innovation going and prices low.
Their product&#8217;s
ability to play Playstation 2 games, for example, is an advantage&#8230;
but I doubt that will be enough, because the old games won&#8217;t
exploit any of the advantages of a next-generation platform.
If Sony can get a massive number of amazingly-good platform-unique
games &#8212; ones so good that people will choose the Playstation 3 specifically
for them &#8212; then <i>maybe</i> they can survive.
But I doubt they can get that strong a corner on good games;
many independents will not want to risk their companies
by making single-platform games, especially one as risky as this one,
and Sony is unlikely to have the finances to buy them all up or
back them enough to eliminate the risks.
What is more likely to happen is that there will be a few platform-unique
games for Playstation 3, a few platform-unique games for its competitors
(particularly XBox 360), and a
few multiple-platform games&#8230; which means no lock for Sony.
In short, things do not look very good right now for Sony;
Sony seems to have
<a href="http://en.wiktionary.org/wiki/hoist_by_one%27s_own_petard">
hoisted themselves on their own petard</a>.
I don&#8217;t even see what they can do now to recover.
</p>
<p>
I think Jonathan V. Last of the Philadelphia Inquirer has it right:
&#8220;Obsessed with owning proprietary formats, Sony keeps picking fights.
[And] It keeps losing. And yet it keeps coming back for more, convinced that
all it needs to do is push a bigger stack of chips to the center of the
table. If Blu-ray fails, it will be the biggest home-electronics failure
since Betamax. If it drags PlayStation 3 down with it, it will be one
of the biggest corporate blunders of our time.&#8221;
</p>
<p>path: <a href="http://www.dwheeler.com/blog/misc">/misc</a> | <a href="http://www.dwheeler.com/blog">Current Weblog</a> | <a href="http://www.dwheeler.com/blog/2006/06/05#playstation3">permanent link to this entry</a></p>
<h1>Sun, 04 Jun 2006</h1>
<p><a name="presentations"><font size="+2"><b>My upcoming presentations - Date change and a new page</b></font></a></p><p></p>
<p>
I&#8217;m still giving a presentation at NovaLUG, but the
date has been changed from July 1 to July 8 (2006).
This is because July 4 is a U.S. holiday (independence day),
and there was concern that some people might not be able to come.
So it will now be July 8, 10am,
&#8220;Free-Libre/Open Source Software (FLOSS) and Security&#8221;.
Washington Technology Park/CSC (formerly Dyncorp),
15000 Conference Center Drive, Chantilly, VA.
</p>
<p>
This has convinced me that I need a page to help people
find when and where I&#8217;m speaking, so that they don&#8217;t have to
march through my blogs to get the information.
So here it is&#8230;
</p>
<p>
<a href="http://www.dwheeler.com/presentations.html">
Presentations by David A. Wheeler.</a>
Just click on it, and you&#8217;ll get the latest times, places, etc., of
where to go if you just can&#8217;t find something better
to do with your life :-).
</p>
<p>path: <a href="http://www.dwheeler.com/blog/website">/website</a> | <a href="http://www.dwheeler.com/blog">Current Weblog</a> | <a href="http://www.dwheeler.com/blog/2006/06/04#presentations">permanent link to this entry</a></p>
<h1>Fri, 02 Jun 2006</h1>
<p><a name="firefox-autonumbering"><font size="+2"><b>Autonumbering supported in Firefox 1.5!</b></font></a></p><p></p>
<p>
Here&#8217;s another reason to use Firefox as your web browser, besides the fact that
<a href="http://www.dwheeler.com/blog/2005/08/06/#ie-horrific">Firefox has
a better security record</a> and that Firefox has
<a href="http://news.com.com/Microsoft+yielding+to+IE+standards+pressure/2100-1032_3-5620988.html">better
support for web standards in general.</a>
Firefox 1.5 has added autonumbering support, and sites like mine are
starting to use it.
If you&#8217;re using a non-compliant web browser,
like the current version of Internet Explorer,
you&#8217;re missing out.
But let&#8217;s back up a bit to the basics: HTML.
</p>
<p>
<a href="http://www.w3.org/MarkUp/">HTML</a> has been a
spectacularly successful standard for sharing
information - web pages around the world use it.
I write a lot of my papers directly in HTML, because it&#8217;s easy,
using HTML makes them easily accessible to everyone, and it&#8217;s a
<a href="http://www.dwheeler.com/essays/opendocument-open.html">
completely open standard</a>.
</p>
<p>
But HTML has several weaknesses if you&#8217;re writing long or
technical reports.
One especially important one is automatic numbering of headers:
the original HTML specification can&#8217;t do it.
When you&#8217;re reading a long report, it can be hard to keep track of
where you are, so having every heading numbered (such as
&#8220;section 2.4.3&#8221;) is <i>really</i> helpful.
This can be solved by having programs directly insert the heading numbers
numbers into the HTML text, but that&#8217;s a messy and kludgy solution.
It&#8217;d be much
better if browsers automatically added numbered headings where appropriate,
so that the HTML file itself is simple and clean.
</p>
<p>
The W3C (the standards group in charge of HTML and related
standards) agreed that automatic numbering was important,
and included support for automatic numbering in the
<a href="http://www.w3.org/Style/CSS/">
Cascading Style Sheets (CSS) standard</a> way back in 1998.
CSS is an important support standard for HTML, so that should have been it&#8230;
but it wasn&#8217;t.
Both Netscape and Microsoft decided to not fully implement the standard,
nor try to fix the standard so that they <i>would</i> implement it.
Soon afterwards Microsoft gained dominant market share, and then
let their browser stagnate (why bother improving it, since there was
no competition?).
It looked like we, the users, would never get basic
capabilities in HTML like auto-numbering.
</p>
<p>
I&#8217;m happy to report that
<a href="http://www.spreadfirefox.com/?q=affiliates&amp;id=31988&amp;t=60">
Firefox 1.5 has added support for auto-numbering</a> headings and
other constructs too.
So I&#8217;ve modified my
<a href="http://www.dwheeler.com/essays/paper.css">CSS file for
papers and essays</a> so it auto-number headings;
I&#8217;ve released my CSS file under the MIT/X license, so anyone else can use it.
If you develop web content you may want to look
at examples like mine for a reason, because&#8230;
</p>
<p>
It turns out that the story is more complicated.
In the process of implementing auto-numbering,
the Firefox developers found a serious problem with the CSS specification.
Oops!
The <a href="https://bugzilla.mozilla.org/show_bug.cgi?id=3247">
Mozilla Firefox bug #3247</a> and
<a href="http://www.davidflanagan.com/blog/2005_08.html#000075">
David Flanagan&#8217;s blog</a> discuss this further.
The Firefox developers talked with the W3C, and the W3C ended up creating
&#8220;CSS 2.1&#8221;, an updated/patched version of the CSS2 standard
that is in the process of being formally released.
</p>
<p>
What this means is that the examples for autonumbering
in the &#8220;official&#8221; original CSS2 standard won&#8217;t actually work!
Instead, you need to follow a slightly different approach as defined
in the patched CSS2.1 specification.
<blockquote>
<b>Technical stuff:</b>
The basic problem involves scoping issues.
To solve it, the counter-reset property <i>must</i> be in the
main heading names (like h1, h2, etc.), and <i>not</i> in the
&#8220;before&#8221; sections (like h1:before, h2:before, etc.) - in spite of
all the examples in the original CSS2 spec.
You can put counter-increment in either place, though the spec
puts them in the :before sections so I have too.
<!-- Here's the W3C example.  I don't think it matters; I think
     what matters is the location of the counter-reset.
H1:before {
    content: "Chapter " counter(chapter) ". ";
    counter-increment: chapter;  /* Add 1 to chapter */
}
H1 {
    counter-reset: section;      /* Set section to 0 */
}
The latest examples in CSS2.1 working draft of 11 April 2006
move the counter-reset property to main heading names, but keep the
counter-increment property in the ":before" sections.
-->
</blockquote>
</p>
<p>
Now people have yet another reason to upgrade to Firefox.
Firefox has had better standards support for some time;
there are now many sites that won&#8217;t display properly
(or as well) if your browser doesn&#8217;t support the standards well.
But here is a clear and functionally important difference.
</p>
<p>
I&#8217;m a big believer in standards, but they can only help users if
they are implemented, and they will only be implemented if
users demand standards compliance.
I think that the more people switch to standards-compliant browsers,
and the more that sites use standards (to encourage people to switch),
the more pressure it will bring on the other browser makers to catch up.
And that would be great for all computer users.
</p>
<p>
More broadly, this is also a good example of why it&#8217;s important to have
implementations try out standards before they are frozen;
they help avoid mistakes like this.
Today,
<a href="http://www.dwheeler.com/essays/open-standards-open-source.html">
essentially every successful open standard is implemented by
free-libre/open source software (FLOSS)</a> - this makes sure that the
standard is implementable, helps all understand what the standard
means, and also helps other developers understand at least one way
to implement it.
This doesn&#8217;t mean standards aren&#8217;t important; standards are vital!
And this also shows that when a mistake is made by a standards body,
life is not over;
standards bodies can work with implementors to fix problems.
In fact, this
shows that the best standards are those created from an
interplay between standards developers and implementors, where
standards are then made official after actual implementation experience.
</p>
<p>path: <a href="http://www.dwheeler.com/blog/oss">/oss</a> | <a href="http://www.dwheeler.com/blog">Current Weblog</a> | <a href="http://www.dwheeler.com/blog/2006/06/02#firefox-autonumbering">permanent link to this entry</a></p>
<h1>Wed, 17 May 2006</h1>
<p><a name="lugs2005"><font size="+2"><b>I&#8217;ll be speaking at some Linux User Groups (LUGs)</b></font></a></p><p></p>
<p>
A while ago I was asked to speak at some of the
<a href="http://www.tux.org/">Linux User Groups (LUGs)</a>
in the Washington, DC area, and I agreed to do so.
Here are the current plans, if you are interested in hearing me speak:
<ul>
<li>May 17, 2006, 7pm: <a href="http://dclug.tux.org/">DCLUG</a>,
Washington, DC. &#8220;FLOSS and security.&#8221;
<a href="http://dclug.tux.org/dclug_meetings.html">Get directions</a> to
their meeting location: 2025 M Street NW, Washington DC.
</li>
<li>July 1, 2006, 10am:
<a href="http://novalug.tux.org/">NovaLUG</a>.
Meeting is at
Washington Technology Park/CSC (formerly Dyncorp),
15000 Conference Center Drive, Chantilly, VA.
(I&#8217;ve just sent an email to confirm, but that&#8217;s the date I have.)
</li>
<li>July 12, 2006, 7pm: <a href="http://www.calug.com/">Columbia LUG</a>.
&#8220;Open Standards and Security (and OpenDocument too)&#8221;
Meeting is at HP, 8890 McGaw Rd Ste 100, Columbia, MD.
</li>
</ul>
<p>
Plans may change, but this is the information I have for now.
</p>
<p>path: <a href="http://www.dwheeler.com/blog/oss">/oss</a> | <a href="http://www.dwheeler.com/blog">Current Weblog</a> | <a href="http://www.dwheeler.com/blog/2006/05/17#lugs2005">permanent link to this entry</a></p>
<h1>Sat, 06 May 2006</h1>
<p><a name="high-assurance-floss"><font size="+2"><b>High Assurance (for Security or Safety) and Free-Libre / Open Source Software (FLOSS)</b></font></a></p><p></p>
<p>
Recently I spoke at an
<a href="http://www.opengroup.org/">Open Group</a>
conference and gave my presentation on
<a href="http://www.dwheeler.com/essays/oss_software_assurance.pdf">
Open Source Software and Software Assurance (Security)</a>.
While there, someone asked a very interesting question:
&#8220;What is the relationship between high assurance and
open source software?&#8221;
That&#8217;s a fair question, and although I gave a quick answer,
I realized that a longer and more thoughtful answer was really needed.
</p>
<p>
So I&#8217;ve just posted a paper to answer the question:
<a href="http://www.dwheeler.com/essays/high-assurance-floss.html">High Assurance (for Security or Safety) and Free-Libre / Open Source Software (FLOSS)</a>.
For purposes of the paper,
I define &#8220;high assurance software&#8221;
as software where there&#8217;s an <i>argument
that could convince skeptical parties</i>
that the software <i>will always perform or never perform</i>
certain key functions <i>without fail</i>.
That means you have to show convincing evidence that there are
<i>absolutely no software defects</i>
that would interfere with the software&#8217;s key functions.
Almost all software built today is <i>not</i> high assurance;
developing high assurance software is currently a specialist&#8217;s field.
But I think all software developers <i>should</i> know a little about
high assurance.
And it turns out there are lots of connections between
high assurance and FLOSS.
</p>
<p>
The relationships between high assurance and FLOSS are interesting.
Many tools for developing high assurance
software are FLOSS, which I can show by examining the areas of
software configuration management, testing,
formal methods, analysis implementation, and code generation.
However, while high assurance components are rare, FLOSS high assurance
components are even rarer.
This is in contrast to medium assurance, where there are a vast number
of FLOSS tools and FLOSS components, and the security record of FLOSS
components is quite impressive.
The paper then examines why this is the circumstance.
The most likely reason for this appears to
be that decision-makers for high assurance components
are not even considering the possibility of FLOSS-based approaches.
The paper concludes that in the future,
those who need high assurance components should
consider FLOSS-based approaches as a possible strategy.
</p>
<p>
Anyway, it&#8217;s a thought piece; if you&#8217;re interested in making software
that is REALLY reliable, I hope you&#8217;ll find it interesting.
</p>
<p>
Again, the paper is here: <a href="http://www.dwheeler.com/essays/high-assurance-floss.html">High Assurance (for Security or Safety) and Free-Libre / Open Source Software (FLOSS)</a>.
</p>
<p>path: <a href="http://www.dwheeler.com/blog/oss">/oss</a> | <a href="http://www.dwheeler.com/blog">Current Weblog</a> | <a href="http://www.dwheeler.com/blog/2006/05/06#high-assurance-floss">permanent link to this entry</a></p>
<h1>Fri, 14 Apr 2006</h1>
<p><a name="oin"><font size="+2"><b>Open Invention Network (OIN), software patents, and FLOSS</b></font></a></p><p></p>
<p>
Software patents continue to threaten the software industry in the
U.S. and some other countries.
They&#8217;re especially a threat to smaller software development
organizations and individuals (who do most of the innovating),
but even large software organizations are vulnerable.
</p>
<p>
If you&#8217;re not familiar with the problems of software patents, here&#8217;s
some information and resources to get you started.
The <a href="http://webshop.ffii.org/">FFII patented webshop</a> is a short
demonstration of why the problem is so serious &#8212; practically all
commercial websites infringe on patents already <i>granted</i> (though
not currently enforceable) in Europe.
<a href="http://www.researchoninnovation.org/softpat.pdf">
Bessen and Hunt</a> found good evidence that software patents
<i>replace</i> innovation instead of encouraging it.
In particular, they found that those who create software patents
are those who do <i>less</i> research, and the primary use of
software patents appears to be in creating a &#8220;patent thicket&#8221; to inhibit
competition.
There&#8217;s also the evidence of history; software is the only product that
can be protected by <i>both</i> copyright and patent, yet there&#8217;s
general agreement that the software industry
was far more innovative when patents were <i>not</i> permitted.
Copyright is sufficient; there&#8217;s no need for software patents, which
simply impede innovation.
Software patents were not even voted in; software patents are an example
of U.S. courts creating laws (which they&#8217;re not supposed to do).
India and Europe have (so far) wisely rejected software patents.
Some large companies (such as Oracle Corporation and Red Hat)
have clearly said that they oppose software patents, but that they
must accumulate such patents to defend against attack by others.
Some organizations are working against software patents, such as the
<a href="http://ffii.org/">FFII</a>,
<a href="http://www.nosoftwarepatents.com/">No Software Patents</a>,
and
<a href="http://lpf.ai.mit.edu/Patents/patents.html">League for
Programming Freedom</a>.
<a href="http://www.groklaw.net/staticpages/index.php?page=20050402193202442">
Groklaw has a massive amount of information on software patents</a>.
Yet currently the U.S. continues this dangerous practice, and so people are
trying to figure out how to deal with it until the practices can be
overturned.
In particular, free-libre / open source software (FLOSS) developers have
been trying to figure out how to deal with software patents, and there
has been some progress on that front.
</p>
<p>
Recently a new organization has been added to the mix: the
<a href="http://www.openinventionnetwork.com">&#8220;Open Invention Network&#8221; (OIN)
</a>.
Its website is short on details, but Mark H. Webbink&#8217;s article
&#8220;The Open Invention Network&#8221; in Linux Magazine (April 2006, page 18)
gives more information.
Webbink reports that
OIN was founded by IBM, Novell, Philips, Red Hat, and Sony, for two
distinct purposes:
<ol>
<li>&#8220;To provide a network, or commons, around Linux and Linux-related applications; and</li>
<li>To assure that the resulting, large, patent-free &#8220;safe area&#8221; remains large, safe, and patent-free.&#8221;</li>
</ol>
Joining OIN costs nothing; OIN&#8217;s
<a href="http://www.openinventionnetwork.com/press.html">November 2005
press release</a> says that joining OIN requires that the
participant agree to
&#8220;not assert its patents against the Linux operating system or
certain Linux-related applications.&#8221;
In return, the participants get
to use patents owned by OIN &#8212; some of which may not be related to
Linux or the key applications at all, but may be valuable to them.
In particular, OIN has the Commerce One patents (donated by Novell) that
cover web services, which potentially threaten anyone who uses web services.
By having a number of participants, OIN reduces the risks
of software patent lawsuits to all users and developers
of FLOSS products &#8212; whether they&#8217;re an OIN member or not.
</p>
<p>
The list of key applications considered by OIN, according to Webbink, includes
Apache, Eclipse, Evolution, Fedora Directory Server, Firefox, GIMP,
GNOME, KDE, Mono, Mozilla, MySQL, Nautilus, OpenLDAP, OpenOffice.org,
Perl, PostgreSQL, Python, Samba, SELinux, Sendmail, and Thunderbird.
Of course, it&#8217;d be nice if OIN protected everything with a FLOSS license,
not just a listed set.
Webbink says they chose to not do that because the &#8220;size of the safe area is
critical to OIN&#8217;s future.  If the commons is too narrow, it offers
little protection.  If it&#8217;s too broad, it makes it difficult for a
company to join the commons&#8230;&#8221;
Then, if someone tries to bring a patent infringment lawsuit against
any of these projects, the OIN can take various kinds of actions.
You can get more information via the
<a href="http://en.wikipedia.org/wiki/Open_Invention_Network">
Wikipedia article on the Open Invention Network (OIN)</a>.
I suspect that even a FLOSS product that&#8217;s not on the list might
still get some protection, because a software patent claim brought
against that product might
also apply to one of the covered applications&#8230; and OIN might try
to pre-empt that.
But still, even given its many limitations, this is a step forward in
reducing risks of developers and users.
</p>
<p>
Another patent commons project for FLOSS programs is
the <a href="http://www.patent-commons.org/">Patent Commons Project</a>,
whose contributors and supporters include
Computer Associates, IBM, Novell, OSDL, Red Hat, and Sun Microsystems.
This is a much looser activity; it is simply a repository where
&#8220;patent pledges and other commitments can be readily accessed
and easily understood.&#8221;
</p>
<p>
The U.S. Patent and Trademark Office (PTO)
has had some talks about
<a href="http://developer.osdl.org/dev/priorart/">
using FLOSS as examples of prior art,</a>
as well as other ways to try to reduce the number of unqualified
yet granted patents.
I guess it&#8217;s good that the PTO is trying to prevent <i>some</i> invalid
patents from slipping through; it&#8217;s better than the current practice of
rubber-stamping massive numbers of patents that are actually illegal
(because they actually don&#8217;t meet the current criteria for patents).
It&#8217;s particularly galling that someone
can read another&#8217;s publicly-available code, get a patent,
and then sue the original creator of the code into oblivion &#8212;
it&#8217;s not legal, but those who do it aren&#8217;t punished and are
usually handsomely rewarded.
(Yes, this happens.)
How does that help advance anything?
But the notion that the PTO cannot currently look at prior art, due to its own
boneheaded rules and absurd timelines, merely shows how broken the
whole process is.
I suspect patents are worth their problems in some other industries, but
in software we&#8217;ve now thoroughly demonstrated them as a failure.
</p>
<p>
The U.S. Constitution only permits patents when they
&#8220;advance the arts and sciences&#8221;; since this is not true for
software patents, software patents need to be abolished immediately.
Still, until that happens, half-steps like OIN and getting the U.S. PTO to
reject illegal patents (for a change!) will at least reduce some
of the risks software developers face.
</p>
<p>path: <a href="http://www.dwheeler.com/blog/oss">/oss</a> | <a href="http://www.dwheeler.com/blog">Current Weblog</a> | <a href="http://www.dwheeler.com/blog/2006/04/14#oin">permanent link to this entry</a></p>
<h1>Wed, 12 Apr 2006</h1>
<p><a name="open-standards-open-source"><font size="+2"><b>Open standards, open source, and security too &#8212; LinuxWorld 2006 and a mystery</b></font></a></p><p></p>
<p>
<a href="http://www.dwheeler.com/blog/2006/03/30/#open-standards-security">As
I had previously threatened</a>,
I gave my talk on &#8220;Open Standards and Security&#8221; on April 4, 2006,
at LinuxWorld&#8217;s &#8220;Government Day&#8221; focusing on open standards.
My talk&#8217;s main message was that open standards are necessary
(in the long run)
for security, and I gave various reasons why I believe that.
I also tried to show how important open standards are in general.
In the process, a mystery was revealed, but first let me talk about
the NewsForge article about my talk.
</p>
<p>
Unbeknownst to me, there was a reporter from NewsForge
in the audience, who wrote the article
<a href="http://business.newsforge.com/business/06/04/05/2046210.shtml">
Why open standards matter</a> &#8212; and it specifically discussed my talk!
Which was pretty neat, especially since the article was very
accurate and complimentary.
I used several stories in my talk, which the reporter called
&#8220;parables&#8221;. I didn&#8217;t use that word, but I wish I had, because that&#8217;s
exactly what they were.
For example, I talked about a (hypothetical) magic food, that cost only $1
the first year and you wouldn&#8217;t need to eat anything else
for a year&#8230; but it would make all other foods poisonous to you, and there
was only one manufacturer of magic food.
I created this parable to show that complete dependency on someone else
is a serious security problem&#8230; if you&#8217;re so dependent that you cannot
switch suppliers (practically), you already have a serious security problem.
I was especially delighted that she included my key comment that my
&#8220;magic food&#8221; parable wasn&#8217;t about any particular supplier
(Microsoft or Red Hat or anyone else)&#8230;
we need suppliers, the problem comes when we allow ourselves to become
<i>dependent</i> on a supplier.
I also discussed the 1904 Baltimore fire (where incompatible firehose
couplings were a real problem), and the railroad gauge incompatibilities
in the mid-1860s in the southern U.S.
(this was a contributing factor to the Confederacy&#8217;s loss in the U.S.
Civil War).
</p>
<p>
I did find one
nitnoid about the article, which doesn&#8217;t change anything really
but is great for showing how messy and complicated real history is.
The article says that in the 1904 Baltimore fire,
none of the firetrucks from other cities could connect.
I said something <i>almost</i> like that, but not quite.
What I actually said was that firetrucks from other cities had
firehose couplings that were incompatible with Baltimore&#8217;s hydrants.
I read a lot more about this event than I could mention in my presentation,
which is why I said what I said in that funny way.
It turns out that a few firefighters did manage to jerry-rig &#8220;connections&#8221;
between some of the incompatible couplings,
by wrapping lots of hoses around the hydrants and couplings.
This is a perfect example of a &#8220;correction&#8221; nitnoid that just
doesn&#8217;t matter, because you can probably guess the result &#8212; the
jerry-rigged connections
had lots of water on the ground (around the hydrant),
and disturbingly little on the fire.
So while technically there were some &#8220;connections&#8221; to hydrants
by the firetrucks from other cities, they weren&#8217;t effective enough,
and the bottom line is just as the article indicated: Baltimore burned.
In short, the firehose incompatibilities between cities resulted
in over 2,500 buildings being lost, almost all of them unnecessarily.
I&#8217;m not sure what it says about me that I note this weird little issue,
which isn&#8217;t important at all,
but I&#8217;m sure that correcting it will require a lot of therapy.
</p>
<p>
So if you haven&#8217;t taken a look at it, take a peek at the
<a href="http://www.dwheeler.com/essays/open-standards-security.pdf">
&#8220;Open Standards and Security&#8221;</a> presentation.
I hope to eventually get an audio file posted; look for it.
When I gave the presentation I had several props to make it
more interesting, which you&#8217;ll just have to imagine:
<ul>
<li>a muffin which I claimed was &#8220;magic food&#8221;, squashed because it&#8217;s hard
to bring a muffin from DC to Boston undamaged</li>
<li>a real firehose coupling; they&#8217;re heavy,
and airport security gets real nervous when you bring them through</li>
<li>a toy railroad; I lacked the strength and space to bring a
real one :-)</li>
</ul>
</p>

<hr />
<p>
Now, on to the mystery.
</p>
<p>
One of the people at my talk made the claim that,
&#8220;today, every successful open standard is implemented by FLOSS.&#8221;
That should be easy to disprove &#8212; all I need is a counter-example.
Except that counter-examples seem to be hard to find;
I can&#8217;t find even one, and even if I eventually find one, this
difficulty suggests that there&#8217;s something deeper going on.
</p>

<p>
So as a result of thinking about this mystery, I wrote a new essay, titled
<a href="http://www.dwheeler.com/essays/open-standards-open-source.html">Open Standards, Open Source</a>.
It discusses how open standards aid
free-libre / open source software (FLOSS)
projects, how FLOSS aids open standards, and then examines this mystery.
It appears that it is true &#8212;
today, essentially every successful open standard
really is implemented by FLOSS.
I consider why that is, and what it means if this is predictive.
In particular, this observation suggests that
an open standard without a FLOSS implementation
is probably too risky for users to require,
and that developers of open standards should encourage
the development of at least one FLOSS implementation.
</p>
<p>path: <a href="http://www.dwheeler.com/blog/oss">/oss</a> | <a href="http://www.dwheeler.com/blog">Current Weblog</a> | <a href="http://www.dwheeler.com/blog/2006/04/12#open-standards-open-source">permanent link to this entry</a></p>
<h1>Thu, 30 Mar 2006</h1>
<p><a name="open-standards-security"><font size="+2"><b>Two new presentations: &#8220;Open Source Software and Software Assurance&#8221; and &#8220;Open Standards and Security&#8221;</b></font></a></p><p></p>
<p>
I&#8217;ve put two presentations on my website you might find of interest.
</p>
<p>
The first one is
<a href="http://www.dwheeler.com/essays/oss_software_assurance.pdf">
Open Source Software and Software Assurance</a>.
Here I talk about Free-Libre / Open Source Software (FLOSS) and its
relationship to software assurance and security.
It has lots of actual statistics, and a discussion on review.
I also deal with the chestnut &#8220;can&#8217;t just anyone insert malicious code
into OSS?&#8221; &#8212; many questioners don&#8217;t realize that attackers can change
proprietary software too (attackers generally don&#8217;t worry
about legal niceties); the issue is the user&#8217;s supply chain.
I gave this presentation at FOSE 2006 in Washington, DC, and I&#8217;ve
given variations of this presentation many times before.
</p>
<p>
The second presentation is
<a href="http://www.dwheeler.com/essays/open-standards-security.pdf">
&#8220;Open Standards and Security&#8221;</a>.
Here I focus on the role of open standards in security, which turns out
to be fundamental.
</p>
<p>
I&#8217;ll be giving the &#8220;Open Standards and Security&#8221; presentation at the
<a href="http://www.linuxworldexpo.com/live/12/events/12BOS06A/conference/tracksessions//QMONYA04PWRU">
&#8220;LinuxWorld Government Day: Implementing Open Standards&#8221; track</a>,
April 4, 2006, in Boston, Massachusetts.
I&#8217;ll speak at 12:45, so come hear the presentation&#8230; you&#8217;ll miss much
if you only read the slides.
</p>
<p>path: <a href="http://www.dwheeler.com/blog/security">/security</a> | <a href="http://www.dwheeler.com/blog">Current Weblog</a> | <a href="http://www.dwheeler.com/blog/2006/03/30#open-standards-security">permanent link to this entry</a></p>
<h1>Tue, 28 Mar 2006</h1>
<p><a name="unsigned-char"><font size="+2"><b>Unsigned characters: The cheapest secure programming measure?</b></font></a></p><p></p>
<p>
Practically every computer language has &#8220;gotchas&#8221; &#8212; constructs or combinations
of constructs that software developers are likely to use incorrectly.
Sadly, the C and C++ languages have an
unusually large number of gotchas, and many of these gotchas tend to
lead directly to dangerous security vulnerabilities.
This forest of dangerous gotchas tends to make developing
secure software in C or C++ more difficult than it needs to be.
Still, C and C++ are two of the most widely-used languages in the world;
there are many reasons people still choose them for new development, and
there&#8217;s a lot of pre-existing code in those languages that is not going to be
rewritten any time soon.
So if you&#8217;re a software developer,
it&#8217;s still a very good idea to learn how to develop secure
software in C or C++&#8230; because you&#8217;ll probably need to do it.
</p>
<p>
Which brings me to the &#8220;-funsigned-char&#8221; compiler option of gcc,
one of the cheapest
secure programming available to developers using C or C++
(similar options are available for many other C and C++ compilers).
If you&#8217;re writing secure programs in C or C++, you should use
the &#8220;-funsigned-char&#8221; option of gcc (and its equivalent in other
compilers) to help you write secure software.
What is it, and what&#8217;s it for?
Funny you should ask&#8230; here&#8217;s an answer!
</p>
<p>
Let&#8217;s start with the technical basics.
The C programming language includes the &#8220;char&#8221; type, which is
usually used to store an 8-bit character.
Many internationalized programs encode text using UTF-8, so a user-visible
character be stored in a sequence of &#8220;char&#8221; values.
but even in internationalized programs
text is often stored in a &#8220;char&#8221; type.
<!-- Let's skip wide char, WCHAR... it's complicated and out of scope. -->
</p>
<p>
The C standard specifically says that char CAN be signed OR unsigned.
(Don&#8217;t believe me? Go look at
ISO/IEC 9899:1999, section 6.2.5, paragraph 15, second sentence.
So there.)
On many platforms (such as typical Linux distributions),
the char type is signed.
The problem is that software developers often <i>incorrectly</i> think
that the char type is unsigned, or don&#8217;t understand the ramifications
of signed characters.
This misunderstanding is becoming <i>more</i> common over time, because
many other C-like languages (like Java and C#) define their &#8220;char&#8221; type
to be essentially unsigned or in a way that it wouldn&#8217;t matter.
What&#8217;s worse,
this misunderstanding can lead directly to security vulnerabilities.
</p>
<p>
All sorts of &#8220;weird&#8221; things can happen on systems with signed characters.
For example, the character 0xFF will match as being &#8220;equal&#8221; to the integer -1,
due to C/C++&#8217;s widening rules.
And this can create security flaws in a hurry, because -1 is a common
&#8220;sentinel&#8221; value that many developers presume &#8220;can&#8217;t happen&#8221; in a char.
A well-known security flaw in Sendmail was caused by exactly this problem
(see <a href="http://www.kb.cert.org/vuls/id/897604">US-CERT #897604</a> and
<a href="http://www.securityfocus.com/archive/1/316773/2003-03-28/2003-04-03/0">
this posting by Michal Zalewski</a>
for more information).
</p>

<p>
Now, you could solve this by always using the
unambiguous type &#8220;unsigned char&#8221; if that&#8217;s what you intended,
and strictly speaking that&#8217;s what you should do.
However, it&#8217;s very painful to change existing code to do this.
And since many pre-existing
libraries expect &#8220;pointer to char&#8221;, you can end up with
tons of useless warning messages when you do that.
</p>

<p>
So what&#8217;s a <i>simple</i> solution here?
A simple answer is to force the compiler to <i>always</i>
make &#8220;char&#8221; an UNSIGNED char.
A portable program should work when a char is unsigned,
so this shouldn&#8217;t require any changes to that code.
Since programmers often make the assumption, let&#8217;s
make their assumption correct.
In the widely-popular gcc compiler, this is done with the &#8220;-funsigned-char&#8221;
option; many other C and C++ compilers have similar options.
What&#8217;s neat is that you don&#8217;t have to modify a line of source code;
you can just slip this option into your build system
(e.g., add this option to your makefile).
This is typically very trivial to do; typically you can just modify (or
set) the CFLAGS variable to add this option, and then recompile.
</p>

<p>
I also have more controversial advice.
Here it is: If you develop C or C++ compilers, or you&#8217;re a distributor
who distributes a C/C++ compiler&#8230; <i>make char unsigned by default
on all platforms</i>.
And if you&#8217;re a customer, demand that from your vendor.
This is just like similar efforts going on in operating system sales
to users;
today operating system vendors are changing their systems so that they
are &#8220;secure by default&#8221;.
At one time many vendors&#8217; operating systems were delivered with all
sorts of &#8220;convenient&#8221; options that made them easy to attack&#8230; but
getting subverted all the time turned out to be rather inconvenient to users.
In the same way, development tools&#8217; defaults should try to prevent
defects, or create an environment where defects are less likely.
Signed characters are basically a vulnerability waiting to happen,
portable programs shouldn&#8217;t depend on a particular choice, and non-portable
software can turn on the &#8220;less secure&#8221; option when necessary.
I doubt this advice will be taken, but I can suggest it!
</p>

<p>
Turning this option on does not save the universe; most vulnerabilities
will <i>not</i> be caught by turning on this little option.
In fact, by itself this is a <i>very</i> weak measure, simply because
by itself this doesn&#8217;t counter most vulnerabilities.
You need to know much more to write secure software;
to learn more, <a href="http://www.dwheeler.com/secure-programs">see
my free book on writing secure programs for Linux and Unix</a>.
But stick with me;
I think this is a small example of a much larger concept, which
I&#8217;ll call <i>no sharp edges</i>.
Chain saws are powerful &#8212; and dangerous &#8212; but no one puts
scissor blades next to the chain saw&#8217;s handle.
We try to make sure that &#8220;obvious&#8221; ways of using tools are not
dangerous, even if the tool itself can do dangerous things.
Yet the &#8220;obvious&#8221; ways to use many languages turn out to lead directly
to security vulnerabilities, and that needs to change.
You can&#8217;t prevent all misuse &#8212; a chain saw can be always be misused &#8212;
but you can at least make languages easy to use correctly
and likely to do only what was intended (and nothing else).
</p>
<p>
We need to design languages, and select tools and tool options, to
reduce the likelihood of a developer error becoming a security vulnerability.
By combining
compiler warning flags (like -Wall), defaults that are likely to avoid
dangerous mistakes (like -funsigned-char),
NoExec stacks, and many other approaches, we can
greatly reduce the likelihood of a mistake
turning into a security vulnerability.
The most important security measure you can take in developing secure
software is to <i>be paranoid</i> &#8212; and I still recommend paranoia.
Still, it&#8217;s hard to be perfect all the time.
Currently, a vast proportion of security vulnerabilities come from relatively
trivial implementation errors, ones that are easy to miss.
By combining a large number of approaches, each of which counter
a specific common mistake,
we can get rid of a vast number of today&#8217;s vulnerabilities.
And getting rid of a vast number of today&#8217;s vulnerabilities is
a very good idea.
</p>
<p>path: <a href="http://www.dwheeler.com/blog/security">/security</a> | <a href="http://www.dwheeler.com/blog">Current Weblog</a> | <a href="http://www.dwheeler.com/blog/2006/03/28#unsigned-char">permanent link to this entry</a></p>
<h1>Sun, 12 Mar 2006</h1>
<p><a name="random-quotes-and-code"><font size="+2"><b>Random Quotes and Code - Why You Need a Community</b></font></a></p><p></p>
<p>
You need a community, not just some dump of posted code,
if you want good open source software.
I can demonstrate this through my
trivial hunt for &#8220;random quote&#8221; code&#8230; so let me tell you my story.
</p>

<p>
I recently decided that I&#8217;d like the front page of my website to
show a randomly-selected quote.
For security reasons, I avoid using dynamically-run code on my own site,
so I needed to use Javascript (ECMAscript) to do this.
Easy enough, I thought&#8230; I&#8217;ll just use Google to find a program that did this,
and I searched on &#8220;random quotation Javascript&#8221;.
</p>

<p>
But what I found was that a lot of people don&#8217;t seem to care about
long-term maintenance, <i>or</i> correctness.
<a href="http://www.codelifter.com/main/javascript/randomquote.shtml">
Codelifter&#8217;s sample code by etLux</a> does the job, but also shows the
problem.  The code has a lot of statements like this:
<pre>
 Quotation[0] = "Time is of the essence! Comb your hair.";
 Quotation[1] = "Sanity is a golden apple with no shoelaces.";
 ...
</pre>
Does this work? Sure, but it&#8217;s <i>terrible</i> for maintenance.
Now you have to write extra code, unnecessarily maintain index numbers,
and if you want to delete a quote in the middle, you have to renumber things.
Even for tiny tasks like this, maintenance matters over time.
I&#8217;m going to use this for my personal website, which I plan to have
for decades; life is too short to fight hard-to-maintain code over a long time.
</p>
<p>
Even worse, this and many other examples did a lousy job of
picking a random quote.
Many sample programs picked the random quote using this kind of code
(where Q is the number of quotes):
<pre>
 var whichQuotation=Math.round(Math.random()*(Q-1));
</pre>
This actually doesn&#8217;t choose the values with equal probability.
To see why, walk through the logic if there are only 3 quotes.
Math.random returns a value between 0 and 1 (not including 1); if there
are 3 quotes, Math.random()*(Q-1) produces a floating point
value between 0 and 2 (not including 2).
Rounding a value between 0 and 0.5 (not including 0.5) produces 0, between 0.5
and 1.5 (not including 1.5) produces 1, and between 1.5 and 2 produces 2&#8230;.
which means that the middle quote is <i>far</i> more likely to be selected
(it will be selected 50% of the time, instead of the correct 33%).
The &#8220;round&#8221; operation is the wrong operator in this case!
</p>
<p>
I&#8217;m not really interested in picking on the author of this
code sample; LOTS of different sample codes do exactly the wrong thing.
</p>
<p>
The problem seems to be that once some code snippet gets posted,
in many places there&#8217;s no mechanism to discuss the code or to propose
an improvement.
In other words, there&#8217;s no community.
I noticed these problems immediately in several samples I saw, yet
there was no obvious way for me to do anything about it.
</p>
<p>
In the end, I ended up writing my own code. For your amusement,
here it is.
Perhaps there needs to be a &#8220;trivial SourceForge&#8221; for taking tiny
fragments like this and allowing community improvement.
</p>

<p>
First, I put this in the head section of my HTML:
</p>

<pre>
&lt;script language="JavaScript">
 // Code by David A. Wheeler; this trivial ECMAscript is public domain:
  var quotations = new Array(
    "Quote1",
    "Quote2",
    "Quote3"
  );
 var my_pick = Math.floor(quotations.length*Math.random());
 var random_quote = "Your random quote: &lt;i>" + quotations[my_pick] + "&lt;/i>";
&lt;/script>
</pre>

<p>
I then put this in the body section of my HTML:
</p>

<pre>
&lt;script language="JavaScript">
 document.write(random_quote)
&lt;/script>
</pre>

<p>
I intentionally didn&#8217;t include some defensive measures against bad
software libraries.
Unfortunately, many software libraries are terrible, and that certainly
includes many random number generators.
For example, Math.random() isn&#8217;t supposed to return 1, only values less
than that&#8230; but returning an (incorrect) 1 isn&#8217;t an unknown defect,
and that would cause an out-of-bounds error.
Also, many implementations of random() are notoriously bad; they
often have trivially tiny cycles, or fail even trivial randomness tests.
I would put defensive measures in software designed to be highly
reliable or secure (for example, I might re-implement the random function
to be sure I got a reasonable one).
But in this case, I thought it&#8217;d be better to just rely on the libraries&#8230;
if the results are terrible, then users might complain to the library
implementors.  And if the library implementors fix their implementations,
it helps <i>everyone</i>.
</p>

<p>
I donate the above snippet to the public domain. It&#8217;s not clear
at all that it can be copyrighted, actually; it&#8217;s far too trivial.
But it&#8217;s still useful to have such snippets, and I hope that someone will
organize a community for sharing and maintaining trivial snippets like this.
</p>
<p>path: <a href="http://www.dwheeler.com/blog/oss">/oss</a> | <a href="http://www.dwheeler.com/blog">Current Weblog</a> | <a href="http://www.dwheeler.com/blog/2006/03/12#random-quotes-and-code">permanent link to this entry</a></p>
<p><a name="al-wheeler"><font size="+2"><b>In memorium: William Alson (&#8220;Al&#8221;) Wheeler</b></font></a></p><p></p>
<p>
<img src="http://www.dwheeler.com/images/al_wheeler.jpg" width="334" height="398" align="right" alt="Picture of William Alson (Al) Wheeler" />
William Alson (&#8220;Al&#8221;) Wheeler (2 March 1916 – 28 February 2006)
was my grandfather and a wonderful, godly man.
He recently went to be with the Lord, and I will miss him very much
until my time comes.
Many of my own traits (love of music, math, science fiction, science,
and learning in general) are easily traced back to him.
He loved jokes and humor; he laughed often, and his eyes often twinkled.
</p>

<p>
He demonstrated his extraordinary character throughout his life;
a few anecdotes will have to suffice.
His love of learning was extraordinary; in his 80s he started
learning koine Greek, and near the time of his death
he was reading the 984-page &#8220;Chaos and Fractals: New Frontiers of Science&#8221;
(a book full of mathematical concepts).
He dedicated his life to serving others; he was a music minister for
over 45 years.
He prayed, and prayed often, for his family and friends.
When he last moved to Pennsylvania, he donated his mechanic&#8217;s tools to
the Smithsonian, which had been hunting for the kind of tools he had.
And even in death he served others; rather than having his body be buried,
he donated his body for medical research.
I am honored that I can count myself as one of his grandchildren.
He did not leave riches behind; he left behind something much greater.
</p>

<p>
Below is a short biography of his life, as printed at his memorial service
on March 4, 2006.
May we all strive to have such a positive biography.
&#8220;Children&#8217;s children are a crown to the aged,
and parents are the pride of their children.&#8221;
(Proverbs 17:6, (NIV)) (New International Version)
</p>

<hr />

<p>
Al was born in the small town of Pottsville, PA. He grew up in Oxford, PA and graduated in 1933 from Oxford High School. He worked a couple years with the Wheeler and Sautelle and then the Wheeler and Almond Circuses. In 1935 he moved to Reading, PA, and entered the Wyomissing Polytechnic Institute to become a machinist. He met his future wife, Mary Clouse, in 1938 at a YWCA sponsored dance. It was during this time that Al taught himself to play tennis and the clarinet. He came from a musical family and had begun singing with the Choral Society in Reading. He began working at the Textile Machineworks in Wyomissing, but found a job in early 1940 with the Federal Government at the Naval Gun Factory in Washington, DC. He married Mary in June 1940. While living in southeast Washington, they had three children: Bill, Ray and Joyce. In 1946, after a few interim jobs, he began working for the Bureau of Standards followed by the Naval Research Laboratory. He moved to the Bureau of Naval Weapons in 1957. He moved his family to Maryland in 1958. In 1962 he made his last career move to the Naval Air Systems Command and retired from there in 1974. Throughout this time he should have received a chauffeur’s license since he performed that duty extensively. Mary went home to be with the Lord in 1996. In 1999 he moved to Perkiomenville, PA to live with his daughter, son-in-law Phil, and grandson Bryan.
</p>

<p>
After moving to Washington the war initially kept him out of church activities. Mary had started attending Fountain Memorial Baptist Church. After the war, Al started attending also. They professed Christ as Lord and Savior and were baptized together. In 1950, Al started directing the Junior Choir, grades 4 thru 6. In 1953 he became the Music Director at Fountain Memorial and spent the next 45 years directing music in 6 different churches. He loved his music and his collection of choir music, mini orchestral scores, records, reel-to-reel tapes, cassette tapes, and CDs attest to it. His primary instrument was the clarinet, but he had obtained and played saxophone, flute, and trombone and, at one point, two synthesizers.
</p>

<p>
Although work and family responsibilities cut down on his tennis activities, he never gave them up completely. After retirement he taught tennis part time for the Maryland Department of Recreation and was regularly playing with his friends until moving back to PA in 1999.
</p>

<p>
He loved science fiction and one of his favorite pastimes was solving the “Word Power” article in the Reader’s Digest. He rarely missed those words.
</p>

<p>
Al had a wonderful life and was adored by all his family. He is survived by three children, five grandchildren, and 7 great-grandchildren.
</p>
<p>path: <a href="http://www.dwheeler.com/blog/misc">/misc</a> | <a href="http://www.dwheeler.com/blog">Current Weblog</a> | <a href="http://www.dwheeler.com/blog/2006/03/12#al-wheeler">permanent link to this entry</a></p>
<h1>Tue, 17 Jan 2006</h1>
<p><a name="gpl3-new-compatibilities"><font size="+2"><b>GPL v3: New compatibilities, with potentially profound impacts</b></font></a></p><p></p>
<p>
Finally, there&#8217;s a draft version of the GNU General Public License (GPL)
version 3.  Lots of people have looked at it, and commented on it
in general, so I won&#8217;t try to cover the whole thing in detail.
(<a href="http://www.groklaw.net/article.php?story=20060117122603204">
Groklaw covers the differences between version 2 and 3, for example.</a>)
A few highlights are worth noting, in particular,
it&#8217;s surprisingly conservative.
This GPL draft changes much less in the license than many expected,
and the changes were long expected.
As expected, it continues to combat software patents;
it has more clauses about that, but at first blush its built-in
&#8220;aggression retaliation clause&#8221; is surprisingly narrow.
It counters digital restrictions management (like Sony&#8217;s ham-handed
attacks on customers in 2005), but that is unsurprising too.
It&#8217;s longer, but primarily because it defines previously undefined terms
to prevent misunderstanding, so that is a <i>good</i> thing.
</p>
<p>
What has <i>not</i> gotten a lot of press yet &#8212; and should &#8212; is
that <b>the new GPL will make it <i>much easier</i>
to combine software from different sources</b> to create new products.
This could result in <i>many</i> more
free-libre/open source software (FLOSS) programs being developed, and
might have very profound impacts.
</p>
<p>
A key reason that FLOSS programs have become such a powerful economic
force is because it&#8217;s easy to combine many different pieces together
quickly into a larger solution, without requiring large sums of money
to get use rights, and anything can be modified arbitrarily.
As more people find use for FLOSS programs, a small percentage end up
making improvements (to help themselves), and contribute them to the projects
(typically so they can avoid the costs of self-maintenance).
After it reaches a critical mass, this can snowball into a program becoming
a dominant force in its niche; it&#8217;s hard to compete against a program
used by millions and supported by thousands of developers, even if you
have an unlimited budget.
</p>
<p>
But this snowballing effect only works if you can combine pieces together
and modify them in new, innovative ways.
As I noted in my essay
<a href="http://www.dwheeler.com/essays/gpl-compatible.html">
Make Your Open Source Software GPL-Compatible. Or Else</a>,
it is a <i>serious</i> problem when free-libre/open source software (FLOSS)
is released that isn&#8217;t GPL-compatible.
Since most FLOSS software <i>is</i> released under the GPL, a program that
is <i>not</i> compatible with this dominant license
creates situations where the same software has to be written twice, for
no good reason.
Most people have heeded that advice, but for various reasons not all.
There&#8217;s been a related effort to reduce the number of licenses accepted
(or at least recommended) by the OSI, for the same basic reason:
license incompatibilities create trouble if you want to combine software
components together.
</p>
<p>
The new GPL text addresses this by allowing a few specific restrictions
to be added, such as requiring renaming if you make your own version, or
forbidding the use of certain names as endorsements.
Two licenses in particular that were incompatible with GPL version 2 &#8212;
but <i>appear</i> to be compatible with GPL version 3 draft 1 &#8212;
are the
<a href="http://www.php.net/license/3_01.txt">PHP 3.01 license</a>, used
by the widely-used PHP language and libraries, and the
<a href="http://www.apache.org/licenses/LICENSE-2.0">Apache License version 2.0</a>, used by not only the #1 web server Apache but also by a variety
of other web infrastructure and Java components.
Both of these licenses include limits on how you can use certain names,
for example, and these limitations are acceptable in GPL version 3 draft 1.
No one has had a chance to do an in-depth analysis, yet, and there are
more drafts to come&#8230; but the current direction looks promising.
</p>
<p>
All is not perfect, of course.  One license that causes many problems is the
<a href="http://www.sdisw.com/openssl.htm">OpenSSL</a> license;
it has variations of the old &#8220;obnoxious advertizing clause&#8221; license
that have been thorns in the side of many for years.
I think it&#8217;s unlikely that this would get changed; such clauses can
really harm many FLOSS-based businesses (they can&#8217;t afford to put
10,000 names on every piece of advertisement).
The GPL isn&#8217;t compatible with proprietary software licenses, either,
but that is by design; the whole purpose of the GPL is to allow
software to be shared between users.
</p>
<p>
In any case, this looks like a good start, and will probably mean
that many more people will be able to use (and create)
FLOSS programs in the future.
</p>
<p>path: <a href="http://www.dwheeler.com/blog/oss">/oss</a> | <a href="http://www.dwheeler.com/blog">Current Weblog</a> | <a href="http://www.dwheeler.com/blog/2006/01/17#gpl3-new-compatibilities">permanent link to this entry</a></p>
<h1>Mon, 16 Jan 2006</h1>
<p><a name="python-typechecking"><font size="+2"><b>Python Typechecking: The Typecheck module</b></font></a></p><p></p>
<p>
About a year ago I started creating a Python library to support
better typechecking in <a href="http://python.org/">Python</a>.
Python is a fun language, but errors often hide <i>far</i> longer than
they would in other languages, making debugging more necessary and
painful than it needs to be.
A trivial typo in a field setting cannot be caught by Python, for example,
and type errors are never caught until an attempt is made to USE the value&#8230;
which may be far, far later in the program.
I really miss the ability of other languages to automatically check
types, so that mistakes can be identified more directly.
But I never got around to finishing my typechecking module for
Python - there were just too many other things to do.
Which is just as well, because someone else has done a better job.
</p>
<p>
<a href="http://oakwinter.com/code/typecheck/">Typecheck</a>
is a type-checking module for Python
by Collin Winter and Iain Lowe; it
&#8220;provides run-time typechecking facilities for Python
functions, methods and generators.&#8221;
Their typecheck module provides many more useful capabilities than my
<a href="http://www.dwheeler.com/typecheck/typecheck.py">early
typecheck module</a>.
In particular, they handle variable-length parameter lists and other
goodies.
These capabilities, like the assert statement, make it much easier to
detect problems early&#8230; and the earlier you can detect problems, the
easier it is to figure out why the problem happened.
</p>
<p>
The biggest trouble with the current verison of typecheck is
that it isn&#8217;t easy to specify the <i>right</i> types.
Since Python hasn&#8217;t had typechecking before, it doesn&#8217;t have
built-in names for the types that you actually want to check against.
For example, conceptually int (integer), long (arbitrarily long integer),
and float are all subclasses of another type named &#8220;Number&#8221;&#8230; but
there isn&#8217;t actually a type named Number to compare against (or
inherit from, or implement as an interface).
The same is true for &#8220;Sequence&#8221;&#8230; the Python documentation is full
of discussions about Sequence, but these are merely conceptual, not
something actually in the language itself.
Even in cases where there <i>is</i> a type, such as &#8220;basestring&#8221; meaning
&#8220;any string-like type&#8221;, is a type not known about by many Python
developers.
</p>
<p>
Typechecking only works when people actually specify the right types,
of course.
If you are too restrictive (&#8220;I want only &#8216;int&#8217;&#8221; when any number will do),
then typechecking is a problem not a help.
Hopefully the typecheck implementors
will find a way to define the types that people need.
In my mind, what&#8217;s needed is a way to define an Interface (a list of
required methods) that has an efficient implementation
(e.g., it&#8217;s a singleton that caches the types that match).
Then they can define critical types using the Interface type.
</p>
<p>
I look forward to using the typecheck module, once they add enough
type definitions to use it well!
</p>
<p>path: <a href="http://www.dwheeler.com/blog/oss">/oss</a> | <a href="http://www.dwheeler.com/blog">Current Weblog</a> | <a href="http://www.dwheeler.com/blog/2006/01/16#python-typechecking">permanent link to this entry</a></p>
</body></html>