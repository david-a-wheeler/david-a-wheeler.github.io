<!doctype html system>
<html lang="en-US">
<head>
<title>The Most Important Software Innovations</title>
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<link rel="stylesheet" type="text/css" href="paper.css">
<meta http-equiv="author" content="David A. Wheeler">
<meta http-equiv="copyright" content="2001-2019">
<!-- "scaleability" and "scalability" are alternative spellings. -->
<meta name="keywords" content="software, innovation, innovations, software innovation, software innovations, invention, inventions, software inventions, important, most important, critical, key, patent, patents, history, historical, history of computing, history of computers, history of software, computing history, computer history, Internet history, history of Internet, David A. Wheeler, David Wheeler, hardware, program, programs, software technology, Microsoft, open source, open-source, open source software, Free Software, Tim Berners-Lee, inventor, World Wide Web, TCP/IP, Internet">
<meta name="description" content="This paper identifies the most important innovations in software, removing hardware advances and products that didn't embody significant new software innovations.  Its results may surprise you.">
<meta name="subject" content="This paper identifies the most important innovations in software, removing hardware advances and products that didn't embody significant new software innovations.  Its results may surprise you.">
</head>

<body bgcolor="#FFFFFF">

<h1 class="title">The Most Important Software Innovations</h1>
<h2 class="author">David A. Wheeler</h2>
<h2 class="date">Revised 2021-12-09; First version 2001-08-01</h2>

<h1><a name="introduction">Introduction</a></h1>
<p>
Too many people confuse software innovations with other factors, such as
the increasing speed of computer and network hardware.
This paper tries to end the confusion by
identifying the most important innovations in software, removing
hardware advances and products that didn&#8217;t embody significant
new software innovations.
This paper presents its <a href="#criteria">criteria for the most important
software innovations</a> and <a href="#sources">sources</a>, the
<a href="#innovations">software innovations themselves</a>,
discusses
<a href="#patents">software patents</a> and
<a href="#notinnovation">what&#8217;s not an important software innovation</a>, and
then closes with
<a href="#conclusions">Conclusions</a>.

<p>
The results may surprise you.

<h1><a name="criteria">Criteria</a></h1>

This paper lists the &#8220;most important software innovations,&#8221;
so we first need to clarify what each of those words mean:
<ol>
<li>
To be a &#8220;most important&#8221; innovation, an innovation has to be an
idea that is very widely used and
is critically important where it applies.
Innovations that are only used by a very small proportion of software
(or software users) aren&#8217;t included.
<!--
I haven't included the Fast Fourier Transform (FFT), for example.
The FFT is very important, but only for a very specific field of endeavor -
almost all software does&nbsp; <i>not</i> use the FFT.
-->

<p>
<li>
To be a &#8220;software&#8221; innovation,
it has to be a technological innovation that
impacts how computers are programmed
(e.g., an approach to programming or an innovative way to use a computer).
<p>
I&#8217;m intentionally omitting computer hardware innovations or
major hardware events that don&#8217;t involve software innovation.
People seem to confuse hardware and software, so by intentionally not
including hardware, we get a different (and interesting) picture
we do not see otherwise.
For example,
a court case in 1973 determined that
<a href="http://www.cs.iastate.edu/jva/court-papers/">John Vincent Atanasoff
is the legal inventor of the electronic digital computer</a>, but
that&#8217;s a hardware innovation and thus not included.
<!-- Decision rendered by Judge Larson in 1973, as part of
Honeywell vs. Sperry-Rand.
See discussion in
Robert Slaters "Portraits in Silicon", chapter
titled "The inventor of the digital computer - by law!".  -->
I&#8217;ve omitted other strictly hardware innovations such as the
transistor (1947) and integrated circuits (1958).
I&#8217;ve also omitted Ethernet, which Bob Metcalfe developed in 1973, for
the same reason (Ethernet used early Internet Protocol software,
so the software ideas already existed.)
<!--
1973 Local Area Networks
Bob Metcalfe invents the
http://inventors.about.com/science/inventors/library/weekly/aa111598.htm
Ethernet (in his PhD thesis),
with critical refinements occurring through 1976.
It can be argued that this was really a product, not an invention.
In any case Ethernet is a hardware invention, not a software
invention; the software used was borrowed from the Internet Protocol.
-->
<p>
I&#8217;ve omitted inventions that aren&#8217;t really technological inventions
(e.g., social or legal innovations), even if they are important for
software technology and/or are widespread.
For example, the concept of a
<a href="http://www.gnu.org/copyleft/copyleft.html">copylefting</a> license
is an innovative software licensing approach that permits modification while
forbidding the software from becoming proprietary;
it is used by a vast array of software via the
<a href="http://www.gnu.org/copyleft/gpl.html">General Public License (GPL)</a>.
The first real copylefting license
(the <a href="http://www.dsl.org/copyleft/timeline">Emacs Public License</a>)
was developed by Richard Stallman in 1985 -
but since copyleft is really a social and legal invention, not
a technological one, it&#8217;s not included in this list.
<!--
Here's history of the GPL:
 See also http://www.free-soft.org/gpl_history/
 and http://www.dsl.org/copyleft/timeline/
It appears that the Don Hopkins in 1983 coined the phrase "copyleft",
which Stallman later used to define the approach.
The first real license with the concept, created by
Richard Stallman, was the
<a href="http://www.dsl.org/copyleft/timeline">"Emacs Public License"
in 1985</a>.
The GPL, an generalization of the Emacs Public License,
was first announced in June 1988 in a bulletin:
  http://www.gnu.org/bulletins
<a href="http://www.gnu.org/copyleft/copying-1.0.html">the first formally
published version of the GPL was relesed on February 1989</a>
GPL version 2 came out June 1991.
-->
Also, the &#8220;smiley&#8221; marker <tt>:-)</tt> is not included -
it&#8217;s certainly widespread, but it&#8217;s
a social invention not a technological one.
<!--
One set of research found that the
<a href="http://research.microsoft.com/~mbj/Smiley/Smiley.html">
"smiley" marker <tt>:-)</tt> was invented by
Scott E. Fahlman at Carnegie Mellon University (CMU)
on September 19, 1982</a>.
More recent research at:
 http://listserv.linguistlist.org/cgi-bin/wa?A2=ind
From: Bapopik@AOL.COM
Subject: Smiley (March 1953)
Comments: To: ADS-L@LISTSERV.UGA.EDU
Content-Type: text/plain; charset=ISO-8859-1

This continues discussion of the pictograph known as the "smiley."
It's authorship was credited to the late Harvey Ball
(who drew it in the 1960s).
"Smiley" is in an ad in the NEW YORK HERALD TRIBUNE, 10 March 1953,
pg. 20, cols. 4-6. See for yourself. The ad is for the film LILI,
with the "delightful" Leslie Caron.
The "World Premiere Today" is at the Trans-Lux 52nd on Lexington.
The film opened nationwide, and this ad possibly ran in many newspapers.

Today

You'll laugh :)
You'll cry :(
You'll love (Heart-shaped face-ed.)
_Lili_ 
-->

<p>
<li>
We also have to define &#8220;innovation&#8221; carefully.
For purposes of this paper,
an &#8220;innovation&#8221; is not simply combining two functions into a single
product - that&#8217;s &#8220;integration&#8221;
and usually doesn&#8217;t require any significant innovation (just hard work).
In particular, integrating functions to prevent customers from using
a competitor&#8217;s product is &#8220;predation,&#8221;
not &#8220;innovation.&#8221;
An &#8220;innovation&#8221; is not a product, either,
although a product may embody or contain innovations.
Re-implementing a product so that it does the same thing on a different
computer or operating system isn&#8217;t an innovation, either.
An innovation is a&nbsp; <i>new idea</i>.
And in this paper, what&#8217;s meant is an important new idea
in <i>software technology</i>.
</ol>

<p>
As a result, you may be surprised by the number of events in computing history
that are <i>not</i> on this list.
Most software products are not software innovations by themselves,
since most products are simply re-implementations of another idea.
For example, WordStar was the first microprocessor word processor,
but it wasn&#8217;t the first word processor - WordStar was
simply a re-implementation of a previous product on a different computer.
Later word processors (such as Word Perfect and Microsoft Word)
were later re-implementations by other vendors, not innovations themselves.
Some major events in computing are simply product announcements of hardware,
and have nothing to do with innovations in software.
Thus, while the IBM PC and Apple ][&#8217;s appearances were important to the
computing world, they didn&#8217;t represent an innovation in software - they
were simply lower-cost hardware, with some software written for them using
techniques already well-known at the time.

<p>
Occasionally a product is the first appearance of an innovation
(e.g., the first spreadsheet program), in which case the date of the
product&#8217;s release is the date when the idea was announced to the public.
Some innovations are innovative techniques, which aren&#8217;t directly
visible to software users but have an extraordinary effect on software
development (e.g., subroutines and object-orientation) - and these&nbsp;
<i>are</i> included in this list of software innovations.
For the more debatable entries, I&#8217;ve tried to discuss why I believe they
should be included.

<p>
Matt Ridley's <i>How Innovation Works</i> (and Why it Flourishes in Freedom)
(2020)
is an interesting book on invention and innovation.
Ridley defines innovation as "finding new ways to apply energy to
create improbable things, and to see them catch on... developing
an invention to the point where it catches on because it is
sufficiently practical, affordable, reliable and ubiquitous to be
worth using... [and] applying new ideas to raising living standards" (page 4).
Ridley walks through inventions and innovations throughout history,
From that point of this, this article focuses on key software inventions
that were applied and thus were important innovations.
Ridley walks through history noting various innovations (his term), and
notes that "most innovation is a gradual process" (page 9 and 240+).
He also argues that the "people who find ways to drive down the costs and
simplify the product who make the biggest difference" (page 246),
not just whoever came up with an idea.
He argues that "the main ingredient [for] innovation is freedom"
(page 359).
His terminology is not identical to this papers', but I think the
connections are clear enough.

<p>
I&#8217;ve tried to identify and date the earliest
public announcement of an idea,
rather than its embodiment in some product.
The first implementation and first widespread implementation are often
noted as well.
&#8220;Public&#8221; in this case means, at least, an announcement to a wide
inter-organizational audience.
In some cases identifying a specific date or event
is difficult; I welcome references to earlier works.
For example, sometimes it is difficult to identify
a &#8220;first&#8221; because an idea
forms gradually through the actions of many.



<h1><a name="sources">Sources</a></h1>

<p>
Since I haven&#8217;t found some sort of consensus of what the most important
computing innovations are, I&#8217;ve developed this list by
selecting events from many other sources.
I used many sources so I wouldn&#8217;t miss anything important, in particular,
I used
<a href="http://www.computer.org/50/history">IEEE Computer&#8217;s historical
information</a>
(including their
<a href="http://www.computer.org/computer/timeline">50-year timeline)</a>,
the
<a href="http://vlmp.museophile.com/computing.html">Virtual Museum
of Computing</a>,
<a href="http://info.isoc.org/guest/zakon/Internet/History/HIT.html">Hobbes&#8217;
Internet Timeline</a>,
Paul E. Ceruzzi&#8217;s <i>A History of Modern Computing</i>, and
John Naughton&#8217;s&nbsp; <i>A Brief History of the Future</i>.
I also used Janet Abbate&#8217;s&nbsp; <i>Inventing the Internet</i> in a few
cases, but I tried to double-check everything in that source because
(unfortunately) Abbate makes several errors
that make its use as a source suspect.
For example, Abbate (page 22)
doesn&#8217;t realize that although both
Strachey and John McCarthy used the same 
word (&#8220;time-sharing&#8221;) for their ideas,
they didn&#8217;t mean the same thing at all.
Also, Abbate (page 201) claims Steve Bellovin was at Duke, but
this is wrong.
<!-- I found Naughton to be more reliable than Abbate.. Abbate makes
     several mistakes, which is a serious problem!  See my comments below. -->
I&#8217;ve also examined other sources, such as James Durham&#8217;s
<a href="http://www-106.ibm.com/developerworks/java/library/co-tmline/index.html">History-Making Components</a> and
<a href="http://www.async.caltech.edu/~kp/history/">A History and
Future of Computing</a>.
<!-- History-Making Components wasn't very helpful for this purpose as
  of May 9, 2001.  It's almost exclusively focused on the later events
  (where something was widely deployed) and hardware,
  and not so much on the date of the original innovation. -->
Note that, in general, these sources mix computer hardware and software
together.
Another source is the
<a href="http://www.sdm.de/conf2001/index_e.htm">&#8220;Software Pioneers&#8221;</a>
conference (June 28-29, 2001, Bonn) sponsored by
Software Design and Management.
Many specific sources such as
<a href="http://www.groklaw.net/article.php?story=20050219170121955">
&#8220;OSI and TCP: A History&#8221; by Peter H. Salus</a> were checked too.
The
<a href="http://awards.acm.org/software_system/">
Association for Computing Machinery (ACM) Software Systems Award</a>
was helpful, but this rewards the developers of influential software systems;
the recipients are certainly worthy, but in many cases the influential
software systems represent good engineering and refinement of already-existing
ideas, instead of being the first implementation of a new idea themselves.
As is discussed futher later, we need to distinguish between
<i>innovations</i> and <i>important products</i>; a product can be
important or useful without being innovative.
<p>
If you find computing history interesting, you might also enjoy the
<a href="http://www.google.com/googlegroups/archive_announce_20.html">
20 Year Usenet Timeline</a>, a
<a href="http://www.catb.org/~esr/writings/cathedral-bazaar/">
Brief History of Hackerdom</a>, and
<a href="http://www.landley.net/history/mirror/index.html">
Landley&#8217;s Computer history page</a>,
though they aren&#8217;t significant sources for the material here.

<p>
After I started identifying innovations, many asked me about software patents.
I have done what I can to find applicable patents, though the
problems are legion.
Software patents are often incomprehensible, even by software experts.
Search systems cannot find all relevant software patents;
unlike drugs, there is no good indexing system, either for software patents or
for software ideas in general (different words can be used for the same idea).
This inability to find patents causes many other problems.
Software patents are often granted for prior art,
even though they are not supposed to be.
Indeed, someone else can hear of an idea (possibly years later),
file a software patent, and the patent office is likely to grant it.
The patent office may even grant a software patent on something already
patented.

<p>
Yet if the real question about software patents is,
"do patents provide an incentive to innovate
in software", then things can be simplified.
If that were true, it is reasonable to presume that
(a) the innovator (or his company) would file the patent,
(b) that it would have a form corresponding to the original innovation, and
(c) he would file within the legal grace period
(12 months from date of public knowledge).
Also, patents generally were not granted on software before 1980.
My thanks to Jim Bessen for these insights.
These factors make patent searching far more tractable, e.g., using
<a href="http://www.google.com/advanced_patent_search">
Google's advanced patent search</a>.
My thanks to many, including Jim Bessen, for searching for
patents on these key innovations to find relevant patents.
Where found, this article identifies the US patent number.
If no patent has been identified, that means that people have looked
but not found a plausibly-valid patent for it.
The <a href="#patents">section on software patents</a> discusses this further.

<p>
Since this paper was originally published, I&#8217;ve received several
additional suggestions which rounded out this paper.
My thanks to those who have provided those suggestions.
It&#8217;s quite possible this paper is still missing some important
innovations; please
<a href="http://dwheeler.com/contactme.html">contact me
if you have a correction or addition
(dwheeler, at dwheeler.com, no spam please)</a>.
<!--
Someday I should also examine this:
http://ox.compsoc.net/~swhite/history/timelines.html
-->



<h1><a name="innovations">The Most Important Software Innovations</a></h1>

Here is a list of the most important software innovations:

<table border="border" nosave="nosave">
<thead>
<tr><th>Year</th><th>Innovation</th><th>Comments</th></tr>
</thead>
<tbody>

<tr><td>1837</td><td>Software (Babbage&#8217;s Analytical Engine)</td><td>
Charles Babbage was an eminent scientist;
he was <a href="http://www.fourmilab.ch/babbage">
elected Lucasian Professor of Mathematics at Cambridge in 1828</a>
(the same chair held by Isaac Newton and Stephen Hawking).
In 1837 he publicly described an <i>analytical engine</i>, a mechanical device
that would take instructions from a program
instead of being designed to do only one task.
Babbage had apparently been thinking about the problem for some time before
this; as with many innovations, pinning down a single date is difficult.
This appears to be the first time the concept of software
(computing instructions for a mechanical device) is seriously contemplated.
Babbage even notes that the instructions can be reused
(a key concept in how today&#8217;s software works).
In 1842 Ada Augusta, Countess of Lovelace, released a translation of
&#8220;Sketch of the Analytical Engine&#8221; with extensive commentary of her own.
That commentary has a clear description of computer architecture and
programming that is quite recognizable today,
and Ada is often credited as being the &#8220;first computer programmer&#8221;.
Unfortunately, due to many factors the Analytical Engine was never built
in Babbage&#8217;s lifetime, and it would be many years before general-purpose
computers were built.
No patent identified.
</td></tr>

<tr><td>1845</td><td>Boolean Algebra</td><td>
George Boole published &#8220;An Investigation of the Laws of Thought&#8221;.
His system for symbolic and logical reasoning became the basis of computing.
No patent identified.
</td></tr>

<tr><td>1936-37</td><td>Turing Machines</td><td>
Alan Turing wrote his paper
&#8220;On computable numbers, with an application to the Entscheidungsproblem&#8221;,
where he first describes
<a href="http://www.turing.org.uk/turing/scrapbook/machine.html">
Turing Machines</a>.
This mathematical construct showed the strengths - and fundamental
limitations - of computer software.
For example, it showed that there were some kinds of problems that
could not be solved.
No patent identified.
</td></tr>

<tr><td>1945</td><td>Stored program</td><td>
In the &#8220;First Draft of a Report on the EDVAC&#8221;, the concept of storing
a program in the same memory as data was described by John von Neumann.
This is a fundamental concept for software manipulation
that all software development is based on.
Eckert, Mauchly, and Konrad Zuse have all claimed prior invention,
but this is uncertain and this draft document is the one that spurred its use.
<a href="http://www.turing.org.uk/turing/scrapbook/computer.html">
Alan Turing published his own independent conception</a>, but went further in
showing that computers could be used for the logical manipulation
of symbols of any kind.
The approach was first implemented (in a race) by the prototype Mark I computer
at Manchester in 1948.
<!-- http://www.man.ac.uk/Science_Engineering/CHSTM/teaching/hs228_41.htm -->
No patent identified.
</td></tr>

<tr><td>1945</td><td>Hypertext</td><td>
Hypertext was first described in Vannevar Bush&#8217;s
<a href="http://www.w3.org/History/1945/vbush/vbush-all.shtml">&#8220;As we may think&#8221;</a>, though of course
it was heavily
influenced by previous library-related work (e.g., the work of
<a href="http://en.wikipedia.org/wiki/Paul_Otlet">Paul Otlet</a>).
The word
<a href="http://www.w3.org/History.html">
&#8220;hypertext&#8221; itself was later coined by Ted Nelson</a> in his
1965 article
<i>A File Structure for the Complex, the Changing,
and the Indeterminate</i> (20th National Conference, New York,
Association for Computing Machinery).
No patent identified.
</td></tr>

<tr><td>1951</td><td>Subroutines</td><td>
Maurice Wilkes, Stanley Gill, and David Wheeler (not me)
developed the concept of subroutines in programs
to create re-usable modules and began formalizing the concept
of software development.
<!-- they produced the first textbook on
  "The Preparation of Programs for an Electronic Digital Computer",
 (Addison-Wesley Publ. Co., New York, 1951).
http://www.computer.org/history/development/1951.htm
-->
No patent identified.
</td></tr>

<tr><td>1952</td><td>Assemblers</td><td>
Alick E. Glennie wrote &#8220;Autocoder&#8221;, which translated symbolic statements
into machine language for the Manchester Mark I computer. 
Autocoding later came to be a generic term for assembly language programming.
<!-- http://www.instantweb.com/foldoc/foldoc.cgi?AUTOCODER -->
No patent identified.
</td></tr>

<tr><td>1952</td><td>Compilers</td><td>
Grace Murray Hopper described techniques to select (compile) pre-written code
segments in correspondence with codes written in a high level language, i.e.,
a compiler.
Her 1952 paper is titled &#8220;The Education of a Computer&#8221;
(Proc. ACM Conference), and is reprinted in the&nbsp;
<i>Annals of the History of Computing</i> (Vol. 9, No. 3-4, pp. 271-281),
based on her 1951-1952 effort to develop A-0.
She was later instrumental in developing COBOL.
A predecessor of the compiler concept was developed by Betty Holberton in 1951,
who created a &#8220;sort-merge generator&#8221;.
<!-- http://www.computer.org/history/development/1952.htm -->
<!-- ??? 1949: John Mauchly's "Short Order Code" is the first
  known concept of a language.
  See http://www.computer.org/computer/timeline/ -->
No patent identified.
</td></tr>

<tr><td>1954</td><td>Practically Compiling Human-like Notation (FORTRAN)</td><td>

John Backus proposed the development of a
programming language that would allow
users to express their programs directly in commonly understood
mathematical notation.
The result was
<a href="http://www.computer.org/history/development/1954.htm">Fortran</a>.
The first Fortran implementation was completed in 1957.
There were a few compilers before this point; languages such as A-0, A-1,
and A-2 inserted subroutines, the Whirlwind I included a
special-purpose program for solving equations (but couldn&#8217;t be used for
general-purpose programming),
and an &#8220;interpreter&#8221; for the IBM 701 named Speedcoding had been developed.
However, Fortran used notation far more similar to human notation, and
its developers developed many techniques so that, for the first time,
a compiler could create highly optimized code
[Ceruzzi 1998, 85].
No patent identified.
</td></tr>

<tr><td>1955</td><td>Stack Principle</td><td>
Frierich L. Bauer and Klaus Samelson developed the
<!-- old http://wwwbib.informatik.tu-muenchen.de/Fak_Schriften/Fak_Schrift_97/engl.Version/nobelpreis_eng.html -->
<a href="http://www-sst.informatik.tu-cottbus.de/~db/doc/People/Broy/Software-Pioneers/Bauer_new.pdf">&#8220;stack principle&#8221;</a>
(&#8220;the operation postponed last is carried out first&#8221;)
at the Technische Universit&auml;t M&uuml;nchen.
This served as the basis for compiler construction, and was naturally
extended to all bracketed operation structures and all
bracketed data structures.
<!-- See http://wwwbib.informatik.tu-muenchen.de/Fak_Schriften/Fak_Schrift_99/kapitel3.html -->
No patent identified.
</td></tr>

<tr><td>1957</td><td>Time-sharing</td><td>
In Fall 1957 John McCarthy (MIT, US) began proposing
time-sharing operating systems,
where multiple users could share a single computer (and each believes
they control an entire computer).
On January 1, 1959, he wrote
<a href="http://www-formal.stanford.edu/jmc/history/timesharing-memo.html">a
memo to Professor Philip Morse</a> proposing that this be done for an
upcoming machine.
This idea caused immense excitement in the computing field.
It&#8217;s worth noting that
Christopher Strachey (National Research Development Corporation, UK)
published a paper on &#8220;time-sharing&#8221; in 1959, but his notion of the term
was having&nbsp;
<i>programs</i> share a computer, not that&nbsp;
<i>users</i> would
share a computer
(programs had already been sharing computers, e.g., in the SAGE project).
[Naughton 2000, 73]
<!-- Abbate 1999, page 22 - but Abbate doesn't realize that Strachey's
     approach, while using the same words, is not the same thing.  -->
<!-- http://www-formal.stanford.edu/jmc/history/timesharing/timesharing.html -->
By November 1961 Fernando Corbat&oacute; (also at MIT) had a four-terminal
system working on an IBM 709 mainframe.
Soon afterwards
<a href="http://www.multicians.org/thvv/7094.html">
CTSS (Compatible Time Sharing System)</a> was running,
the first effective time-sharing system.
<!-- http://www.computer.org/history/development/1961.htm -->
Even in those systems of today which aren&#8217;t shared by different users,
these mechanisms are a critical support for computer security.
No patent identified.
</td></tr>

<tr><td>1958-1960</td><td>List Processing (LISP)</td><td>
McCarthy (at Stanford) developed the LISP programming language for
supporting list processing; it continues to be critical for
Artificial Intelligence and related work, and is still widely used.
List processing was not completely new at this point;
at the 1956 Dartmouth Summer Research Project on Artificial Intelligence,
Newell, Shaw and Simon described IPL 2, a list processing language
for Rand Corporation&#8217;s JOHNNIAC computer.
However, McCarthy realized that a program could itself be
represented as a list, refining the approach into a flexible system
fundamentally based on list processing.
In 1956-1958 he began thinking about what would be needed for list
processing, with significant
work beginning in 1958 with hand simulated compilations.
<!-- http://www.man.ac.uk/Science_Engineering/CHSTM/teaching/hs228_b1.htm -->
<!-- http://www-formal.stanford.edu/jmc/history/lisp/lisp.html -->
LISP demonstrated other important innovations used in many later languages,
including polymorphism and unlimited-extent data structures.
No patent identified.
</td></tr>


<tr><td>1959-1960</td><td>Automatic memory management (garbage collection)</td><td>
One of the key capabilities originally developed in Lisp
is automatic memory management aka automatic garbage collection.
This capability is now included in most programming languages,
including JavaScript, Java, Python, and others.
This was developed in 1959, and is described in John McCarthy's paper
<a href="https://dl.acm.org/citation.cfm?id=367177.367199"
>"Recursive functions of symbolic expressions and their computation by machine, Part I", <i>Communications of the ACM</i>, Volume 3, Issue 4, April 1960, pp.184-195</a>.

<tr><td>1959-1960</td><td>Vendor-Independent Exchange Standards for Software (COBOL and ASCII)</td><td>
In the early days of computing every vendor had their own incompatible
method for creating programs and storing data.
IBM, for example, encoded characters using systems such as BCD and EBCDIC.
But this created terrible problems for users, who could not easily
exchange information and were kept hostage by the various vendors.
Thus, vendor-independent exchange standards began to be developed.
The solution was to create vendor-independent exchange standards.
<p>
The basic idea of creating standards was not new, even then.
But creating standards for something ephemeral like software was new,
so vendor-independent exchange standards for software
are being counted as an innovation.
Such standards are critical; standards
finally made it possible for users to choose and change their suppliers, and
since they could work together even with different suppliers.
(Even today,
<a href="http://www.antipatterns.com/vendorlockin.htm">
people fail to understand the need for standards and thus
fall victim to vendor lock-in.</a>)
Two of the first efforts to create such standards were COBOL (for
exchanging programs) and ASCII (for exchanging text).
<p>
In 1959, an industry-wide team was assembled to formulate a
standardized business programming language,
<a href="http://www.legacyj.com/cobol/cobol_history.html">
Common Business Oriented Language (COBOL)</a>.
The initial specification was presented in April 1960, and was
developed in cooperation with
computer manufactures, users (including the U.S. Department of Defense)
and universities.
Soon afterwards, in May 1962, a committee began developing a standard
for the
<a href="http://www.ibiblio.org/pub/languages/fortran/ch1-1.html">
Fortran language</a>.
<p>
<a href="http://en.wikipedia.org/wiki/ASCII">
American Standard Code for Information Interchange (ASCII)</a> is a way
of encoding characters as numbers, so that there is a standard number to
represent each character of text.
Work on ASCII began in 1960, and it was first published in 1963.
For many years ASCII competed with the vendor-specific EBCDIC, but
eventually the open vendor-neutral ASCII beat
the vendor-specific format (a pattern that often repeated over the years) .
No patent identified.
</td></tr>

<tr><td>1960</td><td>Packet-Switching Networks</td><td>
In 1960 Paul Baran (RAND) proposed a message switching system that
could forward messages over multiple paths.
Unlike previous approaches (which required large storage capacities at
each node), his approach used higher transmission speeds, so each node could
be small, simple, and cheap.
Baran&#8217;s approach routed messages to their destination instead of
broadcasting them to all, and these routing decisions were made locally.
In 1961 Leonard Kleinrock (MIT)
published &#8220;Information Flow in Large Communication Nets,&#8221; the
first larger work examining and defining packet-switching theory.
In 1964 Paul Baran wrote a series of papers titled
<a href="http://www.rand.org/publications/RM/baran.list.html">
&#8220;On Distributed Communications Networks&#8221;</a> that expanded on this idea.
<!-- The first volume was published as "On Distributed Communications
     Networks", Transactions on Software Engineering (TSE), 12:1-9. -->
This series described how to implement a distributed
packet-switching network with no single outage point
(so it could be survivable).
In 1966 Donald Davies (NPL, UK) publicly presented his ideas, which he termed
&#8220;packet switching&#8221;, and learned that Baran had already invented the idea
(though we still use Davies&#8217; term &#8220;packet switching&#8221;).
Davies started the &#8220;Mark I&#8221; project in 1967 to implement it,
and ARPANET planning (the ancestor of the Internet) also began in 1967.
<p>
It&#8217;s worth noting here that in a similar time period, ARPA was looking
for solutions to some of the problems that packet-switching solves.
J.C.R. Licklider, head of two ARPA departments for a time, had
formed the jokingly named &#8220;Intergalactic Computer Group&#8221; in the early 1960s.
In 1963 he wrote a memo to its members pleading for
standardization among the various computer systems so they could easily
communicate data between them, a memo that spurred on the search for and
implmentation of ways to link computers together.
In 1965 (after he left ARPA) Licklider wrote the book
&#8220;Libraries of the future&#8221;, which also hinted at the Internet
and World Wide Web of the future; Licklider said that
&#8220;the concept of a &#8216;desk&#8217; may have changed from passive to active:
a desk may be primarily a display-and-control station in a
telecommunication-telecomputation system-and its most vital part may be the cable (&#8217;umbilical cord&#8217;) that connects it [into the] net [to obtain]
&#8220;everyday business, industrial, government, and professional information,
and perhaps, also to news, entertainment, and education.&#8221;
<!-- http://www.cnn.com/2004/TECH/internet/08/29/internet.birthday.ap/index.html
"The Internet at 35: Still evolving"
Sunday, August 29, 2004 Posted: 2:08 PM EDT (1808 GMT) 
-->
<p>
On September 2, 1969, 
UCLA professor Len Kleinrock, along with graduate students
Stephen Crocker and Vinton Cerf, sent the first test data between two
ARPA computers in a system that would eventaully become the Internet.
These packet-switching
concepts are the fundamental basis of the Internet, defining
how the Internet uses packet-switching, though it would be several years
before the TCP/IP protocols we now use would be developed.
Note that TCP/IP and the Internet were not <i>themselves</i> designed
to survive nuclear attack or other security issues like that.
Instead, the later developers of TCP/IP needed their network to have lots
of nice properties, and the packet-switching concept created by Baran
(which <i>was</i> developed to be survivable)
turned out to have the properties they needed.
No patent identified.
</td></tr>

<tr><td>1960-1961</td><td>Quicksort sorting algorithm</td><td>
In 1960 C. A. R. (Tony) Hoare developed the Quicksort algorithm, which was
eventually published in 1961.
Sorting is an extremely common operation, and this algorithm performed
significantly better than the algorithms typically used at the time.
Perhaps even more importantly, it inspired a great deal of research
into improved algorithms, and showed many that recursive algorithms
could be extremely useful.
Today there are many other useful sorting algorithms, such
as heapsort and merge sort, but even now quicksort is often used
for sorting data.
No patent identified.
</td></tr>

<tr><td>1964</td><td>Word Processing</td><td>
The first &#8220;word processor&#8221;,
IBM&#8217;s product MT/ST (Magnetic Tape/Selectric Typewriter),
which combined the features of the Selectric typewriter
with a magnetic tape drive.
For the first time, typed material could be edited without having
to retype the whole text or chop up a coded copy.
Later, in 1972, this would be morphed into a word processing system we would
recognize today.
No patent identified.
</td></tr>

<tr><td>1964</td><td>The Mouse</td><td>
The Mouse was invented in 1964 by
<a href="http://inventors.about.com/science/inventors/library/weekly/aa081898.htm">Douglas C. Engelbart</a> at SRI,
using funding from the U.S. government&#8217;s ARPA program [Naughton 2000, 81].
Although this could be viewed as a hardware innovation, it isn&#8217;t
much of a hardware innovation
(it&#8217;s nothing more than an upside-down trackball).
The true innovations were in the
user interface approaches that use the mouse, which is
entirely a software innovation.
It was patented (US #3541541), though not until 1967,
and this never resulted in much money for the inventor.
<!-- http://www-sul.stanford.edu/depts/hasrg/histsci/ssvoral/engelbart/engfmst3-ntb.html -->
</td></tr>

<tr><td>1964</td><td>System Virtual Machines</td><td>
A system virtual machine (VM), aka full virtualization VMs,
enables a single computer to appear
to be multiple real computers and provide the functionality needed to
install and execute an entire operating system, including an unmodified
"guest" operating system.
These are often hardware/software combinations, and the underlying
software that implements this is called a virtual machine monitor (VMM)
or hypervisor.
In 1964 the IBM Cambridge Scientific Center begins development of the CP-40
mainframe,
an experimental machine designed to implement virtual machines.
The CP-40 was only used in labs (never sold to customers), but it
later evolved into th eCP-67, the first first commercial
mainframe to support system virtual machines.
<a href="http://www.everythingvm.com/content/history-virtualization">History
of Virtualization</a> has more information.
Later products, such as VMWare's, exploited the fact that microprocessors
had become so powerful that it was useful to share them.
Later on, operating-system-level virtualization (aka containers)
would be developed, but we count that here as a separate development.
<!-- https://en.wikipedia.org/wiki/Timeline_of_virtualization_development
 -->
</td></tr>

<tr><td>1965</td><td>Semaphores</td><td>
E. W. Dijkstra defined semaphores for coordinating
multiple processes.
The term derives from railroad signals, which in a similar way
coordinate trains on railroad tracks.
No patent identified.
</td></tr>

<tr><td>1965</td><td>Hierarchical directories,
program names as commands
(Multics)
</td><td>
The <a href="http://www.multicians.org">Multics</a> project spurred
several innovations.
Multics was the first operating system to sport
hierarchical directories, as described in
<a href="http://www.multicians.org/fjcc4.html">a 1965 paper
by Daley and Neumann</a>.
Multics was also the first operating system where,
in an innovation developed by Louis Pouzin,
<a href="http://www.multicians.org/shell.html">
what you type at command level is the name of a program to run</a>.
This caused related innovations like working directories and a shell.
In earlier systems, like CTSS, adding a command requiring recompiling;
to run your own program you had to execute a system command that then
loaded and ran the program.
Louis Pouzin implemented a very limited form of this idea on CTSS
as &#8220;RUNCOM&#8221;, but the full approach was implemented on Multics with his help.
Although fewer ordinary users use a command line interface today,
these are still important for many programmers.
The <a href="http://www.multicians.org">Multicians.org</a> site has more
information on
<a href="http://www.multicians.org/features.html">Multics features</a>.
No patent identified.
</td></tr>

<tr><td>1965</td><td>Unification</td><td>
J.A. Robinson developed the concept of &#8220;unification&#8221;.
This concept - and algorithms that implement it - become the basis
of logic programming.
No patent identified.
</td></tr>

<tr><td>1965</td><td>Single-computer Email</td><td>
As best as can be determined, email between users on a single computer
was developed in 1965.
MIT&#8217;s CTSS computer had a message feature in 1965
[Abbate 1999, page 109], called MAIL.
It also had another early program on the same computer called SNDMSG.
This is confirmed in
<a href="http://www.nethistory.info/History%20of%20the%20Internet/email.html">"The history of email" by Ian Peter</a>.
Email distributed across a network is much more powerful, but
single-computer email laid the groundwork.

<tr><td>1966</td><td>Structured Programming</td><td>
B&#246;hm and Jacopini defined the fundamentals of &#8220;structured programming&#8221;,
which showed that programs could be created using a limited set of
instructions (looping, conditional, and simple sequence) - and thus
showing that the&#8220;goto&#8221; statement was actually not essential.
Edsger Dijkstra&#8217;s 1968 letter &#8220;GO TO Statement Considered Harmful&#8221;
popularized the use of this approach, claiming that the &#8220;goto&#8221; statement
produced code that was difficult to maintain.
<!-- Dijkstra's paper was (Comm. ACM, August 1968)
B&ouml;hm, C., and G. Jacopini,
"Flow Diagrams, Turing Machines and Languages with Only Two Formation Rules."
 Communications of the ACM, Vol. 9, No. 5, pp. 366-371. 
http://www.dsi.uniroma1.it/~boehm/bohmpub.html
-->
No patent identified.
</td></tr>

<tr><td>1966</td><td>Spelling Checker</td><td>
Les Earnest of Stanford developed the first spelling checker circa 1966.
He later improved it around 1971 and this version quickly spread
(via the ARPAnet) throughout the world.
Earnest noted that 1970s partipants on the ARPAnet &#8220;found that
both programs and data migrated around the net rather quickly, to the
benefit of all&#8221; - an early note of the amplifying effect of large networks
on OSS/FS development.
<!--
Earnest said ``another thing that happened a lot in the 1970s was
benign theft of software. We didn't protect our files and found that
both programs and data migrated around the net rather quickly, to the
benefit of all.''
-->
<!-- Abbate, page 101. -->
No patent identified.
</td></tr>

<tr><td>1966</td><td>Pseudo-Code (p-Code) Machine (in BCPL)</td><td>
In computer programming, &#8220;a 
<a href="http://en.wikipedia.org/wiki/P-code_machine">pseudo-code
or p-code machine</a>
is a specification of a CPU whose instructions are expected
to be executed in software rather than in hardware (ie, interpreted).&#8221;
Basic Combined Programming Language (BCPL) is a computer
programming language designed by Martin Richards
of the University of Cambridge in 1966.
He developed a way to make it unusually portable, by splitting the
compiler into two parts: a compiler into an intermediate
pseudo-code (which he called O-code), and a back-end that translated that
into the actual machine code.
Since the intermediate code could be exchanged between arbitrary machines,
it enabled portability.
Urs Ammann, a student of Wirth&#8217;s
at the Swiss Federal Institute of Technology Zurich,
used the same approach in the 1970s to implement Pascal;
this intermediate pseudo-machine code was called p-code, and
popularized the technique
(see
<a href="http://portal.acm.org/citation.cfm?doid=155360.155378">
Wirth&#8217;s &#8220;Recollections about the development of Pascal&#8221; </a>)
Java is based on this fundamental approach, which enables
compiled code to be run unchanged on different computer architectures
(see
<a href="http://spectrum.ieee.org/computing/software/javas-forgotten-forbear/">
&#8220;Java&#8217;s Forgotten Forbear&#8221;</a>).
Even many text adventure games have been built with this approach, most
famously the <a href="http://en.wikipedia.org/wiki/Z-machine">Z-machine</a>
used to implement many Infocom games (such as Zork).
BCPL also significantly influenced the C programming language, including its
use of curly brackets {...}.
<!-- See  "The Portability of the BCPL Compiler", Martin Richards, Software
- Practice & Experience, Vol. 1, No. 2, 1971  -->
No patent identified.
</td></tr>

<tr><td>1967</td><td>Object Oriented Programming</td><td>
Object-oriented (OO) programming
was introduced to the world by the Norwegian Computing Centre&#8217;s
Ole-Johan Dahl and Kristen Nygaard when they release Simula 67.
Simula 67 introduces constructs that much later become common in
computer programming: objects, classes, virtual procedures, and inheritance.
OO programming is later popularized in
Smalltalk-80, and still later C++, Java, and C#.
This approach proved especially invaluable later when
graphical user interfaces became widely used.
<!-- http://www.ifi.uio.no/~kristen/FORSKNINGSDOK_MAPPE/F_OO_start.html -->
<!-- ??? When was this invented? -->
<!--
Already known at this point was the abstract data type
(ADT). An ADT is a
type whose internal form is hidden behind a set of access functions.
Objects of the type are created and inspected only by calls to the access
functions. This allows the implementation
of the type to be changed without requiring
any changes outside the module in which it is defined. 
Unfortunately, I can't find any reference to its history.
Guttag's 1975 PhD thesis discussed ADTs, but in his 2001 presentation
at "Software Pioneers", he says "I didn't invent ADTS, I don't think
anyone did."
-->
No patent identified.
</td></tr>

<tr><td>1967</td><td>Separating Text Content from Format</td><td>
The first formatted texts manipulated by computer had embedded
codes that described how to format the document
(&#8221;font size X&#8221;, &#8220;center this text&#8221;).
In contrast, in the late 1960s, people began to use
codes that described the meaning of the text
(such as &#8220;new paragraph&#8221; or &#8220;title&#8221;), with separate information
on how to format it.
This had many advantages, such as
allowing specialists to devise formats and easing searching, and influenced
later technologies such as SGML, HTML, and XML.
Although it is difficult to identify a specific time for this idea,
<a href="http://www.sgmlsource.com/history/sgmlhist.htm">
many credit the use of this approach (sometimes called &#8220;generic coding&#8221;)
to a presentation made by William Tunnicliffe,
chairman of the Graphic Communications Association (GCA)
Composition Committee, during a meeting at the
Canadian Government Printing Office in September 1967</a>
(his topic was on the separation of the information content of documents
from their format).
No patent identified.
</td></tr>


<tr><td>1968</td><td>The Graphical User Interface (GUI)</td>
<td><a href="http://www.dougengelbart.org/firsts/dougs-1968-demo.html">Douglas C. Engelbart gave 
a 90-minute staged public presentation / demonstration of a networked computer
system at the 1968
Fall Joint Computer Conference in San Francisco.
His presentation, &#8220;A Research Center for Augmenting Human Intellect&#8221;</a>,
was the first public appearance of the mouse, windows, hypermedia with
object linking and addressing, and video teleconferencing
(it has sometimes been called the &#8220;mother of all demos&#8221;).
These are the innovations that are fundamental to the
graphical user interface (&#8220;GUI&#8221;).
<!-- also known as the WIMP (windows, icons, mouse, and pointing) interface -->
<!-- Engelbart believes he had hypertext systems implemented in 1965 or 1966,
     see http://noframes.linuxjournal.com/articles/briefs/0082.html -->
This kind of interface made it much easier to implement
a driving idea of J.C.R. &#8220;Lick&#8221; Licklider, who envisioned a
human-computer symbiosis.
One of Licklider&#8217;s central ideas was that &#8220;a close coupling between
humans and computers would result in better decision-making.
In this novel partnership, computers would do what they
excelled at - calculations, routine operations, and the rest - thereby
freeing humans to do what they in turn did best.  The human-computer
system would thus be greater than the sum of its parts.&#8221;
(Summary from [Naughton, page 71]; see &#8220;Man-Computer Symbosis&#8221;,
IRE Transactions on Human Factors in Electronics,
vol. HFE-1, March 1960, pp 4-11.)
No patent identified.
</td></tr>

<tr><td>1968</td><td>Regular Expressions</td><td>
Ken Thompson published in the <i>Communications of the ACM</i>,
June 1968, the paper &#8220;Regular Expression Search Algorithm,&#8221;
the first known computational use of regular expressions.
Regular expressions had been studied earlier in mathematics,
based on work by Stephen Kleene.
Thompson later embedded this capability in the text editor ed to implement
a simple way to define text search patterns.
ed&#8217;s command &#8216;g/<i>regular expression</i>/p&#8217; was so useful that a
separate utility, <i>grep</i> was created to print every line in a file
that matched the pattern defined by the regular expression.
Later, many libraries included this capability, and the widely-used
Perl language makes regular expressions a fundamental underpinning
for the language.
See Jeffrey E.F. Friedl&#8217;s <i>Mastering Regular Expressions</i>,
1998, pp. 60-62, for more about this history.
<a href="https://patents.google.com/patent/US3568156A/en"
>Patent US3568156A (filed 1967)</a> by Ken Thompson covers a particular
approach for implementing this.
However, I have not found any evidence that turning this into a patent
enabled technology in any way.
For one thing, as noted in
<a href="https://www.cs.princeton.edu/courses/archive/spr09/cos333/beautiful.html"
>"A Regular Expression Matcher" by Brian Kernighan (code by Rob Pike)</a>,
"Ken's original matcher was very fast because it combined two independent
ideas...  Matching code in later text editors that Ken wrote, like ed,
used a simpler algorithm that backtracked when necessary. In theory this
is slower but the patterns found in practice rarely involved backtracking,
so the ed and grep algorithm and code were good enough for most purposes."
<!--
My thanks to
Freddy Spierenburg freddy at snarl dot nlxxx
for pointing out the patent to me.
-->
</td></tr>

<tr><td>1969-1970</td><td>Generalized Markup Language (GML), the ancestor of SGML, HTML, and XML</td><td>
In 1969 Charles F. Goldfarb, Ed Mosher, and Ray Lorie developed
what they called a &#8220;Text Description Language&#8221; to enable
integrating a text editing application, an information retrieval system,
and a page composition program.
The documents had to be selectable by query from a repository,
revised with the text editor, and returned to the data base
or rendered by the composition program.
This was an extremely advanced set of capabilities for its time, and one
that simple markup approaches did not support well.
They solved this problem by creating a general approach to identifying
different types of text, supporting formally-defined document types,
and creating an explicit nested element structure.
Their approach was
<a href="http://www.sgmlsource.com/history/jasis.htm">
first mentioned in a 1970 paper</a>,
renamed after their initials (GML) in 1971, and use began in 1973.
GML became the basis for the
Standard Generalized Markup Language (SGML), ISO Standard 8879.
While SGML itself is less-used today, descendents of SGML
are used all over the world.
HTML, the basis of the World Wide Web,
is an application of SGML,
and the widely-used XML (another critically important technology)
is a simplified form of SGML.
For more information see
<a href="http://www.sgmlsource.com/history/roots.htm">
The Roots of SGML -- A Personal Recollection</a> and
<a href="http://www.sgmlsource.com/history/sgmlhist.htm">
A Brief History of the Development of SGML</a>.
These standard markup languages, particularly their
descendents HTML and XML, have been critical for supporting
standard interchange of data that support a wide variety of display
devices and querying from a vast store of documents.
<!--
  http://www.oasis-open.org/cover/general.html
  History of Generalized Markup and SGML
"Note: I once had a research project underway which would have resulted in a written history of "the beginnings of SGML." I discovered, in the process of investigation, that there are a lot of strong feelings about those beginnings; various people have their own versions of "history." It appears certain to me that at least these three ideas were common already in the 1960's, often within distinct communities which rarely talked to each other: (a) the notion of separating "content and structure" encoding from specifications for [print] processing; (b) the notion of using names for markup elements which identified text objects "descriptively" or "generically"; (c) the notion of using a (formal) grammar to model structural relationships between encoded text objects. Some of these intellectual streams eventually flowed into the standards work where they took a particular canonical shape, and some important intellectual work developed outside the standards arena. How many of the "fundamental" notions of current SGML (ISO 8879:1986) were (first, best) articulated within efforts that may be reckoned as belonging, genetically or otherwise, to "the beginnings of SGML" will probably remain a matter of personal interpretation rather than of public record. If I ever complete the writeup from the materials collected so far, the picture will reveal a somewhat broader base for the "beginnings of SGML" than is documented in other published treatments of this topic to date. - Robin Cover"
-->
No patent identified.
</td></tr>

<tr><td>1970</td><td>Relational Model and Algebra (SQL)</td><td>
E.F. Codd introduced the relational model and relational algebra in a
famous article in the Communications of the ACM, June 1970.
This is the theoretical basis for relational database systems
and their query language, SQL.
<a href="http://www.multicians.org/history.html">
The first commercial relational database,
the Multics Relational Data Store (MRDS),
was released in June 1976.</a>
<!-- "A Relational Model of Data for Large Shared Data Banks",
                E. F. Codd
Codd's famous article from the June 1970 issue
of Communications of the ACM (reprinted in the Special 25th
Anniversary Issue, January 1983).
The author clearly depicts the new, relational, model
and contrasts it with
the hierarchical and network models, prevalent at that time. 
http://www.palslib.com/Fundamentals/The_Relational_Model.html
-->
No patent identified.
</td></tr>

<tr><td>1970</td><td>Backpropagation (deep learning of neural networks)</td><td>
The backpropogation algorithm is a key algorithm that enables
deep learning of neural networks (that it, it permits learning even when
there are more than 2 or 3 layers).
In 1970 Linnainmaa published the general method for
automatic differentiation (AD) of discrete connected networks
of nested differentiable functions.
This corresponds to backpropagation.
In 1974 Werbos mentioned the possibility of applying this principle
to artificial neural networks, and in 1982 he applied Linnainmaa's AD
method to neural networks in the way that is used today.
In 1986 Rumelhart, Hinton and Williams showed experimentally that this
method can generate useful internal representations of incoming data in
hidden layers of neural networks.
For more information, see
<a href="https://en.wikipedia.org/wiki/Backpropagation">Backpropogation
(Wikipedia)</a>.
For some notes about its limitations, see
<a href="https://www.technologyreview.com/s/608911/is-ai-riding-a-one-trick-pony/">"Is AI Riding a One-Trick Pony?" by James Somers</a>.
</td></tr>

<tr><td>1971</td><td>Distributed Network Email</td><td>
Richard Watson at the Stanford Research Institute suggested that a system
be developed for transferring mail from one computer to another via the
ARPANET network.
<a href="http://openmap.bbn.com/~tomlinso/ray/firstemailframe.html">
Ray Tomlinson of Bolt, Beranek and Newman (BBN) implemented the first
email program to send messages across a distributed network (ARPANET)</a>,
derived from an intra-machine email program and a file-transfer program.
This is confirmed by
<a href="http://www.nethistory.info/History%20of%20the%20Internet/email.html">"The history of email" by Ian Peter</a>.
This quickly became the ARPANET&#8217;s most popular and influential service.
Note that Tominson defined the &#8220;@&#8221; convention for email addresses.
Email between users on a single computer existed before,
but email that can span computers is far more powerful.
In 1973 the basic Internet protocol for sending
email was formed (though RFC 630 wasn&#8217;t released until April 1974), and in
1975 the Internet mail headers were first officially defined
[Naughton 2000, 149].
In 1975, John Vittal released the email program MSG, the first email program
with an &#8220;answer&#8221; (reply) command [Naughton 2000, 149].
<!--
<a href="https://www.techdirt.com/articles/20120222/11132917842/how-guy-who-didnt-invent-email-got-memorialized-press-smithsonian-as-inventor-email.shtml">V.A. Shiva Ayyadurai says he is the "inventor of e-mail",
based on software he copyrighted in 1982</a>
(see also
<a href="https://arstechnica.com/tech-policy/2017/06/shivas-war-one-mans-quest-to-convince-the-world-that-he-invented-e-mail/">here</a>).
-->
No patent identified.
</td></tr>

<tr><td>1972</td><td>Modularity Criteria</td><td>
David Parnas published a
definition and justification of modularity via information hiding. 
<!--
Parnas, D L, "On the Criteria to be Used in Decomposing Systems into Modules," Communications of the ACM,
                   December 1972, pp. 1053-1058. 
http://www.stsc.hill.af.mil/CrossTalk/1999/sep/jarvis.asp

???: The IEEE 50-year timeline puts this at 1971, but I'm not sure why.
-->

No patent identified.
</td></tr>

<tr><td>1972</td><td>Screen-Oriented Word Processing</td><td>
Lexitron and Linolex developed the first word processing system
that included video display screens and tape
cassettes for storage; with the screen, text could
be entered and corrected without having to produce a hard copy.
Printing
could be delayed until the writer was satisfied with the material.
It can be argued that this was the first &#8220;word processor&#8221; of the
kind we use today.
(see <a href="http://inventors.about.com/gi/dynamic/offsite.htm?site=http://www.stanford.edu/%7Ebkunde/fb%2Dpress/articles/wdprhist.html">a brief
history of word processing</a> for more information).
Other word processors were developed since.
In 1979, Seymour Rubenstein and Rob Barnaby released &#8220;WordStar&#8221;,
the first commercially successful word processing software program
produced for microcomputers, but this was simply a re-implementation
of a previous concept.
In March of 1980,
<a href="http://fitnesoft.com/AlmostPerfect/index.html">
SSI*WP (the predecessor of Word Perfect) was released</a>.
No patent identified.
</td></tr>

<tr><td>1972</td><td>Pipes</td><td>
<a href="http://cm.bell-labs.com/cm/cs/who/dmr/hist.html#pipes">Pipes</a>
are &#8220;pipelines&#8221; of commands, allowing programs to be easily
&#8220;hooked together&#8221;.
Pipes were originally developed for Unix and
widely implemented on other operating systems (including all Unix-like systems
and MS-DOS/Windows).
M. D. McIlroy insisted on their original implementation in Unix; after
a few months their syntax was changed to today&#8217;s syntax.
Redirection of information pre-existed this point (Dartmouth&#8217;s system
supported redirection, as did Multics), but
it was only in 1972 that they were implemented in a way that didn&#8217;t
require programs to specially support them and permitted programs to be
rapidly connected together.
No patent identified.
</td></tr>

<tr><td>1972</td><td>B-Tree</td><td>
Rudolf Bayer and Edward M. McCreight publish the seminal paper on
B-trees, a critical data structure widely used for handling large datasets.
<!--
Rudolf Bayer, Edward M. McCreight:
   Organization and Maintenance of Large Ordered Indices. Acta Informatica 1:
   173-189(1972)
-->
No patent identified.
</td></tr>

<tr><td>1972,1976</td><td>Portable operating systems
(OS6, Unix)
</td><td>
By this date high-level languages had been used for many years to
reduce development time and increase application
portability between different computers.
But many believed entire operating systems could not be practically
ported in the same way, since operating systems needed to control
many low-level components.
This was a problem, since it was often difficult to port applications
to different operating systems.
Significant portions of operating systems had been developed using
high-level languages;
(<a href="http://www.multicians.org/pl1.html">
Burroughs wrote much of the B5000&#8217;s operating system in a dialect of Algol,
and later much of Multics was written in PL/I</a>, but both were tied to
specific hardware.
In 1972 J.E. Stoy and C. Strachy discussed OS6, an experimental operating
system for a small computer that was to be portable.
<!-- J. E. Stoy and C. Strachey, "OS6 An
experimental operating system for a small computer. Part 1:
General principles and structure," Comp. J., 15 (May 1972), pp. 117-124. -->
In 1973 the fledgling
<a href="http://cm.bell-labs.com/cm/cs/who/dmr/hist.html">
Unix operating system was rewritten</a>
in
<a href="https://web.archive.org/web/20151122020808/http://www.bell-labs.com/usr/dmr/www/chist.html">C, a high-level programming language</a>
that had just been developed,
though at first the primary goal was
not general machine portability of the entire operating system.
<a href="http://cm.bell-labs.com/cm/cs/who/dmr/portpapers.html">
In 1976-1977 the Unix system was modified further to be portable</a>,
and the Unix system did <i>not</i> limit itself to being small - it
intentionally included significant capabilities such as a
hierarchical filesystem and multiple simultaneous users.
This allowed computer hardware to advance more rapidly, since it was no longer
necessary to rewrite an operating system when a new hardware idea or
approach was developed.
<!-- Portability of C Programs and the UNIX System,
S. C. JOHNSON D. M. RITCHIE.
-->
No patent identified.
</td></tr>

<tr><td>1972</td><td>Internetworking using Datagrams (leading to the Internet&#8217;s TCP/IP)</td><td>
The Cyclades project began in 1972 as an experimental network project
funded by the French government.
It demonstrated that computer networks
could be interconnected (&#8220;internetworked&#8221;) by the simple mechanism of
transferring data packets (datagrams), instead of trying to build session
connections or trying to create highly reliable &#8220;intelligent&#8221; networks
or &#8220;intelligent&#8221; systems which connected the networks.
Removing the requirement for &#8220;intelligence&#8221; when trying to hook
networks together had great benefits: it made systems less dependent on
a specific media or technology, and it also made systems less
dependent on central authorities to administer it.
<p>
At the time, networks were built and refined for a particular media,
making it difficult to make them interoperate.
For example, the ARPANET protocols (NCP) depended on highly reliable networks,
an assumption that broke down for radio-based systems (which used
an incompatible set of protocols).
NCP also assumed that it was networking specific computers, not networks
of networks.
The experience of Xerox PARC&#8217;s local system
(PARC Universal Packet, or PUP), based on Metcalfe&#8217;s 1973 dissertation,
also showed that &#8220;intelligence&#8221; in the network was unnecessary - in their
system, &#8220;subtracting all the hosts would leave little more than wire.&#8221;
<p>
On June 1973, Vinton Cerf organized a seminar at Stanford University
to discuss the redesign of the Internet, where it was
agreed to emphasize host-based approaches to internetworking.
In May 1974, Vinton Cerf and Robert E. Kahn published
&#8220;A Protocol for Packet Network Interconnection,&#8221;
which put forward their ideas of using gateways between networks and
packets that would be encapsulated by the transmitting host.
This approach would later be part of the Internet.
<p>
In 1977, Xerox PARC&#8217;s PUP was
designed to support multiple layers of network protocols.
This approach resolved a key problem of
Vinton Cerf&#8217;s Internet design group.
Early attempts to design the Internet
tried to create a single protocol, but this required too
much duplication of effort as both network components and hosts tried to
perform many of the same functions.
By January 1978, Vint Cerf, Jon Postel, and
Danny Cohen developed a design for the Internet, using two layered protocols:
a lower-level internetwork protocol (IP) which did not require
&#8220;intelligence&#8221; in the network and
a higher-level host-to-host transmission control protocol (TCP) to
provide reliability and sequencing where necessary (and not requiring
network components to implement TCP).
This was combined with the earlier approaches of using gateways to
interconnect networks.
By 1983, the ARPANET had switched to TCP/IP.
This layering concept was later expanded by ISO into the &#8220;OSI model,&#8221;
a model still widely used for describing network protocols.
Over the years, TCP/IP was refined to what it is today.
<!-- Naughton, 151-167 -->
<!-- Minor source: Abbate, page 130 -->
<p>
The origins of the Internet are actually quite complex, and I am
necessarily omitting some detail.
<a href="http://www.nethistory.info/History%20of%20the%20Internet/origins.html">
Ian Peter maintains</a> that
while there were significant contributions of a number of individuals
to claims as &#8220;fathers of the Internet&#8221;, most of these individuals
are at pains to point out the crucial involvement of others;
he argues that
&#8220;the Internet really has no owner and no single place of origin&#8221; and that
&#8220;the history of the Internet is better understood as the history of an era&#8221;.
<p>
In the early 1980s,
<a href="http://en.wikipedia.org/wiki/History_of_the_Internet">
DARPA sponsored or encouraged the development of TCP/IP implementations</a>
for many systems.
<a href="http://oreilly.com/catalog/opensources/book/kirkmck.html">
BSD implemented TCP/IP as open source software</a>, which led to its
being available to many.
After TCP/IP had become wildly popular,
<a href="http://www.kuro5hin.org/story/2001/6/19/05641/7357">Microsoft
added support for TCP/IP to Windows</a>
(originally by licensing TCP/IP code from Spider systems as well as using
BSD-developed code; Microsoft later rewrote portions).
<p>
As an aside, there are two different misconceptions about the Internet
and TCP/IP that should be clarified.
Some mistakenly claim that the Internet and TCP/IP were specifically
created to resist nuclear attacks; this is absolutely not true,
since its parent the ARPANET was specifically created to share large systems.
Yet it&#8217;s also a mistake to claim that there was no connection between
the Internet and survivable networks;
the Internet TCP/IP technology is an internetwork of data packets, and as
noted earlier, packet-switching of data packets
was created was to be survivable in case of disaster.
<p>
In 2005,
<a href="http://www.groklaw.net/article.php?story=20050216080321140">
Vinton Cerf and Robert Kahn were awarded the prestigious
Turing award for their role in creating the Internet&#8217;s basic
components, particularly the TCP/IP protocol</a>.
One of the reasons given for the adoption of the TCP/IP protocols
was they were unencumbered by patent claims;
&#8220;Dr. Cerf said part of the reason their protocols took
hold quickly and widely was that he and Dr. Kahn made
no intellectual property claims to their invention.&#8221;
No patent identified.
</td></tr>

<tr><td>1973</td><td>Font Generation Algorithms</td><td>
There have been many efforts to create fonts using mathematical techniques;
Felice Feliciano worked on doing so around 1460.
However, these older attempts generally produced ugly results.
In 1973-1974 Peter Karow developed Ikarus, the first program to
digitally generate fonts at arbitrary resolution.
In 1978, Donald Knuth revealed his program Metafont, which generated
fonts as well
(this work went hand-in-hand with his work on the open source typesetting
program TeX, which is still widely used for producing typeset
papers with significant mathematical content).
Algorithmically-generated fonts were fundamental to
the Type 1 fonts of Postscript and to True Type fonts as well.
Font generation algorithms
made it possible for people to vary their font types and sizes to whatever
they wanted, and for displays and printers to
achieve the best possible presentation of a font.
Today, most fonts displayed on screens and printers are generated
by some font generation algorithm.
<!-- Source: Donald Knuth, "TEX and Metafont", pg 18. -->
<!--
Ikarus was originally developed in 1973-74 at the Rudolf Weber company in Hamburg (now URW) by Peter Karow. This was the first time type had been digitized as outlines. The native curves of Ikarus are Hermite splines, having the important property that all the control points lie on the outline itself. (It must be disappointing to URW that designers have generally accepted off-curve Bzier handles.) Ikarus was written in FORTRAN and has been used by foundries on VAX and Sun workstations to store thousands of type designs.
http://www.truetype.demon.co.uk/othertls.htm

 Bigelow predictions are significant because they demonstrate that, well before the development of PostScript, those at the forefront of the industry were expecting a standard typesetting software to emerge. At the time he wrote the article there were already several software systems competing to become that standard. The two that Bigelow mentioned were Ikarus, which was commercially developed at URW in Hamburg by Peter Karow, and Metafont which was written, initially as an academic exercise, by Donald Knuth at Stanford. These systems took very different views of the letterform as their starting point. In Ikarus the parameters of a letter, such as stem weight and x-height, were defined as contours. Metafont allowed one to imagine a virtual tool which defined the parameters of the letters.
http://www.typotheque.com/articles/EK_PhD_chapter1.html

Metafont describes _pen_ motions, while True Type etc.
describe _outlines_.  Knuth uses cubic Bezier curves as paths along
which he drags a particular pen shape. Adobe's Postscript Type 1
also uses Bezier curves, but to define the outlines of the letter
shape. For rasterizing Type 1 uses an unusual algorithm for
determining whether any particular pixel lies "inside" the curve
(thus gets painted), or "outside".
-->
No patent identified.
</td></tr>

<tr><td>1974</td><td>Monitor</td><td>
Hoare (1974) and Brinch Hansen (1975) proposed the monitor, a higher-level
synchronization primitive; it&#8217;s now built into several programming languages
(such as Java and Ada).
No patent identified.
</td></tr>

<tr><td>1975</td><td>Communicating Sequential Processes (CSP)</td><td>
C. A. R. Hoare published the concept of
Communicating Sequential Processes (CSP) in
&#8220;Parallel Programming: an Axiomatic Approach&#8221;
(<i>Computer Languages</i>, vol 1, no 2, June 1975, pp. 151-160).
This is a critically important approach for reasoning about parallel processes.
</td></tr>

<!-- Is this really an innovation?
<tr><td>1975</td><td>Mythical Man-Month</td><td>
Frederick Brooks writes "The Mythical Man-Month",
which captures lessons learned from a mammoth software development project.
In it, he identifies critical limiting factors when developing software,
and notes that adding more people to a late software development project
only makes it later.
</td></tr>
-->

<tr><td>1977</td><td>Diffie-Hellman Security Algorithm</td><td>
The
<a href="http://www.ics.uci.edu/~ics54/doc/security/pkhistory.html">Diffie-Hellman</a>
public key algorithm was created in a way that the
public could read about it.
According to the United Kingdom&#8217;s GCHQ,
<a href="https://web.archive.org/web/20011217171744/http://www.counterpane.com/crypto-gram-9805.html#nonsecret">M. J.
Williamson had invented
this algorithm (or something very similar to it) in 1974</a>, but
it was classified, and
I&#8217;m only counting those discoveries made available to the public.
<!--
For more on Ellis' discovery of public key cryptography,
http://www.cesg.gov.uk/publications/media/nsecret/ellis.pdf

http://www.cesg.gov.uk/publications/
-->
This algorithm
allowed users to create a secure communication channel without meeting.
<!--
This was patented, causing a
later furor; the patent never brought much money to the inventors or Stanford.
-->
US patent #4200770; I have a report that it was later
found to be defective in litigation, so if someone could confirm/deny this,
that'd be great.
</td></tr>


<tr><td>1977</td><td>Make (automated build system using dependencies)</td><td>
In 1977 Stuart Feldman developed <i>make</i> at Bell Labs.
Make allows developers to briefly state how components depend on other
components; the make tool can then automatically re-build
(or do other operations) in the right order, skipping what does not
need to be done.
In 2003 Dr. Feldman received the ACM Software System Award
for creating this now-widespread tool.
No patent identified.
</td></tr>



<tr><td>1978</td><td>RSA security algorithm</td><td>
Rivest, Shamir, and Adleman, published their
seminal paper describing the
<a href="http://theory.lcs.mit.edu/~rivest/rsapaper.pdf">RSA algorithm</a>,
a critical basis for security.
The RSA algorithm
permits authentication or encryption without having to previously
exchange a secret shared key, greatly simplifying security.
It&#8217;s amusing to note that this paper also introduced &#8220;Alice&#8221; and &#8220;Bob&#8221;,
fictious characters who are trying to securely communicate, and
<a href="http://www.nwfusion.com/news/2005/020705widernetaliceandbob.html">
Alice and Bob have become a standard part of security notation
ever since the RSA paper</a>.
According to the United Kingdom&#8217;s GCHQ,
<a href="http://www.counterpane.com/crypto-gram-9805.html#nonsecret">
Clifford Cocks had invented the RSA algorithm in 1973</a>, but
it was classified.
US patent #4405829.
</td></tr>

<tr><td>1978</td><td>Spreadsheet</td><td>
Dan Bricklin and Bob Frankston invented the spreadsheet application
(as implemented in their product, VisiCalc).
<a href="http://www.bricklin.com/history/intro.htm">Bricklin and Frankston
have made information on VisiCalc&#8217;s history available on the web</a>.
No patent identified.
</td></tr>

<tr><td>1978</td><td>Lamport Clocks</td><td>
Leslie Lamport published
&#8220;Time, Clocks, and the Ordering of Events in a Distributed System&#8221;
(<i>Communications of the ACM</i>, vol 21, no 7, July 1978, pp. 558-565).
This is an important approach for ordering events in a distributed system.
No patent identified.
</td></tr>

<tr><td>1979</td><td>Distributed Newsgroups (USENET)</td><td>
Tom Truscott and Jim Ellis (Duke University, Durham, NC), along with
Steve Bellovin (University of North Carolina, Chapel Hill),
set up a system
for distributing electronic newsletters, originally between
Duke and the University of North Carolina using dial-up lines and the
UUCP (Unix-to-Unix copy) program.
This was the beginning of the informal network USENET,
supporting online forums on a variety of topics, and took off once Usenet
was bridged with the ARPANET.
ARPANET already had discussion groups (basically mailing lists).
However, the owner of ARPANET discussion groups determined who received the
information - in contrast, everyone could read USENET postings
(a more democratic and scaleable approach)
[Naughton 2000, 177-179]
<!-- Abbate 1999, page 201 claims Steve Bellovin was at Duke, but
     this is wrong.  See Steve Bellovin's web page at
     http://www.research.att.com/~smb/ -->
No patent identified.
</td></tr>

<tr><td>1979</td><td>Operating system level virtualization (containers)</td><td>
In operating-system-level virtualization (aka containers),
the underlying operating system kernel (which typically provides
filesystem mechanisms and networking primitives) is shared among a set
of run-time units called containers.
The containers have at least isolated filesystems, and in many cases
are isolated in other ways (e.g., they may appear to have separate
process lists and/or network addresses).
Since there is a shared underlying kernel, these can be much more efficient
than full virtualization (though this sharing also creates a larger
attack surface that must be secured).
The "chroot" system call was introduced to Unix v7 in 1979, which could
change the "root" directory visible to a process and its children.
This made it possible to create independent isolated views of a system's
files.
Later refinements were implemented by FreeBSD jails, Solaris Containers,
Open VZ (Open Virtuzzo), LXC, and Docker.
See: <a href="http://blog.aquasec.com/a-brief-history-of-containers-from-1970s-chroot-to-docker-2016">A Brief History of Containers: From 1970s chroot to Docker 2016</a>
<!-- https://en.wikipedia.org/wiki/Timeline_of_virtualization_development
 -->
</td></tr>

<tr><td>1980</td><td>Model View Controller (MVC)</td><td>
The &#8220;Model, View, Controller&#8221; (MVC) triad of classes for developing
graphical user interfaces (GUIs) was first introduced as part of the
Smalltalk-80 language at Xerox PARC.
This work was overseen by Alan Kay, but it appears that many people were
actually involved in developing the MVC concept, including Trygve Reenskaug,
Adele Goldberg, Steve Althoff, Dan Ingalls, and possibly Larry Tesler.
Krasner and Pope later documented the approach extensively and described it
as a pattern so it could be more easily used elsewhere.
This doesn&#8217;t mean that all GUIs have been developed using MVC, indeed,
in 1997 and 1998, the Alan Kay team moved their Smalltalk 
graphic development efforts and research to another model based on
display trees called Morphic, which 
they believe obsoletes MVC.
However, this design pattern has since been widely
used to implement flexible GUIs, and has influenced later thinking about
how to develop GUIs.
<!-- Krasner and Pope -->
<!--
Jerry Fass did some extensive work and found earlier references than 1988
to Model View Controller.  Here's his email:

Date:  Tue, 26 Nov 2002 01:03:32 -0600
From: "Jerry Fass" <fass, at, pitnet.net>
To: "David A Wheeler" <dwheeler@dwheeler.com>
Subject: Software Innovation error: MVC?

On your page, 

The Most Important Software Innovations:
http://dwheeler.com/innovation/innovation.html

This entry:
"1988"
"Model View Controller"
"Krasner and Pope define the ``Model, View, Controller'' triad of 
classes, a design pattern widely used to implement flexible GUIs."

gives the impression that Model-View-Controller, MVC, was developed in 
the late 1980s, specifically 1988. Computer history suggests that this 
date is about a decade too late.

As with so many other developments that form the basis for, and 
created, modern computing, MVC is rooted in the work done at Xerox PARC during 
the 1970s, by the team directed by Alan Kay. The proper credit for MVC 
belongs there, see: 
http://st-www.cs.uiuc.edu/users/smarch/st-docs/mvc.html 

MVC was invented late in Smalltalk's gestation period, which was 
1972-1980. MVC was originally implemented as a set of Smalltalk classes. You 
show correctly that MVC was the work of more than one person, but give 
the wrong people the credit. Krasner and Pope wrote their works on MVC 
from ParcPlace Systems, the renamed Xerox PARC, which did not start 
until 1985.

I am no expert on the evolution of MVC, so before I wrote this, I asked 
an expert, someone who was there, for the specifics. A friend of mine, 
Ted Kaehler http://www.squeakland.org/~ted/ is one of the original 
co-developers of Smalltalk. He participated in PARC during its glory years.

According to Kaehler, what the Alan Kay team calls "Models, Views, and 
Controllers" (plural) was developed by Adele Goldberg and Steve Althoff 
in 1979 or 1980. They did the work and contributed the main ideas. MVC 
was "already firmly in place" when first introduced in the Smalltalk-80 
system, released in 1980, and licensed to other firms by 1982. Krasner 
worked on MVC some at PARC, but not in a major role. Pope never worked 
at PARC. Later, Krasner and Pope reworked MVC and built follow-on ways 
to put windows up. Persons who credit Krasner and Pope with developing 
MVC may be thinking of the later system from ParcPlace, which name 
Kaehler can't remember.

The first extensive MVC documentation was in:
Adele Goldberg; Smalltalk-80: The Interactive Programming Environment, 
Addison-Wesley Publishers, 1983.

Kaehler had another PARC alum, Diana Merry-Shapiro, comment to me via 
email. She wrote:

"Somewhere along about 1980, I think, when Smalltalk-80 was 
being conceived, some of us had notions about separating view from 
domain (there were also some of us in the group that thought, ala 
Marshall McLuhan, that there are only views - we lost ;-)).  Lots 
of people had their fingers in this pie, but important players were Dan 
Ingalls, Adele Goldberg and I think Larry Tesler.  But the person who 
really crystallized the idea was Trygve Reenskaug, a visiting 
scientist in our group from Norway - another one of those Norwegian 
guys thinking seminal thoughts ;-).  I believe it was Trygve who coined 
the triplet Model-View-Controller."

In view of the above, it seems that what Krasner and Pope contributed 
to MVC was to rework, extensively document, and reduce it to a design 
pattern. This is limited software innovation, refinement really, compared 
to inventing MVC in the first place.

Notably, in 1997 and 1998, the Alan Kay team moved their Smalltalk 
graphic development efforts and research to another model, Morphic, which 
they believe obsoletes MVC. Morphic is based on display trees, and 
originated at Sun Microsystems Labs, in the Self programming language, from 
Randy Smith, John Maloney, and Dave Ungar.

Morphic is now implemented in Self:
http://research.sun.com/research/self/
and in Alan Kay's direct Smalltalk-80 descendent, Squeak:
http://squeak.org/

Hope this helps.

-->
No patent identified.
</td></tr>

<tr><td>1981</td><td>Remote Procedure Call (RPC)</td><td>
In 1981 Bruce J. Nelson published <i>Remote Procedure Call</i>,
his PhD thesis in Computer Science from Carnegie Mellon University.
An RPC (Remote Procedure Call) allows one program to request a service
from another program, potentially located in another computer,
without having to understand network details.
The requestor usually waits until the results are returned, and local calls
can be optimized (e.g., by using the same address space).
This calling is facilitated through an &#8220;interface definition language&#8221;
(IDL) to define the interface.
Most people today instead refer to the slightly later paper
&#8220;Implementing Remote Procedure Calls&#8221;,
by A.D. Birrell and B.J. Nelson,
ACM Tranactions on Computer Systems,
Vol. 2, No. 1 1984, pp. 39-59.
Sun&#8217;s RPC (later an RFC) were derived from this, and later on
DCE, CORBA, component programming (COM, DCOM),
and web application access
(SOAP / WDDI, RPC-XML, and even REST approaches) all derive from this.

In
<a href="http://awards.acm.org/software_system/">1994 the prestigious
Association for Computing Machinery (ACM) Software System Award went to
Bruce J. Nelson and Birrell for the Remote Procedure Call</a>.

<!--
ONC RPC is a Remote Procedure Call technology that originated in Sun
Microsystems in the early 1980s. ONC RPC was
modelled on Xerox's Courier RPC protocols.
http://www.ietf.org/proceedings/94mar/charters/oncrpc-charter.html
-->
No patent identified.
</td></tr>

<tr><td>1982</td><td>Computer Virus</td><td>
A computer virus is a program that can &#8216;infect&#8217; other programs
by modifying them to include a possibly evolved copy of itself.
While not a positive development, this was certainly an innovation.
The program &#8220;Elk Cloner&#8221; is typically identified as the first
&#8220;in the wild&#8221; computer virus.
<a href="http://www.securityfocus.com/infocus/1286">
Elk Cloner written in 1982 by junior high school student
Richard Skrenta</a>,
as a <a href="http://www.skrenta.com/2007/01/the_joy_of_the_hack.html">
practical joke</a>.
It attached to the Apple DOS 3.3 operating system, and
spread through floppy disks that were inserted afterwards.
The
<a href="http://news.com.com/2009-7349_3-5111410.html">
history of computer viruses is more complicated</a>, and some
consider <a href="http://www.viruslist.com/en/viruses/encyclopedia?chapter=153310937">earlier programs named Creeper, Rabbit, or Animal</a> as the
first virus.
In particular,
in 1975 John Walker released
<a href="http://www.fourmilab.ch/documents/univac/animal.html">&#8221;Animal&#8221;
on Univac systems with a PERVADE subroutine</a>
that caused copies of Animal to reappear elsewhere.
But since it was careful to only use this to update its own software,
it&#8217;s not clear that it fits the definition above, and few (other than
Walker) understood the dangers of the idea.
Fred Cohen later wrote academic works studying computer viruses.
No patent identified.
</td></tr>

<tr><td>1984</td><td>Distributing Naming (DNS)</td><td>
The &#8220;domain name system&#8221; (DNS) was invented, essentially the
first massively distributed database, enabling the Internet
to scale while allowing users to use human-readable names of computers.
Every time you type in a host name such as &#8220;dwheeler.com&#8221;, you&#8217;re
relying on DNS to translate that name to a numeric address.
Some theoretical work had been done before on massive database
distribution, but not as a practical implementation on this scale,
and DNS innovated in several ways to make its implementation practical
(e.g., by not demanding complete network-wide
synchronicity, by distributing data maintenance
as well as storage, and by distributing &#8220;reverse lookups&#8221; through
a clever reflective scheme).
No patent identified.
</td></tr>

<tr><td>1986</td><td>Lockless version management (CVS)</td><td>
Dick Grune released to the public the Concurrent Versions System (CVS),
the first lockless version management system for software development.
In 1984-1985, Grune wanted to cooperate with two of his students when
working on a C compiler.  However,
existing version management systems did not support cooperation well, because
they all required that versions &#8220;locked&#8221; before they could be edited, and
once locked only one person could edit the file.
While standing at the university bus stop, waiting for the bus home
in bad autumn weather, he created an approach for supporting distributed
software development that did not require project-wide locking.
After initial development,
CVS was
<a href="http://groups.google.com/groups?q=Grune+cvs+group:mod.sources.*&amp;hl=en&amp;lr=&amp;ie=UTF-8&amp;selm=122%40mirror.UUCP&amp;rnum=2">
publicly posted by Dick Grune
to the newsgroup mod.sources on 1986-07-03 in volume 6 issue 40,
(and also to comp.sources.unix)</a>
as source code (in shell scripts).
CVS has since been re-implemented, but its basic ideas have influenced
all later version management systems.
CVS was a major step forward for its time, but it was a centralized
version control system (VCS).
if you're only familiar with distributed
VCSs (like git), the article
<a href="https://twobithistory.org/2018/07/07/cvs.html">Version Control Before Git with CVS (by Sinclair Target, Two-Bit History)</a>
may help you understand CVS better.
The initial CVS release did not formally state a license
(a common practice at the time), but in keeping with the common
understanding of the time, Mr. Grune intended for it to be 
used, modified, and redistributed;
he has specifically stated that he
&#8220;certainly intended it to be a gift to the international community...
for everybody to use at their discretion.&#8221;
Thus, it appears that the initial implementation of
CVS was intended to be open source software / free software (OSS/FS) or
something closely akin to it.
Certainly CVS has been important to OSS/FS since that time;
while OSS/FS development can be performed without it, CVS&#8217;s ideas were
a key enabler for many OSS/FS projects, and are
widely used by proprietary projects as well.
CVS&#8217; ideas have been a key enabler in many projects for
scaling software development
to much larger and more geographically distributed development teams.

<!--
Date:  Mon, 24 Feb 2003 11:50:20 +0100
From: "Dick Grune" dick@cs.vu.nl
To: dwheeler@dwheeler.com
Subject: Re: Was CVS the first lockless version management system? (History of CVS)

+ + Hi - Can you help me? I'm trying to verify a few things:
+ 1. Was Dick Grune's CVS the FIRST lockless version management system?
+ I think it was, but I really want to confirm that.

Not being omniscient, I can't be sure.  There was the FUSS (Free University
Storage Server) by Sape Mullender, with which I was familiar, but I don't
think it had any influence.  I wasn't thinking in terms of lockless, I just
wanted to be able to hack at a project with three people who had vastly
different time schedules.  As far as I am aware the ideas in CVS were my own;
I remember thinking them up.  But nobody lives in a void.

+ 2. Was CVS was originally "open source software /
+ Free Software"?  I've looked the shar archive posted by
+ Dick Grune, but I don't see any license statement that says
+ whether or not it was modifiable.  I'm guessing
+ it was presumed that modifying it was okay, but I can't be sure.

I am not certain that there was any such thing then, but if there was, 
it was
far away in America for me, so it didn't apply.  When I posted it on
comp.sources.unix I certainly intended it to be a gift to the international
community.  I'm pretty sure that posting something on a newsgroup those days
meant that it was for everybody to use at their discretion.

+ 3. Was CVS really released in 1986, or in 1983, or some other time?
+ Per Cederqvist reports that Grune released it in 1986, but

I created CVS to be able to cooperate with my students Erik Baalbergen and
Martin Waage on the ACK (Amsterdam Compiler Kit) C compiler.  Their project
ran from July 1984 to August 1985.  I remember thinking about the complicated
interrelated decisions that have to be made, standing at the bus stop 
at the university waiting for the bus home, in bad autumn weather (I then 
decided to
make a big table and just fill in all the combinations and see what 
came of
it).  So that would make it autumn 1984.  But maybe I'm one year off, 
since my
initial commit of CVS under itself is of 1985/11/23 23:24:37, and the 
table is
in the ChangeLog with date 14-Dec-1985.  Also, during 1984-1985 I may 
have
been too busy with the C compiler to bother about niceties.  So I 
suppose I
wrote an ad-hoc thingee somewhere between 1984 and 1985, which served 
us well,
and once the C compiler project was over, I thought that it might 
continue to
serve and went on to program it in earnest.  (Do you still wonder why 
we
cannot find out exactly which pharaoh did what when?)

+ Grune's paper has a date of 1986 and claims that it was in use
+ for 3 years.

Does it?  My initial commit for it is 1987/12/27 17:40:5, so I suppose 
I wrote
it in the autumn of 1987.  If I indeed wrote CVS in the winter of 
1984-1985,
the phrase "The system has been in use for almost three years now" 
makes
sense.
Where does the 1986 date for the paper come from?

Is the old comp.sources.unix posting still around?  I don't think I 
have a
copy.

+ I've really tried to answer these. Cederqvists's history is:
+ + "CVS started out as a bunch of shell scripts written by Dick Grune,
+ + posted to the newsgroup comp.sources.unix in the volume 6 release
+ + of December, 1986. While no actual code from these shell scripts
+ + is present in the current version of CVS much of the CVS conflict
+ + resolution algorithms come from them.

That's all true, as far as I am aware of

Hope this helps,

Regards,

- 
Dick Grune					| email: dick@cs.vu.nl
Vrije Universiteit				| ftp://ftp.cs.vu.nl/pub/dick
de Boelelaan 1081				| http://www.cs.vu.nl/~dick
1081 HV  Amsterdam, the Netherlands		| tel: +31 20 444 7744

-->
No patent identified.
</td></tr>

<tr><td>1989</td><td>Distributed Hypertext via Simple Mechanisms
(World Wide Web)</td><td>
The World Wide Web (WWW)&#8217;s
Internet protocol (HTTP), language (HTML), and addressing scheme
(URL/URIs) were created by Tim Berners-Lee.
The idea of hypertext had existed before, and Nelson&#8217;s Xanadu had tried
to implement a distributed scheme, but Berners-Lee developed a new approach
for implementing a distributed hypertext system.
He combined a simple client-server protocol,
markup language, and addressing scheme in a way that was new, powerful,
and easy to implement.
It was platform-independent, as opposed to some previous hypertext systems.
There were no patent restrictions to its implementation, another
key to its wide and rapid adoption.
Each of the pieces had existed in some form before,
and there were some related/similar projects such as
<a href="http://en.wikipedia.org/wiki/Andrew_Project">Andrew</a>.
However, the combination
was obvious only in hindsight.
<a href="http://www.w3.org/History/1989/proposal.html">Berners-Lee&#8217;s original
proposal was dated March 1989</a>, and he first implemented the approach
in 1990.
No patent identified.
</td></tr>

<tr><td>1991</td><td>Design Patterns</td><td>
Erich Gamma published in 1991 his PhD thesis which first
seriously examined software design patterns as a subject of study
including a number of specific design patterns.
In 1995 Gamma, Helm, Johnson, and Vlissides (the &#8220;Gang of Four&#8221;)
published &#8220;Design Patterns,&#8221;
which widely popularized the idea.
The concept of &#8220;design patterns&#8221; is old in other fields,
specific patterns had been in use for some time, and algorithms had
already been collected for some time.
Some notion of patterns is suggested in earlier works (see the references
in both).
However, these works crystallized software design patterns in a way
that was immediately useful and had not been done before.
This has spawned other kinds of thinking, such as trying to identify
anti-patterns (&#8220;solutions&#8221; whose negative consequences exceed their
benefits; see
<a href="http://www.antipatterns.com">the Antipatterns website</a>,
including information on
<a href="http://www.antipatterns.com/dev_cat.htm">development
antipatterns</a>).
No patent identified.
(Erich Gamma is listed as the inventor on US patent #5544301,
but that appears to be different.)
</td></tr>

<tr><td>1991</td><td>Distributed Version Control System (DVCS)</td><td>
In 1991 Sun began developing TeamWare
(<a href="http://www.usenix.org/publications/library/proceedings/bos94/full_papers/adams.a">The Old Man and the C&#8221;, Evan Adams, Sun Microsystems</a>).
This appears to be the first distribution version control system (DVCS),
aka
<a href="http://en.wikipedia.org/wiki/Distributed_revision_control">
distributed revision control system</a>,
distributed source code control system, or
distributed (software) configuration management system).
In such systems,
instead of using a central repository, each user has a repository,
and the system enables users to synchronize their work.
Later influential DVCS systems include BitKeeper,
Monotone, Mercurial (hg), Bazaar (bzr), and git.
No directly relevant patent identified;
Sun did get US patent #5313646 on TeamWare for a
"Method and apparatus for translucent file system" but this was for
the transparent overlay TeamWare provided, not for the distributed
version control approach itself.
</td></tr>

<tr><td>1992</td><td>Secure Mobile Code (Java and Safe-Tcl)</td><td>
A system supporting secure mobile code can
automatically download potentially malicious code from
a remote site and safely run it on a local computer.
Sun built in 1990-1992, and demonstrated on September 1992, its
<a href="http://java.sun.com/features/1998/05/birthday.html">new
programming language, Oak (later called Java)</a>,
as part of the Green project&#8217;s demonstration of its *7 PDA.
<!-- http://java.sun.com/people/jag/green/index.html -->
<!-- http://www.ibiblio.org/javafaq/javafaq.html -->
Oak combined an interpreter
(preventing certain illegal actions at run-time) and a bytecode verifier
(which examines the mobile code for certain properties
before running the program, speeding later execution).
Originally intended for the &#8220;set-top&#8221; market, Oak was modified to
work with the World Wide Web and re-launched (with much fanfare) as Java
in 1995.
<!-- In the future, perhaps contrast this with:
James W. Stamos and David K. Gifford.
Remote evaluation. ACM Transactions on Programming Languages and
Systems, 12(4):537-565, October 1990.
Looked at: http://www.w3.org/MobileCode/
 http://www-swiss.ai.mit.edu/~jmiller/mobile.html
-->
Nathaniel Borenstein and Marshall Rose implemented a prototype
of Safe-Tcl in 1992; it was first used to implement &#8220;active email messages.&#8221;
An expanded version of Safe-Tcl was incorporated into
regular Tcl on April 1996 (Tcl 7.5).
<!-- From John K. Ousterhout -->
US patents #5748964 and #5668999 may be related, though they were filed
in 1994.
</td></tr>

<tr><td>1993</td><td>Refactoring</td><td>
Refactoring is the process of changing a software system that does not
alter its external behavior but improves its internal structure.
It&#8217;s sometimes described as&#8220;improving the design after it&#8217;s written&#8221;,
and could be viewed as design patterns in the small.
Specific refactorings and the general notion of restructuring programs
were known much, much longer, of course.
However, the idea of creating a list of specific source code
refactoring processes, so they could be discussed and studied,
was essentially a new idea.
This date is based on
<a href="ftp://st.cs.uiuc.edu/pub/papers/refactoring/opdyke-thesis.ps.Z">William
F. Opdyke&#8217;s PhD dissertation</a>, the first lengthy discussion of it
(including a set of standard refactorings) I&#8217;ve found.
Martin Fowler later published his book &#8220;Refactoring&#8221; which
popularized this idea.
No patent identified.
</td></tr>


<tr><td>1994</td><td>Web-Crawling Search Engines</td><td>
The World Wide Web Worm (WWWW) indexed 110,000 web pages by crawling
along hypertext links and providing a central place to make search requests;
this is one of the first (if not the first) web search engines.
Text search engines far precede this, of course, so it can be easily
argued that this is simply the reapplication of an old idea.
However, text search engines before this time assumed that they had
all the information locally available and would know when any content changed.
In contrast, web crawlers have to locate new pages by crawling
through links (selectively finding the &#8220;important&#8221; ones).
<!--
http://www.cs.colorado.edu/home/mcbryan/mypapers/www94.ps
[McBryan 94], referenced by
http://www7.scu.edu.au/programme/fullpapers/1921/com1921.htm
http://www.lub.lu.se/desire/radar/reports/D3.11/
-->
No patent identified.
</td></tr>

<tr><td>1996</td><td>Content-Based Addressing (rsync)</td><td>
<!-- Also at: http://www.linuxworld.com/news/2007/111207-hash.html
     but with a hideous, 2-paragraphs per page format that is close to
     unreadable. -->
<a href="http://www.linuxworld.com/cgi-bin/mailto/x_linux.cgi?pagetosend=/export/home/httpd/linuxworld/news/2007/111207-hash.html">
Content-based addressing</a>
(aka content-based storage) calculates cryptographic hashes of data
(usually whole files) and uses that value as the &#8220;address&#8221; of the data.
This dramatically saves network bandwidth - applications need only exchange
hashes over the network instead of the actual content.
The application &#8220;rsync&#8221; by Andrew Tridgell used it in 1996,
and later applications such as BitTorrent and git use it as well.
Although bandwidth has dramatically increased over years, the amount of
data we want to send has grown as well - and this makes many large data
transfers much more efficient.
A more formal analysis is in
<a href="http://www-serl.cs.colorado.edu/~carzanig/papers/cucs-902-00.pdf">
&#8220;Content-Based Addressing and Routing: A General Model and its Application&#8221;
by Antonio Carzaniga, David S. Rosenblum, and Alexander L. Wolf</a>.
No patent identified.
</td></tr>

<!--
<tr>
<td>1998</td><td>PageRank</td><td>
<a href="http://www7.scu.edu.au/programme/fullpapers/1921/com1921.htm">Brin
and Page</a> developed a system for ranking web pages
based on not just how many web pages link to it, but the ``quality''
of those web pages (determined by applying the algorithm recursively).
This, along with other heuristics, resulted in the
<http://www.google.com">Google</a> search engine.
Ranking documents based on the number of citations and their quality
is an old approach in the research community, so it could be argued that this
isn't really innovative.
It's also questionable if this is really such a critical innovation
compared to, say, subroutines;
other search engines work without this.
</td></tr>

relevance in part based on the
number of hypertext links to the page (as well as other heuristics
such as the text in the link).
Their resulting implementation was Google, first implemented at Stanford.
However, it looks like Lycos patented something like that earlier, see
http://www.searchenginewatch.com/sereport/97/10-patent.html
http://www.research.microsoft.com/research/db/debull/A00sept/issue.htm
-->

<tr><td>2004</td><td>Massively-parallel MapReduce</td><td>
<a href="http://labs.google.com/papers/mapreduce.html">
In 2004,
Google&#8217;s Jeffrey Dean and Sanjay Ghemawat revealed massively-parallel
MapReduce</a>,
a programming model that enables processing and generating large data sets
with huge clusters of parallel machines, yet is remarkably easy to program.
In this approach,
developers &#8220;specify a map function that processes a key/value pair to
generate a set of intermediate key/value pairs,
and a reduce function that merges all intermediate values associated
with the same intermediate key.&#8221;
The developers also specify an input reader, a partition function,
a compare function, and an output writer.
These are then fed to the MapReduce framework, which executes those definitions
on a potentially large distributed computer cluster, which
handles complications such as computer and network failure.
They note that &#8220;many real world tasks
are expressible in this model&#8221;.
Programmers without any experience with parallel or distributed systems
can, using this model, use large distributed systems to
handle large data sets.
The basic
<a href="http://en.wikipedia.org/wiki/MapReduce">
MapReduce approach</a> has since been implemented in other tools such as
<a href="http://lucene.apache.org/hadoop/">Hadoop</a> and
<a href="http://labs.trolltech.com/page/Projects/Threads/QtConcurrent">
Qt Concurrent</a>;
<a href="http://www.theserverside.com/tt/articles/article.tss?l=MapReduceRedux">
Eugene Ciurana has an article demonstrating how to use MapReduce approaches
(using Mule)</a>.
<a href="http://www.theregister.co.uk/2010/09/09/google_caffeine_explained/">
Google&#8217;s reworked search engine no longer uses mapreduce</a>, but
it is still widely-applicable to other projects.
<p>
It could easily be argued that this is obvious.
The sequential MapReduce algorithm has been
around for many decades, the idea of parallelizing a common sequential
algorithm is obvious, and the idea of automatically handling failed
computations is also obvious.
In particular, it could be argued that the real change here is that
parallel computers have become so inexpensive such algorithms are now
more useful.
But I must make a decision, and so I have decided to add it to the list.
US patent #7650331 and #7756919.
</td></tr>

</tbody>
</table>

<h1><a name="patents">Software Patents</a></h1>

<p>
One source that was <i>not</i> helpful for this analysis
were software patents.  The reason?
Software patents are actually harmful, not helpful, to software innovation, as
confirmed by a myriad of data.
Those unfamiliar with software patents may find that shocking.

<p>
There are several basic problems with software patents,
compared to actual innovation:
<ol>
<li>almost all truly important innovations in software were never
covered by patents, so using patents as a primary source would omit almost
all of the most important software innovations;
<li>as software patentability has increased, the number of
key software innovations has decreased; and
<li>software patents are often granted to cover ideas that are
obvious to practitioners of the art or have
prior art (even though these aren&#8217;t supposed to be patented).
</ol>

<p>
There are many reasons most of the most important software innovations
were never patented.
Historically, software was not patentable, and
it&#8217;s still not patentable in vast number of countries (including the EU).
Many believe software should never be patentable, and many of them
oppose software patents on ethical or moral grounds
as well as on pragmatic grounds (and many of them will not apply for patents
for these reasons).
For more about the many who oppose software patents, you can see the
<a href="http://ffii.org">ffii.org site</a> and the
<!-- Old: http://lpf.ai.mit.edu -->
<a href="http://progfree.org/">League for Programming Freedom</a>,
including statements by software vendor
<a href="http://progfree.org/Links/prep.ai.mit.edu/oracle.statement">Oracle</a> and
and a
<a href="http://progfree.org/Links/prep.ai.mit.edu/famous.people">list
of software luminaries opposed to software patents</a>
(including
<a href="http://progfree.org/Patents/knuth-to-pto.txt">Donald Knuth</a>).
<a href="http://www.bricklin.com/patentsandsoftware.htm">Dan Bricklin</a>
(inventor of the spreadsheet) explains why introducing patents to the
software industry, about 50 years after the industry began
(and after it had already been flourishing without them), is
a mistake and hardship.
AutoCAD&#8217;s co-author and Autodesk founder
<a href="http://www.fourmilab.ch/autofile/www/chapter2_105.html">John Walker
wrote &#8220;Patent Nonsense&#8221;</a>, where he states that
&#8220;Ever since Autodesk had to pay $25,000 to &#8216;license&#8217; a patent
which claimed the invention of XOR-draw for screen cursors
(the patent was filed years after everybody in computer graphics
was already using that trick), I&#8217;ve been convinced that software patents
are not only a terrible idea, but one of the principal threats
to the software industry... the multimedia industry
is shuddering at the prospect of paying royalties on every product
they make, because a small company in California has obtained an
absurdly broad patent on concepts that were widely discussed
and implemented experimentally more than 20 years earlier.&#8221;
<a href="http://www.forbes.com/asap/2002/0624/044.html">Forbes&#8217;
article &#8220;Patently Absurd&#8221;</a> also notes the problems of patents, as does
<a href="http://www.eweek.com/article2/0,1759,1666755,00.asp">eWeek</a>.
<!-- by Gary L. Reback, 06.24.02 -->
<a href="http://progfree.org/Whatsnew/survey.html">
One survey of professional programmers</a>
found that by a margin of 79.6% to 8.2%, computer programmers said
that granting patents on computer software impedes,
rather than promotes, software development
(the remaining 12.2% were undecided).
By 59.2% to 26.5% (2:1), most went even further,
saying that software patents should be abolished outright.  
Professors Bessen and Maskin,
two economists at the Massachusetts Institute of Technology (MIT),
have demonstrated in
<a href="http://www.researchoninnovation.org/patent.pdf">a report</a>
that introducing patenting into the software economy
only has economic usefulness if a monopoly is the most useful
form of software production.
This is concerning, because few believe that a monopoly is truly
the most useful (or desirable) form of software production.

<p>
Paul Vick, lead architect for Visual Basic .Net at Microsoft,
was required by his employer to file for a patent on an obvious
pre-existing idea (the IsNot operator), which the patent office
nevertheless granted --
<a href="http://www.panopticoncentral.net/archive/2004/11/20/2321.aspx">
Paul&#8217;s posting on Software patents</a>
states, &#8220;I don&#8217;t believe software patents are a good idea...
software patents generally do much more harm than good.
As such, I&#8217;d like to see them go away and the US patent office focus
on more productive tasks...
One of the most unfortunate aspect of the software patent system is that
there is a distinct advantage, should you have the money to do so,
to try and patent everything under the sun
in the hopes that something will stick....
[overwhelming the patent system.]
Microsoft has been as much a victim of this as anyone else,
and yet we&#8217;re right there in there with everyone else, playing the game.
It&#8217;s become a Mexican standoff, and there&#8217;s no good way out
at the moment short of a broad consensus to end the game
at the legislative level.
as far as the specific IsNot patent goes,
I will say that at a personal level, I do not feel particularly
proud of my involvement in the patent process in this case.&#8221;

<p>
As patentability has increased, there&#8217;s good evidence that
the number of software innovations has decreased.
<a href="http://www.researchoninnovation.org/patent.pdf">
Bessen and Maskin also demonstrated a statistical correlation between
the spread of patentability in the United States
and a decline in innovation in software</a>.
In particular, between 1987 and 1994 ,
software patents issuance rose 195%, yet real company funded R&amp;D
fell by 21% in these (software) industries
while rising by 25% in industries in general.
This paper gives additional evidence that software patents are
inversely related to innovation; it&#8217;s hard to not notice that
as patenting become more common (e.g., 1987 and later) that the number
of major innovations slowed down and are almost always not patented anyway.
Although these only show correlation and not causality, other data
suggest that there <i>is a causal relation</i>.
Their more recent book,
<a href="http://www.researchoninnovation.org/dopatentswork/">
&#8220;Patent Failure: How Judges, Bureaucrats, and Lawyers Put Innovators at Risk&#8221;
by James Bessen and Michael J. Meurer
(Princeton University Press, March 2008)</a> provides more information
about the failures of software patents.
Chapter 9 notes,
&#8220;In Chapter 7, we noticed that patents on software and especially
patents on business methods (which are largely software patents) stood
out as being particularly problematic. These patents had high rates of
litigation and high rates of claim construction review on appeal. This
chapter [argues] that there is,
in fact, something crucially different about software: software is an
abstract technology. This is a problem because at least since the 18th
century, patent law has had difficulty dealing with patents that claimed
abstract ideas or principles...
Such patents often have unclear boundaries and give rise to
opportunistic litigation...
Software also seems to be an area with large numbers of
relatively obvious patents. For these reasons, it is not surprising that a
substantial share of current patent litigation involves software patents...
no other technology has experienced anything like the broad industry
opposition to software patents that arose beginning during the 1960s.
Major computer companies opposed patents on software in their input
to a report by a presidential commission in 1966 and in
amici briefs to the Supreme Court in Gottschalk v. Benson in 1972.
Major software firms opposed software patents through the mid-1990s
(for example in USPTO hearings in 1994). Perhaps more
surprising, software inventors themselves have mostly been opposed to
patents on software. Surveys of software developers in 1992 and 1996
reported that most were opposed to patents...
Software patents... play a
central role in the failure of the patent system as a whole.
Any serious effort at patent reform must address these problems
and failure to deal with the problems of software patents...
will likely doom any reform effort.&#8221;
Thus, not only do software patents fail to help encourage innovation - they
actually inhibit innovation.

<p>
The book
<a href="http://levine.sscnet.ucla.edu/general/intellectual/againstfinal.htm">
&#8220;Against Intellectual Monopoly&#8221;
by Michele Boldrin and David K. Levine</a> presents a number of
evidences that software patents are harmful to the software industry
and users.
Actually, it goes much further, presenting evidence against patents
and copyrights in general, but it&#8217;s the evidence
against software patents that I find especially compelling.
<a href="http://fare.tunes.org/articles/patents.html">
Patents are an economic adsurdity</a> argues against patents in general,
but has some additional words on the specific problems of software patents.

<p>
<a href="http://online.wsj.com/article/SB121599469382949593.html">
L. Gordon Crovitz&#8217;s &#8220;Patent Gridlock Suppresses Innovation&#8221;
(Wall Street Journal, July 14, 2008, Page A15)</a>
states that
&#8220;for most industries [including software],
today&#8217;s patent system causes more harm than good...
Our patent system for most innovations has become patently absurd.
It&#8217;s a disincentive at a time when we expect software and other
technology companies to be the growth engine of the economy.
Imagine how much more productive our information-driven
economy would be if the patent system lived up to the intention of the
Founders, by encouraging progress instead of suppressing it.&#8221;


<p>
<a href="http://perens.com/Articles/PatentFarming.html">Bruce Perens
explains why patents cause serious problems in creating and implementing
standards.</a>
Since patents retard the creation and use of standards,
they also retard the industry as a whole (since relevant,
widely-implemented standards are a key need in the software industry).

<p>
The
<a href="http://webshop.ffii.org/">patented European webshop</a> is
an excellent illustration of the problem - it shows a few of the
many obvious, widely-used ideas of <i>granted</i>European patents.
In short, it demonstrates why patents are a poor match for software.

<p>
There are also many reasons why software patents are often granted
that cover obvious ideas and prior art
(which can give the <i>illusion</i> of innovation without actually having any).
As noted by an
<a href="http://www.law.com/jsp/article.jsp?id=1067350952811">
FTC analysis of patents</a>,
in the U.S. about 1,000 patent applications now arrive each day, so
patent examiners have from eight to 25 hours to read and understand
each application, search for prior art,
evaluate patentability,
communicate with the applicant, work out necessary revisions,
and reach and write up conclusions. 
(The article also notes -- somehow without irony -- that most
granted patents are in fact obvious to practitioners,
even though that is illegal.)
Many other studies have noted that
patent examiners have a poor database of prior art in
software, so it&#8217;s hard for them to find prior art.
But the biggest problem is that
there are <b>no incentives for anyone in the patent process to
reject bogus patents</b>.
The patent applicant has every incentive to ignore prior art, the
patent examiner has little time or resources to do this search, and
a patent examiner who doesn&#8217;t commit enough resources to the search is
rewarded (in contrast, a patent examiner who spends
too much time on each patent will be punished).
And it&#8217;s difficult for a patent examiner to declare something is &#8220;obvious&#8221;;
after all, the people who are paying money say that their
patent request isn&#8217;t obvious, and there&#8217;s little downside for an examiner to
agree with the petitioner.
Also, other areas of the software industry generally pay more than a
patent examiner&#8217;s salary, decreasing the likelihood that a
software patent examiner has the best software experience.
The entire software patent examination
process favors granting software patents for obvious and prior art.
The patent &#8220;review&#8221; process has become so much of a rubber stamp that
<a href="http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO1&amp;Sect2=HITOFF&amp;d=PALL&amp;p=1&amp;u=/netahtml/srchnum.htm&amp;r=1&amp;f=G&amp;l=50&amp;s1='6,368,227'.WKU.&amp;OS=PN/6,368,227&amp;RS=PN/6,368,227">
Steven Olson managed to obtain a patent on swinging sideways on a swing,</a>
an absurd patent that
<a href="http://www.newscientist.com/article.ns?id=dn2178">
was granted the U.S. patent process.</a>

<p>
It&#8217;s really difficult to figure out if something is really innovative.
Several of the &#8220;key innovations&#8221; listed above are actually
quite debatable.
For example, is massively-parallel MapReduce obvious?
It&#8217;s easy to argue that it is, and at the least,
I am certain that someone else would have created it within a year or two,
and I am very doubtful that the patent system incentivized its
creation at all.
Giving a monopoly on an idea that would have been created anyway,
for no societal gain, is bad policy.

<p>
Frankly, I think permitting software patents in the U.S. was a
tremendous mistake, and a misuse of the original patent laws.
Very few of the innovations listed here were patented, and of the few
that were (e.g., the mouse and RSA),
there&#8217;s little evidence that granting
the patents encouraged innovation.
The mouse patent never made much money for its inventor, and although the
developers of RSA did make money, there&#8217;s no evidence that they
would <i>not</i> have developed RSA without the offer of a patent.
All evidence seems to show that
these ideas would have occurred without the patents!
In short,
<i>patents impeded deployment and increased customer costs
without encouraging innovation</i>.
The patent laws were originally written to specifically
prevent patenting mathematical algorithms,
and lower courts have basically rewritten the laws to re-permit patenting of
mathematical algorithms (which is fundamentally what any software patent is).
Permitting software patents has done
almost nothing to encourage innovation nor award innovators,
and the harm that it&#8217;s done far, far exceeds any claimed good.
Most key software technology innovations were never patented,
so tracking patents is certain to miss most of the most important innovations.
Conversely, since patent examiners have a poor database of prior art in
software and there are no incentives for anyone in the patent process
to seriously search for prior art,
software patents are routinely granted for previous and obvious inventions
in software technology.
Basically, the number of patents granted for software
primarily shows how much money
an organization is willing to spend to submit patent applications -
it has nothing to do with innovation.
The W3C has noted that its policy of ensuring that
<a href="http://techupdate.zdnet.com/techupdate/stories/main/0,14179,2861820,00.html">all W3C standards were royalty free</a>
has been key to universal web access; anything else would cause
dangerously harmful balkanization.
<a href="http://www.groklaw.net/article.php?story=20050216080321140">
Vint Cerf stated that part of the reason the Internet protocols
took hold so quickly and widely was that he and Dr. Kahn
made no intellectual property (patent) claims to their invention.</a>
&#8220;It was an open standard that we would allow anyone to have access to
without any constraints.&#8221;

<p>
<a href="http://www.researchoninnovation.org/softpat.pdf">
&#8220;The Software Patent Experiment&#8221; by
James Bessen
(Research on Innovation and Boston University) and
Robert M. Hunt (Federal Reserve Bank of Philadelphia)</a>
is a sobering less-technical summary of important research they
did on software patents.
They found that in the 1990s, the firms that were <i>increasingly</i>
patenting software were the ones that were
<i>decreasing</i> their research and development -- that is,
patents are <i>replacing</i> research and development, not encouraging it.
They found strong statistic evidence that patents in the software field
<i>do not</i> provide an incentive for research and development --
the vast majority of software patents are obtained by
firms outside the software industry which have little investment in
the software developers required to develop software inventions.
They don&#8217;t say it directly, but their research results seem
to clearly show that
software patents have become legalized extortion,
instead of a means to encourage innovation.
<!-- Not as directly on-topic:
Another interesting perspective is
<a href="http://video.google.com/videoplay?docid=280262988255234681&amp;hl=en">
Stephan Kinsella's "The Intellectual Property Quagmire" aka
"Rethinking IP Completely"</a>.
-->

<p>
The software industry&#8217;s solution has been to cross-license patents
between companies, creating a sort of software patent detente.
More recently, this has included cross-licensing
patents with the open source community
(through mechanisms like the
<a href="http://www.openinventionnetwork.com/">Open Invention Network</a>).
Of course, such mechanisms tend to inhibit newcomers, so
software patents&#8217; primary impact is to prevent new ideas from
becoming available to end-users, subverting the official justification
for them.
The only group that seems to be unambiguously aided by software patents
are patent lawyers - and since they make the rules, they are
happy to have them.

<p>
Of course, this fails when someone decides to sue.
So-called "patent trolls" do not make products, only lawsuits, and
thus have no reason to acknowledge detente.
In the mobile space, Apple has decided to try to prevent the sales of
all competition by filing patent lawsuits.
The result has been more lawsuits, and less innovation.

<p>
Any statistic based on software patents is
irrelevant when examining software innovation --
because today&#8217;s software patents have nothing to do with innovation.
<a href="http://endsoftpatents.org/">End Software Patents</a> is an
organization that is trying to eliminate the nonsense of software patents;
I hope they succeed, since they are harming instead of helping innovation.


<h1><a name="notinnovation">What&#8217;s Not an Important Software Innovation?</a></h1>

<p>
As I noted earlier, many important events in computing aren&#8217;t
software innovations, such as the announcements of new hardware platforms.
Indeed, sometimes the importance isn&#8217;t in the technology at all;
when IBM announced their first IBM PC, neither the hardware nor software
was innovative - the announcement was important primarily because IBM&#8217;s
imprimateur made many people feel confident that it
was &#8220;safe&#8221; to buy a personal computer.
<p>
An obvious example is that smartphones are not a software innovation.
In the mid-2000s, smartphones rapidly became more common.
By "smartphone" I mean a phone that can stay connected to the Internet,
access the internet with a web browser capable of running programs
(e.g., in Javascript), and install local applications.
There's no doubt that widespread smartphone availability
has had a profound impact on society.
But while smartphones have had an important social impact, smartphones
do not represent any siginificant software innovation.
Smartphones typically run operating systems and middleware that are
merely minor variants of software that was already running on other systems,
and their software is developed in traditional ways.

<p>
Note that there are few software innovation identified in recent times.
I believe that part of the reason is that over the last number of years
some key software markets have been controlled by monopolies.
Monopolies typically inhibit innovation; a monopoly has a strong financial
incentive to keep things more or less the way they are.
Also,
it&#8217;s difficult to identify the &#8220;most important&#8221; innovations
within the last few years.
Usually what is most important is not clear until years after its development.
<a href="http://www.softwarereality.com/soapbox/softwarefashion.jsp">
Software technology, like many other areas, is subject to fads</a>.
Most &#8220;exciting new technologies&#8221; are simply fashions that will turn out
to be impractical (or only useful in a narrow niche),
or are simply rehashes of old ideas with new names.
It is even possible that the emergence of software patents has
impeded, instead of promoted, innovation in software, since many innovations
occurred when software patents were not permitted.

<p>
Standards are extremely important in computing (just as they are in many
other fields).
In earlier versions of the document I noted that
standards long preceded computing, and did not note them as an innovation.
However, I&#8217;ve since added an entry for vendor-independent standards.
The notion of having computing standards was not something that
immediately came to mind in the computing industry - so the <i>notion</i>
of having <i>computer-related</i> standards is now included above as
an innovation.
There are many important events in computing history involving standards,
but very few standards are listed above as innovations... and for good reason.
Standards themselves generally do not try to create significant
new innovations, and rarely work well when they do.
Instead, standards usually attempt
to create agreements based on well-understood technology, where the
innovations have already been demonstrated as being useful.
Any significant innovation embodied in a standard was usually developed and
tested many years before the standards&#8217; development.

<p>
<h2>Other Technologies that are not innovative</h2>
<p>
Here are a few technologies that, while important, aren&#8217;t really
innovative:
<ol>
<li>XML.  XML is simply a simplified version of SGML, which has been
around for decades.
<li>SOAP.  SOAP is yet another remote procedure call system, employing
XML (and often HTTP).
</ol>

<p>
<h2>It&#8217;s okay to not be innovative</h2>

<p>
There&#8217;s nothing wrong with a technology or product not being innovative.
Indeed, a technology or product should primarily be measured as whether or not
it solves real world problems (without causing more problems than it solves).
<a href="https://lwn.net/Articles/445687/">Linus Torvalds,
creator of the Linux kernel</a>,
has stated that a pet peeve of his is that
&#8220;there is a great deal of talk about &#8216;innovation&#8217; and
&#8216;vision.&#8217; People want to hear about the one big idea that changes the
world, but that&#8217;s not how the world works. It&#8217;s not about visionary ideas;
it&#8217;s about lots of good ideas which do not seem world-changing at the
time, but which turn out to be great after lots of sweat and work have
been applied.&#8221;
Instead, the Linux kernel (which has been wildly successful)
is the result of lots of small ideas contributed by lots
of people over a long time.
<p>
The focus of this paper is innovation, not utility.
Do not confuse innovation with utility.

<h1><a name="conclusions">Conclusions</a></h1>

<p>
Clearly, humankind has been impacted by major new innovations
in software technology.
But the number of major new innovations is smaller than you might expect,
especially given the many who declare that software technology
&#8220;changes rapidly.&#8221;
If you only consider major new innovations in software technology,
instead of various updates to software products,
fundamental software technology is&nbsp; <i>not</i> changing as
rapidly as claimed by some.

<p>
I believe that this list is evidence that
people are far more affected by other issues in computing than by
major new software innovations.
In particular,
I believe that there are at least three reasons for the illusion of
rapid changes in major software technology:
<ol>
<li>
People have been able to apply computing technology to
more and more areas due to rapidly decreasing costs.
Computer hardware performance has improved exponentially, its
size has dropped significantly, and its cost has decreased exponentially,
making it possible to apply computing technology in more and more situations.
The increasing hardware performance has also allowed developers to use
techniques that decrease development time by increasing computing time;
this trade reduces the development cost and time for software, again making
it less costly to apply computing technology (by reducing the cost
of software development).
The idea of automating actions is not, by itself innovative, but automation
can certainly change an environment.
<li>
Increasing use begets increasing use.
And when a technology is widespread or ubuiquitous, it often enables
many widespread uses and social changes.
In those cases, people are feeling multiple rapid social changes,
caused by the widespread availability of a technology, rather than
multiple rapid changes and innovations in the technology itself.
When the web browser was first introduced, comparatively few people used
it because there was relatively little information or services
of interest to them available through it.
But once some was available, other providers of information and services
had users/customers, enticing them to use the WWW, causing an
exponential increase in use.
A service can become particularly influential if it becomes a standard
(either because it&#8217;s formally specified as a &#8220;de jure&#8221; standard,
or simply through widespread use as a &#8220;de facto&#8221; standard).
The idea of creating standards is not new, but once something becomes a
standard, the idea&#8217;s
very ubiquity can mean that it will be widely used in places
that it wouldn&#8217;t be used before.
<li>
Software functionality can be changed over time,
adding new functionality and generalizing a particular program&#8217;s capabilities.
However, when functionality is changed over time, this often requires
that the human interfaces change as well.
As a result, people constantly have to learn how to handle changes in
a given program&#8217;s interface.
This gives some the illusion of constant innovative
change in software technology, while
instead, what is changing is a particular implementation.
</ol>

<p>
Intriguingly, one of the richest and most powerful software companies,
Microsoft, did not create any major software innovation
as identified in this list.
Microsoft did not even create the first useful or widely-used
implementation of any major software innovation.
Others have come to the same conclusions, for example, see the
<a href="http://www.vcnet.com/bms/departments/innovation.shtml">
Microsoft &#8220;Hall of Innovation&#8221;</a>.
This could be considered odd, since in the early 2000's Microsoft
was claiming to be an innovative company.
At the least this shows that <i>innovations</i>, on their own, are often not
the most important part of creating software.
For more (dated) information about this, see
<a href="http://dwheeler.com/innovation/microsoft.html">Microsoft,
the Innovator?</a>.

<p>
In contrast, several major innovations were first implemented as
<a href="http://dwheeler.com/oss_fs_refs.html">open source software /
Free Software (OSS/FS)</a> projects, especially for those
involving networks.
Examples of innovations initially released as OSS/FS
or first widely distributed as OSS/FS
include DNS, web servers, the TCP/IP
implementations on BSD systems to create internetworks using datagrams,
the first spell checker, and the initial implementation of
lockless version management.
<a href="http://news.cnet.com/news/0-1014-201-8155733-0.html">
Tim Berners-Lee, inventor of the World Wide Web,
stated in December 2001</a> that
&#8220; A very significant factor [in widening the Web&#8217;s use beyond
scientific research] was that the software was
all (what we now call) open source.
It spread fast, and could be improved fast - and it
could be installed within government and large industry
without having to go through a procurement process.&#8221;
This may be because the ideas of open source software are quite similar to
research approaches in general, e.g., in both systems
publications are available to all
and can be used as the basis of further work (as long as credit is given).
The paper
<a href="http://firstmonday.org/issues/issue9_1/bonaccorsi">
Altruistic individuals, selfish firms?
The structure of motivation in Open Source Software</a>
found in a 2002 survey of 146 Italian firms that their primary reason
for supplying OSS/FS programs was that
&#8220;Open Source software allows small enterprises to afford innovation&#8221;.
For more information, see
<a href="http://dwheeler.com/oss_fs_why.html#innovation">the
information on OSS/FS innovation from my paper,
&#8220;Why OSS/FS? Look at the Numbers!&#8221;</a>

<p>
It&#8217;s important to not overstate the value of innovation.
As noted in Shapin&#8217;s article
<a href="http://www.newyorker.com/arts/critics/books/2007/05/14/070514crbo_books_shapin?currentPage=all">What Else Is New?</a>,
just doing something radically different does not make it important.
Indeed, we are surrounded by &#8220;old&#8221; technology that still serves us well.
An useful innovation has to be <i>useful</i>, and not just be a new idea.
Even more importantly, it has to be <i>implemented well in a form
that can be easily used</i>.
That said, sometimes new ideas truly are useful, and when they are, they
can improve our world.

<p>
In addition, it&#8217;s important to note that innovation is primarily
a matter of incremental improvement and hard work.
While I think it&#8217;s useful to note dates (where possible) for
new innovations, it can give the illusion that innovation is primarily a
matter of &#8220;Eureka!&#8221; moments that change everything.
As noted in
<a href="http://www.nytimes.com/2008/02/03/business/03unbox.html">
Eureka! It Really Takes Years of Hard Work</a>
(by Janet Rae-Dupree, New York Times, February 3, 2008),
innovation is (primarily) &#8220;a slow process of accretion,
building small insight upon interesting fact upon tried-and-true process.&#8221;
<a href="http://www.itworld.com/development/124541/programmers-who-defined-the-technology-industry-where-are-they-now?page=0%2C2">
Jonathan Sachs (co-developer of Lotus 1-2-3) similarly stated</a>,
&#8220;The rate of innovation is rather slow.
There are only a few really new ideas every decade.&#8221;
See Scott Berkun&#8217;s 2007 book &#8220;The Myths of Innovation&#8221; for more.

<p>
Similarly, just writing a paper usually doesn't capture what's important
about an idea.
<a href=https://markusstrasser.org/extracting-knowledge-from-literature/"
>The Business of Extracting Knowledge from Academic Publications (2021)</a>,
explains that,
"Close to nothing of what makes science actually work is published
as text on the web...
Systems, teams and researchers matter much more than ideas...
the complexity threshold kept rising and now we need to grow companies
around inventions to actually make them happen...
incumbents increasingly acqui-hire instead of just buying the
IP and most successful companies that spin out of labs have someone
who did the research as a cofounder. Technological utopians and ideologists...
underrate how important context and tacit knowledge is."

<p>
Software technology changes, but that is not software&#8217;s primary impact on us.
Software primarily impacts us because of its ubiquity and changeability,
as the computers that software controls become ubiquitous and the software
is adapted to changing needs.

<!--
Software is not impacting people primarily due to rapid
fundamental changes in software technology.
It's impacting people
because computing's flexibility and falling cost lend itself to ever
increasing use of software.
-->


<h1 class="unnumbered"><a name="innovations-considered">Appendix: Software Innovations Being Considered</a></h1>

<p>
No list of &#8220;software innovations&#8221; can be complete.
At the least, there is always the hope that there will be new innovations,
ones that we have not even heard of yet.
But innovations must be given time to see if they are truly important,
or simply a fad that will quickly fade away.
Another problem is that it is sometimes difficult to track backwards to
find out <i>when</i> an idea was created, or by <i>who</i>.
There is also the challenge of determining if it was really innovative,
and what its impact was; in some cases
I&#8217;m not sure if they should be
in the list or not.

<p>
Some ideas have been identified that may be added to
future versions of this document.
Here are ones that aren&#8217;t currently on the list,
but I may add someday:
<ol>
<li>Algorithms - Euclid (GCD), and before.
<li>Backus-Naur Form (BNF), a format for defining language syntax.
<li>Abstract Data Types (ADTs) - this preceded object-orientation, but a &#8220;first&#8221; use seems to be very hard to find.
<li>Big O / Complexity theory
<li>Hashes

<li>Transactions (esp. database transactions) - it
can be a little difficult to determine the origins of the idea of transactions.
David Lomet (who did key work in this area)
has helpfully pointed out to me three papers, along with useful
recollections of his:
<ul>
<li>Obermarck, R. (1980) &#8220;IMS Program Isolation Feature&#8221;. IBM San Jose
Research Report RJ 72879. [This was written about 10 years after the
product containing the feature was released. IMS never quite abstracted
the notion completely, but it the properties are there if one looks
carefully.]
<li>Eswaren, K., Gray, J., Lorie, R., Traiger, I. &#8220;The Notions of
Consistency and Predicate Locks in a Database System.&#8221; Comm. ACM 19
(11). 1976 [This is the first paper that clearly identifies
transactions in the context of databases. Several flavors are involved.
Unfortunately, the title isn&#8217;t very transparent on this point.]
<li>Lomet, D. Process structuring, synchronization, and recovery using
atomic actions. ACM Conf. on Language Design for Reliable Software,
Raleigh, NC SIGPLAN Notices 12,3 (Mar 1977). [Yes, I have some claim on
this. I independently discovered the notion of &#8220;atomic procedure&#8221; while
working with Brian Randell at the U. of Newcastle on system
reliability.]
</ul>

<li>Recursion (though this was a mathematical concept predating computers, and might not really be a &#8220;software&#8221; innovation at all)
<li>Database management systems (it is very difficult to trace back to the
  &#8220;first&#8221; ones)
<li>Operating systems (again, it&#8217;s very difficult to trace back to &#8220;first&#8221; ones)
<li>Aspect-oriented programming
<li>The &#8220;Page Rank&#8221; algorithm used by Google
<li>Directories (X.500 and before) - these are read-mostly databases, implemented later by LDAP (and still later by Active Directory)
<li>Bazaar-style programming/development (e.g., Linux; noted by Raymond)
<li>Open-source software / Free Software (see above)
<li>Fourth-generation programming languages (4GLs)
<li>Wiki+P2P (they change Knowledge distribution and coordination!).

<li>Data compression.
It&#8217;s difficult for me to determine if this was &#8220;obvious&#8221; or not;
I suspect it was obvious, which is why it&#8217;s not currently
in the list above.
My thanks to
Aaron Brick for this interesting idea, as well as his pointing
out the Shannon-Fano (1949ish) lossless encoding approach,
as well as the lossy compression landmarks in
the 1928 analog vocoder and the 1981 block truncation coding.
<li><a href="http://en.wikipedia.org/wiki/Sketchpad">Sketchpad</a>
<li>Flowcharts.
<a href="http://en.wikipedia.org/wiki/Flow_process_chart">Flowcharts</a>
are actually older than the computer, and were used
for process improvement, and is related to
<a href="http://en.wikipedia.org/wiki/Scientific_management">Scientific management</a>.
A key person in this field was
<a href="http://en.wikipedia.org/wiki/Frank_Bunker_Gilbreth,_Sr.">
Frank Bunker Gilbreth, Sr.</a> (described in the delightful book
<i>Cheaper by the Dozen</i>).
However, flowcharts are essentially unused today (unless you count
XKCD cartoons), and I think the other material already cited is more
foundational, which is why it&#8217;s not currently on the list.
<li>Chomsky&#8217;s context-free grammars.  I did include BNF (above).
Technically these aren&#8217;t about computing, but are more general concepts
that can be applied to computing as well.
<li>Leibniz&#8217;s work on
<a href="http://en.wikipedia.org/wiki/Gottfried_Leibniz#Symbolic_thought">
Symbolic thought</a> and
<a href="http://en.wikipedia.org/wiki/Gottfried_Leibniz#Computation">
Computation</a>.
I do include Boole and Babbage, and
Leibniz is a towering figure in math, and his idea that human disagreements
could be resolved someday by exclaiming &#8220;Calculemus&#8221;
(&#8220;let us calculate&#8221;) has been inspirational.
His work on binary numbers, and on developing machines to do arithmetic,
make him a plausible addition.

<li>REST.  In some sense, this is a pattern that pre-existed for years,
and has more recently been identified as a pattern for use.
In some ways, this is already covered by the Design Patterns item.

<li>
<a href="http://en.wikipedia.org/wiki/Bulletin_board_system">
Bulletin board systems (BBSs)</a>.  I have Usenet, but Usenet
created an interesting new approach for distributing data across a larger
network.
Bulletin board systems started off by more-or-less implementing an
electronic equivalent of paper-based bulletin board systems, which is why
I haven&#8217;t included BBSs.

<li>Software Plugin (suggested by Ciaran Carthy)
</ol>


<p>
The
<a href="http://awards.acm.org/software_system/">
Association for Computing Machinery (ACM) Software Systems Award</a>
is another good place to look for a list of software innovations.
However, it is a fantastic list of important software systems and
the worthy people who created them,
many of these systems are not themselves fundamentally innovative.
For example, James Gosling rightly received an award for Java in 2002;
Java is an important programming language, but it is basically
a well-engineered design that combined already-existing concepts, and
was not fundamentally innovative in the way intended by this list.
This is not to slight the important work by Gosling and others;
it&#8217;s just that good engineering results are not the focus of this list.


<!--
Extreme programming - I don't think it's all that innovative
-->


<!--
Proposed by Robert Steinke:
"IVY: a Shared Virtual Memory System for Parallel Computing"
by Kai Li
Published in "Proceedings of the 1988 International Conference on Parallel Processing"
vol 2 Aug 1988 pp 94-101

NON-innovations:
  WSDL (DTD, db schemas)
  Microsoft

This is used in at least one course:
http://www.di-mare.com/adolfo/cursos/2006-2/ci-1201.htm
  
"10 papers for the PhD student in Networking"
http://www.cs.wayne.edu/~hzhang/group/miscl/10PapersInNetworking.pdf
tries to list the 10 most important papers in the area, e.g.,
the RPC paper.


From Ciaran Carthy ciaran.carthy at dkit.ie

The Software Plugin.
===============

I refer to the benefit of being able to acquire third party software to 
extend an existing application or to provide interoperability AT RUN 
TIME. Let us focus for now on the case where the plugin executes i the 
context of the process it is interoperating with. Today when used on a 
single computer it might use a combination of :
- shared libraries in C++ or dynamic class loading in Java and
- the specification of thin interfaces to enable the plugging in of 
different implementations of APIs.

When I was implementing Display Drivers for Autocad the plugin mechanism 
was TSR programming !

There are many benefits for the software companies and end users. When 
did it start? Perhaps when dynamic libraries were introduced? But then 
what about Java Servlets plugging into Web servers and the like. That is 
an example of the same thing using a higher level mechanism. I think the 
innovation most have occurred at the point when somebody said "there is 
a way for the software of company A and company B to interoperate 
without IPC AND without the need to distribute their software as a 
single bundle". It must have been a long time during which people 
assumed this would never happen.

I think the plugin is a great innovation. It was a long time becoming 
established and we wouldn't like to do without it today. Now that I 
think of it, it applies to all device drivers, right? For example an 
ext4 file system and an NTFS file system both work on any USB device or 
IDE or SATA device without change, thanks to the the specification of a 
uniform interface to block devices and a means to plug in alternative 
implementations of that interface. Is the innovation in fact abstraction 
? Should abstraction be in your list? Or should we call this one 
software layers?

Isn't it marvellous and convenient that a clean interface between 
application and file system lets you mount a remote directory via SAMBA 
to make it appear part of your overall FSH?

Therefore, would you debate with me the inclusion of one or more of 
these innovations:
-APIs
-Dynamic Libraries
-Abstraction
-The Software Plugin


-->


<p>
<hr>
<p>
My thanks to Robert Steinke for noting CSP and Lamport Clocks as
key software innovations.
<p>
You can get a copy of this document at
<a href="http://dwheeler.com/innovation">http://dwheeler.com/innovation</a>.
Feel free to examine my home page at
<a href="http://dwheeler.com">http://dwheeler.com</a>.

</body>
</html>

