<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html lang="en-US">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<title>High Assurance (for Security or Safety) and Free-Libre / Open Source Software (FLOSS)... with Lots on Formal Methods / Software Verification</title>
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="description" content="A discussion on the relationship of free-libre / open source software (FLOSS) and high assurance">
<meta name="keywords" content="high assurance, high confidence, high integrity, security, software security, software assurance, safety, software safety, formal verification, verification, safety critical, open source software, open source, free software, FLOSS, FOSS, formal methods, formal method, formal verification, Ada, ACL2, PVS, HOL, proof, theorem provers, risk, risks, David, Wheeler, David A. Wheeler, David Wheeler, Otter, Prover9, Mace, Spin, SMV, PVS, SPARK, Ada, GNAT, theorem prover, model checker, verified software">
<link rel="stylesheet" type="text/css" href="paper.css">
<meta name="generator" content="vim">
</head>
<body>

<h1 class="title">High Assurance (for Security or Safety) and Free-Libre / Open Source Software (FLOSS)... with Lots on Formal Methods / Software Verification</h1>
<center>by <a href="https://dwheeler.com">David A. Wheeler</a>, 2006-06-02 (updated 2013-11-24)</center>

<p>
<i>
This paper discusses some relationships between
high assurance software (for security or safety) and
free-libre / open source software (FLOSS).
In particular, it shows that many tools for developing high assurance
software have FLOSS licenses, by identifying FLOSS tools for
<a href="#cm">software configuration management</a>,
<a href="#testing">testing</a>,
<a href="#formalmethods">formal methods</a>,
<a href="#analysisimp">analysis implementation</a>, and
<a href="#codegen">code generation</a>.
It particularly focuses on formal methods, since formal methods are
rarely encountered outside of high assurance.
However, while high assurance components are rare, FLOSS high assurance
components are even rarer.
This is in contrast with medium assurance, where there are a vast number
of FLOSS tools and FLOSS components, and the security record of FLOSS
components is quite impressive.
The paper then examines why this is the circumstance.
The most likely reason for this appears to
be that decision-makers for high assurance components
are not even considering the possibility of FLOSS-based approaches.
The paper concludes that in the future,
those who need high assurance components should
consider FLOSS-based approaches as a possible strategy.
The paper suggests that
government-funded software development in academia
normally be released under a GPL-compatible FLOSS license
(not necessarily the GPL), to enable others to build on what
tax dollars have paid for, and to prevent the vast waste of effort
caused by current processes.
Finally, developers who want to start new FLOSS projects should
consider developing new high-assurance components or tools;
given the increasing attacks and
dependence on computer systems, having more high assurance
programs available will be vital to everyone&#8217;s future.
</i>

<p>
<h1><a name="introduction">Introduction</a></h1>
<p>
This paper discusses some relationships between
high assurance software (for security or safety) and
free-libre / open source software (FLOSS).
First, let&#8217;s define these key terms.

<h2>Definitions</h2>
<p>
<a href="https://dwheeler.com/oss_fs_why.html">
Free-libre / open source software (FLOSS)</a> is software
whose license gives users the freedom to run the program
for any purpose, to study and modify the program, and to
redistribute copies of either the original or modified program
(without having to pay royalties to previous developers).
It&#8217;s also called libre software, Free Software, Free-libre software,
Free-libre / Open source (FLOS) software, or open source software /
Free Software (OSS/FS).
The term &#8220;Free Software&#8221; can be confusing, because
there may be a fee for the Free Software; the term
&#8220;Free&#8221; is derived from &#8220;freedom&#8221; (libre),
not from &#8220;no price&#8221; (gratis).
More formal definitions of
<a href="http://www.opensource.org/docs/definition.html">open source
software</a>
and
<a href="http://www.gnu.org/philosophy/free-sw.html">
free software (in the sense of libre software)</a>
are available on the web.
Examples include the Linux kernel, the gcc compilation suite,
the Apache web server, and the Firefox web browser.
Many FLOSS programs are commercial, while many others are not.

<p>
For purposes of this paper,
let&#8217;s define &#8220;high assurance software&#8221;
as software where there&#8217;s an <i>argument
that could convince skeptical parties</i>
that the software <i>will always perform or never perform</i>
certain key functions <i>without fail</i>.
That means you have to show convincing evidence that there are
<i>absolutely no software defects</i>
that would interfere with the software&#8217;s key functions.
Almost all software built today is <i>not</i> high assurance;
developing high assurance software is currently a specialist&#8217;s field
(though I think all software developers <i>should</i> know a little about it).
To develop high assurance software you must apply many development
techniques much more rigorously, such as
<a href="#cm">configuration management</a> and
<a href="#testing">testing</a>.
You need to use
<a href="#codegen">implementation tools</a> you can trust your life to.
And in practice, I believe that you
need to use mathematical techniques called
&#8220;<a href="#formalmethods">formal methods</a>&#8221;
for a product to be high assurance, for the simple reason
that it&#8217;s usually hard to create truly convincing arguments otherwise.
A significant fraction of this paper covers formal methods, since they are
rarely encountered outside of high assurance.
There isn&#8217;t a single universal definition of the term high assurance,
and products have been labelled &#8220;high assurance&#8221;
without having any formal methods applied to them.
But this definition should be sufficient for my purpose.
Other terms used for this kind of software are &#8220;high integrity&#8221;
and &#8220;high confidence&#8221; software.

<p>
Usually high assurance software is developed because of serious
safety or security concerns.
Strictly speaking, software by itself has no safety or security
properties -- it can only be safe or secure in the context
of a larger system.
A nice discussion of this issue from the safety point of view
is in Nancy Leveson&#8217;s book <i>Safeware</i> (see section 8.3).
But software tends to control safety and security systems, and such
software is often called
&#8220;safe software&#8221; or &#8220;secure software&#8221;.
For this paper I&#8217;ll talk
about the security or safety of the software,
with the understanding that this only makes
sense if you understand the system that the software will be part of.

<p>
For purposes of this paper, the identity of the software&#8217;s supplier
is <i>not</i> part of the definition of high assurance.
By supplier, I mean the
provenance (origin) and pedigree (lineage -- who the software passed through)
of the software.
By keeping the supplier identity out of the definition of high assurance,
I can concentrate on technological issues.
In reality, there may be some people who you wouldn&#8217;t
trust even if they&#8217;d &#8220;proved&#8221; their code correct...
so in practice it&#8217;s quite reasonable to ask questions like,
&#8220;Who developed or modified the software? Can I trust them?&#8221;
For both FLOSS and proprietary software, provenance and pedigree can be
considered in exactly the same way --
in both cases, you&#8217;d consider who originally developed the
software (in terms of each change), and who controlled the software
from development through deployment to you.
In particular, you&#8217;d consider who has rights to modify the software
repository, and whether or not you trusted them.
You might also consider how well the development environment itself
is protected from attack.
Don&#8217;t be fooled into thinking that FLOSS is &#8220;riskier&#8221;
than proprietary software because it can be legally modified by anyone.
Anyone can modify a proprietary program with a hex editor, too --
but that doesn&#8217;t mean you&#8217;ll use that modified version.
The issue with suppliers is who controls <i>your</i> supply chain,
and FLOSS often has an advantage in provenance and pedigree
(because it is often easier with FLOSS to determine
<i>exactly</i> who did what, and who has modification rights).
But provenance and pedigree issues have to be handled on a case-by-case basis,
and trying to cover those issues as well would over-complicate this paper.
For example, the whole issue of
&#8220;who trusts who&#8221; varies depending on the organizations
and the circumstances.
In an ideal world this wouldn&#8217;t
matter, because the proofs would be true and could be
rechecked everywhere.
Given the massive move to globalization,
I think that would be worth trying to make
<i>who</i> created the software irrelevant.
In any case,
let&#8217;s concentrate on the technical aspects in this paper.

<h2>Contrasting levels of assurance</h2>
<p>
More generally, assurance is simply the amount of
confidence we have that the software will do <i>and not do</i>
the things it should <i>and should not</i>.
Sometimes the things it should <i>not</i> do, what I call
"negative requirements" , are the most important.
Any particular piece of software can
be considered by someone to be low, medium, or high assurance.
This is obviously a qualitative difference; two products could be
in the same assurance category, yet one be more secure than another.
<p>
For purposes of this paper,
let&#8217;s define medium assurance to be software
which doesn&#8217;t reach high
assurance levels, but where
there has been significant effort expended to find and remove important
flaws through review, testing, and so on.
Note that when creating medium assurance software,
there&#8217;s no significant effort to <i>prove</i>
that there are no flaws in it,
merely an effort to find and fix the flaws.
<p>
Medium assurance software must undergo testing and/or
peer review to reduce the number of flaws.
Such mechanisms can be really valuable in reducing flaws, and
eliminate a great many of them, but the normal method
of using these mechanisms won&#8217;t guarantee their absence.
You can eliminate some <i>types</i> of flaws completely by some activities,
e.g., you can completely eliminate buffer overflows by choosing almost
any programming language other than C or C++... but doing so
would not eliminate <i>all</i> flaws!
In particular,
testing by itself is impractical to prove anything about real software.
After all,
exhaustively testing a program that just adds three numbers
would take 2.5 billion years (assuming each number was 32 bits,
you could run the program a billion times per second, and you
used 1,000 computers for testing).
Real programs are much more complicated than this,
which is why testing by itself can&#8217;t reach
the highest levels of assurance.

<p>
The differences between medium and high assurance (as I mean the
terms in this paper) seem to confuse people, so let me contrast them directly.
When developing a high assurance program,
the program is presumed to be wrong (guilty) until
a preponderance of evidence proves that it&#8217;s correct.
When a medium assurance program is being developed,
it is spot-checked in various ways throughout its development
to try to detect and remove some of its worst defects.
Medium assurance software development normally
leave some defects in the program afterward.
Few like the presence of latent defects, but
few people are willing to pay for (or invest the time)
for high assurance development techniques today in most software.

<p>
It&#8217;s reasonable to think that as technology improves,
high assurance programs will become more common.
But even today there are some situations where medium assurance is not enough.
Typically this is where people&#8217;s lives,
or the security of a nation, is at stake.
In such cases, some of today&#8217;s customers need
serious evidence that there are no critical defects of any kind.
They need something different: High assurance.

<!--
Medium and high assurance are fundamentally different in concept.
In medium assurance development approaches,
a series of techniques are used to
significantly reduce the likelihood of serious defects in the application.
In high assurance, we prove it&#8217;s &#8220;perfect&#8221;
(for a set of critical requirements).
So far, few customers have been willing to spend the extra money necessary,
and give up functionality, to acquire high assurance software
instead of medium assurance.
-->

<h2>High assurance challenges and standards</h2>
<p>
Ideally, all software would be high assurance, but ideally we&#8217;d all
live in mansions.
It&#8217;s very difficult to create truly high assurance software.
The configuration management and testing requirements are usually
more severe (and time-consuming) than those for other
kinds of software.
Applying formal methods requires significant mathematical training
that most software developers don&#8217;t have, and can be very
time-consuming.
<p>
Because of these challenges,
high assurance software is usually only developed for
critical security or safety components.
When creating critical security or safety components,
a number of regulations are often imposed.
<p>
High assurance software for security is the point of the
<a href="http://niap.nist.gov/cc-scheme/cc_docs/index.html">
Common Criteria for IT Security Evaluation (ISO standard 15408)</a> when
you select EAL 6 or higher -- and EAL 6 is really a compromise!
For purposes of this paper, medium assurance software is in
the EAL 4 to 5 range of the Common Criteria,
so Red Hat Linux and Microsoft Windows would both be considered
medium assurance products.
I consider EAL 2 (or less) to be low assurance; EAL 3 is a compromise,
but it&#8217;s basically low assurance.
<p>
Here are some other standards that are often mentioned in the security
or safety world, which often impact this kind of development:
<ul>
<li>Security:
Besides the Common Criteria,
the older (superceded)
<a href="http://www.radium.ncsc.mil/tpep/library/rainbow/5200.28-STD.html">
Orange Book</a> defined a set of requirements; level B3 had a number of
requirements aimed at high assurance,
and level A1 extended them even further.
(Again, A1 was clearly high assurance, with the next highest level B3
being a kind of compromise.)
While not officially required, the FAA and DoD have developed
<a href="http://www.faa.gov/ipg/">
Safety and Security Extensions for Integrated Capability Maturity Models</a>.
In the U.S. national intelligence world, another software security standard is
<a href="http://www.fas.org/irp/offdocs/DCID_6-3_20Policy.htm">
DCID 6/3</a>; its confidentiality protection level 5 imposes many requirements
(<a href="http://www.fas.org/irp/offdocs/dcid.htm">see the DCID 6/3 manual</a>
if you want to know what each level means).
<!-- Director of Central Intelligence. "Protecting Sensitive Compartmented Information Within Information Systems." Directive 6/3. Washington: DCID, 5 June 1999 www.fas.org/irp/offdocs/DCID_6-3_20Policy.htm. -->
<li>Safety:
<a href="http://www.rtca.org">
RTCA DO-178B</a> (Software Considerations in Airborne Systems and
Equipment Certification), particularly its Level A, is often mentioned
in this context
(DO-178B is known to have problems -- 
<a href="http://www.era.co.uk/assc/DO178B.asp">
following it by rote will not necessarily lead to highly assured systems,
according to a draft ASSC DO-178B study</a>).
Other oft-noted standards related to software and safety are
<a href="http://www.arinc.com/cf/store/index.cfm">
ARINC-653</a> (Avionics Application Software Standard Interface),
EUROCAE ED-12B (Software Consideration in Airborne Systems
and Equipment Certification),
the U.S. Department of Defense&#8217;s MIL-STD-882,
and the UK Ministry of Defence&#8217;s Def Stan 00-55
(Requirements for Safety Related Software in Defence Equipment).
The U.S. Food and Drug Administration has its
<a href="http://www.fda.gov/cdrh/comp/guidance/938.html">
General Principles of Software Validation;
Final Guidance for Industry and FDA Staff</a> for medical devices
(including parts of devices or producers of devices).
</ul>

<h2>Organization of paper</h2>
<p>
The rest of this paper looks at FLOSS tools that can be used
to create high assurance components (there are many), and
FLOSS components that are high assurance themselves (they are rare).
It then contrasts this situation with medium assurance -- there are
many medium assurance FLOSS tools, and FLOSS
components with impressive results.
The paper then speculates why this is the circumstance, and then concludes.

<p>
<h1><a name="high-assurance-tools">Tools to create high assurance components</a></h1>
<p>
It turns out that there are a lot of FLOSS tools that can be
used to help develop high assurance software.
To prove that, I&#8217;ve identified a few important tool categories,
and for each category I identify several FLOSS tools.
The tool categories I discuss below are
configuration management tools,
testing tools,
formal methods (specification and proof) tools,
analysis implementation tools, and
code generation tools.
<p>
There are <i>many</i> other categories of tools, and
many other specific FLOSS tools, that are not listed below.
But the discussion below should prove my point
that there are many FLOSS tools that can be used to help develop
high assurance components.


<h2><a name="cm">Configuration management tools</a></h2>
<p>
There are many FLOSS software configuration management (SCM) tools, indeed,
<a href="https://dwheeler.com/essays/scm.html">
I&#8217;ve written a review of several FLOSS SCM tools</a>.
(I&#8217;ve also written a paper discussing
<a href="https://dwheeler.com/essays/scm-security.html">SCM
and security</a>.)
<p>
<a href="http://www.cvshome.org/">CVS</a> is an old and still <i>very</i>
widely-used SCM tool.  I suspect that most software worldwide, both
proprietary and FLOSS, is still managed by CVS as of 2006.
<a href="http://subversion.tigris.org/">Subversion (SVN)</a> is the
SCM tool rewritten as a replacement for CVS, and it&#8217;s
widely used, too.
But the list of FLOSS SCM tools is amazingly long, including
GNU Arch, git/Cogito, Bazaar, Bazaar-NG, Monotone, mercurial, and darcs
(see my paper for a longer list).
Clearly, there&#8217;s no problem finding a FLOSS SCM tool.


<h2><a name="testing">Testing tools</a></h2>
<p>
All developers test their software, but high assurance software
requires much more testing to gain confidence in it.
But again, there&#8217;s a massive number of FLOSS tools that support testing.
In fact, there are so many FLOSS tools for testing
that there&#8217;s a website
(<a href="http://opensourcetesting.org/">opensourcetesting.org</a>)
dedicated to tracking them;
as of April 2006 they list 275 tools!
This ranges from bug-tracking tools like
<a href="http://www.bugzilla.org/">Bugzilla</a>,
to frameworks for test scripts like
<a href="http://www.gnu.org/software/dejagnu/">DejaGnu</a>.
<p>
Many high assurance projects are required to meet specific
measurable requirements on their tests.
One common measure of testing is &#8220;statement coverage&#8221;
(aka &#8220;line coverage&#8221;), the percentage
of program statements that are exercised by at least one test.
One problem with the statement coverage measure is that statements
that have decisions, such as the &#8220;if&#8221; statement, can cause different
paths.
Thus, another common measure of testing is &#8220;branch coverage&#8221;
the percentage of &#8220;branches&#8221; from decision points that are covered.
Branch coverage has its weaknesses too, so there are
many other test measures as well -- but statement and branch coverage
are the two most commonly-used measures, so we&#8217;ll start with them.
<p>
Some experts believe that unit testing (low-level tests)
should achieve 100% statement coverage and 100% branch coverage,
with the simple argument that if you&#8217;re not even
covering each statement and each branch, your testing is poor.
Most others argue, however, that 80%-90% in each is adequate --
because the effort to create tests to meet the last percent is very
large and less likely to find problems than by spending the effort
in other ways.
No matter what, in my opinion you should create your tests first
and <i>then</i> measure coverage -- don&#8217;t
write your tests specifically to get good coverage values.
That way, you&#8217;ll often gain insight into what portions of the
code are hard to test or don&#8217;t work the way you thought they would.
That insight will help you create much better additional tests to
bring the values up to whatever your project requires.
<p>
(Oh, and why measure both statement and branch coverage?
It turns out it's possible to meet one without the other.
For example, an "if" statement with a "then" clause but
no "else" caluse might have all its tests yield true.. in which case
all the statements are covered, but not all the braches are covered
(the "false" branch is not covered).
Normally, when you cover all branches you cover all statements, but there
are special cases where that is not true.
For example, if your program (or program fragment)
doesn't contain any branches, or if there is an exception handler
without any branches in its body, you can have all branches covered
but not all statements covered.
Exception handlers might be considered
a branch, but that interpretation is not universal.)
<!-- For more information,
 see: http://www.testing.com/writings/coverage-terminology.html -->

<p>
There have been several recent developments in testing that
improve test efficiency:
<ul>
<li><a href="http://en.wikipedia.org/wiki/QuickCheck">QuickCheck</a> is
QuickCheck (BSD license) is a combinator library written in Haskell,
designed to assist in software testing by generating test cases
for test suites. As noted in Wikipedia,
"The author of the program being tested makes certain assertions
about logical properties that a function should fulfill; these
tests are specifically generated to test and attempt to falsify
these assertions."
The assertions are also useful for documenting the program.
Although the original was created for Haskell,
re-implementations exist for Scheme, Common Lisp, Python, Ruby, Standard ML,
and many other languages.

<li>
One of the most important recent developments in testing has been
developed by NIST, and is to be released as a FLOSS tool.
NIST's
<a href="http://csrc.nist.gov/acts/">NIST's
Automated Combinatorial Testing for Software</a>
work has developed new algorithms to efficiently create a minimum number
of tests that nevertheless cover various levels of combinatorials.
This means that you can efficiently create test suites that really
do a good job of testing all combinations.
</ul>

<p>
Even in the case of test case measurement,
there are FLOSS tools that can meet this need.
There are several FLOSS &#8220;test coverage&#8221; tools, such as gcov,
that can report which statements or which branches
were not exercised by your test suite.


<h2><a name="formalmethods">Formal methods tools</a></h2>
<p>
Many software developers have no idea what &#8220;formal methods&#8221; are.
Yet my definition of high assurance
implies that we&#8217;ll usually need to use
&#8220;formal methods&#8221; to create high assurance software.
This section explains what formal methods
are, shows that there are lots of FLOSS tools even in this area, and
then discusses some of the implications.

<!--
Organize better?
The slides on "Why" (see http://why.lri.fr/), Page 103, organize things as:
automatic decision procedures
     provers a la Nelson-Oppen
          Simplify, Yices, Ergo
          CVC Lite, CVC3
     resolution-based provers
          haRVey, rv-sat
     tableaux-based provers
          Zenon
interactive proof assistants
     Coq, PVS, Isabelle/HOL, HOL4, HOL-light, Mizar
-->

<p>
<h3><a name="formalmethodsintro">Formal methods: Introduction</a></h3>
<p>
Formal methods, broadly, are the application
of rigorous mathematical techniques to software development
(see <a href="http://citeseer.ist.psu.edu/rd/0%2C39426%2C1%2C0.25%2CDownload/http://citeseer.ist.psu.edu/cache/papers/cs/86/ftp:zSzzSzhissa.ncsl.nist.govzSzpubzSzformal_methodszSzvol1.pdf/craigen93international.pdf">An International Survey of Industrial Applications of Formal Methods</a> for a lengthier definition and discussion).
Ideally, we&#8217;d like a rigorous mathematical specification
stating exactly what we want the program to do and not do,
and then prove all the way down to the machine code that the software meets
the specification.
This is normally hard to do, so various compromises are often made.
Many have identified three different broad levels of the use of
formal methods, in order of increasing cost and time:
<ul>
<li>Level 0: A formal specification is created
(a specification using mathematics),
and the program is then developed from this informally.
This has been called &#8220;formal methods lite&#8221;.
Creating formal specifications is not easy, because you&#8217;re
trying to take ambiguous, poorly-defined ideas and
turn them into a rigorously defined specification.
Still, creating a formal specification often doesn&#8217;t
take too much time (for someone trained in how to do it),
and they do tend to help clarify what the real issues are.
This is the cheapest way to use formal methods, and
many argue it&#8217;s the most cost-effective way to use formal methods.
<li>Level 1: The mathematical approaches are used further, beyond
a specification but not all the way into code.
Two common ways are to
(a) refine the specification down deeper
to a mathematically-defined design or a more detailed model, and/or
(b) prove important properties of the specification and/or model
(either by hand or with automated help using a theorem prover or
model checker).
<li>Level 2: Theorem provers and/or model checkers
may be used to fully prove that the
actual code matches the design specification.
This is usually <i>very</i> expensive, and when done at all
this is often done with only the most critical portions (where
no other way can give enough confidence).
</ul>

<p>
The &#8220;levels&#8221; are a little misleading, because you can actually do
things partially (perhaps only a part of the software
is formally specified), and level 1 is somewhat ambiguous.
But these levels give the basic flavor;
there is a trade between rigor and effort.

<p>
Now we come to the decision of where to draw the line,
and this isn&#8217;t an easy decision.
For purposes of this paper, to count as &#8220;high assurance&#8221;
there needs to be some carefully-reasoned explanation as to why
the running code meets its key requirements.
How much effort is needed for this justification
depends on the risk you&#8217;re willing to take,
and where you perceive the risks to be.
Thus, while level 0 may be less costly, that is often not enough, so
high assurance development often moves
into level 1 and uses a focused application of level 2 on the parts
that cannot be shown correct otherwise.
Almost no one tries to prove all code down to the machine code;
typically developers with such concerns
will check the machine code by hand to ensure that it
corresponds with the source code.
Some may prove down to the source code, or at least the parts of the
source code that are most worrisome.
Others may use proofs to a detailed software design,
and then use other less rigorous arguments to justify the source code.
You can even back off further, using formal methods
only for the specification (level 0), or not at all.
In all cases, though, there needs to be some careful reasoning that
convinces others that the code actually meets the key requirements,
typically by showing a stepwise refinement from specification through
to the code.
Mantras such as &#8220;correct by construction&#8221;
come into play in these kinds of systems.

<p>
We would love to formally prove that every line of code,
down to the machine code, is correct; doing so has lots of benefits.
Why is it so costly?
Simply put, creating proofs is incredibly hard to do;
often tools and knowledgeable humans must work together to create them.
To prove code correct, you generally must
write the code and proofs simultaneously (so that the code is in
a form that is easier to prove).
Requiring proofs also
creates limits on the size of programs (and thus their functionality),
because our ability to do proofs does not scale that well.
Years ago, the old historical rule of thumb
for the <i>largest</i> amount of
code that can be reasonably proven correct all the way down
to the code level was about 5,000 lines of code.
Cleanly-separated components can be verified separately
(e.g., a computer&#8217;s boot and initialization programs might be separable
from an operating system kernel), and that helps.
This rule of thumb is (I believe) historical;
the tools for verifying code have improved, and
good tools (including languages designed for provability)
can help today&#8217;s developers go significantly beyond this scale.
SPARK Ada&#8217;s developers in particular claim they can go way beyond this.
But it&#8217;s not clear
where the upper bounds really are, and it&#8217;s clear that formally
proving code gets harder as the software gets larger.
<a href="https://dwheeler.com/sloc">
Typical operating systems have millions of
lines of code</a> and are growing fast, so no matter what the upper bound is,
there is a real gap between typical
commercial demands for functionality and the ability of today&#8217;s
formal methods tools to verify it.
Don&#8217;t expect Windows, MacOS, the Linux kernel, or *BSD kernels
to be formally proved down to their code level.
Proving only general models of code (instead of the system itself)
eliminates this problem,
but as I noted above,
this doesn&#8217;t show that the code <i>itself</i> is highly assured.

<p>
Note that all formal methods have a basic weakness: They <i>must</i> make
assumptions, because you have to start somewhere.
In any such system, humans have to check the assumptions very, very carefully.
If you start with a false assumption, a "proof" could produce an
invalid conclusion.
This problem -- that your assumptions may be invalid --
is a key reason that testing and other activities are still needed
for high assurance software, even if you use formal methods extensively.

<p>
Another trade-off in formal methods
is between expressiveness and analyzability.
Fundamentally, any formal method has some sort of language,
a set of axioms, and inference rules (the rules that let you determine
if something else is true).
A language that is extremely flexible (expressive) typically tends to
be harder to analyze.
As a result, there are many different languages, each better and
different things.

<p>
For more information about formal methods, you can see the
<a href="http://en.wikipedia.org/wiki/Category%3AFormal_methods">
Wikipedia information on formal methods</a>
(particularly the <a href="http://en.wikipedia.org/wiki/Formal_methods">
main article on formal methods</a>,
<a href="http://en.wikipedia.org/wiki/Automated_theorem_proving">
automated theorem proving</a>,
and
<a href="http://en.wikipedia.org/wiki/Model_checking">model checking</a>),
the <a href="http://vl.fmnet.info/">
Formal Methods Virtual Library</a>, the
<a href="http://shemesh.larc.nasa.gov/fm/index.html">
NASA Langley Formal Methods Site</a>, and the
<a href="http://www.cs.indiana.edu/formal-methods-education/">
Formal Methods Education Resources</a>.
<a href="http://www.qpq.org/">
QPQ (&#8220;QED Pro Quo&#8221;)</a> is intended to be an
&#8220;online journal for publishing peer-reviewed source code
for deductive software components&#8221;, and has links to various tools
and papers.
<a href="http://www.cs.cornell.edu/Nuprl/PRLSeminar/PRLSeminar01_02/Nogin/PRLseminar7b.pdf">
Aleksey Nogin&#8217;s &#8220;A Review of Theorem Provers&#8221;
</a>
has a nice short summary comparing theorem provers.
<a href="http://www.cs.ru.nl/~freek/comparison/comparison.pdf">
The Seventeen Provers of the World</a> compares 17
proof assistants for mathematics, and some of which are relevant to
software development.
Most surveys seem to be old, unfortunately.
A quick overview is available from the
<a href="http://www.cs.cmu.edu/afs/cs/usr/wing/www/mit/mit.html">
CMU 1996 paper &#8220;Strategic Directions in Computing Research
Formal Methods Working Group&#8221</a>
as
<a href="http://www.cs.cmu.edu/afs/cs/usr/wing/www/mit/paper/paper.ps">
E. Clarke and J. Wing&#8217;s 1996
&#8220;Formal Methods: State of the Art and Future Directions&#8221;</a>
[<a href="https://dwheeler.com/misc/formal-methods-cmu-art-future.pdf">PDF</a>].
<!-- CMU Computer Science Technical Report CMU-CS-96-178, August 1996.
      (22 pages), ACM Computing Surveys, 28(4):626-643, 1996.  -->
I can point out
<a href="http://www.cl.cam.ac.uk/~mjcg/PVS.html">
&#8220;Notes on PVS from a HOL perspective&#8221; by Michael J.C. Gordon (1995)</a>,
<a href="http://www.thedacs.com/techs/2fmethods/title.shtml">
An Analysis of two Formal Methods: VDM and Z (1997)</a>,
and
<a href="http://www.dacs.dtic.mil/techs/fmreview/toc.html">
Vienneau&#8217;s 1993 &#8220;A Review of Formal Methods&#8221;</a>
(even the full version is incomplete for its time,
but at least it is easy to read).
<a href="http://www.cl.cam.ac.uk/Research/HVG/HOL/history.html">The
History of the HOL System</a> explains some of the convoluted history
of LCF, HOL, and their various derivatives, and also notes
some other systems.
<a href="http://www.cetic.be/IMG/pdf/Formal-Security.pdf">
"Formal Methods for IT Security" (May 2007)</a>
has quick overview of tool types
(one quibble: I agree that automatic theorem provers like prover9
take less <i>effort</i> than interactive tools like PVS and HOL, but
automatic tools don't give less <i>assurance</i> - it's just that they
cannot be effectively used on certain classes of problems).
The
<a href="http://mitpress.mit.edu/catalog/item/default.asp?ttype=2&amp;tid=8575">
&#8220;Handbook of Automated Reasoning&#8221;
(Edited by J. Alan Robinson and Andrei Voronkov)</a>
is a survey.
A short 2003
<a href="http://cisr.nps.navy.mil/downloads/03paper_rhas.pdf">survey
of tools</a> commented on many tools, and the
<a href="http://www.cs.utexas.edu/users/csed/formal-methods/docs/FMFramework.pdf">
Formal Methods Framework </a> report (1999) summarizes many tools.
<a href="http://www.dacs.dtic.mil/databases/url/key.php?keycode=53:55">DACS&#8217;
list of formal methods literature</a> is old, but it&#8217;s nicely focused
on key works.
<a href="http://www.logic.at/people/feinerer/">Ingo Feinerer's
"Formal Program Verification: A Comparison of Selected Tools
and Their Theoretical Foundations"</a> (2005) is a much more recent
comparsion of formal methods tools
(in this case, of the Frege Program Prover, KeY, Perfect Developer,
Prototype Verification System).
Griffioen and Huisman's 1998 work compares PVS and Isabelle;
Zolda's 2004 work compares Isabelle and ACL2.
<a href="http://www.fmeurope.org/">Formal Methods Europe</a>
is an independent association with aim of
stimulating the use of, and research on, formal methods for software
development; their website has some
summaries (though it has a very anemic list of tools as of May 2006).
The mathematically-oriented papers
<a href="http://math1.unice.fr/~carlos/preprints/itmath8.pdf">
Information technology implications for mathematics:
a view from the French riviera</a>
and
<a href="http://monet.nag.co.uk/mkm/Final-docs/MKMNetTN-D4-1.pdf">
Deliverable 4.1: Survey of Existing Tools for Formal MKM</a>
compares tools&#8217; mathematical foundations (these have the
informal look of notes not intended for the masses, but they
are still interesting).
<a href="http://ti.arc.nasa.gov/publications/pdf/1999-0153.pdf">Johann Schumann&#8217;s
Automated Theorem Proving
in High-Quality Software Design</a>
discusses integrating automated theorem provers into larger development
approaches and tools.
<a href="http://vl.fmnet.info/hissd/">
High-Integrity System Specification and Design</a>
by Bowen and Hinchey is a collection of older key essays.
Bowen and Hinchey&#8217;s
&#8220;Ten Commandments of Formal Methods ...Ten Years Later&#8221;
(IEEE Computer, January 2006) discusses previously-identified lessons learned
(their &#8220;ten commandments&#8221; of their IEEE Computer April 1995 article)
and argues that they have generally stood the test of time.
Two oft-referenced formal methods
advocacy pieces were published in IEEE Software:
&#8220;Seven Myths of Formal Methods&#8221; by Anthony Hall (Sep/Oct 1990) and
&#8220;Seven More Myths of Formal Methods&#8221; by Bowen and Hinchey (July 1995).
<a href="http://www.computing.co.uk/computing/features/2072361/formal-methods-start-add-again">Richard Sharpe argues that formal methods may be
increasingly used</a> in the future.
<a href="http://www.embedded.com/showArticle.jhtml?articleID=17603352">
Palshikar&#8217;s
&#8220;An introduction to model checking&#8221;</a>
is a gentle introduction to that topic.
Tony Hoare and Jay Misra have proposed a &#8220;Grand Challenge&#8221; effort to
speed maturation of formal methods -- for their pitch, see
<a href="http://vstte.ethz.ch/pdfs/vstte-hoare-misra.pdf">
Verified software: theories, tools, experiments (July 2005)</a>.
The
<a href="http://vstte.ethz.ch">VSETTE conference of October 2005</a>
has a response to this proposal, focusing on
systemic methods for specifying, building, and verifying high-quality software.
Shankar&#8217;s presentation
<a href="http://www.csl.sri.com/users/shankar/VGC05/shankar-hcss.pdf">
The Challenge of Software Verification</a> gives interesting comments
on this challenge.
The paper
<a href="http://portal.acm.org/ft_gateway.cfm?id=1180448">
Formal specification and verification of data separation in a separation kernel for an embedded system</a>
(Constance L. Heitmeyer, Myla Archer, Elizabeth I. Leonard, and
John McLean of the Naval Research Laboratory) describes a very
promising approach to proving all the way down to the code.
These tools generally presume that you already understand the basics
of formal logic; if you don't, books such as
<a href="http://www.fecundity.com/logic/">P. D. Magnus' "forall x"</a> may
be of use to you.
(A serious problem in the U.S. is that many software
developers have never studied discrete math, including logic, even though
that's the basis of their field; few would allow a civil engineer
to design a bridge without first learning calculus.)
No doubt there are many other sources of information.
<a href="http://www.cypherpunks.to/~peter/04_verif_techniques.pdf">
Peter Gutman's article on "Verification Techniques"</a>
(a chapter of his thesis) is a much more pessimistic view of verification
techniques, and has important insights on the limitations of
formal methods and some other verification techniques.

<p>
<!--old: http://gulliver.eu.org/ateliers/fv-tools/fv-tool-list.html -->
After I wrote this paper, I discovered the very interesting list
<a href="http://gulliver.eu.org/wiki/FreeSoftwareForFormalVerification">
Free software tools for formal verification of computer programs</a>
by David Mentré.
You should definitely take a look at this paper as well if you're interested
in the topic!
<a href="https://babel.ls.fi.upm.es/trac/slamsl/wiki/TheoremProvingEnvironment">
Trac's list of Theorem Proving systems</a> identifies their licenses,
many of which are FLOSS.

<p>
Here are some other lists of formal methods related tools:
<ul>
<li>
The
<a href="http://caml.inria.fr/cgi-bin/hump.en.cgi?sort=0&amp;browse=103">
Caml Maths and Logic</a> list includes a number of tools, some of which
are FLOSS.
<li>
<a href="http://innovationstartups.wordpress.com/2008/05/10/formal-methods-links-by-mark-utting/">Formal methods links by Mark Utting</a>
<li>
<a href="http://www-unix.mcs.anl.gov/AR/others.html">ANL's list</a>
<li>
<a href="http://www-formal.stanford.edu/pub/clt/ARS/systems.html">
Database of Existing Mechanized Reasoning Systems (at Stanford)</a>
(Warning: Extremely outdated list)
<li>
<a href="http://tigris.org/">Tigris</a>
is an open source community focused on building better
software engineering tools (for collaborative software development).
They don't specifically focusing on formal methods, but
they have interesting tools like
<a href="http://delta.tigris.org/">Delta</a>
(BSD license)
which minimizes "interesting" files subject to a test
of their interestingness
(e.g., to isolate a small failure-inducing substring
of a large input that causes your program to exhibit a bug).
Many of their tools can usefully work with the tools listed below.
</ul>


<!--
See also:
"A Comparison of PVS and Isabelle/HOL"
David Griffioen and Marieke Huisman
Lecture Notes in Computer Science
Publisher: Springer Berlin / Heidelberg
ISSN: 0302-9743
Subject:  Computer Science
Volume 1479 / 1998
Title:  Theorem Proving in Higher Order Logics: 11th International Conference, TPHOLs '98, Canberra, Australia, September 27 - October 1, 1998. Proceedings
Editors:  John Grundy, Malcolm Newey
ISBN: 3-540-64987-5
DOI: 10.1007/b68195
Chapter: p. 123
(1)  	CWI, Amsterdam, The Netherlands
(2)  	Computing Science Institute, Univ. Nijmegen, PO Box 9010, 6500 GL Nijmegen, The Netherlands

Abstract
There is an overwhelming number of different proof tools available and it is hard to find the right one for a particular application. Manuals usually concentrate on the strong points of a proof tool, but to make a good choice, one should also know (1) which are the weak points and (2) whether the proof tool is suited for the application in hand. This paper gives an initial impetus to a consumersrsquo report on proof tools.
The powerful higher-order logic proof tools PVS and Isabelle are compared with respect to several aspects: logic, specification language, prover, soundness, proof manager, user interface (and more). The paper concludes with a list of criteria for judging proof tools, it is applied to both PVS and Isabelle.

Various Conferences. One noted: TACAS,
"Tools and Algorithms for the Construction And Analysis of Systems".
2005 is at: http://www-verimag.imag.fr/TACAS2005.html

-->

<p>
Note - don&#8217;t treat &#8220;formal methods&#8221; as a checklist item
for high assurance
(oh look, some math, we&#8217;re done!).
The point in high assurance is to identify the risk areas, and
then use tools (like formal methods) to convincingly
show that there isn&#8217;t a problem.
There is more than a little overlap between those developing
high assurance software and the research community;
applying these techniques can be difficult for some domains,
if you need to get <i>really</i> high levels of confidence for
complex systems.

<p>
<h3><a name="categoriesformalmethods">Formal methods: Categories of tools</a></h3>
<p>

<p>
There are many different kinds of formal methods tools, which
I will group into these categories:
<ul>
<li>Specification tools: These help you write and check specifications written
using a formal notation (such as Z, VDM, B, etc.).
These tend to be designed for people who are working at level 0 or level 1;
they are often connected with other tools to go to level 1 or 2.
<li>Theorem provers/proof checkers: Theorem provers
take a set of assumptions and rules, and try to
prove claims about them using traditional mathematical proof
techniques (generally they need human help).
They vary on many factors, such as
what information they can use (from specifications or
general theorems down to program code or annotations).
Proof checkers check a proof created elsewhere.
<li>Model checkers: These try to prove claims, but unlike theorem
provers, model checkers do this
by trying to find all possible circumstances (states) and showing that
they meet the criteria.
<li>Other: Other tools exist which don&#8217;t easily fit into these categories.
</ul>

<p>
Note that these are very rough and imprecise categories.
All formal methods tools must support some kind of specification notation,
tools often have multiple capabilities, and there is a general
trend of combining these tools into larger interoperable capabilities.
A general discussion about issues in integrating tools is in the paper
&#8220;PVS: Combining Specification, Proof Checking, and Model Checking&#8221;.
Thus, any categorization is imperfect, but hopefully this division will help.

<p>
There are some interesting competitions, particularly for the
automated tools.
Here are the
<a href="http://www.cs.miami.edu/~tptp/CASC/J4/WWWFiles/ResultsSummary.html">
CASC results of 2008 using TPTP</a>, for example.

<p>
It turns out that there are many
FLOSS tools that support using formal methods, in all of those categories,
as the following sections will show.

<p>
<h3><a name="specification">Formal methods: Specification tools</a></h3>
<p>
All formal tools have some sort of specification language,
but some languages are often focused on higher-level
specifications -- helping users enter, syntactically check, and
cleanly display the specifications with a minimum of effort.
These are often used for level 0 and 1 type of work
(though they <i>can</i> be used for more -- often by devising connections
to other tools).
Here is a partial list of specification languages,
and FLOSS tools that support them:
<ul>
<li>
Z.
Z is pronounced &#8220;zed&#8221; even in the U.S., and
in 2002 was standardized as ISO/IEC standard 13568:2002(E). There are several
toolsuites that support Z.
The
<a href="http://czt.sourceforge.net">Community Z tools (CZT) project</a>
is developing and coordinating FLOSS projects that develop Z support tools.
<!-- I was told CADiz is FLOSS, but source is not available.
     CADiz is from U of York, typesetting and theorem proving.
     Z/Eves was once distributed, but now it isn't... which is a
     perfect example of the risks of non-FLOSS programs.
     It's in flux, Odyssey has something to do with it.  Unknown if it'll
     ever return.
     Fuzz from Oxford is supposed to be a good typechecker for Z,
       supporting Z embedded in MS Word.   -->
<a href="http://spivey.oriel.ox.ac.uk/mike/fuzz/">fuzz</a> (MIT license)
is a type-checker for Z.
<a href="http://uebb.cs.tu-berlin.de/zeta/">ZETA</a> (GPL + public domain)
is an environment for developing specification documents based on Z.
&#8220;It provides an integration framework for tools to edit, analyse
and animate Z specifications and formalisms which are mapped to Z.&#8221;
It supports type-setting, type checking, and &#8220;execution&#8221; of pure Z.
<a href="http://www.lemma-one.com/ProofPower/index/">
ProofPower</a> (GPL license, except for the Ada plug-in)
is a suite of tools supporting specification and proof in
Higher Order Logic (HOL) and in the Z notation.
<a href="http://www.cs.waikato.ac.nz/~marku/jaza/">Jaza</a> (GPL)
is an "Animator" for the Z formal specification language,
developed at the University of Waikato (primarily by Mark Utting).
More information about Z is available at
<a href="http://zuser.org/">Z User Group</a>, including the
<a href="http://vl.zuser.org/">Z User Group virtual library</a>.
<li>
Alloy.
<a href="#alloy">Alloy</a> is a tool that's hard to categorize.
Alloy implements a specification language that's intentionally similar to Z,
but makes it very easy to analyze and find counter-examples for.
Its analysis capabilities are far beyond what a "pretty printer" or
"type checker" can do, but it can't prove arbitrary properties; see
its description for more information.
You give up some capabilities, but receive a massive ease-of-use bonus
in return.
<li>
CASL.  The <a href="http://www.lsv.ens-cachan.fr/CoFI/">
"Common Framework Initiative for algebraic specification and development"
(CoFI)</a> is a voluntary organization for an open collaborative effort
to produce a Common Framework for Algebraic Specification and Development.
In particular, they have produced the
<a href="http://www.lsv.ens-cachan.fr/CoFI/CASL.html">
Common Algebraic Specification Language (CASL)</a>,
a specification language that is designed to be a careful selection of
known constructs, intended to be expressive, simple, and pragmatic.
Their goal was to create a language suitable for
specifying requirements and design for conventional software packages;
it has restrictions to various sublanguages, and
extensions to higher-order, state-based, concurrent, and other languages.
<a href="http://www.informatik.uni-bremen.de/agbkb/forschung/formal_methods/CoFI/hets/index_e.htm">Hets</a> (LGPL-like license),
the successor of the CATS tool, supports CASL,
several extensions of CASL, and Haskell.
Hets is a parsing, static analysis and proof management tool
for combining various tools for different specification languages;
its "single" language
is the heterogeneous specification language HetCASL.
Hets includes parsing, static analysis, and proof support.
<li>
VDM-SL (Vienna Development Methodology - Specification Language).
<a href="http://www.overturetool.org/">Overture</a> is a set of
FLOSS tools (both current and under development) to support the VDM++
specification language (an enhanced version of VDM).
VDM-SL is standardized by ISO/IEC as ISO/IEC 13817-1: 1996.
VMD is really a whole method, of which VDM-SL is the specification language
piece.
VDM seems to be less active than Z to me, but
that is simply an impression and may not be true.
<li>
<a href="http://www.omg.org/technology/documents/formal/uml.htm">
Unified Modeling Language (UML) Object Constraint Language (OCL)</a>.
UML is defined by the Open Management Group (OMG); UML version 2.0
added OCL.
<a href="http://www.key-project.org/">KeY</a> (GPL license)
supports formal specification and verification of programs
in conjunction with UML.
UML OCL is part of the UML standard; KeY can then analyze the constrains.
The target language of KeY based development is Java CARD,
a proper subset of Java for smart card applications and embedded systems.
KeY currently requires the use of a proprietary UML tool, but this
does not seem fundamental to KeY; it should be possible to integrate
KeY into a FLOSS UML tool as well.

<li>B method.
The
<a href="http://rodin-b-sharp.sourceforge.net">RODIN Project</a>
(Common Public License and Eclipse Public License)
is developing a platform focused on the B method, as is
<a href="https://gna.org/projects/brillant">Brillant</a>
(LGPL license).
<a href="http://vl.fmnet.info/b/">The virtual library for the B-method</a>
has more information about B.
<li>ProMela.
ProMela is a language for specifying distributed software systems;
it was originally developed for the model-checking tool
<a href="http://spinroot.com/spin/whatispin.html">Spin</a>, and the
<a href="http://anna.fi.muni.cz/divine/tool/index.html">DiVinE</a> tool
supports it too.
See the text below for more about these tools.
</ul>

<p>
<h3><a name="theoremprovers">Formal methods: Theorem Provers/Proof Checkers</a></h3>
<p>
Here are FLOSS theorem provers and checkers
(increasingly they are combined with model checkers, in which case
I list them here and not under model checkers):
<ul>
<li>
<a href="http://www.cs.utexas.edu/users/moore/acl2/">ACL2</a>
(GPL license)
is an industrial-strength theorem prover,
part of the Boyer-Moore family of provers
(winner of the 2005 ACM Software System Award).
It takes expressions using LISP notation and tries to automatically
prove the expression.
ACL2 is one of the more commonly-used such tools for industrial-strength
proving of real world programs, though it&#8217;s certainly not the only one.
I&#8217;ve talked to users of ACL2, who claim that ACL2 strikes a nice balance
between trying to do everything automatically (which sadly isn&#8217;t
practical yet) and forcing users to do everything &#8220;by hand&#8221;
(which is painful) -- it tries to do much automatically, while still making
it easy to control.
ACL2 is the intellectual successor of the Nqthm theorem-prover.
This family has been used to prove correctness for
<a href="http://www.cs.utexas.edu/users/moore/acl2/v2-9/How_Long_Does_It_Take_to_Become_an_Effective_User_lparen_Q_rparen_.html">
many processor designs, microcodes, and machine object codes</a>,
including AMD microprocessors and pieces of the Berkeley string library.
See <a href="http://citeseer.ist.psu.edu/220177.html">Boyer and Yu&#8217;s</a>
and
<a href="http://citeseer.ist.psu.edu/boyer96mechanized.html">
Boyer and Moore&#8217;s &#8220;Mechanized Formal Reasoning...&#8221;</a>
for work at the machine code level, where they found 3 defects at the
machine code level (the same idea works for bytecode, too).
I should note that this family of tools (ACL2/Nqthm)
is rather different from other tools.
ACL2&#8217;s developers claim that
<a href="http://www.cs.utexas.edu/users/moore/acl2/v2-9/How_Long_Does_It_Take_to_Become_an_Effective_User_lparen_Q_rparen_.html">
it only takes several months to become an effective ACL2 user</a>
for someone who has
&#8220;a bachelor&#8217;s degree in computer science or mathematics,
has some experience with formal methods,
has had some exposure to Lisp programming and is comfortable
with the Lisp notation,
is familiar with and has unlimited access to a Common Lisp [implementation],
is willing to read and study the ACL2 documentation, and
is given the opportunity to start with &#8220;toy&#8221; projects&#8221;.
<p>
ACL2 is powerful enough to be very useful, and has been
used for many important commercial projects.
ACL2 directly supports mathematical induction, meaning that ACL2 can directly
handle computer programs with loops or recursion (something many other
theorem-provers cannot handle as directly and thus must handle in other ways).
ACL2&#8217;s defchoose and defun-sk abilities add the ability to handle
&#8220;there-exists&#8221; and
&#8220;for all&#8221; statements to ACL2, which the ACL2 developers say
adds &#8220;all the abilities of full first order logic&#8221;
(but see below for the limits in how they can be applied; ACL2&#8217;s support for
the quantifiers (for-all and there-exists) is limited).
ACL2 supports encapsulation (the &#8220;encapsulate&#8221; form lets you
describe general properties of a function,
instead of having to define all functions), so you do not <i>have</i>
to define an executable function to use ACL2.
ACL2 supports some capabilities of second-order logic (higher-order functions,
though not variables).
<!--
(defchoose lets you "introduce an undefine function whose value is constrained
to be some object satisfying a certain formula, provided such an
object exists. This effectively extends the ACL2 logic to full
first order logic."
defun-sk lets you "introduce a function whose body is a universally
or existentially quantified formula."
End of Chapter 6, Page 84, "Computer-Aided Reasoning: An Approach"
by Matt Kaufmann, Panagiotis Manolios, J Strother Moore. August, 2002.
-->
<!--
 Here are some notes about induction:

 http://www.cs.uiowa.edu/~hzhang/induc.html
 Who developed RRL said:
  Mathematical induction is required for reasoning about objects or events containing repetition, e.g. computer programs with recursion or iteration, electronic circuits with feedback loops or parameterized components. Thus automating mathematical induction is a key enabling technology for the use of formal methods in information technology. Failure to automate inductive reasoning is one of the major obstacles to the widespread use of formal methods in industrial hardware and software development.

 http://www.cs.nott.ac.uk/~lad/research/challenges/challenge_manager.html
 "Induction Challenge Management" says:
 Inductive Theorem proving is a small field. The main theorem provers within this field are Nqthm (now re-engineered as ACL2) [Boyer and Moore, 79, Kaufmann and Moore, 96], INKA [Autexier et. al, 99], the clam series [Bundy et. al, 90, Richardson et. al, 98] and RRL [Kapur and Zhang, 95]. Twelf [Pfenning and Schurmann, 99] also looks at the automation of inductive proof inthe context of logical frameworks. Within the field it is hard to assess claims for the superiority of any given system since there is naturally a tendency to report "successes" - difficult or challenging problems automatically proved. There is also a desire within the community to develop a store of shared knowledge about the challenges that face the automation of proof by mathematical induction.

 TPTP (Thousands of Problems for Theorem Provers) is a library of test problems for first-order ATP systems. They provide the ATP community with a comprehensive library complete with unambiguous names and references. All the problems are stated in a standardised formulation of first-order logic and are widely used to benchmark first-order systems. They are also used as the test set for the CASC competition which compares such systems. One of the benefits of the TPTP library to the ATP community is the existence of a common set of problems by which comparisons can be made.

 It is not practical for inductive theorem provers to follow the pattern of the TPTP library. Various attempts have been made to build a similar corpus of problems requiring inductive reasoning. The most mature of these was based on the Boyer-Moore corpus (This has become known as the Dmac corpus after David McAllester who translated a fragment of the NQTHM corpus into a simpler language.). This corpus was unpopular partly because there was repetition within the problem set and partly because many problems depended on a few particular function definitions. But the major objection was that inductive theorem provers use a number of different logics, some of which are typed and some of which are not, which made it difficult to agreed on a standard format. The use of other logics also raised translation issues and a fully automated process for converting the theorems, even into an agreed typed language was never produced.

 A group of researchers within the community (At the 2000 CADE Workshop on the Automation of Proof by Mathematical Induction.) agreed that instead of a large set of benchmarks in a standard logic they would each put forward a number of "Challenge Problems". These should present interesting challenges to the automation of inductive proof or illustrate important features which an inductive prover should be able to handle. A set of these problems would be collected which would remain sufficiently small that an individual could represent them within their own theorem proving system as they saw fit.
-->
<p>
ACL2 is one of the strongest of any theorem-prover
in its support of executability;
<!-- Stated by "Certifying Compositional Model Checking Algorithms in ACL2" -->
you can interactively enter runnable LISP functions, and begin proving
properties about them. That is really powerful!
There are good LISP compilers, so the execution can be really fast
(especially if you use the usual LISP speedup tricks, such as
tail recursion, arrays, and declaring numeric types).
It also supports &#8220;single threaded objects&#8221;,
which you can use to make models with state run much faster.


<p>
ACL2 has weaknesses (and perceived weaknesses)
as well, though there is ongoing work to address many of them:
<ul>
<li>It is not well integrated into other tools.
For one, there&#8217;s no strong connection with
higher-level specification languages like Z or B.
There&#8217;s little support to call out to other tools like other
proof checkers or model checkers (e.g., Otter/Mace/Prover9) -- such
integration would make it possible to use their capabilities to
automatically prove theorems when ACL2 cannot find the proof without help.
The latter would be very useful because although
many find ACL2&#8217;s theorem prover relatively
easy to guide, ACL2 needs to be guided
in cases where other theorem provers could automatically find the proof.
For more information see the work on Ivy and Mu-Calculus in
&#8220;Computer-Aided Reasoning: ACL2 Case Studies&#8221;, which could perhaps
be the basis for connecting ACL2 to other theorem provers
(like Otter/Prover9) and model checkers.
<li>ACL2 doesn&#8217;t have a lot of support for reasoning about quantifiers
(for-all and there-exists).
<a href="http://citeseer.ist.psu.edu/ray03certifying.html">
Certifying Compositional Model Checking Algorithms in ACL2
(by Ray, Mattherws, and Tuttle)</a>
identifies several ACL2 weaknesses:
its logic &#8220;has little support for modeling or reasoning about infinite
sequences&#8221; and does not &#8220;permit recursive function definitions with
quantifiers in the body&#8221;.
They also note how these weaknesses could be eliminated;
note that this work would also help integrating ACL2 with other tools.
<li>ACL2 does not support typing well.
ACL2 is basically untyped, and functions essentially have to be defined
for all types that exist in ACL2 -- even if that&#8217;s unnecessary.
&#8220;Guards&#8221; make execution faster if certain typing restrictions are met,
and constructs like (declare (type ...)) and initial requirements
can specify type requirements, but ACL2 is not really focused on supporting
types.
There have been discussions about these issues, such as
<a href="http://www.cs.utexas.edu/users/moore/acl2/workshop-2004/contrib/austel/acl2-types.pdf">
Vernon Austel&#8217;s &#8220;Adding a typing mechanism to ACL2&#8221;</a> and
<a href="http://citeseer.ist.psu.edu/486132.html">
Manolios and Moore&#8217;s &#8220;Partial Functions in ACL2&#8221;</a> (describing a &#8220;defpun&#8221;),
and
<a href="http://citeseer.ist.psu.edu/kaufmann02efficient.html">
Kaufmann and Sumners&#8217; &#8220;Efficient Rewriting of Operations on
Finite Structures in ACL2&#8221;</a>.
<li>LISP&#8217;s notation is regular but different than what many are used to.
In LISP, function names always precede operations, and parentheses are
used (*&nbsp;(+&nbsp;2&nbsp;1)&nbsp;3) is &#8220;(2+1)*3&#8221;.
For those who prefer more conventional notations
(such as using infix operators), there are some solutions.
<a href="http://www.cs.utexas.edu/users/moore/infix/README.html">
There are tools to (1) &#8220;pretty-print&#8221; results (including using infix notation)
and to (2) accept statements in a more conventional infix syntax
(this is IACL2, aka Infix ACL2)</a>.
IACL2 works, but it is not as portable, and its input notation
does not support some of ACL2&#8217;s advanced capabilities.
Maturing IACL2 to support all ACL2 features, and improving its
documtation so that users could just use the syntax directly,
would be valuable in my opinion.
</ul>
<p>
If you are thinking about using ACL2, I highly recommend getting two books
by the tool&#8217;s creators. These are
<i>Computer-Aided Reasoning: An Approach</i>, which describes how to use
the tool, and <i>Computer-Aided Reasoning: ACL2 Case Studies</i>, which
gives worked examples on various problems.
They are absurdly pricey in hardback (around $215-$224 each!), so
<a href="http://www.cs.utexas.edu/users/moore/publications/acl2-books/OrderingInformation.html">buy softcovers from the authors instead</a>.

<li>
<a href="http://pvs.csl.sri.com/">PVS Specification and Verification System</a>
(GPL License, as of the 4.0 release of December 2006) is one of the
other major theorem provers/verifiers, and it's also FLOSS.
As they say,
"PVS is a verification system: that is, a specification language
integrated with support tools and a theorem prover. It is intended
to capture the state-of-the-art in mechanized formal methods and to be
sufficiently rugged that it can be used for significant applications."

<li>
<a href="http://twelf.org">Twelf</a> (2-clause BSD license)
is a programming system and language
"used to specify, implement, and prove properties
of deductive systems such as programming languages and logics."

<li>
<a href="http://sal.csl.sri.com/">Symbolic Analysis Laboratory (SAL)</a>
(GPL license) is
a framework for combining different tools to calculate
properties of concurrent systems.
At its heart is a language devised by SRI, Stanford, and Berkeley,
for specifying concurrent systems in a compositional way. Here's
how they describe it: 'It is supported
by a tool suite that includes state of the art symbolic (BDD-based)
and bounded (SAT-based) model checkers, an experimental "Witness"
model checker, and a unique "infinite" bounded model checker based on
SMT solving. Auxiliary tools include a simulator, deadlock checker and
an automated test generator.'

<p>
<a href="http://mcrl2.org/wiki/index.php/Home">mCRL2</a>
(<a href="http://www.boost.org/users/license.html">BOOST license</a>)
stands for micro Common Representation Language 2.
"It is a specification language that can be used to specify and analyse
the behaviour of distributed systems and protocols and is the successor
to μCRL.
It is a formal specification language with an associated toolset.
Using its accompanying toolset systems can be analysed
and verified automatically.
The toolset can be used for modelling, validation and verification of
concurrent systems and protocols.
The toolset supports a collection of tools for linearisation, simulation,
state-space exploration and generation and tools to optimise and analyse
specifications. Moreover, state spaces can be manipulated, visualised
and analysed.
<br>
<br>
"mCRL2 is based on the Algebra of Communicating Processes (ACP)
which is extended to include data and time. Like in every process algebra, a fundamental concept in mCRL2 is the process. Processes can perform actions and can be composed to form new processes using algebraic operators. A system usually consists of several processes (or components) in parallel."
This uses the BOOST license, which is OSI-approved.

<li>
<a href="http://www-unix.mcs.anl.gov/AR/otter/">Otter/MACE</a>
(public domain),
developed at the Argonne National Laboratory,
is the &#8220;first widely used high-performance theorem prover&#8221;
(according to Wikipedia).
It includes a built-in model checker (MACE2).
This is very powerful theorem prover, and has proved theories
unsolved by mathematicians. A sister project even proved a 60-year-old
conjecture, the Robbins problem, and made the New York Times;
many mathematicians failed to find the proof,
yet Otter handles it easily.
That makes Otter noteworthy, but Otter has since been superceded by
Prover9/Mace4, noted next.
<!--
Otter and its sister program
<a href="http://www-unix.mcs.anl.gov/AR/eqp/">EQP</a>
were used to create a mathematical
proof of a 60-year-old mathematical mystery
(the &#8220;Robbins problem&#8221;), a result so striking that it made
the New York Times.
-->
<!-- Prover9 is intended to be Otter's successor.
I sent an email to William McCune about Prover9's license; he said on
May 5, 2005:
It hasn't been determined yet. Probably it will be GPL or
something similar.
-->
<li>
<a href="http://www.cs.unm.edu/~mccune/prover9/">
Prover9/Mace4
</a>
(GPL license)
is a combination of two programs:
Prover9 is an automated theorem prover for first-order and equational logic,
(based on resolution/paramodulation), while
Mace4 searches for finite models and counterexamples.
Prover9 is a successor of the Otter Prover, with a tagline
"the future of theorem proving".
You can check the proofs produced by Prover9 using
<a href="http://www.cs.unm.edu/~mccune/papers/ivy/">Ivy</a>,
a preprocessor and proof checker proved using ACL2.
I've used this one personally - if you have a problem that's easily
expressed in its language, this is a <i>very</i> good tool.

<p>
<a href="http://www.cs.manchester.ac.uk/~hoderk/sine/">
SInE (Sumo Inference Engine)</a> (GPLv3)
is "a metaprover targeted on large theories, especially on SUMO".
This is actually a support program designed to make other other
first-order theorem-provers (like prover9 and E) much more effective
on large problems.
Programs like prover9 and E take the assumptions (axioms) and negated goal
and try to derive everything that can be derived from them, looking for
a contradiction.
That's fine, but if there are a vast number of irrelevant assumptions,
they get overwhelmed, and that's where SInE comes in.
SInE selects only "relevant" axioms that "define" the meaning of symbols
and then runs an underlying theorem prover.

<p>
<a href="http://www.spass-prover.org/">SPASS</a> (GPLv2)
is an automated theorem prover for first-order logic with equality.
It can be used for the "formal analysis of software, systems, protocols,
formal approaches to AI planning, decision procedures,
and modal logic theorem proving."
<a href="http://www.mpi-inf.mpg.de/~uwe/software/">SPASS+T</a> (GPLv2)
is an extension of SPASS that "enlarges the reasoning capabilities of
SPASS using some built-in arithmetic simplification rules and
an arbitrary SMT procedure for arithmetic and free function symbols
as a black-box."
Unfortunately, SPASS+T requires Yices (proprietary) or
CVC Lite (license currently unacceptable to distributors due to a
dangerous legal clause), so it cannot be included in the main repostitory
of a typical Linux distribution.

<li>
<a href="http://www.ai.sri.com/~stickel/snark.html">
SRI's New Automated Reasoning Kit (SNARK)</a> (MPL)
is an "automated theorem-proving program being developed in Common Lisp.
Its principal inference rules are resolution and paramodulation.
SNARK's style of theorem proving is similar to Otter's [and Prover9's].
Some distinctive features of SNARK are its support for special unification
algorithms, sorts, nonclausal formulas, answer construction for program
synthesis, procedural attachment, and extensibility by Lisp code.
SNARK has been used as the reasoning component of SRI's High Performance
Knowledge Base (HPKB) system, which deduces answers to questions
based on large repositories of information, and as the deductive core
of NASA's Amphion system, which composes software from components to
meet users' specifications, e.g., to perform computations in planetary
astronomy. SNARK has also been connected to Kestrel's SPECWARE environment
for software development."
Note that it directly supports numbers (Prover9 does not).

<li>
<a href="http://www.ags.uni-sb.de/~leo/">LEO-II</a> (BSD)
is a "standalone, resolution-based higher-order theorem prover designed for fruitful cooperation with specialist provers for natural fragments of higher-order logic. At present LEO-II can cooperate with the first-order automated theorem provers E, SPASS, and Vampire.
LEO-II is implemented in Objective CAML and its problem representation language is TPTP THF."

<li>
<a href="http://code.google.com/p/csisat/">csisat</a> (Apache 2.0 license)
is a Tool for LA+EUF Interpolation.
That is, it is
"an interpolating decision procedure for the quantifier-free theory of rational linear arithmetic and equality with uninterpreted function symbols. Our implementation combines the efficiency of linear programming for solving the arithmetic part with the efficiency of a SAT solver to reason about the boolean structure."


<li>
<a href="http://focal.inria.fr/zenon/">Zenon</a>  (new BSD)
is an "automated theorem prover for
first order classical logic (with equality), based on the tableau method.
Zenon is
intended to be the dedicated prover of the Focal environment, an object-
oriented algebraic specification and proof system, which is able to pro-
duce OCaml code for execution and Coq code for certification. Zenon can
directly generate Coq proofs (proof scripts or proof terms), which can be
reinserted in the Coq specifications produced by Focal. Zenon can also be
extended, which makes specific (and possibly local) automation possible
in Focal."
Note in particular that Zenon generates proofs in a Coq-checkable format.
It doesn't seem to be maintained as of 2009, and that's a problem.

<li>
<a href="">Muscadet3</a> (<a href="http://www.math-info.univ-paris5.fr/~pastre/muscadet/LICENSE">new BSD</a>)
is a knowledge-based theorem prover written in Prolog
(it's known to work with SWI-Prolog).
It is able to work with first-order and second-order predicate calculus.
It is based on natural deduction and uses methods which resemble those
used by humans (vs. the resolution principle).
It is composed of an inference engine, which interprets and executes rules,
and of one or several bases of facts.
It accepts TPTP Problem library syntax.
It supports the usual infix connectives: &amp; (and), | (or), ~ (not),
=&gt;, and &lt;=&gt;.
It supports prefix quantifiers using TPTP's syntax:
! (for-all) and ? (there-exists).
Note that the Muscadet3's syntax is TPTP's, whereas Muscadet2 used
a slightly different syntax.
TPTP doesn't have a syntax for second-order expressions, and
Prolog cannot handle P(A,B) where P is a variable predicate.
Muscadet (as described in its manual, section 11)
predicate variables are expressed using ".."; e.g.,
P(A,B) where P is variable is written as ..[P,A,B].
<!--
As you are interested by 2nd order, I have added an example
http://www.math-info.univ-paris5.fr/~pastre/muscadet/example-2ndorder-th

Here are some precisions. Muscadet may work in 2nd order but Prolog
cannot handle P(A,B) where P is a variable predicate.
(Muscadet1, which was written in Pascal could accept such formulas).
So Muscadet2 and 3 use a constant Prolog predicate ".." and P(A,B) is
given as ..[P,A,B] .
(It would be easy to automatically translate formulas such that P(A,B)
into ..[P,A,B] before the call to Prolog.
I chose this symbol ".." because if p is constant, in Prolog we have
p(A,B) =.. [p,A,B] :-) )
If P is instanciated by a constant p, Muscadet rewrites ..[p,A,B] into
p(A,B).
(See http://www.math-info.univ-paris5.fr/~pastre/muscadet/manual-en.pdf
section 11)

I proposed such problems to be added to the TPTP Library, where P(A,B)
is written apply(P,A,B) as usual in the Library, but "apply" may not be
known from provers, so I had to add hypotheses such as
! [X,Y] : ( apply(subset,X,Y) <=> subset(X,Y) ) )) for example in
SET807+4.p
Moreover, Geoff Sutcliffe asked me to not use the same symbol for the
constant and the predicate because some other provers may not be able to
accept it. So it became
! [X,Y] : ( apply(subset_predicate,X,Y) <=> subset(X,Y) ) ))
and the conjecture became
! [E] : pre_order(subset_predicate,power_set(E)) )).
instead of simply
! [E] : pre_order(subset,power_set(E)) )).
-->

<li>
<a href="http://www.michaelbeeson.com/research/otter-lambda/index.php">
Otter-λ (Otter-lambda)</a> (MIT-style license)
is "a theorem-proving program. It accepts as input a list of axioms
and a theorem to try to prove, and if successful, it outputs a
proof of that theorem from those axioms... 
Otter-λ is a first-order theorem prover (Otter) augmented by
lambda calculus and an algorithm for untyped lambda unification."

<li>
<a href="http://www.functologic.com/info/KeYmaera-guide.html">
KeYmaera (Hybrid Theorem Prover for Hybrid Systems)</a> (GPL) is
"a verification tool for hybrid systems
and built as a hybrid theorem prover for hybrid systems.
KeYmaera separates the overall verification workflow into two phase.
In the first phase you specify the hybrid system that you would
like to verify along with its correctness properties.
In the second phase, you can use KeYmaera and its automatic proof strategies
to verify the specified property of the hybrid system."
Originally it depended on the proprietary tool Mathematica.
However, on 2009-03-26 Andre Platzer reported to me that they've
been a lot of work on it - it no longer
requires Mathematica, and that it supports "a much more 
flexible structure and even a nice out-of-the-box webstart to run".

<li>
<a href="http://jape.comlab.ox.ac.uk:8080/jape/">JAPE</a> (GPL)
is a configurable, graphical proof assistant.
It allows user to define a logic, decide how to view proofs, and so on.
It works with variants of the sequent calculus and natural deduction.

<li>
<a href="http://www.irit.fr/ACTIVITES/LILaC/Lotrec/">LoTREC</a>
(CeCILL License) is
"a generic tableau theorem prover for modal logic. It is a suitable educational tool for students and researchers for creating, testing and analysing tableau method implementations."

<li>
<a href="http://gilith.com/software/metis/">Metis</a> (GPLv2)
is "an automatic theorem prover for first order logic with equality".
Its website reports these features:
"Coded in Standard ML (SML), with an emphasis on keeping the code as
simple as possible; Compiled using MLton to give respectable performance
on standard benchmarks; Reads in problems in the standard .tptp file
format of the TPTP problem set; Outputs detailed proofs in TSTP format,
where each proof step is one of 6 simple rules; Outputs saturated clause
sets when input problems are discovered to be unprovable."
MLton is an "open-source, whole-program, optimizing Standard ML compiler"
(released under a BSD-style license).

<li>
<a href="http://kti.ms.mff.cuni.cz/cgi-bin/viewcvs.cgi/MPTP2/MaLARea/">MaLARea</a> (GPLv2+ except for snow)
is a metasystem for "automated theorem proving in large theories
where symbol and formula names are used consistently.
It uses several deductive systems (now E,SPASS,Paradox,Mace),
as well as complementary AI techniques like machine learning 
(the SNoW system) based on symbol-based similarity, model-based
similarity, term-based similarity, and obviously previous
successful proofs...
The basic strategy is to run ATPs on problems, then use the machine learner
to learn axiom relevance for conjectures from solutions, and use
the most relevant axioms for next ATP attempts. This is iterated,
using different timelimits and axiom limits. Various features
are used for learning, and the learning is complemented by other criteria
like model-based reasoning, symbol and term-based similarity, etc."


<!--
<li>
<a href="http://hsinfosystems.com/taujay/index.html">Tau</a>
is a
"robust and general purpose, interactive, user-configurable automated theorem prover for first-order predicate logic with equality. Tau proves both theorems and arguments expressed in unrestricted first-order notation in the KIF (knowledge interchange format) language. It combines rule-based problem rewriting with model elimination (both full and weak), uses Brand’s modification method to implement equality handling, and accepts user-configurable heuristic search to speed the search for proofs. Tau optionally implements mathematical induction (both strong and weak). Formulas are input and output in KIF or its own infix first-order syntax, and other syntactic forms can be added. Tau is operated from a Web interface or from a command-line interface. It is implemented entirely in Java.
Other features include tautology and subsumption deletion; depth-, breadth-, and modified-best-searching; use of unit lemmas; instantiation and generalization strategies; finite model checking; extensibility."

 Quote from Wikipedia, 2008-06-28.

Can't find source code.
-->



<li>
<a href="http://coq.inria.fr/">Coq</a> (LGPL 2.1 license)
is a formal proof management system: a proof done with Coq
is mechanically checked by the machine.
(Coq does not <i>create</i> proofs for the most part,
it checks and manages them.)
Coq was used by Trusted Logic to evalute the Java Card (TM) system
at Common Criteria EAL 7 (see Why and Krakatoa, which are FLOSS tools
for verifying Java programs and can use Coq).
Coq supports defining functions or predicates,
stating mathematical theorems and software specifications,
interactively developing formal proofs of these theorems, and
checking these proofs by a small certification &#8220;kernel&#8221;.
Coq is based on a logical framework called
&#8220;Calculus of Inductive Constructions&#8221;.
If you want to learn more about Coq, consider the book
<a href="http://www.labri.fr/perso/casteran/CoqArt/index.html">
"Interactive Theorem Proving and Program Development Coq'Art:
The Calculus of Inductive Constructions"</a>.
There many tools that run on top of Coq, too.
Coq has been used by Xavier Leroy (main developer of OCaml) to write a
certified compiler (
<a href="http://pauillac.inria.fr/~xleroy/research.html#compcert">
compcert</a>) that guarantees that semantics of a C
source program is kept up to PowerPC assembly.
The
<a href="http://pauillac.inria.fr/~xleroy/compcert-backend/">
specification of the compiler back-end is available as GPL software</a>
(though unfortunately not the Coq proofs).
Although the compcert work is not entirely FLOSS,
the fact that it exists shows that complete formal methods
can be applied to a nontrivial software project.

<li>
Agda is in an interesting transition.
<a href="http://unit.aist.go.jp/cvs/Agda/">Agda 1</a>
(MIT license)
is "an interactive proof editor, or proof assistant, developed in Chalmers University of Technology, in the tradition of succession of such proof assistants (ALF, Cayenne, Alfa).  Its input language, called Agda language (or simply Agda), is based on a constructive type theory á la Martin-Löf, extended with dependent record types, inductive definitions, module structures and a class hierarchy mechanism."
A research development of its successor, the
<a href="http://appserv.cs.chalmers.se/users/ulfn/wiki/agda.php">
Agda2 language and its interactive proof editor</a> (MIT license,
with a few pieces GPL), is going on.
Agda2 is "a dependently typed programming language with good support for programming with inductively defined families of types."

<li>
<a href="http://matita.cs.unibo.it/">Matita</a> (GPL, in Debian)
is an
"experimental, interactive theorem prover under development at the
Computer Science Department of the University of Bologna.  Authoring
interface Matita is based on the Calculus of (Co)Inductive Constructions,
and is compatible, at some extent, with Coq. It is a reasonably small
and simple application, whose architectural and software complexity is
meant to be mastered by students, providing a tool particularly suited
for testing innovative ideas and solutions. Matita adopts a tactic based
editing mode; (XML-encoded) proof objects are produced for storage and
exchange.  The graphical interface has been inspired by CtCoq and Proof
General. It supports high quality bidimensional rendering of proofs and
formulae transformed on-the-fly to MathML markup."

<li>
<a href="http://www.eprover.org/">
E equational theorem prover</a>
(GPL license)
is a high performance automatic theorem prover for full first-order logic
with equality.
Here's
<a href="http://www4.informatik.tu-muenchen.de/~schulz/WORK/eprover.html">
Here's another link for the E Equational Theorem prover</a>

<li>
<a href="http://www.ai.sri.com/~stickel/pttp.html">PTTP</a> (BSD-style)
(Prolog Technology Theorem Prover) is a theorem prover based on
model elimination.
The term &#8220;Prolog&#8221; here is a little misleading; PTTP extends Prolog to the
full first-order predicate calculus.
There are two implementations;
the Lisp version is faster (and is intended here).
PTTP is extremely fast and has low memory requirements,
at a cost of being unable to solve difficult theorems
(the author recommends using Otter for difficult problems that are
intractable for PTTP).
<li>
<a href="http://www.cl.cam.ac.uk/Research/HVG/Isabelle/">Isabelle</a>
(BSD-like license) is a
generic theorem proving environment developed at Cambridge University
and TU Munich, building on Standard ML.
It&#8217;s an &#8220;LCF-style theorem prover&#8221; --
that means its ideas are descended from the old
&#8220;Logic for Computable Functions&#8221; (LCF) theorem prover, via
another system called HOL (see below).
In these kinds of theorem provers (including Isabelle, HOL 4, and
HOL Light),
you &#8220;drive&#8221; (control) how it tries to prove things using
commands written in the programming language Standard ML
(it does <i>not</i> automatically find a proof for you, but lets you
command it and it does the manipulations for you).
<!--
 Confirmed "doesn't do it for you" via:
 http://michaelbeeson.com/research/otter-lambda/index.php?include=comparison
-->
<li>
<a href="http://hol.sourceforge.net/">HOL 4</a> (BSD-like license) is
an HOL-based automated proof system for higher order logic.
<a href="http://www.cs.ucsd.edu/users/goguen/projs/lfm.html">Joseph
A. Goguen claims that Cambridge University&#8217;s HOL (now HOL 4) is
&#8220;Perhaps the most widely used theorem proving system today&#8221;</a>.
<!-- Yes, it's the same system -->
Again, you &#8220;drive&#8221; the prover using a programming language.
It has support for induction and infinite data sets.

<li>
<a href="http://www.cl.cam.ac.uk/users/jrh/hol-light/">HOL Light</a>
(BSD-like license) is similar to HOL 4 (which is derived from HOL Light), but
is an unusually light theorem-proving system
running on OCaml (Objective Caml).
You still need to drive the program to make a proof; HOL Light includes
a MESON command which is an automated proof search method
called &#8220;model elimination&#8221; -- this automated search sometimes works, instead
of guiding the proof by hand.
<li>
<a href="http://metaprl.org/">MetaPRL</a> (GPL license)
is (1) &#8220;a general logical framework where multiple logics can
be defined and related&#8221;, and (2) &#8220;a system implementation with
support for interactive proof and automated reasoning&#8221;.
It has a &#8220;semantic connection to programming languages,
that allows the system to be used as a logical programming environment,
where programs are constructed as a mixture of specifications,
implementations, and verifications.&#8221;
An extract from their website should explain its purpose best:
&#8220;The MetaPRL system was implemented with the purpose of
supporting relations between logics. There is a huge investment
in formal work in systems like PVS, HOL, Coq, ELF,
Nuprl, and others. These systems use different logics
and different methodologies, but they have common goals and their
results share fundamental mathematical underpinnings.
Mathematical developments are expensive; our first goal
is to expose the logical foundations that the systems share,
to allow the results to be shared between systems...
Work is underway to relate the PVS, HOL, Isabelle,
and Nuprl mathematical foundations.&#8221;
MetaPRL is part of the Cornell Prl Automated Reasoning Project, and is
thus related to NuPrl.
MetaPRL is built using OCaml.

<li>
<a href="http://maude.cs.uiuc.edu/tools/scc/">
Maude Sufficient Completeness Checker</a> (GPL license)
is an experimental tool designed to check
that operations are defined on all valid inputs,
given a Maude-based specification (see below).

<li>
<a href="http://imps.mcmaster.ca/">Interactive Mathematical Proof
System (IMPS)</a> (special license, MIT-like plus requirement to
identify changes) is &#8220;intended to provide organizational and
computational support for the traditional techniques of mathematical
reasoning. In particular, the logic of IMPS allows functions to be
partial and terms to be undefined. The system consists of a database
of mathematics (represented as a network of axiomatic theories linked
by theory interpretations) and a collection of tools for exploring,
applying, extending, and communicating the mathematics in the database.&#8221;
It was developed by MITRE.

<li>
<a href="http://www.leancop.de/">LeanCoP</a> (GPL license) is
a compact theorem prover written in Prolog for classical
first-order logic which is based on the connection calculus.
It's actually only a few lines long! It's certainly not as powerful
as some of the other provers listed here (although it does perform more
strongly than you might expect), but its short length might make
it a good starting point for special purposes, or for learning
a little about how these tools work.
(Originally there was no license statement, but on 2006-05-31
Jens Otten sent me an email saying he intended to license under the GPL
shortly; on 2008-05-21 I confirmed that there's a license statement.)
<li>
<a href="http://deepthought.ttu.ee/it/gandalf">Gandalf</a> (GPL license)
is an automated theorem proving (ATP) system.
It has won several times in the CASC contest.
</ul>

<p>
I have not included some tools in this list because I can't confirm
that they have a FLOSS license.
<a href="http://www.dfki.de/~inka/maya.html">
MAYA</a> (originally part of Inka, something that
supports graphs and connects to various other useful components) has no
license that I can find; its "mathweb" component is clearly GPL'ed,
but it's unclear it's entirely GPLed, and it depends on the
proprietary Allegro Common LISP.
<a href="http://www.cs.uiowa.edu/~hzhang/induc.html">
RRL</a> has no license I can find, and I can't download it.
The lesson here is that if you develop a tool, you need to clearly
identify its license so that others can use it.

<p>
<h3><a name="modelcheckers">Formal methods: Model checkers</a></h3>
<p>
Here are tools that are model checkers that at least say they are FLOSS:
<ul>
<li>
<a href="http://spinroot.com/spin/whatispin.html">Spin</a>
(Spin license, which is an issue)
is a model-checking tool for
formal verification of distributed software systems
(using ProMeLa, its modeling language).
Spin has been used in a variety of applications, e.g.,
to verify the control algorithms
of a new flood control barrier in the Netherlands, and to
verify selected algorithms for a number of space missions
(including Deep Space 1, Cassini, the Mars Exploration Rovers, and
Deep Impact).
The big problem in model checkers is &#8220;state explosion&#8221;; Spin
counters this proble using a technique called
&#8220;partial order reduction&#8221;.
Spin won the ACM&#8217;s prestigious
<a href="http://awards.acm.org/software_system/">Software System Award</a>
in April 2002.
<a href="http://lwn.net/Articles/243851/">Here's an article about how
to use Spin and Promula to verify parallel algorithms</a>.
<p>
However, although the front page of the Spin project
says it has an open source license, and I believe that was their intent,
there are significant concerns that suggest it may <i>not</i>
be a FLOSS license at all.
Spin created their own unique license, an unwise practice that is
broadly discouraged (because it's so easy to get it wrong).
When people create their own licenses but are serious about making them FLOSS,
they generally submit it to opensource.org or the Free Software Foundation,
but neither the
<a href="http://www.opensource.org/licenses/">Opensource.org license list</a>
nor the
<a href="http://www.fsf.org/licensing/licenses/index_html">FSF license list</a>
identify the Spin license as a FLOSS license.
That's rather suspicious.
What's worse, the
<a href="http://lists.debian.org/debian-legal/2004/01/msg00273.html">
Debian-legal team noted some very serious problems with the Spin license</a>,
suggesting that it's not a FLOSS license at all.
Thankfully, there are other tools available now which do not have a cloud
of licensing problems hanging over them.
<li>
<a href="http://anna.fi.muni.cz/divine/tool/index.html">DiVinE tool</a>
(libraries GPL; tools appear to be as well)
is a model-checking tool for verifying concurrent systems
(and is thus similar to Spin).
DiVinE can itself run on a parallel distributed system,
making it possible to handle larger systems than Spin can.
It has its own native DiVinE modeling language.
Perhaps more interestingly, it can process C and C++!
<!--, and also supports
Spin&#8217;s ProMeLa language.
-->
<li>
<a href="http://www.luigidragone.com/hlmc/">Hybrid Logics Model Checker</a>
(GPL) is a model checker for the
hybrid logics MCLite and MCFull.
<li>
<a href="http://nusmv.irst.itc.it/">
NuSMV 2</a> (LGPL license) is a model checker that is a
re-implementation of SMV (so that a FLOSS version is available).
NuSMV, like SMV, counters the &#8220;state explosion&#8221; problem using
a construct called &#8220;BDDs&#8221;.
<li>
<a href="http://sprout.stanford.edu/dill/murphi.html">
Murphi</a> (BSD-new license + must rename changed version)
uses a language
based on a collection of guarded commands (condition/action rules),
which are executed repeatedly in an infinite loop
(similar to Misra and Chandy&#8217;s Unity model).
The language includes common data types
(subranges, enumerated types, arrays, and records), as well as
&#8220;Multiset&#8221; (for describing a bounded set of values whose order
is irrelevant to the behavior) and &#8220;Scalarset&#8221; (for
describing a subrange whose elements can be freely permuted).
Murphi has been used to verify many hardware components
and protocols.
<li>
<!-- http://embedded.eecs.berkeley.edu/blast/ -->
<a href="http://mtc.epfl.ch/software-tools/blast/">BLAST
(Berkeley Lazy Abstraction Software Verification Tool)</a>
(BSD license)
is a software model checker for C programs.  BLAST checks
that software satisfies behavioral properties of the interfaces it uses.
Their description:
"BLAST is a software model checker for C programs.  The goal of BLAST is to be able to check that software satisfies behavioral properties of the interfaces it uses. BLAST uses counterexample-driven automatic abstraction refinement to construct an abstract model which is model checked for safety properties. The abstraction is constructed on-the-fly, and only to the required precision."
<!--
(Note: This depends on ESC/Java, which is being rewritten to be FLOSS;
see the discussion below.)
-->
A key limitation: It has only been tested with non-recursive programs
(recursive programs require use of an untested option).
It also has a licensing issue; it requires a solver, and the only ones
it is written to use are Vampyre, Simplify, and Cvc.
(Vampyre and Simplify aren't FLOSS; CVC was intended to be, so perhaps
it will have a license change.)
<li>
<a href="http://javapathfinder.sourceforge.net/">Java PathFinder</a>
(NASA Open Source Agreement) is a model checker for Java bytecode.
</ul>

<p>
<h3><a name="sat">Formal methods: SAT Solvers</a></h3>
The
<a href="http://en.wikipedia.org/wiki/Boolean_satisfiability_problem">
Boolean satisfiability (SAT) problem</a>
is
the problem of determining if the variables of a given Boolean formula
(where all variables can only be true or false)
can be assigned in such a way as to make the formula evaluate to TRUE;
alternatively, it's to determine
if no such assignments exist (i.e., if it's unsatisfiable).
SAT programs are low-level programs/algorithms that
many other formal methods tools (like theorem provers) build on.
In the last number of years there have been a lot of improvements in
SAT solvers, resulting in improvements on anything built on them.
SAT is a big area;
<a href="http://www.satlive.org/">SAT live</a> tracks SAT goings-on.
<a href="http://www.cse.unsw.edu.au/~tw/sat/">Here are some SAT surveys</a>.
There are a number of competitions, including the
<a href="http://www.satcompetition.org/">International SAT competition</a>,

<ol>
<li>
<a href="http://www.cs.chalmers.se/Cs/Research/FormalMethods/MiniSat/Main.html">
MiniSat</a> (MIT license).
In the
<a href="http://www.satcompetition.org/">
SAT 2005 competition</a>,
MiniSAT all by itself won Silver in the industrial categories
SAT+UNSAT and SAT.
MiniSAT is a "conflict driven solver", one of main (modern) styles of
SAT solvers.
SatELiteGTI is the combination of
SatELite (used as a preprocessor) with MiniSat (the &#8220;GTI&#8221; component).
SatELiteGTI won Gold in all three industrial categories:
SAT+UNSAT, SAT, and UNSAT.
I cannot find the license for SatELite, but the developers are making
SatELite obsolete anyway by incorporating its capabilities into their
updated version of MiniSAT.
<li>
<a href="http://www.st.ewi.tudelft.nl/sat/march_dl.php">MarchDL</a> (GPLv2+)
is a SAT solver based on the "look-ahead" approach (one of the other
main modern styles of SAT solvers).
It won a prize at the 2007 SAT competition.
<li>
<a href="http://dudka.cz/fss">Fast SAT Solver</a> (GPL) is a SAT
solver based on genetic algorithms.
<li>
<a href="http://fmv.jku.at/picosat/">PicoSAT</a> (MIT-style)
is a recent and strong SAT solver.
It did very well in the
<a href="http://www.satcompetition.org/">SAT'07 SAT Solver competition</a>;
Version 535 won the category of "satisfiable industrial instances"
and came second on all industrial instances (satisfiable <i>and</i>
unsatisfiable).
<li>
<a href="http://vallst.berlios.de">Vallst</a>
(Reciprocal Public License, a GPL-like but stricter
<a href="http://www.opensource.org/licenses/rpl1.0.php">OSI-certified</a>
license)
is another SAT solver.
It won two golds and one bronze in the SAT 2005 &#8220;world championships&#8221;.
</ol>

<p>
<h3><a name="sat">Formal methods: SMT Solvers</a></h3>
<p>
The
<a href="http://en.wikipedia.org/wiki/Satisfiability_modulo_theories">
Satisfiability Modulo Theories (SMT) problem</a>
is an extension of the SAT problem (above).
Basically, given expressions with boolean variables and/or
predicates (functions that take potentially non-boolean values yet
return boolean values), determine the conditions that would make it
true (or conversely, show it's false).
An SMT solver adds one or more "theories" for various predicates, e.g.,
it might add real numbers (adding predicates like
less-than and equal-to), integers, lists, and so on.
SMT solvers are sometimes implemented on top of SAT solvers.
<p>
Many other systems build on top of SMT solvers.
The
<a href="http://combination.cs.uiowa.edu/smtlib/">
SMT-LIB: The Satisfiability Modulo Theories Library</a> and
<a href="http://www.smtcomp.org/">
SMT-COMP: The Satisfiability Modulo Theories Competition</a> are
important to many SMT solver implementors.
<a href="http://combination.cs.uiowa.edu/smtlib/">Here is a list of
SMT solvers (current and abandoned, FLOSS and not)</a>.
<p>
Examples include:
<ol>
<li>
<a href="http://ergo.lri.fr/">Ergo (Alt-Ergo)</a> (CeCILL-C license) is
an automatic theorem prover focused on program verification.
It supports equational theory (=) and linear arithmetic, and it's relatively
small.
One significant problem is that this is licensed under the extremely rare
CeCILL-C license, not the CeCILL license.
I can't find a major FLOSS organization who has ruled that the
CeCILL-C license is FLOSS
(including the FSF, OSI, Debian, or Fedora).
This license is intended to be FLOSS, but that is as yet untested.

<li>
<a href="http://www.cs.nyu.edu/acsys/cvc3/">CVC3</a> (BSD)
is "an automatic theorem prover for Satisfiability Modulo Theories (SMT) problems. It can be used to prove the validity (or, dually, the satisfiability) of first-order formulas in a large number of built-in logical theories and their combination."
CVC3 is the successor to CVC Lite.
The license of earlier versions of CVC3 included some highly-controversial
non-standard clauses, one of which was an "indemnification" clause
that to some appeared highly dangerous to any user or distributor.
Fedora eventually ruled that the license was "non-free" and thus unacceptable.
I contacted the developer, and although it took a long time, I'm delighted
to report that as of October 2009, the CVC3 license was changed to a
simple, normal BSD license, resolving the issue.

<li>
<a href="http://lipforge.ens-lyon.fr/www/gappa/">Gappa</a>
(CeCILL or GPL, libraries LGPL)
is a tool "intended to help verifying and formally proving properties
on numerical programs dealing with floating-point or fixed-point
arithmetic. It has been used to write robust floating-point filters for
CGAL and it is used to certify elementary functions in CRlibm.
It requires Coq support library 0.8.
("Why" can invoke Gappa.)

<li>
The
<a href="http://sourceforge.net/projects/dpt">
Decision Procedure Toolkit (DPT)</a> (Apache license)
is "a system of cooperating decision procedures for answering satisfiability
queries. The DPT implementation in OCaml comprises a DPLL-style SAT
solver with theory-specific decision procedures".

<li>
<a href="http://absolver.sourceforge.net/">
Arithmetic and Boolean solver (ABSolver)</a> (Common Public License 1.0)
is a framework for combining other tools to solve mixed
arithmetic and Boolean problems, and is designed to make it easy to
add new solvers.
<!-- Determined license by opening up source and looking at file header,
as well as COPYING file -->
ABSolver is remarkable in its ability to solve non-linear problems.
However,
"Efficient Solving of Large Non-linear Arithmetic Constraint
Systems with Complex Boolean Structure"
(Journal on Satisfiability, Boolean Modeling and Computation 1 (2007) 209–236)
warns that ABSolver's "currently reported implementation
uses the numerical optimization tool
IPOPT (https://projects.coin-or.org/Ipopt) for
solving the non-linear constraints.
Consequently, it may produce incorrect results due to
the local nature of the solver, and due to rounding errors."

<li>
<a href="http://www.matf.bg.ac.yu/~filip/argo/ArgoLib/">Argo-lib</a>
(GPLv2) is an SMT-LIB solver.
It is "a C++ library which provides a generic support for using decision
procedures in automated reasoning systems and also support for several
schemes for combining and augmenting decision procedures. This platform
follows the SMT-lib initiative which aims at establishing a library of
benchmarks for satisfiability modulo theories. ARGO-lib platform can
be easily integrated into other systems, but it should also enable
comparison and unifying of different approaches, evaluation of new
techniques and hopefully help advancing the field. ARGO-lib follows a
range of techniques and different systems. The latest version of ARGO-lib
provides support for DPLL(T) scheme and for producing object-level proofs."
<!--
<p>
Note: the
<a href="http://www.matf.bg.ac.yu/~filip/argo/ArgoLib/">source code</a>
includes the file
<a href="http://www.matf.bg.ac.yu/~filip/argo/ArgoLib/license">license</a>,
which has the text of GPL version 2.
However, when I examined this on 2008-05-30,
the source files didn't include any license information, nor was there a
clear statement about exactly what the GPLv2 applied to.
I've contacted the author; he will soon make the license much clearer.

As of 2008-08-01, GPLv2 is clearly in the files; that's sufficiently
clear that I haven't pressed further.
-->

<li>
<a href="http://code.google.com/p/opensmt/">OpenSMT</a> (GPLv3)
is a "compact and open-source SMT-solver written in C++,
with the main goal of making SMT-Solvers easy to understand.
OpenSMT is built on top of MiniSAT (http://minisat.se)...
Currently OpenSMT supports only the theory of
Equality with Uninterpreted Functions [QF_UF]...
In the future we plan to extend OpenSMT with other theories."


<li>
<a href="http://nemerle.org/~malekith/smt/en.html">Fx7</a>
(BSD-like license).

<li>
<a href="http://harvey.loria.fr/">
haRVey SMT prover has two branches:
haRVey-FOL (LGPL) and haRVey-SAT (BSD license)</a>.
From the website:
<ul>
<li>
"haRVey-FOL integrates a First-Order Logic theorem prover (hence
its name), i.e. the E-prover. It uses the superposition calculus as
implemented by the E-prover, to determine the satisfiability of Boolean
combinations of atoms with functions interpreted in a first-order theory
with equality."
haRVey-FOL includes a pre-processor (by Augusto Antonio Viana da Silva)
that removes axioms that are not relevant for the proof of the current goal,
which should make it more capable than provers without one.
haRVey-FOL (aka "Harvey") depends in turn on SPASS (for some utilities) and E.
Unfortunately, haRVey-FOL also depends on zchaff, which is definitely
<i>not</i> FLOSS (and thus can't be pre-packaged into various distribution's
main repositories); my hope is that a future version will be able to use
miniSAT2 or some other FLOSS SAT solver.
<li> 
"haRVey-SAT is based on congruence closure, the Nelson-Oppen framework,
and rudimentary instantiation techniques to decide the satisfiability of
a set of atoms written with uninterpreted symbols, linear arithmetics,
some lambda-expressions, and some quantifiers. The Boolean engine is a
SAT solver (zChaff or MiniSAT), hence its name."
Although rv-SAT has promise, it's not appropriate for use with anything
related to high assurance as of August 2008, for it says:
"rv-sat is in early development stage.  In particular, it is not
complete for (linear) arithmetics.  However, rv-sat gives only two
answers: "sat" or "unsat".  In the case the formula belongs to a fragment
for which rv-sat is incomplete, "sat" should be understood as "rv-sat
has not been able to prove unsatisfiability of the input formula".  In
short: "sat" should only be trusted if QF_UF.
These incompleteness issues will be solved in future versions of the software."
</ul>
"Current developments aim at merging both branches, and provide one
uniform tool. The main issues are
the logics are different (haRVey-SAT is multi-sorted, haRVey-FOL is not)
[and]
there is some technical and theoretical difficulties to combine first-order provers within a Nelson-Oppen scheme.
So, haRVey is still in development stage...".
<a href="http://harvey.loria.fr/haRVey-download.php">haRVey downloads
are available</a> (but watch out, some links are broken, so it
can be hard to find).
</ul>
<li>
<a href="http://people.csail.mit.edu/vganesh/STP_files/stp.html">
STP</a> (MIT license) is
a Decision Procedure for Bitvectors and Arrays.
"STP is a constraint solver (also referred to as a decision procedure
or automated prover) aimed at solving constraints generated by program
analysis tools, theorem provers, automated bug finders, intelligent
fuzzers and model checkers. STP has been used in many research projects
at Stanford, Berkeley, MIT, CMU and other universities. It is also
being used at many companies such as NVIDIA, some startup companies,
and by certain government agencies.
The input to STP are formulas over the theory of bit-vectors and arrays
(This theory captures most expressions from languages like C/C++/Java
and Verilog), and the output of STP is a single bit of information that
indicates whether the formula is satisfiable or not. If the input is
satisfiable, then it also generates a variable assignment to satisfy
the input formula.
We are currently adding the theory of finite sets and the theory of
uninterpreted functions to STP."
There is a
<a href="http://sourceforge.net/projects/stp-fast-prover/">
SourceForge home page for STP</a>.
It uses MINISAT.
</ol>


<!--
Maybe add "Termination" finder?

See Termination competition, e.g.:
http://www.lri.fr/~marche/termination-competition/2007/
AProVE 07: Proprietary

But this seems really specialized, and not really useful to most
developers at this time.  Please prove (ha!) me wrong...!

-->

<p>
<h3><a name="otherformalmethods">Formal methods: Other Tools</a></h3>
<p>
Here are FLOSS tools that are don&#8217;t easily fit into the above categories:
<ul>
<li>
<a name="#alloy"></a>
<a href="http://alloy.mit.edu">Alloy</a> (GPL)
implements a simple structural modeling language based on first-order logic.
This is a really interesting project; its language is similar
to Z, VDM, or UML constraints, but it can analyze the
results <i>completely</i> automatically (no theorem-proving or other
complexities) and display results graphically,
making it unusually easy to use.
The tool can generate instances of invariants,
simulate the execution of operations (even those defined implicitly),
and check user-specified properties of a model.
&#8220;The motivation for the Alloy project was to bring to Z-style
specification the kind of automation offered by model checkers.
The Alloy Analyzer is designed for analyzing state machines with
operations over complex states...&#8221;
<p>
Alloy
includes the Alloy Analyzer, &#8220;which is a model finder (not a model checker):
given a logical formula, it finds a model of the formula.
When an assertion is found to be false,
the Alloy Analyzer generates a counterexample...
Alloy Analyzer is essentially a compiler. It translates the problem
to be analyzed into a (usually huge) boolean formula. This formula is
handed to a SAT solver, and the solution is translated back by the Alloy
Analyzer into the language of the model. All problems are solved within
a user-specified scope that bounds the size of the domains, and thus
makes the problem finite.&#8221;
Because of its different approach, Alloy supports many higher-level
structures (such as sets, relations, tables, and trees);
&#8220;most model checking languages provide only relatively
low-level data types (such as arrays and records)&#8221;.
The tool is written in Java, and includes a GUI interface.
<p>
This tool looks like it&#8217;d be very useful for specifying
in some medium assurance environments, and I think it would be useful
for high assurance at level 0 (with a little more strength than usual
at level 0).
The notation is fairly clear, and the notation is specifically designed
so that assertions can be analyzed in a completely automated way
(unlike today&#8217;s theorem-proving).
Those are big advantages, and thus this is a good example of a
&#8220;formal methods light&#8221; tool.
<p>
However, note that it cannot prove that certain things can <i>never</i>
happen;
instead it can prove something like "X cannot happen within Y steps"
(you can choose Y to be as large as you like).
The phrase they use to describe it is a "model finder" approach;
basically, they try to create a model that falsify the claims.
Thus, while it can certainly give <i>some</i> confidence that the specification
is right, it often cannot &#8220;prove&#8221; things to the strength
that you usually want at level 1 or 2 for high assurance.
Notationally, Alloy is very different from the tools called "model checking"
tools; model checkers are typically designed to analyze compositions of
state machines running in parallel, and usually only support arrays and
records inside the state machines.
In contrast, Alloy supports more abstract notations such
as sets and relations.
I can easily imagine this tool being combined with other tools
(a theorem-prover or model checker)... this approach supports
quick tests for sanity, and then you could
prove in more depth if you needed to.
<p>
<a href="http://sdg.lcs.mit.edu/alloy/papers/p16-hashii.pdf">
Brant Hashii&#8217;s &#8220;Lessons Learned Using Alloy to Formally Specify MLS-PCA Trusted Security Architecture&#8221;</a> describes using Alloy to model security.
<a href="http://sdg.lcs.mit.edu/alloy/papers/interop.pdf">
Pamela Zave&#8217;s &#8220;A Formal Model of Addressing for Interoperating Networks&#8221;</a>
describes using Alloy to model network addressing.
A number of places use Alloy as a teaching tool as well, because its
ability to easily generate graphically-displayed examples seems to help
people understand its analysis results.

<li>
<a href="http://www.openproofs.org/wiki/SPARK">SPARK</a>
is a subset/superset of Ada.
SPARK builds in the ability to define preconditions and postconditions;
its tools can then determine if the postconditions are met.
The SPARK language is designed so that SPARK code can be passed into
an Ada compiler (unchanged) for code generation.
Tokeneer is a serious example of a system implemented using SPARK.
SPARK doesn't support dynamic constructs, so it's not a good fit
for some applications.
On the other hand, there are many applications (say, control systems)
where this omission is considered a <i>good</i> thing, and if high
reliability is critically important,
SPARK should <i>definitely</i> be considered.

<li>
<a href="http://why.lri.fr/">Why</a> /
<a href="http://why.lri.fr/caduceus/">Caduceus</a> /
<a href="http://krakatoa.lri.fr/">Krakatoa</a> 
These are tools for verifying implementations (code, with emphasis
currently on C and Java); all are released under the GPL.
<p>
"Why" is a software verification tool; it is a general-purpose
verification conditions generator (VCG) for other
verification tools (including Coq and PVS), which it can call on.
<a href="http://why.lri.fr/manual/manual003.html">
It can be used as a front-end for many tools</a>, including
calling out to many automated tools (so it can actually combine the results
of many different tools in a useful way).
On top of "Why" are two very interesting tools:
<ul>
<li>
<a href="http://why.lri.fr/caduceus/">Caduceus</a>
is a verification tool for C programs, built on top of Why.
It can even handle C programs with pointers (C pointers are notoriously
hard to handle, but tools that can't handle C pointers are useless for C).
This is obsolete; instead, use the "Jessie" tool included
in Why (Jessie requires Frama-C).
<li>
<a href="http://krakatoa.lri.fr/">Krakatoa</a> is a verification
tool for Java programs, also built on top of Why.
</ul>

<li>
<a href="http://frama-c.cea.fr/">Frama-C</a> (LGPL)
is a framework for the development of collaborating
static analyzers for the C language. Many analyzers are provided in the
distribution, including a value analysis plug-in that provides variation
domains for the variables of the program, and Jessie, a plug-in
for computing Hoare style weakest preconditions (building on <i>Why</i>,
above).
It provides a formal behavioral specification language for C programs
named ACSL.
<a href="http://lists.gforge.inria.fr/pipermail/why-discuss/2008-March/000027.html">Frama-C (particularly its "Jessie" plug-in)
is to eventually replace Caduceus</a>.
Like Caduceus, it can handle C pointers but currently it has trouble with
unions and casts; there is
<a href="http://www.citeulike.org/user/_Anne_/article/2868946">
work to essentially remove many of those restrictions</a>.
<!--
Union and Cast in Deductive Verification
by: Yannick Moy
Vol. Technical Report ICIS-R07015 (jul 2007)
-->

<li>
<a href="http://saturn.stanford.edu/">Saturn</a> (BSD-like license)
is a program to statically and automatically
verify properties of large (meaning multi-million line) software systems. 

<li>
<a href="http://www-sop.inria.fr/everest/soft/Jack/jack.html">
JACK: Java Applet Correctness Kit
</a>
(Cecill C licence)
"The Jack tool provides an environment for verification of Java and Java Card programs with JML annotations. It implements a fully automated weakest precondition calculus that generates proof obligations from annotated Java sources. Those proof obligations can be discharged using different theorem provers.
An important design goal of Jack is that it is easy to use for normal Java developers, who use it to validate their own code. To allow developers to work in a familiar environment, Jack is integrated as a plugin in the eclipse IDE. Care has been taken to hide the mathematical complexity of the underlying concepts. Therefore Jack provides a dedicated proof obligation viewer, that presents the proof obligations connected to execution paths within the program. For each proof obligation, the relevant source code is highlighted. Moreover goals and hypothesis are displayed in a Java/JML like notation.
Our goal is to allow formal method experts to prove the correctness of Java applets, and moreover, to allow Java programmers to obtain a high confidence in the correctness of their application.
Currently proof obligations can be generated for
the Simplify theorem prover (notably used by ESC/Java) and
the Coq proof assistant.
The Jack proof manager sends the proof obligations to the different provers, and keeps track of proven and unproven proof obligations."


<li>
<a href="http://sdg.csail.mit.edu/forge/">Forge / JForge</a> (GPLv3).
"Forge is a program analysis framework that allows a procedure in a conventional object oriented language to be automatically checked against a rich interface specification. The framework uses a bounded verification technique, in which all executions of a procedure are examined up to a user-provided bound on the heap and number of loop unrollings. If a counterexamples exists within the bound, Forge will find and report the complete program trace, but defects outside the bound may be missed. To facilitate modular analysis, specifications can be embedded as statements in code, an idea borrowed from the refinement calculus.
<p>
The core Forge library... operates on programs constructed in the Forge Intermediate Representation (FIR), a simple, relational programming language. To analyze a program written in a conventional programming language, like Java or C, that program and its specification must first be encoded in FIR. We have built a command-line tool called JForge that analyzes Java code against specifications written in the Java Modeling Language (JML) by translating them both to FIR, and we have made this tool available for download as well. Others are working on a translation from C to FIR, and we welcome and encourage you to encode your own favorite language in FIR."

<li>
<a href="http://www.splint.org">Splint, formerly named LCLint</a>
(GPL license)
does static analysis of C programs, and is usually used in a medium assurance
mode that requires very little specification work from a developer to
help find some security flaws.
But splint is actually based on a long trail of research
into formal methods (on &#8220;Larch&#8221; specifically), and it supports far
stronger annotation and proof methods if developers choose to use them
that move into high assurance.


<li>
<a href="http://groups.csail.mit.edu/pag/daikon/">Daikon</a>
(MIT-style; includes some components with other OSS licenses).
"Daikon is an implementation of dynamic detection of likely invariants; that is, the Daikon invariant detector reports likely program invariants. An invariant is a property that holds at a certain point or points in a program; these are often seen in assert statements, documentation, and formal specifications. Invariants can be useful in program understanding and a host of other applications...
Dynamic invariant detection runs a program, observes the values that the program computes, and then reports properties that were true over the observed executions. Daikon can detect properties in C, C++, Java, Perl, and IOA programs; in spreadsheet files; and in other data sources. (Dynamic invariant detection is a machine learning technique that can be applied to arbitrary data.) It is easy to extend Daikon to other applications; as one example, an interface exists to the Java PathFinder model checker."


<li>
<a href="http://web.mit.edu/emina/www/kodkod.html">
Kodkod</a> (MIT license) is a constraint solver for relational logic.
It is "an efficient SAT-based analysis engine for first order logic with relations, transitive closure, and partial instances. The current prototype, which includes a finite model finder and a minimal unsatisfiable core extractor, is being used as a backend to the Karun, Forge, and Miniatur code checkers, a course scheduler, the Alloy Analyzer 4.0, a network configuration tool, etc.
Unlike traditional model finders (e.g. Alloy Analyzer 3, Paradox, and MACE), Kodkod is designed to take advantage of partial instance information..."


<li>
<a href="http://www.inf.ethz.ch/personal/daniekro/satabs/">
SATABS</a> (BSD-old style license, but with odd notification requirement
that may be non-FLOSS) is a verification tool
for ANSI-C programs.
It allows verifying array bounds (buffer overflows), pointer safety,
exceptions and user-specified assertions.
<li>
<a href="http://manju.cs.berkeley.edu/ccured/">
CCured</a> (BSD-new license)
is a &#8220;source-to-source translator for C. It analyzes the C program to determine the smallest number of run-time checks that must be inserted in the program to prevent all memory safety violations. The resulting program is memory safe, meaning that it will stop rather than overrun a buffer or scribble over memory that it shouldn&#8217;t touch.&#8221;
I am skeptical that this would be used in a high assurance setting, but
I can&#8217;t help but mention it.

<li>
<a href="http://www.jakstab.org/">Jakstab (GPLv2)</a>
is an "Abstract Interpretation-based, integrated disassembly and static analysis framework for designing analyses on executables and recovering reliable control flow graphs. It is designed to be adaptable to multiple hardware platforms using customized instruction decoding and processor specifications similar to the Boomerang decompiler. It is written in Java, and in its current state supports x86 processors and 32-bit Windows PE or Linux ELF executables.
Jakstab translates machine code to a low level intermediate language on the fly as it performs data flow analysis on the growing control flow graph. Data flow information is used to resolve branch targets and discover new code locations."

<li>
<a href="http://deputy.cs.berkeley.edu/">Deputy</a> (revised BSD license)
is "a C compiler that is capable of preventing common C programming errors, including out-of-bounds memory accesses as well as many other common type-safety errors. It is designed to work on real-world code, up to and including the Linux kernel itself.  
Deputy allows C programmers to provide simple type annotations that describe pointer bounds and other important program invariants. Deputy verifies that your program adheres to these invariants through a combination of compile-time and run-time checking.
Unlike other tools for checking C code, Deputy provides a flexible annotation language that allows you to describe many common programming idioms without changing your data structures. As a result, using Deputy requires less programmer effort than other tools. In fact, code compiled with Deputy can be linked directly with code compiled by other C compilers, so you can choose exactly when and where to use Deputy within your C project."
<p>
"Unlike many other safe C variants such as Cyclone and CCured,
Deputy is incremental and thread safe. That is, programmers are free to add annotations and modify code function-by-function. This is possible because Deputy does not change the representation of the data visible across function boundaries, which allows “deputized” modules to interoperate with standard modules. While the initial version of the file may contain several blocks of trusted code, subsequent versions will gradually eliminate this trusted code in favor of fully annotated and checked code."
[<a href="http://www.usenix.org/event/hotos07/tech/full_papers/anderson/anderson_html/">Beyond Bug-finding</a>].

<li>
<a href="http://cyclone.thelanguage.org/">Cyclone</a>
(GPL and LGPL)
is "a safe dialect of C."

<li>
<a href="http://theoretica.informatik.uni-oldenburg.de/~pep/">
PEP (Programming Environment based on Petri Nets)</a>
[<a href="http://sourceforge.net/projects/peptool">PEP SourceForge site</a>]
(GPL)
is a broad set of modelling, compilation, simulation
and verification components,
linked together within a Tcl/Tk-based graphical user interface.
It is based on Petri Nets (which are aimed at addressing concurrency
and nondeterminism).
<li>
<a href="http://manta.sourceforge.net/">ManTa</a> (GPL + public domain)
is a programming/specification language, and also the name of
the supporting development environment.
It is fundamentally based on letting users write algebraic specifications
of ADTs (abstract data types).  &#8220;Its theoretical bases ensure that every program written has &#8220;mathematical meaning&#8221; (i.e. a model)&#8221;
It then lets you &#8220;Evaluate expressions by using a Rewriting Motor,
&#8220;Demonstrate ADT properties by using an inductive theorem prover..., [and]
Generate correct code which implements an ADT in ANSI C or Ocaml.&#8221;
This looks interesting, but as of May 2006 it seems to have stalled since 2001.
Thankfully, any FLOSS project can get restarted by anyone else, so if
there is interest, that&#8217;s all that is needed.
<li>
<a href="http://modulogic.inria.fr/focal/site/index.php">FoCaL / FoCaLize</a>
(BSD-style).
The <a href="http://focal.inria.fr/site/index.php?option=com_content&amp;task=view&amp;id=16&amp;Itemid=49">FoCaL overview</a> says
"The Focal project attempts to provide a programming environment in
which certified programs can be developed. This environment is based on a
language including functional and object-oriented features. Moreover, this
language provides means for the programmers to write formal specifications
and proofs of their code, and to have them verified by a proof checker.
Thanks to inheritance and refinement mechanisms, Focal allows to make
several refinements of a specification until providing an efficient
executable code (obtained via a translation to OCaml).
Focal provides a library which implements mathematical structures up
to multivariate polynomial rings and includes complex algorithms with
performances comparable to the best CAS in existence."
<p>
<a href="http://ralyx.inria.fr/2006/Raweb/gallium/uid47.html">
Another FoCaL overview</a> adds some detail:
"Focal, a joint effort with LIP6 (U. Paris 6) and Cedric (CNAM), is
a programming language and a set of tools for software-proof codesign. The
most important feature of the language is an object-oriented module system
that supports multiple inheritance, late binding, and parameterisation
with respect to data and objects. Within each module, the programmer
writes specifications, code, and proofs, which are all treated uniformly
by the module system.
Focal proofs are done in a hierarchical language invented by Leslie
Lamport. Each leaf of the proof tree is a lemma that must be proved
before the proof is detailed enough for verification by Coq. The Focal
compiler translates this proof tree into an incomplete proof script. This
proof script is then completed by Zenon, the automatic prover provided
by Focal."
<p>
As of May 2008 it looks more like early research work, but
it will probably mature over time.
They seem to have focused primarily on implementing
computer-aided algebra (CAS) systems so far.

<!-- BSD-style license
http://focal.inria.fr/site/index.php?option=com_docman&task=doc_download&gid=15&Itemid=54

Copyright (c) 2003, by:
  Universite Pierre et Marie Curie (UPMC)
  Institut National de Recherche en Informatique et en Automatique (INRIA)
  Conservatoire National des Arts et Metiers (CNAM)

All rights reserved.

Redistribution and use in source and binary forms, with or without
modification, are permitted provided that the following conditions are
met:
        
. Redistributions of source code must retain the above copyright
  notice, this list of conditions and the following disclaimer.
        
. Redistributions in binary form must reproduce the above copyright
  notice, this list of conditions and the following disclaimer in the
  documentation and/or other materials provided with the
  distribution.
        
. Neither the names of LIP6, INRIA, CNAM, nor the names of the
  contributors may be used to endorse or promote products derived from
  this software without specific prior written permission.


THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
"AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
A PARTICULAR PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT
OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
(INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
-->


<li>
<a href="http://banshee.sourceforge.net/">Banshee</a>
(most BSD License, some GPL) is &#8220;a toolkit that simplifies the task of
building constraint-based program analyses.
Program analyses are widely used in compilers and
software engineering tools for discovering or verifying
specific properties of software systems...
the analysis designer provides a short specification file
describing the kinds of constraints used in the analysis.
From this specification, BANSHEE builds a customized constraint
resolution engine which solves those constraints very efficiently.&#8221;
Banshee is the successor of
<a href="http://www.eecs.berkeley.edu/Research/Aiken/bane.html">BANE</a>
(MIT license).
"The pointer analysis application, which includes a C parser derived
from GCC, is also included and is released under the GNU General Public
License."

<li>
<a href="http://code.google.com/p/iprover/">iProver</a> (GPLv3).
iProver can is a general-purpose automated theorem prover, using
"a modular combination of first-order reasoning with ground reasoning. In particular, iProver currently integrates MiniSat for reasoning with ground abstractions of first-order clauses... [it]
can solve around 4843 problems out of 8984 in the TPTP-v3.2.0 library (with the default options)."

<li>
<a href="http://combination.cs.uiowa.edu/Darwin/">Darwin</a> (GPL).
"Darwin is an automated theorem prover for first order clausal logic. It accepts problems formulated in tptp or tme format, non-clausal tptp problems are clausified using the eprover. Equality is not built into the currently implemented version of the calculus, it is instead automatically axiomatized for a given problem. Darwin is a decision procedure for function-free clause sets, and is in general faster and scales better on such problems than propositional approaches."

<li>
<a href="http://www.cs.chalmers.se/~koen/paradox/">
Paradox</a> (GPL license)
is a tool that processes first-order logic problems and
tries to find finite-domain models for them.
Paradox won the SAT/Models class (generated most models)
in the CASC 2003 competetion for first-order logic tools.
Paradox can read problems in both TPTP and Otter syntax.
It is written in Haskell, and depends on MiniSAT.
Paradox is co-developed with Equinox, a first-order theorem prover.
<li>
<a href="http://www.lama.univ-savoie.fr/~raffalli/pml/">
Proved ML</a> (CeCILL-B license)
is a variant of the ML language
"which focuses both on being able to prove programs and be really usable".
As of May 2008 it was not ready for serious use.
<li>
<a href="http://chicory.stanford.edu/dill/Murphi/">Murphi</a>
(CMC license, BSD-like) is
Finite-state Concurrent System Verifier.
It was developed by David L. Dill,
who's done a lot of work related to
<a href="http://verifiedvoting.org/">verified voting</a>.
<li>
<a href="http://proofgeneral.inf.ed.ac.uk/">Proof General</a> (GPL)
is a generic front-end for interactive theorem provers
(aka "proof assistants") based on the customizable text editor Emacs.
It works with Isabelle, Coq, PhoX, and LEGO, and has experimental support
for other tools like HOL and ACL2.
<li>
<a href="http://jmleclipse.projects.cis.ksu.edu/">JMLEclipse</a>
is "an Eclipse plugin that allows the integration of JML into
Eclipse's Java Development Tools (JDT)...
The whole idea behind JMLEclipse is to have an open framework that can be used as frontend for different JML tools."

<li>Computer Algebra System (CAS).
Historically, CASs have been separate programs.
There are a large number of FLOSS CAS programs to choose from.
<p>
It's worth noting the integrating program
<a href="http://www.sagemath.org">SAGE</a>
(<a href="http://sage.math.washington.edu/home/aklemm/sage-1.4/COPYING.txt">GPL and GPL-compatible licenses</a>), a Python-based
program that integrates several CAS and other mathematical programs with
the goal of "Creating a viable free open source alternative
to Magma, Maple, Mathematica, and Matlab."
<p>
Examples of specific CAS programs include:
<ol>
<li><a href="http://maxima.sourceforge.net/">Maxima</a> (GPL), with a GUI
provided by
<a href="http://wxmaxima.sourceforge.net/">wxMaxima</a> (GPL).
This is a large system with a very long history; it is very mature.
<li><a href="http://www.axiom-developer.org/">Axiom</a> (BSD-style library).
This is a large system, also with a long history.
One interesting aspect is that all values are "mathematically typed", that is,
it includes a type system.
"In its current state it represents about 30 years and 300 man-years of research work."
<li><a href="http://yacas.sourceforge.net/homepage.html">Yacas</a> (GPL).
This is implemented in C++ and is designed to require relatively few
resources or dependencies.
<li><a href="http://www.ginac.de/">GiNaC</a>.  This is meant to be
embedded in C++ programs.
<li><a href="http://code.google.com/p/sympy/">SymPy</a> is a simple CAS
implemented in Python, where simple implementation in an
easily-read language is important.
<li>
<a href="http://www.gap-system.org/">GAP
(Groups, Algorithms, Programming)</a> (GPL)
is a system for computational discrete algebra.
<li>
<a href="http://swiss.csail.mit.edu/~jaffer/JACAL.html">Jacal</a> (GPLv3)
is "an interactive symbolic mathematics program. JACAL can manipulate and simplify equations, scalars, vectors, and matrices of single and multiple valued algebraic expressions containing numbers, variables, radicals, and algebraic differential, and holonomic functions.
JACAL is a GNU package."
<li>
<a href="http://kayali.sourceforge.net/">Kayali</a> (GPL)
is a GUI front-end of a CAS system.
It's a Qt-based GUI front-end to (a subset of) Maxima and gnuplot.
It is implemented in Python.
</ol>
<li>General-purpose upper ontologies.
For purposes of this paper, an ontology is something that provides
(1) identification of basic categories of objects (real or abstract),
(2) a way of determining what kinds of entities fall into those categories, and
(3) a way of determining the relationships between and among the categories.
An ontology is extremely useful when handling unrestricted
natural language; ontologies help you infer
some of the information that is implied but unstated in ordinary text.
They also help you structure problems if they have a rich set of object types.
See
<a href="http://en.wikipedia.org/wiki/Upper_ontology_(computer_science)">
Wikipedia's article on upper ontology</a> and
<a href="http://www.formalontology.it/">formalontology.it</a>
for more about ontologies;
you should probably also know about
<a href="http://www.w3.org/TR/owl-ref/">W3C's
OWL Web Ontology Language</a>.
<a href="http://www.disi.unige.it/person/MascardiV/Download/DISI-TR-06-21.pdf">
A Comparison of Upper Ontologies
(Technical Report DISI-TR-06-21)</a> gives a brief comparison of ontologies.
Some general-purpose upper ontologies are available as FLOSS:
<ol>
<li>
<a href="http://www.ontologyportal.org/">
Suggested Upper Merged Ontology (SUMO)</a> (GPL for extensions).
<a href="http://suo.ieee.org/SUO/SUMO/index.html">IEEE has a working group
on SUMO</a>.
For more information on SUMO, see: Niles, I., and Pease, A. 2001.
<a href="http://home.earthlink.net/~adampease/professional/FOIS.pdf">
Towards a Standard Upper Ontology</a>.
In Proceedings of the 2nd International Conference on Formal Ontology in Information Systems (FOIS-2001), Chris Welty and Barry Smith, eds, Ogunquit, Maine, October 17-19, 2001.
SUMO's top-level item is the "Entity", which it breaks down as follows:
<pre>
  Physical
     Object
         SelfConnectedObject
            ContinuousObject
            CorpuscularObject
         Collection
     Process
  Abstract
     SetClass
            Relation
     Proposition
     Quantity
         Number
         PhysicalQuantity
     Attribute
</pre>
SUMO maps to the widely-used
<a href="http://www.cogsci.princeton.edu/~wn/">Wordnet</a> (next!).
<li>
<a href="http://www.cogsci.princeton.edu/~wn/">Wordnet</a> (BSD-style).
is itself an ontology of words along with other information (parts of speech
and definitions).
Thus, Wordnet is useful as a dictionary, and it tends to be used for
a lot linguistic work.
Wordnet doesn't have much about interrelationships between concepts
(other than synonyms/antonyms) in the way SUMO and other specs do, though,
so for some purposes Wordnet is paired with other information.
<li>
<a href="http://www.loa-cnr.it/DOLCE.html">
DOLCE - a Descriptive Ontology for Linguistic and Cognitive Engineering</a>
(LGPL)
has a cognitive bias (it aims at capturing the ontological categories
underlying natural language and human commonsense), and particularly
focuses on social structures (organizations and so on).
<li>
<a href="http://www.opencyc.org/">OpenCyc</a>
(Apache License Version 2) derives from the Cyc project.
This is one of the oldest serious computer-processable ontologies.
Note that the <i>data</i> provided by OpenCyc is FLOSS, and
some programs that manipulate it are also FLOSS
(those tend to be Apache-licensed), but other Cyc-related
programs are <i>not</i> FLOSS.
</ul>
<!-- End of ontologies -->

<!-- Add more things here -->

</ul>

<!--
Here are some other tools, which either I can't tell or aren't FLOSS>

Can't tell if license for Vampire , Waldmeister. SETHEO.
E-SETHEO is not available foe download.

NRL Protocol Analyzer - can't find a site for it at all,
nor info on if it even CAN be gotten, so I have no idea what its license is.
* http://citeseer.ist.psu.edu/71085.html
  Catherine Meadows",
    title = "The {NRL} Protocol Analyzer: An Overview",
    journal = "Journal of Logic Programming",
    volume = "26",
    number = "2",
    pages = "113-131",
    year = "1996",
    url = "citeseer.ist.psu.edu/article/meadows96nrl.html"
* http://tor.eff.org/cvs/tor/doc/design-paper/tor-design.html
  Tor: The Second-Generation Onion Router
  "Tor, a circuit-based low-latency anonymous communication service. This second-generation Onion Routing system addresses limitations in the original design by adding perfect forward secrecy, congestion control, directory servers, integrity checking, configurable exit policies, and a practical design for location-hidden services via rendezvous points. Tor works on the real-world Internet, requires no special privileges or kernel modifications, requires little synchronization or coordination between nodes, and provides a reasonable tradeoff between anonymity, usability, and efficiency." NRL Protocol Analyzer used to analyze.
* IPSEC.  FreeS/WAN docs noted this:
  http://www.freeswan.org/freeswan_trees/freeswan-1.99/doc/HowTo.html


Not all such tools are FLOSS.
The Spark Ada analysis tools (SPADE),
LDRA Testbed,
Polyspace, MALPAS,....

SRI's PVS originally said "non-commercial only", else pay, so
it was obviously not FLOSS.  That changed since, so noted as such below.

http://michaelbeeson.com/research/otter-lambda/index.php
Otter-Lambda - combines Otter with lambda calculus. No download, probably
not FLOSS.  Interesting comparison page, though:
 http://michaelbeeson.com/research/otter-lambda/index.php?include=comparison

zChaff - http://www.ee.princeton.edu/~chaff
 Noncommercial use only.
SATO - http://www.cs.uiowa.edu/~hzhang/sato.html
 Noncommercial use only.
http://sat.inesc.pt/~jpms/grasp
 CANNOT FIND

HOL has a harness for calling "SAT" programs:
http://www.cl.cam.ac.uk/~mjcg/HolSatLib/
"The HOL library HolSatLib provides a very simple harness for invoking SAT solvers on HOL terms. Currently the following solvers are supported...
These solvers all require input in the standard DIMACS format for conjunctive normal form (CNF). It should be straightforward to add other DIMACS compatible SAT solvers."
DIMACS defined in:
ftp://dimacs.rutgers.edu/pub/challenge/satisfiability/doc/satformat.dvi

Concurrency Factory
http://www.cs.sunysb.edu/~concurr/
Could not find download.

Software Cost Reduction (SCR): tabular state transition-like format.
 http://chacs.nrl.navy.mil/5540/personnel/heitmeyer/
 http://www.softwaretechnews.com/stn3-4/scr.html
 http://www.ics.uci.edu/~alspaugh/software/SCR.html
Could not find download.

Tatami
 http://www.cs.ucsd.edu/groups/tatami/
 " supports distributed cooperative design, specification and validation of (software and/or hardware) systems, especially distributed concurrent systems. The Tatami system integrates formal with informal methods, has an online tutorial capability, runs over the web, and is intended to be useful to ordinary software engine"  
Could not find download.

SMV
http://www.cs.cmu.edu/~modelcheck/smv.html
But license says:
http://www.cs.cmu.edu/~modelcheck/license.htm
Won't transfer outside.  Which is weird, because you can download it.

VIS
http://embedded.eecs.berkeley.edu/Respep/Research/vis/getting_VIS_1.2.html
Must send request via email to get; also, looks dead.
(It runs on Linux 1.3.99 and Digital Ultrix!  How would I find either?)
It appears it was replaced by Mocha.

CADP
http://www.inrialpes.fr/vasy/cadp/registration/
"The CADP software is distributed free of charge to universities and public research centers. Industrialists can obtain an evaluation license for non-commercial use during a limited period of time"

Mocha
http://embedded.eecs.berkeley.edu/research/mocha/download/
http://www.ucop.edu/ott/permissn.html
Non-commercial use only.

Trull (MIT license)
http://sourceforge.net/projects/trull/
Trull is the successor of Triveni.
(http://www.cs.luc.edu/projects/triveni/).
"Trull (Triveni UltraLite) is a JavaBeans-compliant component framework for the modular development of event-driven concurrent systems from building blocks, including basic abstract behaviors and combinators that operate on abstract behaviors"
It's FLOSS, but I don't think it fits the formal methods (etc.) topic.


VeriSoft
http://cm.bell-labs.com/who/god/verisoft/files.html
"This version of VeriSoft is for educational use and evaluation purposes"

Stanford Temporal Prover (STeP)
http://www-step.stanford.edu/
Non-transferable licsence.
Developed by REACT research group.

RAISE tools
http://www.iist.unu.edu/home/Unuiist/newrh/III/3/1/page.html
Hard to tell, but compiling requires Gentile (educational use only).

PV
(Spin-like tool)
http://www.cs.utah.edu/formal_verification/software/pv/install.html

Escher Tech
http://www.eschertech.com/products/
Perfect Developer

Berkmin
http://embedded.eecs.berkeley.edu/Alumni/kenmcmil/smv/dld2.html
Non-commercial use only.


Termination detectors seem another useful lower-level tool.
You can see competition results here:
http://www.lri.fr/~marche/termination-competition/2005/webform.cgi?command=trs
Competing programs:
AProVE 1.2pre 	CiME 2.02 	Matchbox 	TEPARLA 	TPA 	TTT

-->

<p>
<h3><a name="unmaintained">Formal methods: Unmaintained tools</a></h3>
<p>
These tools appear to be no longer maintained, but they may still be
useful as basis for new work:
<ul>
<li>
<a href="http://kind.ucd.ie/products/opensource/OBJ3/">OBJ3</a> (BSD-style).
The OBJ languages are broad spectrum algebraic programming
and specification languages, based on order sorted equational logic.
OBJ3 is a particular instance of this family.
Maude descends from OBJ3 (so see the entry for Maude).
<li>
<a href="http://sra.itc.it/tools/getfol/GETFOL.html">
GETFOL</a> (BSD-style license).
This is old and doesn't appear recently maintained, but it
has a long history of maintenance so it may be of interest.
</ul>

<p>
<h3><a name="proprietary">Formal methods: Proprietary tools</a></h3>
<p>
Not all formal methods tools are free-libre / open source software (FLOSS).
I thought I should briefly note a few, so that people can save time instead
of trying to track down their licenses.
<p>
"Proprietary" simply means that the program is not FLOSS, and thus users
do not have the right to use the software for any purpose,
view the source code, modify it, and redistribute it (modified or not).
Many tools were developed at public expense at universities, but are
nevertheless proprietary.
Even if the software's development was completely paid for using public funds,
do <i>not</i> assume that software will be released as FLOSS to the public.
In some cases there are "demo" binaries of the tool available to the public,
but the source code is
not distributed and/or there are significant limitations on the tool's use.
<p>
The following are proprietary tools: Barcelogic,
Boolector, HySAT, Spear, Yices, and Microsoft's Z3.
ESC/Java and Simplify are not FLOSS; they were once widely distributed,
but since they were never FLOSS, when their company's direction changed
they became abandoned, legally risky to use, and impractical to maintain.
Similarly, Z/Eves was once widely distributed, but it was never FLOSS, so
when its distributor stopped distributing it, users had no legal recourse.
<p>
<a href="http://mathsat4.disi.unitn.it">MathSAT</a> is <i>not</i> FLOSS,
even though it has the text of the GPL and LGPL licenses in its release.
Alberto Griggio explained the real situation to me in 2008:
"It is linked with the GNU multiprecision library GMP,
which is covered by the LGPL. So, the sourcecode doesn't
have to be available, and in fact at the moment it
is not, sorry. The tarball includes a copy of the GPL as well as of
the LGPL, because, as far as we know, you must ship a copy of the LGPL
if you link against an LGPL'd library, and the LGPL itself requires
that you also ship a copy of the GPL. Sorry if this was unclear,
<p>
<a href="http://www.e-pig.org/epigram1.html">Epigram</a>
has no clear license statement.
I've sent an email requesting that this be clarified.
<p>
<a href="http://www.cprover.org/cbmc/">CBMC</a> is a model-checker
with an <i>almost</i> free license, so it's probably not FLOSS.
It allows change, but requires notification before installation;
that certainly fails Debian's requirements (e.g., it fails the
"Desert Island" test).
Once again, it's frustrating that people want to create new licenses,
please STOP!
<p>
Bogor is non-commercial-use only, and thus not FLOSS.
<p>
<a href="http://www.lsv.ens-cachan.fr/~fl/cmcweb.html">HCMC</a> is
a model-checking tool, but I haven't found a clear license statment
about it.


<!-- I believe other programs should move here, but it's a pain to
     check.  Suggestions welcome.

<a href="http://www.cs.nyu.edu/acsys/cvc3/">CVC3</a> (nonstandard license; BSD-new plus highly controversial clauses)

Barcelogic: http://www.lsi.upc.edu/~oliveras/bclt-main.html
  No source available
HySAT: No source available
Spear: no source code available, so not OSS
Yices is not OSS.
Z3 is non-OSS; Microsoft.
-->

<p>
<h3><a name="formalmethodscommentary">Formal methods: Commentary</a></h3>
<p>
My point is simply that there are many FLOSS tools in this space, and
I think I have proven that convincingly.
This is certainly not a complete list; see the resources listed above,
the
<a href="http://anna.fi.muni.cz/yahoda/">Yahoda verification tools database</a>,
<a href="http://ar.colognet.org/tools.php">
CoLogNET&#8217;s list of automated reasoning tools </a>,
the
<a href="http://www.cs.miami.edu/~tptp/CASC/J2/SystemDescriptions.html">
CASC entrants</a>,
<a href="http://www.cs.indiana.edu/formal-methods-education/Tools/">
Formal Methods Education (FMEd) Resources tools page</a>, the
<a href="http://vl.fmnet.info/">Formal methods virtual library</a>,
<a href="http://www.rbjones.com/rbjpub/cs/ai003.htm">
Automation of reasoning net links</a>, and
<a href="http://www.niii.ru.nl/~freek/digimath/index.html">
Freek Wiedijk&#8217;s overview</a>
for even more programs.
There are many non-FLOSS formal methods tools, just as
there are non-FLOSS SCM and testing tools, but that does not
invalidate my point.

<p>
Many of these FLOSS tools are considered very strong, innovative,
and/or have been used for serious applications.
Spin and the Boyer-Moore Theorem Prover (the basis of ACL2) each
received the ACM&#8217;s prestigious
<a href="http://awards.acm.org/software_system/">Software System Award</a>,
which recognizes a
&#8220;software system that has had a lasting influence&#8221;.
Both have been used for extremely important applications,
from checking space probe algorithms through microprocessor design.
PVS is also widely considered to be one of the better tools of its kind.
Otter has been used to find new proofs previously unknown to mathematics.
HOL 4 and Isabelle are widely used among these kinds of tools.
MiniSat, Paradox, and Vallst have won awards in recent competitions
against other similar tools.
Alloy is a new tool, but I think it&#8217;s pretty innovative.

<p>
One type of tool I&#8217;m not including are
probabilistic / statistical / Monte Carlo model checkers, such as
<a href="http://www.cs.bham.ac.uk/~dxp/prism/">PRISM</a> (GPL license).
They appear to be valuable for medium assurance, but
I am skeptical that they are appropriate for high assurance.
One tool in this space I should note is
<a href="http://www.cs.sunysb.edu/~gmc/download.htm">GMC</a>
(GPL license, likely), which is a highly experimental
Monte Carlo-based software model checker for the gcc compiler suite.
<a href="http://www.cs.sunysb.edu/~grosu/softmc05.pdf">
Open-Source Model Checking</a> explains more about GMC.
GMC is very experimental, and does not appear suitable for
development use at this time; I note it here
because it embodies some very interesting ideas, for those who
are interested in the up-and-coming research.
Monte carlo model checkers have great promise for medium assurance,
but because they only cover statistical likelihoods
(not <i>all</i> possible situations), I would be nervous about using
any of them for high assurance.

<p>
Sadly, many tools have completely disappeared from the world because
they were not released under FLOSS licenses.
What&#8217;s particularly galling is that
many governments pay for academics to develop code,
yet fail to require releases of that code (that they paid for!)
under FLOSS licenses.
In my mind, this is shameful; if my taxes paid for the tool,
then I should have the right to see, use, and improve it,
unless there is <i>strong</i> and specific evidence
that an alternative license would be better for that particular circumstance.
FLOSS licenses allow others to study, use, improve, and release
those original or improved versions, and are thus a much better
vehicle for making continued research possible.
Instead, often a tool is released (if it&#8217;s released at all)
only as a proprietary binary file
(which is unmodifiable and will eventually be unrunnable) or possibly
as restricted source code;
in either case its license often has lots of anti-FLOSS restrictions
(such as you can&#8217;t modify it or you can&#8217;t use it for
a commercial purposes).
The tool often cannot be used for commercial purposes except through
special licensing deals;
that might sound fine, but in practice this generally squelches
research through trial commercial use, and it also
prevents distributors (such as Linux distributors)
from making the tool widely available.
Then either (1) the academic loses interest, or
(2) a proprietary company builds on the tool and tries to build a business
based on it.
If the original academic loses interest, then no one else (even
other academics!) has the rights to build on that code.
The second option (proprietary commercialization)
<i>sounds</i> good, but remember,
we've had decades of almost uniform failure in trying to sell
formal methods tools
(Most <i>restaurant</i> starts fail, never mind niches like this
where there are few users and the tools take time to learn.)
In the end, the project usually fails.
In either case, the software ends up being unavailable to all --
even if government funds were used to develop it.
This is incredibly wasteful, and in my opinion this is one of the
primary reasons people don&#8217;t use formal methods as often: most of the
research work is locked up in software with proprietary licenses
that is eventually thrown away.
A company can do what it likes with its own money, of course,
but if it will not sell it as a proprietary product, I think they should
at least release it as FLOSS so others can build on it and improve
the field.
Governments have no such excuse; it is their citizens&#8217; money
they are squandering.
I believe that, with a little creativity, governments could ensure
that such projects can continue and grow.

<p>
Obviously, it is possible to have proprietary tools with long support,
but investigate any such vendor <i>very</i> carefully.
If you use such tools, depending on proprietary versions can be a big risk;
if the company goes out of business (which is historically very likely),
your largest cost suddenly depends on an unsupportable tool.
In large markets,
<a href="open-standards-open-source.html">open standards</a> help --
by having multiple competing implementations which you can switch between,
competition lowers costs and incentivizes improvement.
There are a few standards in this area (e.g., those for Z and VDM).
But depending on competition often fails
when there is a thin market, as in the case of formal methods tools.
You might consider requiring the vendor to escrow their code as FLOSS if they
decide to stop selling or supporting their product, so at least
you <i>can</i> have a support option if the vendor leaves the business
(as most do).
This isn&#8217;t an empty concern;
here are some examples where a proprietary tool license has caused problems
for its users:
<ul>
<li><a href="http://www.ora.on.ca/z-eves">Z/Eves and Eves</a> were once
distributed by ORA Canada with a non-FLOSS license.
They distributed the software no charge for non-commercial use
until June 3, 2005, but then decided to cease distribution.
As of May 2006, I know of no legal way to acquire
these tools (which were used in many places).
Even if they eventually become available elsewhere, it still illustrates
the problem.
<li>
The commercial product
&#8220;VDM through Pictures&#8221; (VtP) by IDE was once prominent, but appears
to have disappeared.
VtP was once prominently displayed in comparisons like
<a href="http://www.dacs.dtic.mil/techs/2fmethods/tools.shtml">
this one</a>.
But <a href="http://www.aonix.com/about.html">Aonix</a> was formed by merging
IDE and another company in 1996, and in May 2006 I cannot find the
product at all.
(If anyone can help me find this -- perhaps its named changed! --
I would really appreciate it.)
<li>
<a href="http://secure.ucd.ie/products/opensource/ESCJava2/license.html">
KindSoftware had this problem with ESC/Java2</a> -- Compaq/HP did not
make a FLOSS release of ESC/Java, and now has decided to abandon the work.
Suddenly, a tool that many people depended on is on
doubtful (and constraining) legal ground.
At first some researchers tried to make improvements anyway...
but as a result, they don&#8217;t have clear legal rights
to release their own work!
KindSoftware is now going back and redeveloping from scratch something
like the original ESC/Java software, so that
improvements can be legally released using a standard OSS license
(they currently plan to release ESC/Java3 as part of Mobius).
They&#8217;ve learned their lesson, and plan to release the results under the
GPL (which is a mainstream FLOSS license, and thus
<i>does</i> permit improvements by all).
</ul>

<p>
In contrast, some of the tools that <i>have</i> been released as FLOSS
have resulted in incredible benefits to the world, and lower
risks for their users.
ACL2, Isabelle, HOL4, and splint (for example) come from <i>very</i>
long lines of research, and their continued use today demonstrates that
releasing software developed during academic research under FLOSS
licenses can have tremendous, long-lasting benefits.
(The computer algebra system
<a href="http://maxima.sourceforge.net/">Maxima</a> has demonstrated
the same thing; it&#8217;s been around since the late 1960s and is still
actively maintained.)
The
<a href="http://nusmv.irst.itc.it/">NuSMV</a> project specifically
re-implemented the SMV tool, so they could get the benefits of being
a FLOSS project (permitting extensions like
<a href="http://www.lsv.ens-cachan.fr/Publis/PAPERS/PDF/MS-qest2004.pdf">
TSMV</a>, an extension of NuSMV to deal with timed versions of circuits).
Any company doing research would be wise to consider releasing its code
as FLOSS -- if it&#8217;s research, they can often receive far more than they
release.
I think it would be much wiser to <i>require</i> that
government-funded software development in academia
be released under a FLOSS license under usual circumstances.
That way, anyone can start with what was developed through
government funds and build on it, instead of starting over.

<p>
Indeed, in some ways, FLOSS is an ideal way to commercialize
formal methods tools.
Formal methods tools require people to learn and apply new skills,
so for bigger projects you generally need someone to help you 
understand how to apply the tool.
Thus, the FLOSS commercial model of &#8220;give away the code and
sell support services&#8221; is especially easy to apply in this area.
And if the commercial company flops, the work is still available
for future research or for combining with other components.

<p>
I do not think that within a few years suddenly everyone will be
using formal methods, for a variety of reasons.
But I do think that over the next many years we will see a very
gradual increase in use of these tools in very critical areas.

<p>
Of course, one challenge is that assurance tools are often not
assured themselves.
Assurance tools could even be maliciously undermined;
see the discussion under compilers for more about &#8220;trusting trust&#8221;
types of attacks.
Here are few items related to assuring the assurance tools:
<ul>
<li>In the proof area, one approach that really helps
is to separate <i>creating</i> proofs
from <i>checking</i> proofs -- it&#8217;s often incredibly hard for computers
to find a proof, even with lots of human direction, because doing so
in reasonable time requires lots of clever heuristics.
But checking a proof afterward is fairly easy, so it can be done by
a relatively tiny program designed for just that purpose, which
can then be examined carefully.
Two systems written using ACL2 are worth noting;
Ivy (noted above) is a proof-checker of Otter-like theorems (those created
by Otter or MACE);
<a href=" http://www.cs.utexas.edu/users/moore/acl2/workshop-2002/contrib/caldwell-cowles/revised.pdf">
Representing Nuprl Proof Objects in ACL2: towards a proof checker for Nuprl</a>
talks about writing a proof-checker for Nuprl
(an LCF-style prover, a family that also includes HOL 4 and Isabelle).
One of many papers about this approach is
<a href="http://www.cs.princeton.edu/~appel/papers/flit.pdf">
A Trustworthy Proof Checker</a>.
<li>This idea of using proof checkers is particularly exploited in
<a href="http://www.cs.princeton.edu/sip/projects/pcc/">
proof-carrying code</a>, where
code producers include with the code a formal safety proof (that
they must create);
the code consumer uses a simple proof validator to check, before executing
the code, that the proof is valid.
<li>Another approach (which could be combined with the first one)
is followed by HOL, Isabelle, and other descendents of the
LCF theorem prover.
LCF-style provers separate the problem of creating proofs into two parts:
a lower-level engine which implements the logical rules,
and higher-level part that decides which rule to use.
For the latter, they
generally use a general-purpose programming language
(such as ML) where the theorem proving &#8220;tactics&#8221; are written.
The theorems themselves, however, can only be modified by
the lower-level engine using inference rules known to be valid.
That way, the higher-level driver choose poor strategies and
fail to prove something that is true, but it won&#8217;t
accidentally implement an invalid rule.
<li>ACL2 is written mostly in itself, which is essentially a subset
of LISP.  It does prove some properties of itself.
<li>There have been some proofs of provers.
<a href="ftp://ftp.cs.utexas.edu/pub/moore/acl2/v2-9/acl2-sources/books/workshops/1999/ivy/ivy-v2/README">Ivy is a proof checker written in ACL2</a>;
ACL2 was then used to prove it.
</ul>

<p>
Tools can&#8217;t do everything for you; humans have to help create proofs,
and they often have to try many different paths to find the proof.
<a href="http://www.cl.cam.ac.uk/Research/HVG/haiku.html">
This haiku by Larry Paulson</a> expresses some of
the challenges of that process:
<blockquote><i>
    Hah!  A proof of False.<br>
    Your axioms are bogus.<br>
    Go back to square one.<br>
</i></blockquote>
<p>
(Yes, many tools are designed to counter bogus axioms, but
the basic point is still true.
Namely, anyone trying to prove properties of <i>real</i> systems often
struggles and has to restart several times,
going through many different approaches, before succeeding.)


<p>
<h2><a name="analysisimp">Analysis implementation tools</a></h2>
<p>
A different issue is how to run the various formal methods tools
listed above.
A few of them are implemented in widely-available
languages also used for other purposes (like C, C++, or Java).
Obviously, they&#8217;ll need an operating system,
and usually need other common tools like text editors.
(Warning: Most analysis tools run on Linux/Unix and are either
<i>not</i> available for Windows, or only work on Windows with an
emulation tool like <a href="http://www.cygwin.com/">Cygwin</a> --
making the tool slower.)
But there are already well-known FLOSS implementations
for these, so we don&#8217;t need to discuss them in more detail.
<p>
However, specification and proof systems are often built
&#8220;on top&#8221; of other (less common) programming languages.
These other languages are often specialized themselves, and
in some cases using the specification or proof tools also involves
interacting directly with the underlying implementation tools as well
(e.g., to control/&#8220;drive&#8221; the proof system).
<p>
Programming languages which are functional programming languages,
or have a functional programming subset, are very common for these purposes.
A functional programming language is simply a language where assignment is not
(normally) used, and thus there are no &#8220;side-effects&#8221; -- instead,
functions accept values and return values (like a spreadsheet does).
There are many arguments for the advantages of such systems, but
one reason is simply that such systems make it possible to use
much more of the arsenal of mathematics.
Functional languages usually have built-in support
for lists and other constructs useful for the purpose.
<!-- Note: His name really is "J", it's not the abbreviation "J." -->
<a href="http://vstte.inf.ethz.ch/pdfs/vstte-moore-position.pdf">
J Strother Moore&#8217;s position is that all highly-assured software should
be written in a functional programming language</a>,
because it is <i>much</i> easier to prove properties about programs
written in them.
(Most widely-used languages are &#8220;imperative&#8221; languages, including
C, C++, Ada, Java, C#, Perl, Python, PHP, and Ruby.
Techniques for proving programs in imperative languages are known;
<a href="http://en.wikipedia.org/wiki/Hoare_logic">C. A. R. Hoare&#8217;s 1969
paper on Floyd-Hoare logic</a> did so, as did
<a href="http://en.wikipedia.org/wiki/Weakest_precondition">
Edsger Dijkstra&#8217;s weakest precondition work</a> that was part of his
1975 work on predicate transformers.
Moore argues that their complexities are not
worth it, and that using a functional approach makes proofs much easier.)
<p>
In some cases it&#8217;s hard to figure out where to place some language.
In particular I&#8217;ve placed Maude here, but Maude could arguably
be considered an analysis tool (and be placed above).
In any case, there are many useful FLOSS implementations of these tools too:
<ul>
<li>Lisp.
Lisp is one of the oldest programming languages in the world and
the Lisp family is still widely used for these kinds of applications.
Today there are three major variants in wide use: Common Lisp,
Scheme, and Emacs Lisp (the latter is built into the Emacs text editor).
There are also a host of other variants.
There are many FLOSS implementations of the Lisp family.
Emacs lisp is implemented, unsurprisingly, by Emacs (GPL).
ACL2, noted above, builds on Common Lisp.
<p>
A good FLOSS Common Lisp implementation is
<a href="http://www.gnu.org/software/gcl/">GNU Common LISP (GCL)</a>
(LGPL license); this implementation isn&#8217;t quite ANSI compliant,
but since it compiles to efficient machine code it&#8217;s often used
for proof-checking work (because this task is very compute-intensive).
Another high-performance Common Lisp implementation is
<a href="http://www.cons.org/cmucl">CMUCL</a> (public domain).
<a href="http://clisp.cons.org">GNU CLISP</a> (GPL license)
<i>also</i> implements Common Lisp; it has a
bytecode implementation which makes it a little slower, but it's
very capable.
<p>
There are many FLOSS implementations of the Scheme variant of Lisp as well.
These include
<a href="http://www.gnu.org/software/guile/">GNU guile</a> (LGPL;
small and quick to start up,
used in many GNU programs as an extension language),
<a href="http://www-sop.inria.fr/mimosa/fp/Bigloo/">Bigloo</a>
(compiler and tools GNU GPL, library is GNU LGPL;
this has a higher-performance compiler and other components
focused on bigger development efforts, including a compile-time
type checking system),
<a href="http://www.cs.umb.edu/~wrc/scheme/umb-scheme-3.2.tar.gz">
umb-scheme</a> (public domain),
<!-- http://www.cs.umb.edu/~bill/bio.html
"I have implemented a public domain version of the
Scheme programming language. It's called UMB Scheme and
you can get it [by ftp]
at http://www.cs.umb.edu/~wrc/scheme/umb-scheme-3.2.tar.gz."
-->
and
<a href="http://www.shiro.dreamhost.com/scheme/gauche/">Gauche
(command-line "gosh")</a>
(<a href="http://sourceforge.net/projects/gauche">BSD license</a>).
<a href="http://practical-scheme.net/index.html">Practical Scheme</a>
is a useful place to start looking for libraries.
<a href="http://www.ccs.neu.edu/home/dorai/t-y-scheme/t-y-scheme.html">
Teach yourself Scheme in Fixnum days</a>
is a reasonable intro to Scheme.
<p>
I should note that although Scheme and Common Lisp have a lot of
shared history, as languages
<a href="http://en.wikipedia.org/wiki/Common_Lisp#Comparison_with_other_Lisps">
Scheme and Common Lisp
are basically incompatible with each other</a>.
Common Lisp has multiple namespaces (versus Scheme's one),
Scheme uses the special values #t and #f for true and false, with distinct
values for #f, NIL, and the empty list'();
Common Lisp uses the older Lisp convention of using the symbols T and NIL,
with NIL also representing the empty list, and non-nil being considered true.
In practice, Scheme programs use recursion where a Common Lisp program
would not (because Scheme guarantees tail-call optimization while
Common Lisp does not).
Most of the built-in functions have different names (because of different
naming conventions), and many have subtly different semantics (because of their
fundamentally different notions about lists and boolean values).
The program
<a href="http://www.ccs.neu.edu/home/dorai/scmxlate/scmxlate.html">
scmxlate</a> by Dorai Sitaram translates Scheme
into Common Lisp; the license in the package itself
isn't FLOSS, but Sitaram has separately
<a href="http://www.ccs.neu.edu/home/dorai/">released scmxlate and other
packages under the LGPL</a>, so it <i>is</i> FLOSS even if at first
it does not appear to be.
<p>
Both Common Lisp and Scheme have formal standards, so as long as you
stay with the standards, you can generally port from one to another.
Although Lisp's usual programming notation is different most other languages
(strictly prefix s-expression), and it has a history of being an "AI"
language, studies have found that both development time and performance
can be quite good.
<a href="http://www.flownet.com/gat/papers/lisp-java.pdf">
Ron Garret (nee Erann Gat) did a study in writing a test program in
Lisp</a>; the resulting Lisp programs ran faster on average than C, C++ or
Java programs (although the fastest Lisp program was not
as fast as the fastest C program), and the Lisp programs took
less development time than the other languages. 
<a href="http://www.norvig.com/java-lisp.html">
Norvig's "Lisp as an Alternative to Java"</a> adds some commentary about this.
<p>
A new and interesting variant of Lisp is implemented in
<a href="http://www.lambdassociates.org/aboutqi.htm">Qi</a>
(GPL license).
This is another Lisp variant implemented on top of Common Lisp; it supports
pattern-matching, optional static type checking, and is
&#8220;lambda-calculus consistent&#8221; (supporting partial things, like
partial applications).
The authors claim that the type system of Qi is more powerful and flexible than
the very powerful capabilities of ML or Haskell, because it is based on
<a href="http://www.lambdassociates.org/advtypes.htm">sequent notation</a>.
Their manual is interesting in its own right, they even discuss
implementing simple automated reasoning systems.
One problem with Lisps is that their usual programming notation
(s-expressions) is painful for most people to read; see my
<a href="https://dwheeler.com/readable">Readable s-expression work
(including sweet-expressions)</a> for techniques to overcome that.
<li>Prolog.
Prolog is a logic programming language.
FLOSS implementations include
<a href="http://gnu-prolog.inria.fr/">
GNU Prolog</a> (GPL license),
<a href="http://www.swi-prolog.org/">
SWI-Prolog</a> (LGPL license),
<a href="http://www.clip.dia.fi.upm.es/Software/Ciao/">
Ciao Prolog</a> (GPL license),
<a href="http://yap.sourceforge.net/">YAP (Yet Another Prolog)</a>
(Artistic license).
<!-- Source: http://en.wikipedia.org/wiki/Prolog -->
Warning - there&#8217;s an ISO standard for Prolog, but the
<a href="http://www.cs.unipr.it/~bagnara/Papers/Abstracts/ALPN99a">
ISO standard for Prolog is ignored or not fully followed by many</a> -- so
there can sometimes be problems porting Prolog code
(<a href="http://www.inf.bme.hu/~pts/stdprolog/stdprolog_paper.pdf">
there have been some efforts to measure and improve conformance</a>).
<li>Maude.
Maude is a reflective language and system supporting
both equational and rewriting logic specifications; it's descended from OBJ3.
<a href="http://maude.cs.uiuc.edu/">Maude 2</a> (GPL license) is available.
Maude requires
<a href="http://buddy.wiki.sourceforge.net/">BuDDy</a>
(Public Domain), an implementation of BDDs.
<li>ML.
ML is a mostly-functional language featuring static typing
(types are usually inferred so they do not need to be declared)
and eager evaluation (i.e., it&#8217;s strict;
all parameters are evaluated before a function is called).
Major variants of ML include Standard ML and Objective Caml
(<a href="http://www.ps.uni-sb.de/~rossberg/SMLvsOcaml.html">
this page compares ML and Objective Caml</a>).
There are a
<a href="http://en.wikipedia.org/wiki/Standard_ML">
large number of FLOSS implementations of Standard ML</a>, including
<a href="http://smlnj.sourceforge.net/">Standard ML of New Jersey (SML/NJ)</a>
(MIT-style license).
There is also the Caml variant of ML; the most popular variant of Caml is
<a href="http://en.wikipedia.org/wiki/Ocaml">
Objective Caml</a>, which integrates object-oriented approaches
with the functional approach of ML.
<a href="http://caml.inria.fr/ocaml/index.en.html">Inria&#8217;s
OCaml system</a>
(<a href="http://caml.inria.fr/ocaml/license.en.html">compiler Q
Public License, library LGPL</a>) is a very high-performance implementation
of OCaml;
<a href="http://shootout.alioth.debian.org/">
The Computer Language Shootout Benchmarks</a>
reports that this implementation
often beats the performance of other high-level languages;
it even sometimes beats gcc C at maximum optimization
while still having very short, clear programs.
<li>Haskell.
Haskell is a fully functional programming language.
Unlike most other functional programming languages, Haskell is
lazy (non-strict): it doesn&#8217;t compute something unless it needs to,
and typical Haskell programs define infinitely-long constructs
(from which it&#8217;ll only compute the parts it needs).
Haskell also is <i>completely</i> functional;
most functional programming languages make
exceptions for I/O and other parts, but Haskell supports special constructs
(particularly monads) so that even I/O is handled completely functionally.
There is a public specification for Haskell.
Many consider the canonical implementations of Haskell to be
<a href="http://www.haskell.org/ghc/">GHC</a> (for speed)
and
<a href="http://www.haskell.org/hugs/">Hugs</a>
(for nice interactivity and smaller size).
Both of them are FLOSS, and GHC works hard to be very efficient.
There&#8217;s at least one kernel implemented in Haskell.
I&#8217;m not sure if
Haskell will be used to <i>implement</i> many high-assurance programs,
because it&#8217;s hard to reason about the execution time
of a Haskell program.
Others will disagree with me on this point!
But there&#8217;s no doubt that Haskell is used in many places
for reasoning <i>about</i> programs.
</ul>



<h2><a name="codegen">Code generation</a></h2>
<p>
Clearly, you need to have a way to execute the highly-assured
source code... which requires either a compiler or interpreter.
<p>
If you write code in C or C++, it quite common to use the
gcc compiler suite (GPL license), which is FLOSS.
Developers of high-assurance software who choose to use
C or C++ often use gcc as well, so clearly there are FLOSS
tools covering these languages.
<p>
However, programming languages often used for
other assurance levels don&#8217;t work as well for high assurance.
Here are comments on some programming languages if you are
interested in high assurance:
<ul>
<li>
C and C++ are widely used for a variety of good reasons, and in
many applications their weaknesses are not a big problem or are
easily surmountable.
But C and C++ are not well-suited for high assurance, because
for high assurance their weaknesses are a serious problem.
One problem is that it&#8217;s notoriously easy to make a mistake
in C and C++ that will be missed both by tools and by other reviewers.
Studies have shown that, on average, C/C++ programs tend to
have more mistakes
than those in many other languages; if <i>any</i> mistake
is a disaster, that&#8217;s a bad place to start.
Also, C/C++&#8217;s designs make it incredibly hard to
prove anything about code written in them.
In particular, most formal method proof systems cannot handle pointers well,
yet both languages are fundamentally designed around pointers.
It is <i>possible</i> to use C/C++ for high assurance programs;
some organizations go ahead and do it, and compensate using
draconian style guides, massive extra reviews, and so on.
But the &#8220;savings&#8221; from using common C or C++ tools
is often completely overwhelmed by the vast amount of extra
time and money to compensate -- and
that&#8217;s if it&#8217;s possible at all.
When creating high assurance components
it&#8217;s hard to justify using a language so poorly designed for the task.
If you do choose to use C, I would suggest looking at
<a href="http://leshatton.org/ISOC_subset1103.html">
Les Hatton's EC</a> set - it's a set of rules that slightly subset C,
based on measurements of failures (from Safer C, T, and many other
studies) - that way, you're less
likely to make the same mistakes as your predecessors.
<li>
Java and C# are much easier to verify than C or C++, and
are specifically designed so that the common mistakes people make
in C and C++ are automatically detected by the compiler.
One challenge in using these languages are relevant implementations.
The most common Java implementation (Sun&#8217;s proprietary implementation)
expressly forbids the use of Java in safety-critical applications.
The only C# implementations I&#8217;m aware of, both FLOSS and proprietary,
depend on lots of other lower-assurance components (medium assurance
operating systems and so on).
There are FLOSS implementations
(gcj implements Java and Mono implements C#),
but as of May 2006 these are less mature FLOSS programs and
it&#8217;s not clear anyone should use them
for high assurance programs (I am sure they will mature over time,
just as gcc&#8217;s support for C, Ada, and C++ has matured).
<p>
One additional problem, too:
many high assurance programs must also be hard real-time systems
(with <i>guaranteed</i> execution times), and/or must prevent
&#8220;covert channels&#8221; of communication.
In practice, this often means that
runtime memory heap allocation can&#8217;t be used, including
automated garbage collection.
Without automated garbage collection, many languages -- including
Java and C# -- are impractical; their designs fundamentally require it.
<li>
Python and Smalltalk are wonderfully malleable,
making it easy to create complex things.
But the malleability becomes a problem when you&#8217;re trying to prove
what a program does.
In these languages it&#8217;s easy to redefine fundamental constructs
(say, a library function) in the language, while the program is running,
far away from their use or definition
(aka &#8220;action at a distance&#8221;).
The result is that it&#8217;s incredibly hard to be <i>certain</i> what
some code in these languages does -- again,
making them poorly suited for high assurance.
They lack strong static typing, so many would be
nervous about using them to implement high assurance -- static typing
can detect many problems during compilation.
(There is an extension for Python that supports static typing, though
it is not often used.)
These languages also require automatic garbage collection; as noted in the
text about Java and C#, sometimes that is not allowed.
<li>
Lisp is a useful language, and is comparatively easy to analyze.
However, it also lacks strong support static typing
(the basics of static type declaration exist but are optional in Common Lisp,
and type checking tends to be run-time not compile time).
It requires automatic garbage collection, again, a problem for some
applications.
<a href="https://dwheeler.com/readable">
Many people find Lisp programming syntax hard to read</a>, which is
a severe disadvantage if you want people to review your code!
<li>
<a href="http://www.coyotos.org/">BitC</a> is a <i>very</i> promising
new language that is being developed.
It is currently in the research stage (as of May 2006),
so I would not recommend it for general use at the moment,
but it is worth watching.
The <a href="http://www.coyotos.org/docs/bitc/spec.html">BitC
specification</a> says this:
&#8220;BitC is a systems programming language that combines the
&#8216;low level&#8217; nature of C with the semantic rigor of Scheme or ML.
BitC was designed by careful selection and exclusion of language features
in order to support proving properties (up to and including
total correctness) of critical systems programs...
<p>
BitC is conceptually derived in various measure from Standard ML, Scheme,
and C. Like Standard ML, BitC has a formal semantics,
static typing, a type inference mechanism, and type variables.
Like Scheme, BitC uses a surface syntax that is readily
represented as BitC data. Like C, BitC provides full control over
data structure representation, which is necessary for high-performance
systems programming. The BitC language is a direct expression of
the typed lambda calculus with side effects,
extended to be able to reflect the semantics of explicit representation.&#8221;
<p>
&#8220;In contrast to ML, BitC syntax is designed to discourage currying
[because it] requires dynamic storage allocation... Since there
are applications of BitC in which dynamic allocation is prohibited,
currying is an inappropriate idiom for this language.
In contrast to both Scheme and ML, BitC does not provide or require
full tail recursion [but restricts it in a way that] preserves all of
the useful cases of tail recursion that we know about,
while still permitting a high-performance translation of BitC code to C code.
Building on the features of ACL2, BitC incorporates explicit support
for stating theorems and invariants about the program as part of
the program&#8217;s text.
As a consequence of these modifications, BitC is suitable for the
expression of verifiable, low-level &#8216;systems&#8217; programs.
There exists a well-defined, statically enforceable
subset language that is directly translatable to a low-level language
such as C.&#8221;
<p>
An implementation is being developed (BitCC) which generates C code
(which is then compiled), as part of the Coyotos project.
Currently BitC uses a Scheme (LISP)-like syntax, though a C-like syntax
may be built eventually.
BitC development work is being done as part of the Coyotos project.

<li>
Ada is widely used in the high assurance world, even though it&#8217;s
uncommon at lower levels of assurance.
This shouldn&#8217;t be surprising; Ada was specially designed for
high-assurance applications.
Ada has all sorts of type-checking and other kinds of built-in static
checks to detect defects before you can finish compiling,
which is a good thing if a single software defect could kill people.
Various studies (such as
<a href="http://www.stsc.hill.af.mil/crosstalk/2003/11/0311German.html">German&#8217;s</a>) find that Ada programs tend to have fewer defects than C/C++.
Ada can be used quite easily without pointers or heap allocations
(though it has both if you need them).
Ada is usually compiled directly to machine code, yielding fast results and
predictable performance.
Ada has abilities such as built-in commands so you can compare the
source lines and generated object code (this is part of annex H,
&#8220;Safety and Security&#8221;).
You can easily add &#8220;restrict&#8221; statements to forbid certain
structures (e.g., if your chosen formal method system
can&#8217;t easily handle them, or if never using them aids optimization).
For example, the
<a href="http://www.sigada.org/ada_letters/jun2004/ravenscar_article.pdf">
Ravenscar profile of Ada</a> is a predefined set of restrictions
useful for many real-time programs.
<a href="http://std.dkuug.dk/JTC1/sc22/WG9/n359.pdf">
ISO/IEC has produced a guide for using Ada in high integrity systems.</a>
If you&#8217;re curious about Ada, feel free to visit my
<a href="https://dwheeler.com/lovelace">
Ada95 Lovelace tutorial</a>,
including my discussion about
<a href="https://dwheeler.com/lovelace/s17s4.htm">
safety and Ada</a>.
The paper
<a href="http://portal.acm.org/citation.cfm?id=376545">
Refinement of Z specifications using reusable software components in Ada</a>
by Hayward and Bale shows a simple formal Z specification and an
implementation in Ada, concluding that
&#8220;Ada has proved an ideal language to implement Z [formal] specifications&#8221;.
Not all high assurance programs are in Ada, but it&#8217;s
not an unusual choice.
<p>
But choosing Ada doesn&#8217;t prevent the use of FLOSS.
The FLOSS
<a href="http://www.gnat.com/home/">GNAT Ada compiler</a> (GPL license)
is also part of the gcc toolsuite that compiles Ada code,
and commercial support is available for GNAT from AdaCore.
GNAT is one of the very best Ada compilers around; it is <i>widely</i> used.
In any case,
you don&#8217;t have to use Ada for high assurance software development,
but if you choose it, you can use a very good FLOSS implementation.
Also, check out SPARK (discussed separately).

<!--
Older work on Ada and verification includes Anna:
 http://pavg.stanford.edu/previous_research/index.html#anna
 released under "Anna Public License" (GPLish, appears FLOSS).
But it appears abandoned, probably won't run on modern systems
and would probably require massive rewriting to run.

Also, Frege Program Prover at:
http://psc.informatik.uni-jena.de/Fpp/fpp-intr.htm
"The Frege Program Prover (FPP) is an experimental system which performs correctness proofs for simple annotated programs.
The programs can be formulated in a small subset of Ada (see syntax). The annotations are preconditions, postconditions, loop invariants, and termination functions for loops. The correctness proofs are based on the weakest precondition approach and other proof rules as described in the Literature. We also included something about the person Frege."
-->
</ul>

<p>
Sometimes the best approach is to create domain-specific
language, use that to define at least part of your system,
and then create a code generator for your language.
This makes it much easier to use languages you might not
be able to use otherwise.
This can be very effective, but you must still decide how to
implement the code generator (and the rest of the code, if applicable),
and you must somehow verify that the generated program does what you want.

<p>
There are a lot of guidance documents out there on how to use
various languages to implement high assurance software, or have
more general guidance on development.
<a href="http://www.aitcnet.org/isai/Resources.html">ISO/IEC JTC 1/SC 22
has a list of some related documents</a>.
If you're interested in how to develop secure software,
see <a href="https://dwheeler.com/secure-programs">my book
developing secure programs</a>.
Some relevant documents include:
<ul>
<li><a href="http://ftp2.ansi.org/download/free_download.asp?document=ISO%2FIEC+TR+15942%3A2000">ISO/IEC TR 15942:2000,
"Information technology - Programming languages - Guide for the use of the Ada programming language in high integrity systems"</a>
(part of <a href="http://webstore.ansi.org/ansidocstore/free_standards.asp">
ANSI's set of freely-available standards</a>).
<li>
"EC--, a measurement based safer subset of ISO C suitable for embedded system development", Les Hatton, 2005.
[<a href="http://www.aitcnet.org/isai/Resources/ISOC_subset.pdf">PDF</a>].
Much of
<a href="http://leshatton.org/index_SA.html">
Les Hatton's work on safe language subsets</a> (much of it emphasizing C)
is online.
<li>
<a href="http://www.aitcnet.org/isai/Resources/nist204.pdf">
Dolores R. Wallace, Laura M. Ippolito, D. Richard Kuhn, "High Integrity Software Standards and Guidelines," NIST SP 500-204, 1992</a>.
<li>
Motor Industry Software Reliablity Association (MISRA),
<a href="http://www.misra-c.com/buy.htm">
"MISRA-C:2004, Guidelines for the Use of the C Language in Critical Systems"</a>,
ISBN 0 9524156 2 3 (or 4 X for pdf).
(Not freely available.)
<li>
NUREG/CR-6463, Rev. 1,
<a href="http://www.aitcnet.org/isai/Resources/NUREG%20CR-6463%2C%20Rev.%201/index.html">
Review Guidelines for Software Languages for Use in Nuclear Power Plant Safety Systems: Final Report</a>, 1997, US Nuclear Regulatory Commission.
</ul>



<p>
In all cases, you want to use the tools in a way that tries
to catch as many mistakes as possible before the code gets out to the user.
For example, you would normally turn on essentially all warning flags,
and you would normally set up guidelines on how to use the language
(to avoid things that are known to be problematic).

<p>
Compilers are hard to verify too.
There&#8217;s been some research progress for formally verify that
a compiler correctly implements its language, but it&#8217;s not
ready for typical use yet with real compilers;
current work focuses on tiny toy languages, and even these are difficult.
The article
<a href="http://www.stsc.hill.af.mil/crosstalk/2006/04/0604KorneckiZalewski.html">
<i>Qualification of Software Development Tools From the DO-178B
Certification Perspective</i></a> notes some of the challenges in
qualifying tools directly (from the DO-178B perspective).
An approach more commonly used today is to have the compiler generate code,
and then do hand-checking to make sure that the generated code matches the
source code.
This is obviously not ideal, but it easily beats writing the
assembly code by hand, and if the compiler directly supports it
(the GNAT Ada compiler does) this is not to hard to do.
Using a widely-used compiler, and avoiding &#8220;stressing&#8221; the compiler,
reduces the risk of accidentally inserting an error into the final
machine code to a very small amount, and hand-checking can detect
such errors.

<!--
ftp://ftp.cs.indiana.edu/pub/scheme-repository/doc/pubs/vlisp/README
"The Verified Programming Language Implementation project has developed
a formally verified implementation of the Scheme programming language,
called Vlisp.  The contributors were Joshua D. Guttman, John D.
Ramsdell, William M. Farmer, Leonard G. Monk, and Vipin Swarup, of The
MITRE Corporation, and Mitchell Wand and Dino Oliva of Northeastern
University.  This directory contains the ten final reports generated
by the members of the project.  An overview of the project is
presented in the Vlisp Guide.  You can obtain paper copies of these
reports by sending a request to ramsdell@mitre.org, or to the
following U. S. Mail address:
 John D. Ramsdell
 MS A118
 The MITRE Corporation
 202 Burlington Road
 Bedford MA, 01730-1420 "
More accessible PDFs can be found here:
http://library.readscheme.org/

http://www.swiss.ai.mit.edu/ftpdir/users/jar/archive/whole.ps
Jonathan A. Rees. "A Security Kernel Based on the Lambda-Calculus".
PhD. Thesis. February 1995.
-->

<p>
Intentional malicious subversion of compilers turns
out to be much more difficult to counter.
An Air Force evaluation of Multics,
and Ken Thompson&#8217;s famous Turing award lecture
<a href="http://www.acm.org/classics/sep95/">
&#8220;Reflections on Trusting Trust&#8221;</a> showed that compilers can
be subverted to insert malicious Trojan horses into critical software,
including themselves, and it&#8217;s shockingly hard to counter the attack.
Thompson even performed the attack under controlled
experimental conditions, and the victim never discovered it.
For decades it&#8217;s been assumed that this was
the &#8220;uncounterable attack&#8221;.
Thankfully, my own academic work on
<a href="https://dwheeler.com/trusting-trust">
Countering Trusting Trust through Diverse Double-Compiling</a>
shows how to counter subversive compilers, so even this
nasty problem is now solvable.

<p>
<h1><a name="highassurancecomponents">High assurance components</a></h1>
<p>
There are very few high assurance FLOSS components available.
A later section will discuss this, but for now,
here are the closest to high assurance FLOSS
components that I&#8217;ve been able to identify
(some are quite a stretch):

<p>
<ul>
<li>
The
<a href="http://www.adacore.com/home/gnatpro/safety-critical">
GNAT Pro High-Integrity Edition (HIE)</a>
for the GNAT Ada compiler is focused on supporting
requirements such as TRCA/DO-178B level A and B, EUROCAE ED-12B
and DEF Stan 00-55, according to the vendor AdaCore.
It features a configurable run-time library (so you only need
to include what you need, simplifying certification).
<!--
http://www.adacore.com/home/gnatpro/embedded/bareboard_powerpc
-->

<li>
Various L4-microkernel-related projects.
L4 is a family of microkernels (with a common interface),
and some recent work has focused on
making high-assurance L4-related microkernels.
Most of them are open source software.
Parts of the L4::Pistachio have been formally proved, and that is
definitely FLOSS.
There's an
ongoing effort to create a new L4 implementation that is formally verified
(seL4), though at this time I don't know if it will be FLOSS.
<a href="http://l4hq.org/">L4HQ</a> is a useful general source of L4 information;
<a href="http://l4ka.org/">L4Ka</a> discusses this to some extent.
<a href="http://portal.ok-labs.com/">Open Kernel Labs maintains
OKL4</a>. This is released under essentially a meta-OSS-license, requiring
that any redistribution must include information on how to get the source
code, and that it be licensed under an OSI-approved license.
They work with the NICTA
<a href="http://www.ertos.nicta.com.au/research/l4/">L4</a>,
<a href="http://www.ertos.nicta.com.au/research/l4.verified">L4.verified</a>, and
<a href="http://www.ertos.nicta.com.au/research/sel4">seL4</a>
efforts, among others.
This is a large set of interrelated efforts; to understand them,
I suggest reading
<a href="http://www.ok-labs.com/_assets/downloads/Trusthworthy_Computing_Systems.pdf">
Towards Trustworthy Computing Systems: Taking Microkernels to the Next Level</a> and
<a href="http://www.usenix.org/publications/login/2005-12/pdfs/heiser.pdf">
"Secure Embedded Systems Need Microkernels" by Gernot Heiser</a>.
In many ways this is ongoing work; L4 kernels have been around a long time,
but the efforts to really create highly assured versions of L4 are much younger
(and influenced by work such as Shapiro's EROS work).

<!--
 Here's the copyright statements from Open Kernel Labs, Inc., version 1.5.2,
 which appear to clearly be OSS:

/*
 * Copyright (c) 2007 Open Kernel Labs, Inc. (Copyright Holder).
 * All rights reserved.
 *
 * 1. Redistribution and use of OKL4 (Software) in source and binary
 * forms, with or without modification, are permitted provided that the
 * following conditions are met:
 *
 *     (a) Redistributions of source code must retain this clause 1
 *         (including paragraphs (a), (b) and (c)), clause 2 and clause 3
 *         (Licence Terms) and the above copyright notice.
 *
 *     (b) Redistributions in binary form must reproduce the above
 *         copyright notice and the Licence Terms in the documentation and/or
 *         other materials provided with the distribution.
 *
 *     (c) Redistributions in any form must be accompanied by information on
 *         how to obtain complete source code for:
 *        (i) the Software; and
 *        (ii) all accompanying software that uses (or is intended to
 *        use) the Software whether directly or indirectly.  Such source
 *        code must:
 *        (iii) either be included in the distribution or be available
 *        for no more than the cost of distribution plus a nominal fee;
 *        and
 *        (iv) be licensed by each relevant holder of copyright under
 *        either the Licence Terms (with an appropriate copyright notice)
 *        or the terms of a licence which is approved by the Open Source
 *        Initative.  For an executable file, "complete source code"
 *        means the source code for all modules it contains and includes
 *        associated build and other files reasonably required to produce
 *        the executable.
 *
 * 2. THIS SOFTWARE IS PROVIDED ``AS IS'' AND, TO THE EXTENT PERMITTED BY
 * LAW, ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,
 * THE IMPLIED WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR
 * PURPOSE, OR NON-INFRINGEMENT, ARE DISCLAIMED.  WHERE ANY WARRANTY IS
 * IMPLIED AND IS PREVENTED BY LAW FROM BEING DISCLAIMED THEN TO THE
 * EXTENT PERMISSIBLE BY LAW: (A) THE WARRANTY IS READ DOWN IN FAVOUR OF
 * THE COPYRIGHT HOLDER (AND, IN THE CASE OF A PARTICIPANT, THAT
 * PARTICIPANT) AND (B) ANY LIMITATIONS PERMITTED BY LAW (INCLUDING AS TO
 * THE EXTENT OF THE WARRANTY AND THE REMEDIES AVAILABLE IN THE EVENT OF
 * BREACH) ARE DEEMED PART OF THIS LICENCE IN A FORM MOST FAVOURABLE TO
 * THE COPYRIGHT HOLDER (AND, IN THE CASE OF A PARTICIPANT, THAT
 * PARTICIPANT). IN THE LICENCE TERMS, "PARTICIPANT" INCLUDES EVERY
 * PERSON WHO HAS CONTRIBUTED TO THE SOFTWARE OR WHO HAS BEEN INVOLVED IN
 * THE DISTRIBUTION OR DISSEMINATION OF THE SOFTWARE.
 *
 * 3. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR ANY OTHER PARTICIPANT BE
 * LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
 * CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
 * SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR
 * BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,
 * WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE
 * OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN
 * IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
 */
/*
-->

<li>
<a href="http://www.eros-os.org/">EROS</a> (GPL)
is an operating system based on capabilities, spearheaded by the very smart
<a href="http://www.eros-os.org/~shap">
Jonathan S. Shapiro</a>.
<a href="http://www.eros-os.org/devel/00Devel.html">There are many
documents available about EROS</a>,
including proofs of correctness for its
fundamental security mechanism (confinement).
In a sense the project named &#8220;EROS&#8221; has completed, and now has two
two successors.
One is the <a href="http://www.capros.org/">CapROS</a> project (GPL),
which is a commercial derivative spearheaded by Charlie Landau
(one of architects of KeyKOS, which was the EROS <i>predecessor</i>).
CapROS builds directly on the EROS work as a small, secure, operating system.
Meanwhile, Shapiro is pursuing the
<a href="http://www.coyotos.org/">Coyotos project</a> Coyotos project (GPL),
taking the ideas of EROS but making some significant architectural
changes.
As of April 2006 the Coyotos system is not yet complete, but
the current code can be obtained from the web site. Coyotos has research
objectives in the direction of formal verification, but is also expected
to deliver commercially some time this year. The GNU Hurd project (GPL)
has decided to build their next-generation system using the Coyotos
system as the core OS.

<!--
<li>
Uncertain:
? Smalley - University of Utah
? talk with Blaine Burnham at Georgia Tech? -->


<li>
<a href="http://programatica.cs.pdx.edu/House/">House, an
operating system project in Haskell (BSD3 license)</a>, is reportedly
aiming (eventually) at high assurance.

<li>
<a href="http://www.haskell.org/halfs/">Halfs</a> (LGPL)
is a filesystem implemented in the programming language Haskell.
Halfs can be mounted on Linux (via the FUSE interface)
and used like any other Linux filesystem,
or used as a library.
The background is that,
"In the course of developing a web server for an embedded operating system,
Galois Connections had need of a filesystem which was small enough to
alter to our needs and written in a high-level language so that we
could show certain high assurance properties about its behavior.
Since we had already ported the Haskell runtime to this operating system,
Haskell was the obvious language of choice.
Halfs is a port of our filesystem to Linux.
High assurance development is a methodology for creating software systems
that meet rigorously-defined specifications with a high degree of confidence...".
<p>
To be fair, though, Halfs isn't really high assurance as far as I'm
concerned.
Yes, they implemented a filesystem in Haskell, but you can implement junk
in any language (including Haskell).
Haskell does provide some type guarantees, but that's not enough.
It's true that transformations to machine code can be checked thoroughly,
but on the other hand, it's difficult to analyze resulting Haskell machine
code (in traditional procedural languages the correspondance
is clearer, and thus easier to check).
What's potentially interesting from a high assurance viewpoint
is that Haskell can be deeply analyzed
in a way many other languages can't, and so you <i>could</i> verify many
interesting properties of Halfs.
Unfortunately, I don't see much evidence that this
has actually occurred with Halfs.
<p>
More generally,
Galois's whole business model rests on developing
high-assurance software (including heavy use of formal methods).
They do most coding in Haskell, and they do contribute to many Haskell FLOSS
efforts (eg, the Glasgow Haskell Compiler which is under a BSD3
license), although often the core software they develop is proprietary.
<!--
They are looking at opening more software were possible, but
it's not clear how much software that will be.-->
<a href="http://cufp.galois.com/slides/2007/JohnLaunchbury.pdf">
John Launchbury's talk at CUFP</a> may be of interest.

<li>
The
<a href="http://xmonad.org">tiling window manager X monad</a> is
FLOSS and it uses some techniques to gain assurance
(e.g., the property checker QuickCheck and the
partiality checker catch).
It is implemented in Haskell.
<!--
Manuel M T Chakravarty chak at cse.unsw.edu.au:
"I think this is a nice project
that does show how the advantages of FLOSS wrt to high assurance can
actually be realised in a FLOSS project without leading to too much
development overhead."
-->


<li>
Twenty-one BSD C library string functions were proved correct
(after corrections) using ACL2 in the early 1990s.
In the process, three defects were found and corrected.
This work proved the machine code level (of the 68020), as generated by gcc.
For more information, see
<a href="http://citeseer.ist.psu.edu/220177.html">Boyer and Yu&#8217;s</a>
and
<a href="http://citeseer.ist.psu.edu/boyer96mechanized.html">
Boyer and Moore&#8217;s &#8220;Mechanized Formal Reasoning...&#8221;</a>
papers.
A key basis of this proof is described in
<a href="http://www.cs.utexas.edu/users/boyer/ftp/nqthm/nqthm-1992/examples/yu/mc20-1.pdf">
A Formal Speciﬁcation of Some User Mode Instructions for the Motorola 68020</a>
by Robert S. Boyer and Yuan Yu.
See
<a href="https://db.usenix.org/events/jvm01/full_papers/moore/moore_html/node2.html">comments by Moore</a> for discussion on related topics.

<li>
Minix was released under the BSD license (a FLOSS license) in
<a href="http://minix1.woodhull.com/faq/mxlicense.html">April 2000</a>.
It turns out that in the late 1980s some work had been developed to
demonstrate how to evaluate operating system security, using Minix as a model.
In particular a mathematically formal model was developed and proved
for Minix
(&#8220;Minux Security Policy Model&#8221; by
J. Eric Roskos and Terry Mayfield, IDA Paper P-2112, May 1988, IDA,
4850 Mark Center Drive, Alexandria, VA).
This formally defined the overall security policy 
for a simple Unix-like system which supported the usual user-group-other
permissions on files, with file owners making the permission decisions
(without any &#8220;mandatory&#8221; access controls
that users can&#8217;t override), and proved certain things
about the policy (it was &#8220;level 1&#8221; in the parlance above).
However, this was only a proof of a very high-level policy of the system.
<!--
There was a secondary paper "C2-Minix Operating System Design" by
Alan T. Krantz and J. Eric Roskos,
IDA Memorandum Report M-486, August 1988, but while it discussed the
Minix design, it didn't try to PROVE that the model was met by the
design, never mind go all the way down to the code.
-->
Again, there&#8217;s little doubt that creating simple mathematical models of
systems and then proving their properties can be a very valuable use of
formal methods... but it&#8217;s not enough for high assurance, because
no attempt was made to give strong evidence that the system exactly
met the policy.
Nor did the authors claim otherwise;
this work was only to demonstrate what to do to meet
the old &#8220;Orange Book&#8221; level C2,
a &#8220;medium assurance&#8221; level.
Today&#8217;s operating systems typically support far more functionality
than Minix of the late 1980s, so even this is very limited.

<li>
MK++ (never released)
by the Open Software Foundation Research Institute
(originally part of the Open Group)
was a microkernel designed to support
bounded (real-time) execution time, high assurance
(targeting the Orange Book level B3),
distribution, multiprocessor support, and high performance.
Papers discussing MK++ include
<a href="http://www.opengroup.org/ar/technologies/mk-dbleplus/">
MK++: A High Performance, High Assurance Microkernel</a>,
<a href="http://doi.ieeecomputersociety.org/10.1109/ICFEM.1997.630422">
The Specification-Based Testing of a Trusted Kernel: MK++</a>,
<a href="http://doi.ieeecomputersociety.org/10.1109/ISORC.1998.666781">
Evolution of a Distributed, Real-time, Object-Based Operating System</a>,
<a href="http://citeseer.ist.psu.edu/18899.html">
Specification-based Atomicity Testing of the MK++ Kernel</a>,
and
<a href="http://www.usenix.org/publications/library/proceedings/coots96/full_papers/loepere/index.html">
Composing Special Memory Allocators in C++ (at Usenix COOTS 1996)</a>.

<!-- POC for MK++:
Randy Dean (rwd at osf.org), Keith Loepere (loepere at osf.org)
Open Software Foundation Research Institute(1)
-->
However, MK++ suddenly disappeared after quite a bit of work
had gone into testing it, and I have not found any evidence that
its code was released.
<a href="http://slashdot.org/articles/03/07/25/1517236.shtml">A
posting on Slashdot reports on MK++&#8217;s fate</a>,
saying that MK++ was an unfortunate victim of bad timing as
the Open Group changed focus:
&#8220;[mk++] was a complete re-write of the Mach
microkernel interfaces using C++.
There was a plan to release a book on mk++ along with a CD containing
mk++Linux. Unfortunately, a month or so before it was to be sent off,
all development efforts were shut down, and The Open Group became
a Unix branding organization.&#8221;

<li>
The
<a href="http://cisr.nps.navy.mil/projects/tcx.html">
Trusted Computing Exemplar (TCX) project</a> (license unknown,
probably not FLOSS, no code released)
is working to &#8220;provide a working example that shows how
trusted computing systems and components can be constructed.
The project will develop a high assurance, embedded micro-kernel
and a trusted application built on top of the micro-kernel
as a reference implementation exemplar for trusted computing.&#8221;
Some very smart people, such as
<a href="http://www.nps.navy.mil/cs/facultypages/faculty/irvine/">Cynthia
Irvine</a> and others at the
Center for Information Systems Security Studies and Research,
are involved in this project.
They are focusing on meeting Common Criteria EAL7 and beyond
(the highest defined assurance level).
In particular, they intend to
truly prove every line of code all the way down
(which is beyond what people do today, even in high assurance).
The outputs of the TCX project are intended to be publicly documented,
but it is not yet clear if its source code and specifications will meet
FLOSS license requirements;
<a href="http://cisr.nps.navy.mil/downloads/04paper_tcx.pdf">
the 2002 TCXX white paper</a>
only says that their &#8220;framework will support the dissemination of project
deliverables using a philosophy similar to the open source approaches&#8221;,
which makes no particular commitment.
<p>
Unfortunately, the TCX project doesn&#8217;t seem
to be working to ensure that the project can live on
or be supported as a FLOSS project.
Many FLOSS developers will only work on projects
if they can be supported with FLOSS tools
(see <a href="http://www.gnu.org/philosophy/java-trap.html">The
Java Trap</a> for a discussion).
Yet the project chose to use
the proprietary CM tool Perforce, and was seriously considering the use of a
tool (PVS) that was proprietary at the time instead of using a FLOSS tool
(such as ACL2).
PVS is now FLOSS, but that is not the point.
The point is that
the project failed to even <i>consider</i> licensing as a selection
factor when it performed its
<a href="http://cisr.nps.navy.mil/downloads/03paper_rhas.pdf">survey
of tools</a>.
If the TCX fundamentally depends on non-FLOSS tools, it
will probably be mostly ignored by FLOSS developers, because it
will essentially be unusable to them (after the Java Trap and the
Linux kernel&#8217;s Bitkeeper crisis, few will be interested in depending on
non-FLOSS tools).
There&#8217;s also no evidence of outreach to the FLOSS community;
if FLOSS were being considered as a future maintenance strategy,
that&#8217;s a curious omission.
Also, since TCX is focusing on being a demonstration, it is unclear
if the results will be usable as an implementation in real projects.
And besides all this,
there is no plan for it to undergo a formal Common Criteria evaluation or
any other independent evaluation,
even though that is the specification they are using as their baseline.
And finally, I suspect that the project has been halted;
I did not get any responses from email queries, and I&#8217;ve heard
separately that the project stopped.

<li>
There are a few small published examples of proved programs
using Coq, Why, and Caduceus (for C) or Krakatoa (for Java)
(these are all FLOSS tools).
There are several examples, such as the
<a href="http://why.lri.fr/caduceus/examples/tutorial">
Caduceus tutorial examples</a>,
<a href="http://why.lri.fr/caduceus/examples/">
other Caduceus examples</a>, and
an <a href="http://why.lri.fr/queens/index.en.html">
N-queens problem</a>.
<li>
The
<a href="http://www.cs.utah.edu/flux/fluke/html/dtos/HTML/dtos.html">
Distributed Trusted Operating System (DTOS) project</a>
(no source code release known, but the FLOSS SELinux followed on)
was a joint effort by the National Security Agency (NSA)
and Secure Computing Corporation (SCC) to encourage strong,
flexible security controls in next generation operating systems.
DTOS was a successor of the Distributed Trusted Mach (DTMach) project.
DTOS is no longer active, but the
Flask and Security-Enhanced Linux (SELinux) projects have carried on as its
successors.
SELinux in particular <i>is</i> FLOSS, and since some of the lessons from
DTOS were used in developing SELinux, it may be that some of the DTOS
information would be relevant.
The
<a href="http://www.cs.utah.edu/flux/fluke/html/dtos/HTML/final-docs/lessons.pdf">DTOS lessons learned paper</a>
(27 June 1997) is especially interesting.
</ul>
<p>
There&#8217;s much more available if you are simply looking for a
real-time operating system (RTOS) kernel, and aren&#8217;t really
looking for high assurance.
<a href="http://ecos.sourceware.org/">eCos</a> is a FLOSS RTOS,
for example,
and there are several projects that create &#8220;real-time&#8221; versions of the
Linux kernel.
Also, if you want a hypervisor, Xen is very popular and is FLOSS.
But none of these are high assurance.
<p>
Recently several proprietary
<a href="http://www.cotsjournalonline.com/home/printthis.php?id=100423">
Multiple Independent Levels of Security (MILS) separation kernels</a>,
have become available.
It&#8217;s not clear that there are FLOSS implementations becoming available
as well, though clearly it&#8217;s possible
(the TCX project might do so, but it&#8217;s not clear that the code will
be FLOSS, and it&#8217;s not clear that it will be sufficiently capable to
be useful... it&#8217;s currently not available).
The July 2004 draft <a href="http://niap.nist.gov/pp/">separation kernel
protection profile</a>, which defines the requirements for the
separation kernel, is a Common Criteria-based document requiring
EAL 6 plus augmentation.
More information is about the MILS architecture is available in the
<a href="http://www.stsc.hill.af.mil/crosstalk/2005/08/0508Vanfleet_etal.html">
August 2005 Crosstalk article about the MILS architecture</a>,
and you can look at John Rushby&#8217;s papers that discuss this in more
technical detail.
<a href="http://www.cs.utexas.edu/users/moore/acl2/workshop-2003/contrib/greve-wilding-vanfleet/security-policy.pdf">
Greve, Wilding, and vanFleet have defined
a formal specification for a MILS separation kernel using ACL2</a>
[<a href="ftp://ftp.cs.utexas.edu/pub/moore/acl2/v2-9/acl2-sources/books/workshops/2003/greve-wilding-vanfleet/support/">code</a>];
<a href="http://www.cs.utexas.edu/users/moore/acl2/workshop-2004/contrib/alves-foss-taylor/ACL2004-final.pdf">
Alves-Foss and Taylor comment on it</a>.
I could imagine a variant or re-implementation of Xen meeting this,
however, Xen is designed for performance instead of trying to
meet these requirements.



<h1><a name="mediumassurance">Medium assurance</a></h1>
<p>
There are a lot of FLOSS tools to help achieve medium assurance, and
there are also many FLOSS programs that achieve medium assurance.
Let&#8217;s look at them, so we can contrast the situation of
FLOSS tools and components in medium assurance against those in high assurance.

<h2>FLOSS tools for medium assurance</h2>
<p>
There are a number of FLOSS tools to help find defects in programs.
These include splint and flawfinder; my
<a href="https://dwheeler.com/flawfinder">Flawfinder home page</a>
identifies many of these program analysis tools, such as
<a href="http://pmd.sourceforge.net/">PMD</a> for Java.
The Linux kernel developers even developed their own static analysis
tool to examine the kernel (kernels are very different from application
programs, and their common failure modes are
different than applications&#8217;).
But note that these tools are generally designed to
to look for specific common defects and defects that are easy for
tools to find.
These could have <i>some</i> value in high assurance applications
as well, since these tools could quickly filter out problems before
bringing out the &#8220;big guns&#8221;.
But these tools aren&#8217;t enough;
a program can pass every such tool and still have serious flaws.
It&#8217;s also worth noting that these tools often have many false positives
(&#8220;bug&#8221; reports that aren&#8217;t really defects at all).

<p>
I should note that it&#8217;s been widely and repeatedly
proven that the most efficient method for finding and repairing defects,
in terms of found and repaired defects per person-hour spent, are
<i>peer reviews</i> -- groups of people actually looking at the code.
There are various processes for this, going under names such
as &#8220;software inspections&#8221; (Fagan and Gilb have different processes
sharing that name).
Peer reviews are low-tech, and often not exciting to do... 
but they&#8217;re so effective that
anyone leading a software project should be interested in them.
<a href="http://www.stsc.hill.af.mil/crosstalk/2003/11/0311German.html">
Andy German&#8217;s 2003 paper &#8220;Software Static Code Analysis Lessons Learned&#8221;</a>
reports that &#8220;independent code walkthroughs are
the most effective [static code analysis] technique
for software anomaly removal [, finding]
up to 60 percent of the errors present in the code.&#8221;
I co-edited and co-authored an IEEE Computer Society Press book,
&#8220;Software Inspection: An Industry Best Practice&#8221;, which if you can get it
(it&#8217;s now out of print) presents lots more information about it.
The <a href="http://www2.ics.hawaii.edu/~johnson/FTR">Formal Technical
Review Archive</a> lists many other texts.
For high assurance, 60% is not 100%, but for medium assurance it&#8217;s fantastic.
The known effectiveness of peer review explains why the
&#8220;many eyeballs&#8221; idea of FLOSS really can work.

<h2>FLOSS components at medium assurance</h2>
<p>
Several FLOSS implementations have undergone traditional
medium assurance evaluations.
There are at least two Linux distributions that have undergone
Common Criteria evaluations (Red Hat and Novell/SuSE) at what I
would term medium assurance levels.
Similarly, the cryptographic library OpenSSL has undergone FIPS evaluation.
To be fair, a program that has not undergone a Common Criteria
evaluation process might be more secure than one that has, and
a program that underwent a lower EAL Common Criteria evaluation might
be more secure than one at a higher level.
The Common Criteria is a standardized set of processes
for evaluating the security of software, and higher EAL values add
more evaluation processes.
A product that has not gone through the higher levels might still
be very secure -- all we know for sure is that no one has paid for the
higher-level evaluation to be performed.

<p>
Unsurprisingly, many other popular FLOSS programs and systems have
undergone many tool-based evaluations searching for flaws as well:
<ul>
<li>The paper
<a href="http://www.acsac.org/2005/abstracts/165.html">
Model Checking An Entire Linux Distribution for Security Violations</a>
describes the use of model-checking (with MOPS) to find defects in
a Linux distribution;
it helped find defects in 60 million lines of code.
One clarifying point:
Model-checking is a general technique that <i>can</i> also be used to prove
properties for high assurance, but in this case the technique only
proved the presence or absence of specific types of errors in
certain patterns. It did <i>not</i> determine if a program
had <i>no</i> errors.
(That being said,
<a href="http://www.cs.berkeley.edu/~daw/mops/">MOPS</a> is still
an interesting tool.)
<li>Many tool vendors intentionally apply their tools to FLOSS programs,
since they make great test cases and reporting their results can help
both the tool vendor and the FLOSS project.
<a href="https://dwheeler.com/oss_fs_why.html">Examples
of tool vendors examining FLOSS programs</a> include
the &#8220;Fuzz&#8221; studies, Reasoning&#8217;s analysis of the Linux kernel and MySQL,
and a study by Coverity.
<li>The U.S. Department of Homeland Security (DHS) is
<a href="http://news.com.com/Homeland+Security+helps+secure+open-source+code/2100-1002_3-6025579.html">funding work to examine common FLOSS programs
using Coverity</a>, including the Linux kernel, Apache, BIND, the Firefox
web browser, and so on.
Basically, they repeatedly scan the software, and send defect report
results to the various affected programs.
I think this is a great idea, because it gives them a useful source
of possible bugs, though the
<a href="http://www.gcn.com/print/25_8/40387-1.html">high number of
false positives in the reports</a> do limit their effectiveness.
<li>The
<a href="http://osq.cs.berkeley.edu/">Open Source Quality Project</a>
investigates techniques and tools for assuring software quality,
and focuses on &#8220;designing and building tools to improve
the quality of Open Source software.&#8221;
Their rationale is that FLOSS is &#8220;attractive as a research vehicle
in software quality because of the critical role it plays in
the nation&#8217;s economy and precisely because it has the unique
feature that it is a real-world system that is
completely open and available for study.
Because of the Open Source tradition of incorporating useful
new techniques and tools into the Open Source environment,
there is also an opportunity for direct and widespread impact.&#8221;
There are many other similar academic projects, for the same reasons.
</ul>
<p>
Looking more broadly, it is clear that there are many FLOSS
projects which take significant steps to search for and remove defects.

<h2>General issues: FLOSS and medium assurance</h2>
<p>
Many experts have concluded that FLOSS has a potential advantage over
proprietary software when it comes to security or reliability,
though not all FLOSS
programs are more secure than their competitors.
This is borne out by many studies of actual FLOSS programs, which show
that FLOSS does <i>very</i> well in terms of security
and reliability (often much better than their proprietary competition).

<p>
Several papers and presentations
discuss the general issue of security and FLOSS,
noting some potential advantages of FLOSS.
Examples of such sources include
<a href="http://www.dirc.org.uk/publications/articles/papers/44.pdf">
&#8220;Issues of Dependability in Open Source Software Development&#8221;
(ACM SIGSOFT, Software Engineering Notes, May 2002)</a>
the
<a href="https://dwheeler.com/secure-programs/Secure-Programs-HOWTO/open-source-security.html">
section on open source software in my book
<i>Secure Programming for Linux and Unix HOWTO</i></a>, and my
<a href="https://dwheeler.com/essays/oss_software_assurance.pdf">
presentation on open source software, security, and software assurance</a>.
There&#8217;s no <i>guarantee</i> that a FLOSS program will be more secure
than a proprietary program -- do not mistakenly think otherwise.
However, there are potential advantages to FLOSS, for example:
<ul>
<li>FLOSS better meets the security fundamental &#8220;open design&#8221;
<li>FLOSS programs always have the potential for massive (worldwide) peer
review, both during initial development
(e.g., by a community of developers) or afterward.
Thus, many defects are detected before customers use the results.
For many of the larger and more important components, such as the
Linux kernel and Apache web server, there is ample evidence
that in fact this happens both during initial development and
afterward.
<li>FLOSS developers tend to work to develop better code,
because they know their name will be associated with
the code that is reviewed worldwide.
<li>FLOSS programs tend to have well-defined interfaces and careful
designs, since their developers must usually work worldwide and
cannot depend on walking over to someone else to understand a design.
<li>FLOSS programs are typically less pressured to deliver an
inadequate product; if the product isn&#8217;t ready, the world knows it.
</ul>

<p>
There&#8217;s certainly lots of evidence that FLOSS programs can be
more reliable and secure than their proprietary competition.
My paper
<a href="https://dwheeler.com/oss_fs_why.html">
&#8220;Why Open Source Software / Free Software (OSS/FS or FLOSS)?
Look at the Numbers!&#8221;</a>
has lengthy lists of studies showing that various FLOSS programs
do very well in terms of reliability and security; see them for more.
For example:
<ul>
<li>
The &#8220;Fuzz&#8221; studies in particular studied several sets of
proprietary and FLOSS programs, and found that the FLOSS
programs had markedly superior reliability... this is one
of the few studies that compared FLOSS and proprietary programs
as a set, instead of comparing one instance to another instance.
<li>
A detailed study of two large programs
(the Linux kernel and the Mozilla web browser) found evidence that
FLOSS development processes tend to produce more modular designs.
See Harvard Business School&#8217;s &#8220;Exploring the Structure of
Complex Software Designs: An Empirical Study of
Open Source and Proprietary Code&#8221; by Alan MacCormack, John Rusnak,
and Carliss Baldwin (Working Paper Number 05-016) for the details.
It&#8217;s generally accepted that there are important benefits to
greater modularity, in particular, a more
modular system tends to be more reliable, easier to change over time, and
more secure.
</ul>
<p>
In addition, many FLOSS programs have now built-in countermeasures to
counter security problems in other components.
<a href="http://people.redhat.com/drepper/nonselsec.pdf">
"Security Enhancements in Red Hat Enterprise Linux (beside SELinux)"
by Ulrich Drepper</a> describes many mechanisms in Red Hat Enterprise Linux
that counter or limit damage even if another component has a vulnerability.
I'm a strong advocate of this belt-and-suspenders approach to security;
by all means, eliminate vulnerabilities in components, but also deploy
defensive measures so that unfound vulnerabilities are less serious.
<p>
In short, there is every reason to believe FLOSS components
can be at least as reliable and secure as proprietary components,
if not more so.


<h1><a name="rare">Why are high assurance FLOSS components so rare?</a></h1>
<p>
So now we come to an interesting question --
why are high assurance FLOSS components so rare?
There are very few real high assurance components available, period,
so we would expect there to be few FLOSS components.
Yet FLOSS still seems under-represented.

<p>
Jeffrey S. Norris and Poul-Henning Kamp&#8217;s
&#8220;Mission-Critical Development with Open Source Software: Lessons Learned&#8221;
(IEEE Software) clearly shows that people do develop mission-critical
systems at NASA using FLOSS components;
they relied heavily on FLOSS components, reporting that they
kept the project within budget and resulted in a more robust and
flexible tool.
The text suggests that these were medium assurance applications,
but the lessons are worth considering at any assurance level.

<p>
<a href="http://perens.com/Articles/Economic.html">
Key vendor-differentiating software is usually not FLOSS.</a>
If a company or government
uses a particular piece of software as part of their
competitive advantage, they should usually <i>not</i> release it
at all (as <i>either</i> proprietary or FLOSS).
But most software is actually not company or government differentiating,
and even if the software as a <i>whole</i> is, some of its pieces are
still typically commodity components.
A company that <i>sells</i> software licenses to others
will often not choose to
release that program as FLOSS -- but if they have customers
that <i>use</i> the software, sometimes those users find that
using a FLOSS program instead has its advantages.
Components that are commodities shared among many devices,
such as a separation kernel or real-time operating system kernel,
would make sense as FLOSS.

<p>
There&#8217;s fragmentation in
the high assurance market (e.g., different detailed requirements
in different standards and different circumstances) which
probably doesn&#8217;t help.
But this harms other approaches for developing software too,
and increasingly this problem is being recognized and addressed.

<p>
Clearly high assurance components are
normally required to go through expensive independent evaluations
(due to various regulations).
But again, that is a hurdle that has been overcome before,
many times.

<p>
FLOSS is widely represented in other areas.
There are many medium assurance FLOSS programs that have better security
or reliability records than competing proprietary programs.
FLOSS is certainly well-represented by tools to create medium and
high assurance components as well.
A vast number of FLOSS tools are available, in fact, for creating
high assurance components
(and many have already been used for the purpose,
including ACL2, SPIN, and GNAT).

<p>
It&#8217;s even more bizarre when you compare software proofs with
the way normal mathematical proofs are made.
&#8220;Normal&#8221; mathematicians publish their proofs, and then depend on
worldwide peer review to find the errors and weaknesses in their proofs.
And for good reason; it turns out that many formally published
math articles (which went through expert peer review before publication)
have had flaws discovered later, and had to be corrected later or withdrawn.
Only through lengthy, public worldwide review have these problems surfaced.
If those who dedicate their lives to mathematics often make mistakes,
it&#8217;s only reasonable to suspect that software developers who hide their
code and proofs from others are far more likely to get it wrong.
<a href="http://www.ams.org/notices/200710/tx071001279p.pdf">
Joyner and Stein's
"Open Source Mathematical Software" (<i>Notices of the AMS</i>, Nov. 2007)</a>
notes how well FLOSS matches the methods in mathematics
for producing high-quality results, and includes some interesting quotes.
Andrei Okounkov (2006 Fields medalist) notes that
"I think we need a symbolic standard to make
computer manipulations easier to document
and verify...  An open source
project could, perhaps, find better answers
to the obvious problems such as availability,
bugs, backward compatibility, platform independence, standard libraries, etc...
I do hope that funding agencies are looking into this."
Neubüser notes that with proprietary software
"two of the most basic rules of conduct in mathematics are violated:
In mathematics information is passed on free of charge
and everything is laid open for checking.”

<p>
The nonsense that FLOSS is more vulnerable to insertion of malicious code
doesn't wash, either.  Any FLOSS or proprietary program can be modified -
just get a hex editor.
The trick is to get the maliciously-modified program into a customer's
supply chain, and that's much harder: You have to get the malicious code
into the FLOSS component's trusted repository, and not noticed (else it
will be removed).
<a href="http://www2.csoonline.com/exclusives/column.html?CID=33386">
"Application Security: Is the Backdoor Threat the Next Big
Threat to Applications?" by Scott Berinato
(in <i>CSO Online</i>)
</a>
interviewed security researcher Chris Wysopal of Veracode.
"As detection and scanning technology gets better at finding the
accidental coding errors like buffer overflows, Wysopal believes the
malicious will turn more and more to using backdoors--holes in programs
usually intentionally programmed in to allow access to an application."
Wysopal goes on to note that
"The lifetime of a backdoor in open source is very short. It’s measured
in weeks. The lifetime of a backdoor in closed source is measured
in years. The many eyes concept of open source is working to detect
backdoors. We found that in most open source cases, the malicious or
accidental opening was detected in a matter of days, sometimes a few
weeks. But every backdoor in the binary of proprietary software was
there for years or an indeterminate length of time. It never happened
that closed source backdoors were discovered in months. With an old one,
Borland Interbase, we saw seven years worth of versions where a backdoor
was there."
The interviewer was surprised,
and appears to have been unaware of FLOSS
trusted repositories and how they work.
He asked,
"with so many people manipulating open source code, the number of
backdoors to detect must be exponentially higher than proprietary systems,
and the potential virulence, of spreading backdoors, must be much higher
with open source?"
Rather than explain this, Wysopal went immediately to
the measure that mattered:
"when we looked at special credential backdoors,
the four biggest were all closed source products."
The notion that FLOSS always makes you more vulnerable
to backdoors is contrary to real world experience.

<p>
Yet the source code and proofs for high assurance programs
are almost never published publicly at all (never mind
being released as FLOSS).
This means that many "high assurance" programs fail to exploit the methods
used by mathematics to strengthen claims.
Thus, there&#8217;s a good case to be made that high assurance FLOSS programs
would tend to be <i>much</i> higher assurance than proprietary programs,
because there could be a worldwide review of the proofs.
At least for safety-critical work making FLOSS (or at least world-readable)
code and proofs would make sense; why should we
accept safety software that cannot undergo worldwide review?
Are mathematical proofs really more important than software that
protects people&#8217;s lives?

<p>
Several potential explanations come to mind, and I suspect only the
last two are really true:
<ol>
<li><i>Is high assurance too expensive to do using FLOSS?</i>
Certainly, developing high assurance components requires a great deal
of personnel time, but this is true for other assurance levels too
(though they place their effort in different areas).
As I demonstrated in my paper
<a href="https://dwheeler.com/sloc">&#8220;More than a Gigabuck&#8221;</a>,
FLOSS development approaches are quite capable of employing <i>massive</i>
amounts of development effort.
In 2001, the old Red Hat Linux 7.1
represented more than one billion U.S. dollars of development effort,
and that is a small fraction of today&#8217;s FLOSS development effort.
It&#8217;s true that independent evaluations are normally required at high
assurance levels, and they require cash payments that may be harder to get.
Yet the fact that other FLOSS programs have undergone independent evaluations
(such as Common Criteria evaluations for Red Hat Linux and Novell/SuSE,
and FIPS evaluation for OpenSSL) suggests that this is probably not a real
barrier either.
At least, it&#8217;s not a barrier if there&#8217;s an
economic model to support it, which brings us to
our next point.

<p>
<li><i>Is there no possibility of a rational economic model for FLOSS high assurance?</i>
FLOSS is a licensing model that is typically
associated with certain development and distribution models.
FLOSS is not a single business model;
instead, there are many different business and non-profit economic models
that can be employed by FLOSS projects.
Business/economic issues in FLOSS are discussed in many places, including
<a href="http://perens.com/Articles/Economic.html">Bruce Perens&#8217;
&#8220;The Emerging Economics of Open Source Software&#8221;</a>
and
<a href="http://www.catb.org/~esr/writings/cathedral-bazaar/">
Eric Raymond&#8217;s &#8220;The Magic Cauldron&#8221;</a>.
Indeed, there&#8217;s a micro-industry of
economists who analyze FLOSS business
models, because they think FLOSS &#8220;shouldn&#8217;t&#8221; work
(due to failure to understand it) but it does anyway.
There are many different business models for FLOSS, but for our
purposes they all involve either making money and/or reducing expenditures.
The &#8220;make money&#8221; models typically do this by 
charging for warranties, indemnification, support, or related services
(the latter is sometimes an example of &#8220;commoditizing your complements&#8221;).
The &#8220;reducing expenditures&#8221; models typically involve
cost sharing or
cost avoidance, and often look like consortia with an unusually efficient
legal framework (Apache and the X consortium are examples).
Linux distributors typically do both; they sell support and services
(to make money), while exploiting the far lower expenditures to
compete against established proprietary organizations.
Lots of organizations do this in many other areas, so unless there&#8217;s
something especially unique to high assurance, it&#8217;s hard to believe
the approach can&#8217;t work for FLOSS.
<p>
Commercial organizations often support FLOSS projects when it&#8217;s
in their financial interest to do so.
Essentially all work in major FLOSS projects like the Linux kernel and
Apache website are done through software developers paid to do so.
Even in high assurance tools there are examples;
ACL2 is financially supported by many corporations;
AMD, Rockwell Collins, and Sun Microsystems are three of them, and they&#8217;ve
supported ACL2 because they&#8217;ve used it in microprocessor development.
Not all FLOSS projects are financially supported like this, of course,
but today&#8217;s corporations are willing to do so when
it&#8217;s in their own interests.
<p>
Certainly, there&#8217;s a big problem that there is a very high &#8220;initial fee&#8221;
to create and evaluate an initial product, but that&#8217;s true
for other FLOSS products too.
For years it was assumed that modern compilers and operating system
kernels were &#8220;too hard&#8221; for FLOSS;
that myth has been completely blown.
And besides, these also mitigate against proprietary suppliers;
proprietary suppliers have difficulty raising enough money in these
cases too.
These initial fees could be addressed by having
several large integrators or users (who use
<i>lots</I> of high assurance components) pool their funds to create
such a component, with the idea that they&#8217;ll reduce their overall costs.
Such organizations manage to create consortia in other areas; there&#8217;s
little reason they can&#8217;t handle this too.
<p>
Of course, such organizations could claim that they&#8217;re worried
about &#8220;free riders&#8221; who use the component without paying for initial
development, but that&#8217;s missing the economic point.
Nearly all costs for software are <i>not</i> in development --
they are in maintenance.
One reason FLOSS projects do so well is that many people copy the software
(starting out as &#8220;free riders&#8221; ), but a small fraction eventually
contribute valuable information (bug reports, patches, etc.)... because
they must to cost-effectively use the product.
As a result,
releasing a component as FLOSS allows the maintenance costs -- the majority
of all costs -- to be shared, even among those who did not pay
for initial development.
This means that a proportion of those
who start as &#8220;free riders&#8221; eventually help share the
maintenance costs, reducing the maintenance costs for all.
Since maintenance is the primary cost, there is a reasonable economic
rationale for releasing software this way (in some cases).
The initial investors have an additional reason to invest:
they will decide what will be implemented first, and how, and
many have decided that this additional control
is worth the initial investment.
That doesn&#8217;t mean that FLOSS is always the best economic approach, but
it does mean that it&#8217;s an approach worth exploring.
<p>
The only obvious true difference I see are liability issues, but this could
<i>help</i> FLOSS implementors.
Some people may think that the liability costs of high assurance software
will make it &#8220;impossible&#8221; to use FLOSS -- yet this misses the point.
Liability is something customers are willing to pay for, and even better for
a FLOSS business, customers would typically <i>have</i> to
pay for liability protection and support.
<a href="http://www.gnat.com/home/">AdaCore</a>, who support the GNAT
Ada compiler, depend on this and seem to be doing well.
So this makes high assurance unusually tempting for a FLOSS business --
customers can try out the product (making it more likely to be
considered), but will be required to pay the supplier through laws and
contracts requiring liability protection.
<p>
Many different economic models could be devised; let&#8217;s imagine just one.
A consortium could be established to create such high assurance components,
and give a liability price break to its founders -- with others paying
a higher (but reasonable) price to gain liability protection and support
(perhaps the costs go down after paying double the costs
of the initial founders).
Such a consortium could encourage code and monetary
contributions through a variety of means (e.g., by using the LGPL license,
or using a dual-license approach with the GPL license being one of them and
a for-pay license being the other).
Organizations like major government integrators might be interested in
such a consortium, because it would be a way to reduce their expenditures...
and they would not want to have to compete against those in the consortium,
if they were outside it, since those inside
might have lower future expenditures.
Many variations of these ideas are possible, of course.
There seems to be little evidence that
there&#8217;s no economic model for FLOSS high assurance components.

<p>
<li><i>Is the expertise too specialized and hard to acquire?</i>
Developing high assurance components truly is a specialized art.
More importantly, there are few published examples of how to actually
do it, so it&#8217;s <i>very</i> difficult to learn how to do it.
In fact, there are reasons to fear that some of this knowledge
at the highest end is disappearing and will need to
be re-discovered when its practitioners die.
The lack of published <i>proven</i> programs is a real problem!

<p>
<a href="http://opensource.ucc.ie/icse2002/HalloranScherlis.pdf">
Halloran and Scherlis&#8217; paper
&#8220;High Quality and Open Source Software&#8221;</a>
notes that in the medium assurance realm,
the &#8220;quality and dependability of today&#8217;s
open source software is roughly on par with commercial [proprietary]
and government developed software&#8221;, and then asks,
&#8220;what attributes must be possessed by quality-related interventions
for them to be feasibly adoptable in open source practice?&#8221;
They then note several attributes, which would presumably apply to
high assurance as well.
They note that FLOSS projects generally bootstrap on
top of other FLOSS tools, in part due to ideology, but also
because it lowers barriers for new participants and enables
developers to fluidly shift their attention from tool use to
tool development/repair.
As noted above, there are FLOSS tools available.
But in their conclusions they note these criteria:
&#8220;(1) an incremental model for quality investment
and payoff (e.g., incrementally adding analysis support, test
cases, measurement, or other kinds of evidence collection), (2) incremental
adoptability of methods and tools both within the server
wall and in the baseline client-side tool set, (3) a trusted server-side
implementation that can accept untrusted client-side input, and (4)
a tool interaction style that is adoptable by practicing open source
programmers (i.e., that does not require mastery of a large number
of unfamiliar concepts).&#8221;
Based on this list, they conclude that
&#8220;With the exception of testing technology and some code analysis
technology, these requirements suggest that
some adaptation
will be required before adoption is possible for tools that embody,
say, lightweight formal methods approaches or advanced program
analysis approaches. Clearly, any technique or tool is not feasibly
adoptable if it requires a major (client-visible) overhaul of a project
web portal, collaboration tools, development tools, or source code
base. Discernible increments of benefit from increments of participant
effort is key to adoptability.&#8221;
In short, formal methods require a lot more training
before the benefits can accrue -- and because they aren&#8217;t
incremental (and are rare), it&#8217;s harder to do.

<p>
I think the difficulty of acquiring the
necessary skills before being able to do any work
<i>is</i> a real and valid issue.
This is a real problem for using these techniques
to develop proprietary software, too!
But there is such experience, so while valid
I don&#8217;t think that&#8217;s the primary issue.

<p>
<li><i>Have few considered the possibility?</i>
It may be that many potential customers/users have simply
failed to consider a FLOSS option at all.
In fact, I think this is the most likely reason of all.
<p>
People tend to do what they know how to do,
and repeat approaches they've used before.
In many cases, the people who are interested in developing a small
high-assurance component have done this before and used
a proprietary approach... and since that is what they did before,
they do not think to consider an alternative.
<p>
For example, I went to a detailed economic review in 2006 of
one particular high assurance effort, where the big contrast was between
&#8220;government-owned&#8221; and &#8220;commercial proprietary software vendors&#8221;.
The possibility of a FLOSS approach (e.g., establishing a
consortia to create a FLOSS implementation) simply never entered any
decision-maker&#8217;s mind!
When an alternative is never considered, it&#8217;s not
surprising when it isn&#8217;t chosen.
The <i>developers</i> of high-assurance software
are quite familiar with FLOSS, and in fact are developers of it
themselves (just look at all the FLOSS tools for high assurance!).
But the decision-makers about high-assurance software are not the
same people, and in fact tend to be very conservative people because
they&#8217;re worrying about security and safety.
Conservatism about technology makes sense for these stakes, but
that doesn&#8217;t mean that valid economic alternatives should be ignored.

<p>
The TCX research project is even more bizarre, because it
specifically notes that it hopes to aid both
the proprietary commercial and the &#8220;open-source&#8221; sectors.
The TCX project even mentions planning to release its results
&#8220;using a philosophy similar to the open source approaches&#8221;.
But the information published so far does not suggest that
the materials will ever be released under a FLOSS license, nor
have I found evidence that this option was considered.
Licensing was not even a criteria in its tool survey,
yet depending on a language only implemented by a proprietary tool
would be certain to drive away many FLOSS developers.
The TCX published works so far include a mathematical model, which
could be used as a basis for further work, but it is only released as a
file in (uneditable) PDF format, with no rights to make improvements.
There is no FLOSS license granting
rights to improve any of the works released as of April 2006,
even though the TCX text seems to imply that the
MIT and BSD-new licenses might be appropriate for them.
There&#8217;s also no published rationale for why the
works will <i>not</i> be released under a FLOSS license;
given the many pages discussing other project decisions, the
failure to discuss the licensing of the results
(where dissemination is the <i>whole point</i> of the project) is jarring.
The reader is left with the impression that the ideas of
using FLOSS licensing terms to release their work,
and working with the larger FLOSS community,
never occurred to the project leaders.

<p>
In short, I think this is the primary explanation: the
FLOSS option does not seem to be even considered as a potential strategy.
I don&#8217;t think every high assurance component of the future will be FLOSS,
in fact, I expect there to be a continued number of proprietary
high assurance products.
But there also seems to be no special reason that it should be ignored.
I suspect that the number of FLOSS high-assurance
products will grow as decision-makers start to
consider FLOSS options as well.
In the long term, I expect a mixture of proprietary and FLOSS components,
just as this is already true for compilers,
operating system kernels, web browsers, web servers, and so on.
</ol>


<h1><a name="conclusions">Conclusions</a></h1>
<p>
There are a large number of FLOSS tools available for creating
high assurance components.
There are a vast number of tools supporting configuration management,
testing, formal methods, and code generation.
This paper focused on formal methods, since this is distinctive for
high assurance, and showed that there are many tools in this space
(both research and industrial-strength).
Some of these tools are ACL2, HOL 4, Isabelle, Otter, ProofPower, ZETA, CZT,
Spin, NuSMV, BLAST, and Alloy.
(I find ACL2, Otter, Spin, BitC, and Alloy especially interesting,
though with different reasons for each.)

<p>
In contrast, few high assurance components are FLOSS.
After looking at the options, the most likely reason for this appears to
be that decision-makers are not even considering the possibility of
FLOSS-based approaches.
Decision-makers should consider FLOSS-based approaches
as future high assurance components are needed,
including the possibility of creating consortia,
so that a FLOSS-based strategy can be chosen where appropriate.

<p>
Governments should <i>require</i> that government-funded research
<i>normally</i> release all software it develops under a FLOSS license,
unless the government is convinced that in this particular case there is
a better alternative.
This is amply demonstrated by the many discarded and underused
proprietary projects in these fields; the waste and
lost opportunity alone is enough to justify this.
More fundamentally,
governments use money from their people to do research; it is only fair to
ensure that all the people (who pay for the research) can reap the benefits,
unless they will be better served some other way.
If software research results are FLOSS, then
anyone can start with what was developed and build on it,
instead of starting over.
<a href="http://www.oreillynet.com/pub/a/network/2002/01/11/openinfo.html">
The Open Informatics Petition</a> text goes farther
and suggests that governments simply mandate this no matter what.
I will not go that far; I think there are good reasons for cases where
government-funded research should not be released as FLOSS, but
the petition does explain in more detail the advantages of this approach.
Most detractors are concerned about this being required on <i>all</i> research.
If you simply say that FLOSS is a wiser
&#8220;default&#8221;, I think a much better balance is struck than
the current system in many countries.
<a href="http://openinformatics.sourceforge.net/">The Open Informatics
web site</a> has more information.
<!-- Well-written disagreeing position:
http://mstation.org/solveig_singleton_linux.php

More about Open Informatics Petition:
http://www.sciencemag.org/cgi/content/full/294/5540/27a
-->
Requiring FLOSS release does not prevent commercialization; there are many
FLOSS-based businesses, and many FLOSS licenses permit adding extensions and
making the result proprietary.
<!-- Discuss Bayh-Dole? -->
The many success stories from FLOSS-based approaches (e.g., ACL2,
Security-Enhanced Linux, etc.) suggest that releasing software under
FLOSS licenses is a <i>very</i> effective way to improve tech transition
and establish sustainable research.
<!--
http://www.csis.org/tech/it/#oss
The CSIS 2006 survey (updated January 2006) very interesting, not that
granular.
-->
Since the GPL is the most common FLOSS license
(in formal methods tools and in general),
whatever FLOSS license is used in this case should at least be
<a href="https://dwheeler.com/essays/gpl-compatible.html">
GPL-compatible</a> -- that way, research efforts can be combined
into larger works as needed.
For the same reason, I would recommend that
one of the &#8220;classic&#8221; FLOSS licenses be used in most cases
(i.e., MIT, BSD-new, LGPL, or GPL) -- they are well-understood,
and can be combined as necessary.
<p>
Finally, developers who want to start new FLOSS projects should
consider developing or improving high-assurance components or tools
(including tools that combine other tools).
Improving the user interfaces, capabilities, or integration of tools
would be very valuable.
Sample assured components, especially ones that are useful
(like separation kernels or RTOSs) would be of value too, both for
potential users and for others developing such programs
(because there are few publicly-available examples that people can
experiment with and learn from).
These are technically interesting, and given the increasing attacks and
dependence on computer systems, having more high assurance
programs available will be vital to everyone&#8217;s future.

<p>
<hr>
<p>
<a href="https://dwheeler.com">You can see David A. Wheeler&#8217;s home page, https://dwheeler.com</a>
<p>
You should also see the
<a href="http://www.openproofs.org">Open proofs web site</a>.

</body>
</html>

<!--
Other tools:

ACL2 Sedan (acl2s):
Access ACL2 through Eclipse, adds a termination detector.
Do not know if it's OSS.
http://acl2s.peterd.org/acl2s/doc/

"Sunrise" hooks a small imperitive language to HOL:
 http://www.cis.upenn.edu/~hol/sunrise/
but it's not FLOSS, it's noncommercial-use only; in fact,
there's even a patent claim!
(Yet another example of patent misuse; this stuff is decades old.)

PhoX/Phox tries to be an easy-to-use higher-order math theorem prover:
http://www.lama.univ-savoie.fr/sitelama/Membres/pages_web/RAFFALLI/phox.html#distrib
But can't find any license info.  I sent email 2008-05-03 asking for info.

LEGO Proof assistant:
http://www.dcs.ed.ac.uk/home/lego/
Having trouble finding license info, and in any case it appears
to be a completely dead project (last release November 1998).

SOURCE CODE, but NO LICENSE INFO; Debian package seems to be local so
it has NOT been verified as OSS
(have emailed to schwicht[at]math.lmu.de):
<li>
Schwichtenberg's
<a href="http://www.mathematik.uni-muenchen.de/~minlog/">Minlog</a>
"is an interactive proof system developed by Helmut Schwichtenberg and members of the logic group at the University of Munich.
MINLOG is based on first order natural deduction calculus. It is intended to reason about computable functionals, using minimal rather than classical or intuitionistic logic. The main motivation behind MINLOG is to exploit the proofs-as-programs paradigm for program development and program verification. Proofs are in fact treated as first class objects which can be normalized. If a formula is existential then its proof can be used for reading off an instance of it, or changed appropriately for program development by proof transformation. To this end MINLOG is equipped with tools to extract functional programs directly from proof terms. This also applies to non-constructive proofs, using a refined A-translation. The system is supported by automatic proof search and normalization by evaluation as an efficient term rewriting device...
MINLOG is implemented in SCHEME and runs under every SCHEME version supporting the Revised5 Report on the Algorithmic Language Scheme."
<p>
There are Debian packages for Minlog.
<p>
Warning:
There's another MINLOG, which is
a theorem prover for propositional logic, by John Slaney at Australian National University.

http://users.rsise.anu.edu.au/~jks/software/
Minlog by John Slaney - NOT OSS (forbids commercial use)
"it is no longer supported and its use is not recommended."
(comments suggest it doesn't work well anyway)

-->

<!-- My thanks to Clyde Roby, who reviewed this on 2006-05-03.  -->
<!-- My thanks to LDW11@aol.com, who reviewed this on 2006-05-28 and said,
I read your paper on High Assurance FLOSS.
To me, it was a very good paper and I enjoyed reading it
(unfortunately it took a while for me to get to it, but I'm
sure you can relate).  There are some comments intersperced below -
I put them in red and started them with "***"  One general comment, you
may want to dedicate a section on how one would go about getting something
FLOSSed and the pitfalls (you cover the reasons why very well, and some
of the pitfalls are described here and there throughout the paper).
It would give the next step to someone reading the paper and saying,
"hey, good idea, but I don't have the foggiest idea where to start."



-->

