<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html lang="en-US">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<title>How to Prevent the next Heartbleed</title>
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="description" content="This paper focuses on tools and techniques that would prevent the next Heartbleed-like vulnerability.">
<meta name="keywords" content="Heartbleed, OpenSSL, SSL, TLS, security, vulnerability, tools, techniques, buffer overflow, buffer over-read, fuzzing, fuzz testing, countermeasures, writing secure software, CVE-2014-0160, buffer overread, disaster, learning from disaster">
<meta name="generator" content="vim">
<link rel="stylesheet" type="text/css" href="paper.css">
</head>

<body bgcolor="#FFFFFF">

<h1 class="title">How to Prevent the next Heartbleed</h1>
<h2 class="author"><a href="https://dwheeler.com">David A. Wheeler</a></h2>
<h2 class="date">2024-04-24 (originally 2014-04-29)</h2>

<p>
This paper analyzes the
<a href="http://heartbleed.com/">Heartbleed vulnerability</a>
(<a href="https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2014-0160">CVE-2014-0160</a>) in OpenSSL found in 2014.
After an <a href="#introduction">introduction</a> and
a discussion of
<a href="#why-not-found">why it wasn&#8217;t found earlier</a>,
this paper focuses on
<a href="#counter">identifying and discussing countermeasures that could
have countered Heartbleed-like vulnerabilities</a>.
The paper also discusses
<a href="#preconditions">preconditions</a>,
<a href="#reduce-impact">what would reduce the impact of Heartbleed-like vulnerabilities?</a>,
<a href="#applying">applying these approaches</a>,
and
<a href="#exemplars">exemplars</a>.
It ends with
<a href="#conclusions">conclusions and recommendations</a>.
My hope is that developers can learn to apply
at least some of these techniques in the future to prevent or reduce the
impact of future problems.
This paper is part of my
essay suite <a href="learning-from-disaster.html">Learning from Disaster</a>.

<h1 id="introduction">Introduction</h1>

<img align="right" class="logo" width="85" height="103" src="heartbleed.png" alt="Heartbleed Bug">
<!--
Heartbleed logo is from http://heartbleed.com/
"Heartbleed logo is free to use, rights waived via
<a href="http://creativecommons.org/publicdomain/zero/1.0/">CC0</a>."
-->
<p>
The <a href="http://heartbleed.com/">Heartbleed vulnerability</a>
is a serious security vulnerability formally identified as
<a href="https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2014-0160">CVE-2014-0160</a>
[<a href="http://heartbleed.com/">Heartbleed.com</a>]
and described in
<a href="http://www.kb.cert.org/vuls/id/720951">CERT Vulnerability Note VU#720951</a>.
Heartbleed is a vulnerability in OpenSSL,
a widely-used toolkit that implements
the cryptographic protocol Secure Sockets Layer (SSL) and its successor the
Transport Layer Security (TLS).
When you use a web browser with an &#8220;https://&#8221; URL
you are using SSL/TLS, and in many cases at least one side
(the client or server) uses OpenSSL.
The XKCD cartoon
<a href="http://xkcd.com/1354/">Heartbleed Explanation</a>
is a great explanation that shows how the vulnerability can be exploited
[<a href="http://xkcd.com/1354/">XKCD</a>], pointing out
that it is remarkably easy to exploit.
<p>
The impact of the Heartbleed vulnerability was unusually large.
<a href="http://www.cnet.com/how-to/which-sites-have-patched-the-heartbleed-bug/">Heartbleed affected a huge number of popular websites</a>, including
Google, YouTube, Yahoo!, Pinterest, Blogspot, Instagram,
Tumblr, Reddit, Netflix, Stack Overflow, Slate, GitHub, Yelp, Etsy,
the U.S. Postal Service (USPS), Blogger, Dropbox, Wikipedia,
and the Washington Post.
<!--
In some cases there seems to be a dispute.
CNet reports Facebook and Bing as vulnerable, while Durumeric2014
reports that their HTTPS sites were not vulnerable.
I've removed the ones in dispute; what's left is still pretty large,
and there are many others I have not listed that are not in dispute.
-->
<a href="http://www.mumsnet.com/features/mumsnet-and-heartbleed-as-it-happened">UK parenting site Mumsnet (with 1.5 million registered users) had several user accounts hijacked and its CEO was impersonated</a>.
A breach at Community Health Systems (CHS), initially via Heartbleed,
led to an information compromise that affected an
estimated 4.5 million patients
[<a href="https://www.trustedsec.com/august-2014/chs-hacked-heartbleed-exclusive-trustedsec/">TrustedSec</a>]
[<a href="http://www.csoonline.com/article/2466726/data-protection/heartbleed-to-blame-for-community-health-systems-breach.html">Ragan2014</a>].
One paper stated that
&#8220;Heartbleed&#8217;s severe risks, widespread impact, and costly global
cleanup qualify it as a security disaster&#8221;
[<a href="https://jhalderm.com/pub/papers/heartbleed-imc14.pdf">Durumeric2014</a>].
<a href="https://www.schneier.com/blog/archives/2014/04/heartbleed.html">Bruce Schneier put it succinctly</a>:
&#8220;On the scale of 1 to 10, this is an 11&#8221;
[<a href="https://www.schneier.com/blog/archives/2014/04/heartbleed.html">Schneier2014</a>].
<p>
Google and Codenomicon independently found and reported 
this vulnerability at close to the same time.
Rita Mailheau reports,
based on work by Ben Grubb from the <i>Sidney Morning Herald</i>,
that Neel Mehta and his team from Google Security discovered Heartbleed
on 2014-03-21 during a source code review,
and that engineers at Finnish company Codenomicon
(Antti Karjalainen, Riku Hietamäki, and Matti Kamunen)
separately discovered Heartbleed on 2014-04-02
using a new extension (called Safeguard) in their Defensics fuzz testing tool
[<a href="http://ddosattackprotection.org/blog/heartbleed-bug/">Mailheau</a>].
There is strong evidence that no attacker conducted widespread
scanning for vulnerable servers before the public revelation of Heartbleed
on 2014-04-07, since no Heartbleed-basd attacks were found before that date
in four large network data taps
(though it is possible that targeted attacks occurred before then)
[<a href="http://threatpost.com/research-finds-no-large-scale-heartbleed-exploit-attempts-before-vulnerability-disclosure/108161">Fisher2014</a>]
[<a href="https://jhalderm.com/pub/papers/heartbleed-imc14.pdf">Durumeric2014</a>].
The US government has publicly noted that it
&#8220;had no prior knowledge of the existence of Heartbleed&#8221;
[<a href="https://www.whitehouse.gov/blog/2014/04/28/heartbleed-understanding-when-we-disclose-cyber-vulnerabilities">[WhiteHouse2014]</a>].

<p>
A key reason for Heartbleed&#8217;s large impact was that
many widely-used tools and techniques for finding such defects
did <i>not</i> find Heartbleed.
This paper discusses
<a href="#counter">specific tools and techniques that could have
detected or countered Heartbleed, and vulnerabilities like it,
ahead-of-time</a>.
I will first briefly examine
<a href="#why-not-found">why many tools and techniques
did <i>not</i> find it</a>,
since it&#8217;s important to understand why many common techniques
didn&#8217;t work.
I will also briefly cover
<a href="#preconditions">preconditions</a>,
<a href="#reduce-impact">impact reduction</a>,
<a href="#applying">applying these approaches</a>,
<a href="#exemplars">exemplars</a>, and
<a href="#conclusions">conclusions</a>.
This paper does not describe how to write secure software in general;
for that, see my book
<a href="https://dwheeler.com/secure-programs/"><i>Secure Programming for Linux and Unix HOWTO</i></a>
[<a href="https://dwheeler.com/secure-programs/">Wheeler2004</a>]
or other such works.
I think the most important approach for developing secure software is to
<a href="#simplify-code">simplify
the code so it is obviously correct</a>,
including avoiding common weaknesses, and then limit
privileges to reduce potential damage.
However, here I will focus on ways to detect vulnerabilities,
since even the best developers make mistakes that lead to vulnerabilities.
This paper presumes you already understand how to develop software, and
is part of the larger
essay suite <a href="learning-from-disaster.html">Learning from Disaster</a>.

<p>
If you&#8217;re in a hurry, you can jump directly to the
<a href="#conclusions">conclusions</a>.

<p>
My goal is to help prevent similar vulnerabilities
by helping projects improve how they develop secure software.
As the fictional character Mazer Rackham says in
Orson Scott Card&#8217;s <i>Ender&#8217;s Game</i>,
&#8220;there is no teacher but the enemy...
only the enemy shows you where you are weak&#8221;.
Let&#8217;s learn from this vulnerability how we can <i>avoid</i> similar
vulnerabilities in the future.


<h1 id="why-not-found">Why wasn&#8217;t this vulnerability found earlier?</h1>

<p>
There are many detailed explanations of why the code was vulnerable, e.g.,
[<a href="http://blog.existentialize.com/diagnosis-of-the-openssl-heartbleed-bug.html">Cassidy2014</a>].
However, for our purposes we only need to focus on the broader technical
reasons that this vulnerability existed and stayed undetected for so long.
<p>
This OpenSSL vulnerability was caused by well-known general weaknesses
(a <i><a href="http://cwe.mitre.org/documents/glossary/index.html#Weakness">weakness</a></i> is basically a type of potential <a href="http://cwe.mitre.org/documents/glossary/index.html#Vulnerability">vulnerability</a>).
The key weakness can be classified as a buffer over-read
(<a href="http://cwe.mitre.org/data/definitions/126.html">CWE-126</a>)
in the heap,
which could happen because of improper input validation
(<a href="http://cwe.mitre.org/data/definitions/20.html">CWE-20</a>)
of a <a href="http://tools.ietf.org/html/rfc6520">heartbeat request message</a>.
CWE-126 is a special case of an &#8220;out-of-bounds read&#8221;
(<a href="http://cwe.mitre.org/data/definitions/125.html">CWE-125</a>),
which itself is a special case of
&#8220;improper restriction of operations within the bounds of a memory buffer&#8221;
aka &#8220;improper restriction&#8221;
(<a href="http://cwe.mitre.org/data/definitions/119.html">CWE-119</a>).
These are really well-known weaknesses; many tools <i>specifically</i>
look for improper restriction of operations
within the bounds of a memory buffer.
OpenSSL is routinely examined by many tools, too.

<p>
It's also important to note that <i>fixing</i> this vulnerability was
relatively trivial.
<a href="https://github.com/openssl/openssl/commit/731f431497f463f3a2a97236fe0187b11c44aead">Commit 731f4314...</a> fixed the vulnerability with only a few lines of code (its discussion takes far more space than the code changes;
<a href="https://stackoverflow.com/questions/25035243/how-do-i-interpret-and-make-use-of-github-commit-pages/25035244#25035244"
>here's a discussion of how to read pages like this</a>).
In short, we have a well-known kind of vulnerability that is easy to
fix <i>if</i> it is found.

<p>
Kupsch and Miller specifically examined
the Heartbleed vulnerability and identified several reasons
this vulnerability was not found sooner, even though people and
tools were specifically looking for vulnerabilities like it
[<a href="https://continuousassurance.org/swamp/SWAMP-Heartbleed.pdf">Kupsch2014-May</a>].
They even noted that,
&#8220;Heartbleed created a significant challenge for
current software assurance tools,
and we do not know of any such tools that were able to discover
the Heartbleed vulnerability at the time of announcement.&#8221;
Here I will emphasize a few of its points and add a few points
of my own.

<p>
Please note that I am focusing on the technical aspects here.
<a href="http://veridicalsystems.com/blog/of-money-responsibility-and-pride/">&#8220;Of Money, Responsibility, and Pride&#8221; by Steve Marquess</a>
discusses how the OpenSSL work has been funded in the past
(primarily through work-for-hire contracts).
They did have some funding, and were
even turning away money in a number of cases (see the essay for more).
But at the time of Heartbleed
there was only one person working on OpenSSL full-time,
in spite of the importance of OpenSSL.
Since that time, the
<a href="https://www.coreinfrastructure.org/">Core Infrastructure Initiative
(CII)</a> has invested money in OpenSSL, and things have gotten better.
Obviously money matters, and I'm not discounting that, but
many other authors have discussed funding.
In this essay, I'm focusing on the
technical aspects of Heartbleed (and what we can learn from those aspects).

<p>
But first, a few quick comments on terminology:
<ul>
<li><i>Overflow</i>. Many people, include MITRE&#8217;s CWE,
use the term &#8220;buffer overflow&#8221; to
<i>only</i> mean over-writes (writing outside the buffer region),
and often use the term even more narrowly to only mean
copying information to write past the end of a buffer
(see
<a href="http://cwe.mitre.org/data/definitions/120.html">CWE-120</a>).
Some other people use the term &#8220;buffer overflow&#8221; to mean
<i>either</i> buffer over-read or buffer over-write, and use the
more precise term <i>buffer over-write</i> to specifically mean a write.
This matters because the Heartbleed vulnerability allowed
improperly <i>reading</i> data, instead of the more common
problem of allowing improper <i>writing</i>.
I have tried to write this text so that it will be clear
no matter which meaning you choose.
Heartbleed was an over-read in a buffer stored in the heap.
<li><i>TOE</i> or <i>SUT</i>.
We need some term for the software we are evaluating.
One common term is the <i>Target of Evaluation</i> (TOE); this is the term
used by the Common Criteria (ISO/IEC 15408).
Another term is <i>System Under Test</i> (SUT).
The word &#8220;test&#8221; often implies that you are executing the program,
and not all evaluation processes do this, so I will use the term
TOE instead for consistency.
</ul>


<h2 id="static-not-found">Static analysis</h2>

<p>
Static analysis tools work without executing the program.
The most commonly-discussed type of static analysis tools for finding
vulnerabilities are variously called
<i>source code weakness analyzers</i>,
<i>source code security analyzers</i>,
<i>static application security testing</i> (SAST) tools,
<i>static analysis code scanners</i>,
or <i>code weakness analysis tools</i>.
A source code weakness analyzer searches for vulnerabilities using
various kinds of pattern matches (e.g., they may do
<i>taint checking</i> to track data from untrusted sources
to see if they are sent to potentially-dangerous operations).
There are various reports that evaluate these tools, e.g.,
[<a href="http://infoscience.epfl.ch/record/153107/files/ESSCAT-report">Hofer2010</a>].

<p>
However, it&#8217;s known that many widely-used static analysis tools
would <i>not</i> have found this vulnerability ahead-of-time:
<ol>
<li>Coverity: Coverity would <i>not</i> have found it ahead-of-time. They are currently working to improve their tool so it will find similar vulnerabilities in the future, using some very interesting new heuristics [<a href="http://security.coverity.com/blog/2014/Apr/on-detecting-heartbleed-with-static-analysis.html">Chou2014</a>].
<li>HP/Fortify: HP/Fortify has posted several public statements about
Heartbleed, but I have not found <i>any</i> claims that their static
analysis tool would have found this vulnerability ahead-of-time.  They did
modify their dynamic suite to test for the vulnerability once it was
publicly known, but that is not the same as detecting it ahead-of-time.
Their lack of claims, when specifically discussing it, lead me to
believe that their tool would not have found Heartbleed ahead-of-time.
<li>Klocwork: Klocwork would not have detected this vulnerability in its normal configuration [<a href="http://www.klocwork.com/blog/software-security/saving-you-from-heartbleed/">Sarkar2014</a>].
<li>Grammatech: Grammatech&#8217;s CodeSonar also could not detect
this vulnerability.
They are also working on experimental improvements
that would find vulnerabilities like it in the future
(their approach involves
a new warning class called <i>Tainted Buffer Access</i>
as well as extensions to their taint propagation algorithm)
[<a href="http://www.grammatech.com/blog/finding-heartbleed-with-codesonar">Anderson2014</a>].
<li>GrammaTech could do the taint analysis starting at socket buffers,
but didn't do it because it was too slow in practice.
When they turned it on for the right section of code, it found
the problem.
I don't think that counts;
if you already know where the problem is, then you
don't need a tool to find it.
<!-- 2016-07-12: GrammaTech report via the SWAMP developers. -->
</ol> 

<p>
The <i>only</i> static analysis tool I've found so far that existed at the
time, and was able to find Heartbleed ahead-of-time without a non-standard or
specialized configuration, is the FLOSS tool CQual++.
CQual++ is a polymorphic whole-program dataflow analysis tool for C++,
inspired by Jeff Foster's Cqual tool (indeed, it uses the same backend solver).
Although CQual++ focuses on C++, it is also able to analyze C programs
(including OpenSSL).
CQual++ is the main tool provided by Oink/Elsa
(Oink is a collaboration of C++ static analysis tools;
Elsa is the front-end for Oink).
<a href="http://dsw.users.sonic.net/oink/index.html">Daniel S. Wilkerson
reported in Oink documentation</a> that
"After the Heartbleed bug came out, someone at a government lab that will
not let me use their name wrote me (initially on 18 April 2014), saying:
Yes, you are interpreting me correctly.
CQual++ found Heartbleed while the [proprietary] tools I tried did not."
The paper
<a href="http://people.eecs.berkeley.edu/~daw/papers/fmtstr-plas07.pdf">"Large-Scale Analysis of Format String Vulnerabilities in Debian" by Karl Chen
and David Wagner</a> suggests that this toolsuite can be effective
at detecting vulnerabilities.
However, the same reporter may have also made it clear why CQual++ was
not used to find the problem first:
"I also applied CQual++ to an
important internal project and found it very effective (though a bit
difficult to run and interpret) at identifying places where sanitization
routines weren't being called consistently."

<p>
A fundamental issue is that most of these tools do not guarantee to find
all vulnerabilities; most do not even guarantee to find vulnerabilities
of any particular kind.
Sadly, the terminology about this is confusing, so I will
first need to clarify the terminology.

<p>
In this paper I will call a software analysis tool <i>incomplete</i>
if the tool does not necessarily find all vulnerabilities (of a given kind)
in the software being analyzed.
Previous versions of this paper, and many people,
use the term <i>unsound</i> instead to describe
tools that look for vulnerabilities (aka <i>bug-finders</i>)
that do not claim to find all vulnerabilities.
For example, Bessey et al discuss Coverity&#8217;s static analysis tool
and say,
&#8220;like the PREfix product, we were also <i>unsound</i>.
Our product did not verify the absence of errors but
rather tried to find as many of them as possible...
Circa 2000, unsoundness was controversial in the research community,
though it has since become almost a de facto tool bias
for commercial products and many research projects...&#8221;
[<a href="http://cacm.acm.org/magazines/2010/2/69354-a-few-billion-lines-of-code-later/fulltext">Bessey2010</a>]
This term <i>unsound</i> can cause confusion,
because people who develop or use <i>program checkers</i>
use the term <i>unsound</i> with a different meaning.
One blog post explains why the same
term seems to have two conflicting meanings:
&#8220;most program checkers prove theorems about programs.
In particular,
most aim to prove programs correct in some respect (e.g. type safety).
A theorem prover is sound [if and only if]
all the theorems it proves are true...
People in the program-checking field are accustomed to this,
so they habitually think soundness [means] proving the absence of bugs.
But a bug-finder doesn&#8217;t aim to prove correctness.
Instead, it aims to prove incorrectness: to prove the presence of bugs.
It&#8217;s sound [if and only if] all the bugs it reports are real bugs -
that is, if it has no false positives.
False negatives (overlooking bugs) are OK,
because they don&#8217;t make its claims incorrect.&#8221;
[<a href="http://arcanesentiment.blogspot.se/2014/04/a-sound-bug-finder-is-unsound.html">ArcaneSentiment2014</a>]

<p>
I have adopted the
<a href="http://samate.nist.gov/SATE5OckhamCriteria.html">NIST SAMATE SATE V Ockham Sound Analysis Criteria</a> 
[<a href="http://samate.nist.gov/SATE5OckhamCriteria.html">NIST-Sound</a>]
in this paper to eliminate this confusion.
In the NIST SAMATE terminology,
tools that do not guarantee to find all vulnerabilities (of any particular
kind) are termed <i>incomplete</i>.
Here&#8217;s how NIST differentiates between soundness and completeness:
&#8220;a <b>site</b> is a location in code where a weakness might occur.
A <b>buggy site</b> is one that has an instance of the weakness,
that is, there is some input that will cause a violation.
A <b>non-buggy site</b> is one that does not have
an instance of the weakness, in other words, is safe or not vulnerable...
A <b>finding</b> is a definitive report about a site.
In other words, that the site has a specific weakness (is buggy)
or that the site does not have a specific weakness (is not buggy)...
<b>Sound</b> means every finding is correct.
[A sound] tool need not produce a finding for every site;
that is <b>completeness</b>&#8221;
[<a href="http://samate.nist.gov/SATE5OckhamCriteria.html">NIST-Sound</a>].

<p>
Why are so many source code weakness analyzers incomplete?
First, most programming languages are not designed to be easy to analyze,
making it more difficult to analyze programs in general.
Second, most software is not written to make it easy for static analyzers
to analyze them.
As a result, <i>complete</i> analysis tools may require a lot of
human help to apply to existing programs.
In contrast, incomplete analysis tools can be applied immediately
to existing programs.
They manage this by using
heuristics to help them identify likely vulnerabilities and
complete their analysis within useful times.
However, this presents a major caveat: incomplete
source code weakness analyzers often miss vulnerabilities.

<p>
Clearly Heartbleed is one of those cases where these incomplete
heuristics led to a failure by many static analysis tools
to find an important vulnerability.
The fundamental reason they all failed to find the vulnerability is
that the OpenSSL code is extremely complex; it includes multiple levels
of indirection and other issues that simply exceeded these tools&#8217;
abilities to find the vulnerability.
Developers should <a href="#simplify-code">simplify the code</a>
(e.g., through refactoring) to make it easier for tools and humans
to analyze the program, as I discuss further later.
A partial and deeper reason is that the programming languages
C, C++, and Objective-C are notoriously difficult to statically analyze;
constructs like pointers (and especially function pointers) can be
difficult to statically manage.

<p>
This does <i>not</i> mean that static analyzers are useless.
Static analyzers can examine how the software will behave under a large
number of possible inputs (as compared to dynamic analysis),
and the tool heuristics often limit the number
of false positives (reports of vulnerabilities that are not vulnerabilities).
But - and this is the important point -
the heuristics used by incomplete static analysis tools
sometimes result in a failure to detect important vulnerabilities.

<!--
Here is the Fortify link I was mentioning in my last email:
http://h30499.www3.hp.com/t5/HP-Security-Research-Blog/HPSR-Software-security-content-update-Heartbleed-bug-detection/ba-p/6445654#.U0wwwhtOVId 

Their algorithm did not detect this buffer overflow, but they added a detection means to WebInspect quickly and announced it to all customers. 


I can confirm that Coverity’s static weakness analyzer could *NOT* have found the Heartbleed vulnerability ahead-of-time.  They *HAVE* prototyped a possible modification to their tool that would detect it and vulnerabilities very similar to its structure.  Details here:

http://security.coverity.com/blog/2014/Apr/on-detecting-heartbleed-with-static-analysis.html

http://blog.regehr.org/archives/1125

http://blog.regehr.org/archives/1128


I *cannot* find any evidence that HP Fortify’s static weakness analyzer would have found the vulnerability ahead-of-time.  HP has suggested that Heartbleed is a good example of why people should use their tool, but their statements pointedly do *NOT* claim or even imply that that their tool would have found this particular vulnerability ahead-of-time.  Fortify has invited the OpenSSL project to join their “Fortify Open Source Review” project, and has determined that none of their servers use a vulnerable library, but neither of those statements actually state that they WOULD have found it.  Their formal statement is here:

http://h30499.www3.hp.com/t5/Fortify-Application-Security/Fortify-on-Demand-Heartbleed-Update/ba-p/6444528 

More technical comments are posted in a separate article.  In this article, they state that they have modified their “Fortify on Demand” to test specifically for Heartbleed.  However, that suite includes dynamic tests, and it’s easy to create a dynamic test for this specific vulnerability *once* you know it exists.  This raises doubts that they could detect it ahead-of-time:

http://h30499.www3.hp.com/t5/Fortify-Application-Security/Thoughts-on-the-Heartbleed-Bug/ba-p/6442708

The website “Y Combinator” has a posting claiming to be from ScottBurson who says, “I can confirm that HP Fortify SCA does not find this bug. (I am the architect for the SCA team.)”  Unfortunately, I haven’t been able to verify the validity of this posting, so it’s not clear that this is true: 

  https://news.ycombinator.com/item?id=7571506

Basically, I find *no* evidence that HP/Fortify *would* have found the Heartbleed vulnerability,  and *some* evidence (though of questionable reliability) that it would not have found it.

 

Klocwork would *NOT* have found this vulnerability by default, though with additional skilled configuration specific to this specific program it could have found it.  As they describe it, “The difficulty here is the n2s() macro [defined in OpenSSL and] used to extract payload from p… This macro effectively “hides” the propagation of data through the function…”  More detail here:

http://www.klocwork.com/blog/software-security/saving-you-from-heartbleed/
 

The vulnerable software (OpenSSL) has been analyzed by a variety of static analysis tools in the past, and probably has been evaluated by many over the last 2 years.  E.g., here’s an example (though this discussion covers an older version, 0.9.8 (1.0.1 through 1.0.1f (inclusive) are vulnerable), which even states that it is overall a “quality product”: http://www.viva64.com/en/b/0183/   It’s *NOT* the case that no one has analyzed the program.   Indeed, the 2-year delay between insertion and detection of the vulnerability suggests that *many* static analysis tools probably did not detect this particular vulnerability.

 
-->


<h2 id="dynamic-not-found">Dynamic analysis</h2>

<p>
Dynamic approaches involve running the program with specific inputs
and trying to find vulnerabilities.
<p>
A limitation of dynamic approaches is that it&#8217;s impossible
to fully test any program in human-relevant timetables.
For example, a trivial program that adds two 64-bit integers
has 2<sup>128</sup> possible inputs.
Testing all inputs
(assuming a 4GHz processor and 5 cycles to test each input)
would require 13.5 sextillion years (1.35 x 10<sup>22</sup> years).
Even massively-parallel computing does not really help.
Real programs, of course, have far more complex inputs than this!
Thus, dynamic approaches cannot show that a program is secure in
a strong sense; all they can show is the absence of vulnerabilities
with the tests that were used.

<p>
But this does <i>not</i> mean that dynamic approaches are useless.
Dynamic approaches can be a very useful way to improve security,
as long as their limitations are understood.
Of course, dynamic approaches (aka <i>software testing</i>) is a useful
approach for finding defects.
A general introduction to software testing not specific to security
is available in
<i>Introduction to Software Testing</i> by
Paul Ammann and Jeff Offutt
[<a href="http://cs.gmu.edu/~offutt/softwaretest/">Ammann2008</a>].

<p>
Let me discuss two areas that are widely used,
but would fail to find Heartbleed: a mostly-positive test suite and
traditionally-applied fuzzers.

<h3 id="mostly-positive"><span id="typical-testing">Mostly-positive automated test suites</span></h3>

<p>
One approach is to create a big automated test suite.
Eric S. Raymond and some others have been discussing Heartbleed, and
in our discussion he stated that,
&#8220;I think a lot of people have an intuition
that test suites don&#8217;t work
very well... What I&#8217;ve learned since is that the gap is relatively
narrow - pushing conventional methods hard enough can get you pretty
close to never-break&#8221;.
I completely agree with him that a good automated regression test suite
is powerful, especially for non-security defects.
If you don&#8217;t have one, create one, full stop, we agree.

<p>
However, whether or not a test suite would have found the Heartbleed
vulnerability depends on <i>how</i> you create this test suite.
The way many developers create test suites, which produce
which I call &#8220;mostly-positive&#8221; test suites,
would probably <i>not</i> have found Heartbleed.
I will later discuss
<a href="#negative-testing">negative testing,
a testing approach that <b>would</b> have worked</a>,
but we first need to understand why common testing approaches fail.

<p>
Many developers and organizations almost exclusively create tests for
what should happen with <i>correct</i> input.
This makes sense, when you think about it; normal users
will complain if a program doesn&#8217;t produce the correct output
when given correct input, and most users do not probe what the program
does with incorrect input.
If your sole goal is to quickly identify problems that users would complain
about in everyday use, mostly-positive testing works.
Besides, many software developers have a bias to focus on
making the program work with correct input,
and at most try to handle some error conditions they can easily foresee,
so they have a natural tendency to create tests with correct input.
Many developers simply don&#8217;t
think about what happens when an <i>attacker</i>
sends input that is carefully crafted to exploit a program.

<p>
I will call the approach of primarily creating tests
for what should happen with correct input a
<i>mostly-positive test suite</i>.
Unfortunately, in many cases today&#8217;s
software regression test suites are mostly-positive.
Two widely-practiced test approaches
typically focus on creating mostly-positive test suites:
<ol>
<li><a href="https://en.wikipedia.org/wiki/Test-driven_development"><i>Test-driven development (TDD)</i></a>
is a software development process in which
the developer &#8220;writes an (initially failing) automated test case
that defines a desired improvement or new function,
then produces the minimum amount of code to pass that test,
and finally refactors the new code to acceptable standards&#8221;
[<a href="https://en.wikipedia.org/w/index.php?title=Test-driven_development&amp;oldid=602568836">Wikipedia-TDD</a>].
In nearly all cases the TDD literature emphasizes creating
tests for describe what a modified function <i>should</i> do,
not what they should <i>not</i> do, and many TDD materials do not
even mention creating negative tests.
A developer <i>could</i> create negative tests while implementing TDD,
but this is unusual in practice for those using TDD.
<li><i>Interoperability testing</i> is a system testing process
where different implementations
of a standard are connected together to determine if they can
connect and interoperate (by exchanging data).
Interoperability testing is great for helping developers
correctly implement a standard protocol (such as SSL/TLS).
However, the other implementations are
also trying to <i>comply</i> with the specification, so the other
implementation usually will not test for
&#8220;what should not happen&#8221;.
</ol>

<p>
Mostly-positive testing is practically useless for secure software.
Mostly-positive testing generally isn&#8217;t testing for the right thing!
In the Heartbleed attack, like most attacks,
the attacker sends data in a form <i>not</i> sent in normal use.
TDD and interoperability testing are good things... but you typically
need to augment them if your goal is secure software.

<p id="code-coverage">
<i>Code coverage</i> tools as typically used would not have helped either.
Some developers may, in addition, run code coverage
tools to see what wasn&#8217;t tested, and then
add additional tests so that
a larger percent of the code is covered by tests.
Code coverage tools are actually hybrids of static and dynamic
analysis, but for purposes of this paper we will discuss them here.
The key questions with code coverage are
(1) what specific code coverage measurement(s) are used, and
(2) what are their minimum value(s)?
These vary, but in practice
many people are happy with a test suite that only tests 80%-90% of the
code when measured as statements or branches (aka <i>decisions</i>).
A very few might press all the way up to 100% coverage of
statements or branches.
Some, particularly those in the safety community, may use
a slightly more rigorous coverage measure such as
<a href="https://en.wikipedia.org/wiki/Modified_Condition/Decision_Coverage">modified condition/decision coverage (MC/DC)</a>.
Other coverage measures are possible, but these are the common ones.
Test coverage tools for these common measures have some security value,
for example, they can sometimes detect malicious software
that is waiting for a trigger
(since tests will often not include the trigger).
They can also check if exception handlers seem to run correctly.
But even 100% coverage, as measured by typical code coverage tools,
would not have been enough to counter Heartbleed.
The Heartbleed vulnerability involved the <i>absence</i> of
proper input validation.
Fundamentally, a code coverage tool as typically used
cannot notice <i>missing</i> code;
it can only notice existing statements or branches that are untested.

<p>
It is not clear that a less-common approach, <i>program mutation testing</i>,
would have worked either.
Mutation testing is a different coverage measure and way
to develop new tests.
As applied to programs, in mutation testing
you &#8220;mutate&#8221; a source program, using a set of mutation operators,
to create &#8220;mutants&#8221;.
If a test can detect a difference between a mutant and the original,
then the mutant is &#8220;killed&#8221;.
This would <i>not</i> have detected the lack of input validation,
because there was no input validation test to mutate.
It is conceivable that it might have found the buffer over-read,
and there has been some research work on using program
mutation testing to detect vulnerabilities
(e.g., 
[<a href="http://qspace.library.queensu.ca/handle/1974/1359">Shahriar2008</a>]).
You can also mutate input data structures and use those mutated inputs
as tests.
Mutating input data structures
certainly <i>could</i> find the Heartbleed vulnerability.
Two approaches I discuss later
(<a href="#negative-testing">thorough negative testing</a> and
<a href="#fuzzing-check-standard">fuzzing with address checking and standard memory allocator</a>)
can be viewed as ways to apply mutation testing to input data structures.

<p>
I should note that this is not unique to OpenSSL.
<a href="https://web.nvd.nist.gov/view/vuln/detail?vulnId=CVE-2014-1266">CVE-2014-1266</a>, aka the <i>goto fail</i> error in the Apple iOS implementation
of SSL/TLS, demonstrated that its testing was also mostly-positive.
In this vulnerability, the SSL/TLS library accepted valid certificates
(which were tested).
However, no one had tested to ensure that the
library rejected certain kinds of <i>invalid</i> certificates.
If you only check if valid data produces valid results, you are
unlikely to find security vulnerabilities, since most attacks
are based on invalid or unexpected inputs.


<p>
You <i>can</i> find this vulnerability (and similar ones)
if you create the test suite using a
different approach (<a href="#negative-testing">negative testing</a>)
that I describe below.
But first, let&#8217;s discuss fuzzing.

<h3 id="basic-fuzzers">Traditionally-applied fuzzers and fuzz testing</h3>

<p>
<i>Fuzz testing</i> is the process of
generating pseudo-random inputs and then sending them to
the program-under-test to see if something undesirable happens.
The tools used to implement fuzz testing are called <i>fuzzers</i>.

<p>
Note that fuzz testing is different from traditional testing;
in traditional testing, you have a given set of inputs, and you
know what the expected output should be for each input.
Traditional testing can be expensive as the number of tests grows,
because you have to figure out the expected output.
The mechanism that determines the expected output is
called an <i>Oracle</i>.
The costly problem of invoking an Oracle
for a large number of test inputs is sometimes called the
<i>Oracle problem</i>.

<p>
Fuzz testing approaches the Oracle problem differently, because it
only tries to detect &#8220;something bad&#8221; like a program crash.
Fuzz testing makes it easy to try many more input test cases in fuzz testing,
by making the output checking much less precise.
The fuzzing approach was originally developed by
Barton Miller in 1988 at the University of Wisconsin.
The &#8220;fuzz testing of application reliability&#8221; site at
<a href="http://pages.cs.wisc.edu/~bart/fuzz/">http://pages.cs.wisc.edu/~bart/fuzz/</a>
has more information about fuzzing in general.
For more about fuzzing, see [Takanen2008] and [Sutton2007].

<p>
Fuzzers are often used to help find security vulnerabilities,
because they can test a huge number of unexpected inputs.
In particular,
fuzzers are often useful for finding input validation errors, and
Heartbleed was fundamentally an input validation error.
Yet typical fuzzers completely failed to find the Heartbleed vulnerability!

<p>
Fundamentally, the way fuzzers are typically applied would not have
found Heartbleed.
Heartbleed was a buffer over-read vulnerability,
not a buffer over-write vulnerability.
Most fuzzers just send lots of data and look for program crashes.
However, while buffer over-writes can often lead to crashes,
buffer over-reads typically do not crash in normal environments
(my thanks to Mark Cornwell who pointed this out).

<p>
Several mechanisms are sometimes used to improve the likelihood
of detecting or countering buffer over-write.
But again, Heartbleed involved an over-read not an over-write,
so some of these additional mechanisms would not help at all.
For example, canary-based protection approaches (e.g., ProPolice) and
non-executable stacks are designed to counter over-writes - not over-reads.
GNU libc&#8217;s malloc()
has the option <tt>MALLOC_CHECK_</tt>; this uses
a less-efficient implementation that
tolerates simple memory allocation errors
(such as double-free) and tries to detect
corruption (e.g., caused by writing past the end of an allocated block).
The <tt>MALLOC_CHECK_</tt> option
is a helpful countermeasure against over-writes,
but I have no evidence that it would have detected or countered
an over-read like Heartbleed.
Similarly,
<a href="http://dmalloc.com/docs/latest/online/dmalloc_6.html#SEC8">Dmalloc&#8217;s
fence-post (bounds) checking</a>
&#8220;cannot notice when the program reads from these areas,
only when it writes values.&#8221;

<p>
<a href="#fuzzing-check-standard">Fuzzers <i>can</i>
find vulnerabilities like Heartbleed</a>.
However, to make that happen, we need to extend the
error-detection capabilities that they use
(beyond the simple approaches widely used today).
One way to extend their error-detection capabilities
is to use special address-checking tools that can detect
memory problems like over-reads during fuzzing.
These special address-checking
tools (such as address sanitizer or a guard page system)
turn subtle problems into something the fuzzer can detect, such as a crash.
These special tools typically require that the program
<a href="#allocate-normally">allocate and deallocate memory normally</a>.

<p>
It is known that OpenSSL does <i>not</i> directly allocate and deallocate
memory directly using standard calls.
Instead, it used a caching freelist system internal to OpenSSL
to reuse allocated memory.
Since OpenSSL does not return (deallocate) memory back to the underlying
system once it was done with it (in some cases),
special tools could fail to detect
some common weaknesses such as use-after-free or double-free
that they would otherwise find.
It also sometimes prevented some operating system and run-time
mitigation mechanisms from working.
In short, it is widely agreed that this OpenSSL memory allocation approach
prevented many mitigation and weakness detection mechanisms from working.

<p>
There have been conflicting reports on whether or not these
special tools could have specifically found Heartbleed
without code changes in OpenSSL.
Kupsch and Miller reported in the April 22, 2014 edition of their paper
that OpenSSL uses a custom memory allocator, and that
&#8220;to a dynamic analysis tool, it appears as if the
library is allocating large memory buffers and not returning them,
but in reality, it is subdividing these large blocks of memory
and returning them for use&#8221;
[<a href="https://continuousassurance.org/swamp/SWAMP-Heartbleed-White-aper-22Apr2014.pdf">Kupsch2014-April</a>].
This kind of subdivision completely defeats the ability of these
special tools to detect over-reads like the one in Heartbleed.
Based on this information,
older versions of this paper reported that fuzzers would have been unlikely
to have found Heartbleed in unmodified OpenSSL,
even if some of these special tools were used.
However, Chris Rohlf and I have since
independently investigated the OpenSSL code.
On careful examination it appears that while OpenSSL does have a custom
system, this memory subdivision does <i>not</i> occur in OpenSSL.
The fact that it is so difficult to even determine what the
allocator <i>does</i> is testimony that the memory allocation
system itself is <a href="#simplify-code">too complex</a>!
Based on this more recent information, it appears that
fuzzing <i>could</i> have found Heartbleed, but only if
<a href="#fuzzing-check-standard">special tools were used with fuzzers</a>,
and Kupsch and Miller have updated their paper
[<a href="https://continuousassurance.org/swamp/SWAMP-Heartbleed.pdf">Kupsch2014-May</a>].
This is, however, a minor point.
Kupsch and Miller were and are correct that
typical fuzz testing would not have found this vulnerability,
and that the OpenSSL code
countered many mitigation and defect-detection tools.

<p>
There has been some speculation that fuzzing hasn&#8217;t
been done as rigorously for OpenSSL and other cryptographic libraries
because encryption greatly reduces effectiveness of fuzz testing unless the
fuzzer is given keys and is specially written to attack the library
[<a href="https://plus.google.com/+JustinUberti/posts/Ah5Gwb9jF4q">Uberti</a>].
That might be true.
However, nothing prevents anyone from writing fuzzers that are given
keys (for purposes of testing).
Besides, the Heartbleed vulnerability can be found even without keys.
Thus, it&#8217;s really the fact that it was an over-read
that made traditional fuzzing ineffective.

<p>
Some fuzz testing systems are <i>white box</i> fuzz testing systems.
These systems typically use static analysis to determine
what parts of the program are not tested by earlier fuzz testing,
and then develop new inputs to test those previously-untested portions.
SAGE (Scalable, Automated, Guided Execution)
is an example of a tool that takes this approach
[<a href="http://research.microsoft.com/en-us/projects/atg/ndss2008.pdf">Godefroid2008</a>]
[<a href="http://research.microsoft.com/en-us/um/people/pg/public_psfiles/icse2013.pdf">Bounimova2013</a>].
<!-- Recent afl results make this assertion less likely; removed:
However, as noted earlier, Heartbleed was fundamentally a
<i>lack</i> of validation code, so there was no branch for a white-box
approach to detect.
As a result, it is unlikely that white box approaches (by themselves)
would have detected Heartbleed unless they were also extended by
other measures.
-->

<p>
To summarize: Traditionally-applied fuzzers and fuzz testing
could <i>not</i> find Heartbleed.
As I will soon describe,
<a href="#fuzzing-check-standard">fuzzing <i>can</i>
be effective if special address checking tools
and a standard memory allocator are used</a>.

<h1 id="counter">What would counter Heartbleed-like vulnerabilities?</h1>

<p>
Here is a partial list of tools and techniques that would
have countered Heartbleed ahead-of-time
(either with certainty or with very high confidence).
I will specifically note some
free / libre / open source software (FLOSS) where that makes sense to do so.
<p>
But first, some caveats:
<ul>
<li>Do <i>not</i> use <i>just</i> one of these tools and techniques
to develop secure software.
Developing secure software requires a collection of approaches,
starting with knowing how to develop secure software in the first place.
Most organizations who want to create secure software at least try to
write software in a simple and clear way,
enable and heed compiler warning flags,
apply source code weakness analyzers,
apply multi-person review, run fuzzers,
and apply a large automated regression test suite.
If you only use one technique, you run the risk of fighting
the &#8220;last war&#8221; instead of the current one.
For example, it would be absurd to ignore warning flags, even though
warning flags would not have detected Heartbleed.
(Yes, sometimes warning flags produce false positives, but in most cases
you should modify your code to eliminate the false positives.)
That said, when an attack succeeds, it&#8217;s important to see
how to improve things; otherwise attackers may keep
breaking into the software using that same approach.
Also, the more general an improvement is, the more likely that
same improvement would also counter many other attacks.
<li>There is no one master list of the types of tools and techniques
that exist.  Terminology varies, and different tools do different things.
I am co-author of a report that lists various tools and techniques
for software assurance
[<a href="http://www.acq.osd.mil/se/initiatives/init_pp-sse.html">Wheeler2014a</a>], which gives the most complete list I am aware of.
For additional information about types of tools and techniques, see
[<a href="http://samate.nist.gov/docs/NAVSEA-Tools-Paper-2009-03-02.pdf">BAH2009</a>]
[<a href="http://samate.nist.gov/index.php/Tool_Survey.html">NIST</a>].
I&#8217;ve created this list for this specific paper.
However, I will try to be clear about what I mean.
<li>This is certainly not a complete list of ways that Heartbleed
could have been detected ahead-of-time, as I mentioned above.
I do hope it helps; further suggestions would be welcome!
</ul>
<p>
So given those caveats,
what specifically <i>could</i> have countered this vulnerability
ahead-of-time?
To make this especially useful, I have roughly ordered them in
cost order, with the cheapest approaches listed first.
It is a really rough order, and some are especially debatable;
suggestions on how to improve it are welcome.
In many cases the more expensive approaches are more general and
can counter many other kinds of vulnerabilities, not just Heartbleed.
The subheadings identify in parentheses
which use dynamic analysis, static analysis, or a hybrid.
The final subsection discusses
<a href="#others-might-work">other approaches that might have worked</a>.

<h2 id="negative-testing">Thorough negative testing in test cases (dynamic analysis)</h2>

<p>
<i>Negative testing</i> is creating tests that should cause
failures (e.g., rejections) instead of successes.
For example, a system with a password login screen will typically have
many positive regression tests to show that logins succeed if
the system is given a valid username and credential (e.g., password).
Negative testing would create many tests to show that invalid usernames,
invalid passwords, and other invalid inputs will <i>prevent</i> a login.
One book defines negative testing as
&#8220;unexpected or semi-valid inputs or sequences of inputs...
instead of the proper data expected by the... code&#8221;
[Takanen2008 page 24].
There are many ways to do negative testing,
including creating specific tests (the focus of this section)
and creating semi-random test input
(covered in a later section as <a href="#fuzzing-check-standard">fuzzing</a>).

<p>
Thorough negative testing in test cases creates a set of tests that cover
every type of input that <i>should</i> fail.
I say every <i>type</i> of input, because you cannot test every input,
as explained in the section on
<a href="#dynamic-not-found">dynamic analysis</a>.
You should include invalid values in your regression test suite
to test each input field
(in number fields at least try smaller, larger, zero, and negative),
each state/protocol transition,
each specification rule (what happens when <i>this</i> rule
is not obeyed?), and so on.
This would have immediately found Heartbleed, since Heartbleed
involved a data length value that was not correct according to
the specification.
It would also find other problems like
<a href="https://web.nvd.nist.gov/view/vuln/detail?vulnId=CVE-2014-1266">CVE-2014-1266</a>,
the <i>goto fail</i> error in the Apple iOS implementation of SSL/TLS.
In CVE-2014-1266, the problem was that iOS accepted invalid certificates.
There were many tests with <i>valid</i> certificates... but clearly
not enough tests to check what happened with <i>invalid</i> ones.

<p>
In most cases only negative tests, not positive tests,
have any value for security.
As I noted earlier, what matters about test suites
is how you create them.
This is probably obvious to many readers of this paper.
In particular, I suspect Eric S. Raymond is <i>including</i>
these kinds of tests when he discusses the advantages of testing.
However, this is <i>not</i> obvious to many software developers.
All too many developers and organizations only use a
<i>mostly-positive test suite</i> instead.
Many developers find it very difficult to think like an attacker,
and simply fail to consider widespread testing of inputs that
&#8220;should not happen&#8221;.

<p>
One great thing about thorough negative testing is that this
can at least be partially automated.
You can create tools that take machine-processable specifications
and generate lots of tests to intentionally fail it... and
then see if the implementation can handle it.

<p>
Another great thing about thorough negative testing is that if there&#8217;s a
standard (which there is in this case), it&#8217;s possible to
collaboratively develop a separate common test suite as a
FLOSS project.
Then it&#8217;s possible to quickly test all current and future implementations
and prevent many problems from getting out to users.
I would <i>strongly</i> encourage creating general-purpose test
suites for protocols like SSL/TLS; that would reduce effort
(people only need to create the test suite once),
and it would help increase the security for all implementations
(not just one).
Individual implementations would still need to supplement the general
tests with additional tests, but a common big test suite would be
a big help.

<p>
Software testing is, in fact, an entire field.
There are many different kinds of test approaches and test coverage criteria.
I can only summarize testing in this paper.
For more general information, again, see
<i>Introduction to Software Testing</i> by
Paul Ammann and Jeff Offutt
[<a href="http://cs.gmu.edu/~offutt/softwaretest/">Ammann2008</a>].
But the point still stands: testing with only valid input
will fail to find many security-related problems, including Heartbleed.

<p>
I do <i>not</i> think that you should depend solely on
thorough negative testing, or any other single technique, for security.
Negative testing, in particular, will only find a relatively narrow
range of vulnerabilities, such as especially poor input validation.
Dynamic approaches, by their very nature,
can only test an insignificant portion of the true input space anyway.
But - and this is key - this approach can be very useful for finding
security vulnerabilities <i>before</i> users have to deal with them.


<h2 id="fuzzing-check-standard">Fuzzing with address checking and standard memory allocator (dynamic analysis)</h2>

<p>
Unfortunately traditional fuzz testing approaches were not helpful
in this case.
But there are simple lessons we can learn.
Fuzzing would have been much more effective if a special tool
called an <i>address accessibility checker</i> had also been used.
These kinds of special tools can detect many out-of-bound <i>reads</i> in
addition to out-of-bound <i>writes</i> during execution,
and can often detect other memory problems as well.
They are especially good at detecting when a read or write incrementally
goes beyond the end of the buffer, and that is exactly the
problem with Heartbleed.

<p>
There are a number of special tools that perform some sort of
address accessibility checking;
every tool has its pros and cons.
However, if you haven&#8217;t used anything else,
I <i>strongly</i> recommend that you check out address sanitizer (ASan).

<h3 id="asan">Address sanitizer (ASan)</h3>
<p>
Address sanitizer (ASan) was first released in 2012,
and is now easily available; it&#8217;s just an extra flag
(<i>-fsanitize=address</i>)
built into the LLVM/clang and gcc compilers.
Address sanitizer is nothing short of amazing;
it does an excellent job at detecting nearly all
buffer over-reads and over-writes (for global, stack, or heap values),
use-after-free, and double-free.
It can also detect use-after-return and memory leaks.
It cannot find all memory problems (in particular, it
cannot detect read-before-write), but that&#8217;s a pretty good list.
Its performance overhead averages 73%, with a 2x-4x memory overhead.
This performance overhead is usually fine for a test environment,
and it&#8217;s remarkably small given how good it is at detecting these
problems.
Many other memory-detection mechanisms have a far larger speed and memory 
use penalty, and many
guard page tools (described below) can only detect heap-based problems.
The one big drawback with ASan is that in current implementations you
have to recompile the software to use it; in many cases that is
not a problem.
<p>
For more about ASan, see the USENIX 2012 paper
[<a href="https://www.usenix.org/system/files/conference/atc12/atc12-final39.pdf">Serebryany2012</a>]
or the ASan website
(<a href="http://code.google.com/p/address-sanitizer/">http://code.google.com/p/address-sanitizer/</a>).
The test processes for both the Chromium and Firefox web browsers
already include ASan.

<p id="asan-confirmed">
Christopher T. Celi (of NIST) confirmed to me on 2014-07-10
that address sanitizer <i>does</i> detect Heartbleed if an attacking query is
made against a vulnerable OpenSSL implementation.
He ran OpenSSL version 1.0.1e (released in February 2013),
which is known to be vulnerable to Heartbleed.
He use gcc (version 4.8+) and its <tt>-fsanitize=address</tt> flag to
invoke address sanitizer.
As expected, a normal heartbeat request causes no trouble, but a
malicious heartbeat request is detected by ASan, and ASan then
immediately causes a crash with a memory trace.
In his test suite ASan reported, in its error trace, that the
there was an error when attempting a &#8220;READ of size 65535&#8221;.
He comments that, &#8220;Though the output is a bit more cryptic than that
of Valgrind, ASan is better for testing with a fuzzer as it crashes
upon finding an error. Because of the output however, one would have to
analyze the specific input that caused the crash a bit more heavily than
with Valgrind.&#8221;
As I note later, he also
<a href="#valgrind-confirmed">confirmed that Valgrind works</a>.

<!--
Time: Thu, 10 Jul 2014 19:16:59 +0000   	 
From: 	"Celi, Christopher T." <christopher.celi@nist.gov>
To: 	protocol-testing <protocol-testing@nist.gov>  	 
Subject: 	[Protocol-testing] ASan and Heartbleed   	 
Hello all,
I have continued to run tests to detect heartbleed and would again like to report that Address Sanitizer, (ASan) another FLOSS tool used to detect memory errors, has been able to detect the bug. ASan is built in to current versions of GCC (4.8+) and can be added in any programs compilation with the addition of the -fsanitize=address flag. When an error is detected, ASan terminates the program and reports the error. This is particularly useful for mass testing with a fuzzer as a fuzzer generally only looks to determine if the program has crashed unexpectedly.
After compiling OpenSSL with ASan, I ran a sample server ($ openssl s_server [flags]) using the same version of OpenSSL, (1.0.1e). I sent three different heartbeat requests to the server to see what would happen.
The first request was a normal heartbeat request. ("$ openssl s_client [flags], $ B" does the trick) This lead to no change in behavior in the server. The server processed the request normally and returned the proper data to the client.
The next request was a malicious heartbeat request. This was a custom designed array of bytes you can easily find on the internet searching for a heartbleed client. The server crashed and the usual ASan memory trace was outputted. Here is a portion of the output:
=================================================================
==3255== ERROR: AddressSanitizer: unknown-crash on address 0x608200016a0b at pc 0x7fa3eadd43f7 bp 0x7fffe7740f10 sp 0x7fffe77406d0
READ of size 65535 at 0x608200016a0b thread T0
#0 0x7fa3eadd43f6 (/usr/lib/x86_64-linux-gnu/libasan.so.0.0.0+0xe3f6)
#1 0x7fa3eab339ca (/home/chris/openssl-asan/lib/libssl.so.1.0.0+0x7d9ca)
...
0x60820001af48 is located 0 bytes to the right of 17736-byte region [0x608200016a00,0x60820001af48)
allocated by thread T0 here:
#0 0x7fa3eaddb41a (/usr/lib/x86_64-linux-gnu/libasan.so.0.0.0+0x1541a)
#1 0x7fa3ea544122 (/home/chris/openssl-asan/lib/libcrypto.so.1.0.0+0xc0122)
...
==3255== ABORTING

Most notable about this error trace is the "READ of size 65535." This is the max value of a 4 digit hex value, 0xffff which is the amount of data this particular request is asking for. Though the output is a bit more cryptic than that of Valgrind, ASan is better for testing with a fuzzer as it crashes upon finding an error. Because of the output however, one would have to analyze the specific input that caused the crash a bit more heavily than with Valgrind.

The last request was an arbitrary heartbeat request, one such that there should be no leaked memory, and that the server would not return the proper information. This was processed normally by the server and did not crash the server.
Again, I would be more than happy to answer questions about my process.
Thanks,
Chris Celi
-->

<p id="afl-confirmed">
Even more importantly,
Hanno Boeck confirmed in 2015 that the fuzzing tool american fuzzy lop (afl),
when combined with Address Sanitizer, <i>does</i> automatically find
Heartbleed.
It only took 6 hours on non-fancy hardware,
and that is a short time for a fuzzer.
To be fair, at the time afl was barely known,
harder to use, and had trouble working with Address Sanitzer.
He noted that,
"A lot of other things have been improved in afl, so at the time
Heartbleed was found american fuzzy lop probably wasn't in a state that
would've allowed to find it in an easy, straightforward way."
<a href="https://blog.hboeck.de/archives/868-How-Heartbleed-couldve-been-found.html">[Boeck2015]</a>
However, afl has become a remarkably powerful yet easy-to-use fuzzer.
It tracks the branches that are taken and how often, then prefers using
tests that cover the program differently when it evolves new tests.
This is so successful that afl has pulled JPEGs out of thin air
<a href="http://lcamtuf.blogspot.com/2014/11/pulling-jpegs-out-of-thin-air.html">[lcamtuf2014]</a>.
This suggests that using afl, combined with Address Sanitizer or something
similar, is worth considering today.


<h3>Other address access detection tools (such as guard pages)</h3>
<p>
There are other tools that can detect memory access and allocation problems.
These include binary simulators (e.g., valgrind),
guard page systems (e.g., electric fence),
and the CPU-specific bound checking mechanisms
(such as Intel Memory Protection Extensions (MPX)).
You can use several different tools (possibly on different fuzzer runs).
For fuzzing to detect Heartbleed and vulnerabilities like it,
the mechanism must be able to detect an over-read (not just an over-write)
and eventually lead to a crash or other problem detectable by fuzzing.
Some of these approaches have very significant performance overheads;
where significant, these overheads can reduce the amount of fuzz testing
that can be done in a fixed amount of time.

<p>
Binary simulators (such as valgrind and Dr. Memory)
indirectly execute a program, while performing
additional functions such as tracking memory accesses.
A widely-used and widely-respected tool in this category
is valgrind; valgrind&#8217;s
<a href="http://valgrind.org/docs/manual/mc-manual.html">memcheck</a>
plug-in can detect a variety of errors including over-reads on the heap.
Valgrind works by creating a &#8220;synthetic processor&#8221;
and monitoring execution.
There are various plug-ins for valgrind; the memcheck tool tracks
if memory is valid (if it has been initialized) and and if it can be
accessed (e.g., if it has been allocated or not).
Valgrind can be used on programs when you do not have the source code.
However, valgrind greatly slows down the program, often 25-50 times,
and often increases code size by a factor of 12 [Takanen2008, page 182],
but this may be fine for testing.
Valgrind&#8217;s memcheck is powerful for detecting heap-based vulnerabilities
like Heartbleed, but it has an important limitation:
<a href="http://www.valgrind.org/docs/manual/faq.html#faq.overruns">Memcheck cannot do bounds checking on global or stack arrays</a>.
A tool that works in a similar way to valgrind,
but focuses especially on memory access issues, is
<a href="http://www.drmemory.org/">Dr. Memory</a>.
Both valgrind and Dr. Memory are FLOSS.
These are very useful tools for finding memory-related errors,
especially if you lack source code.
However, ASan tends to be better if you have the source code
and you want to do dynamic bounds-checking;
ASan is much faster, takes less memory, and can do bounds-checking
for heap, stack, and global data.

<p id="valgrind-confirmed">
Christopher T. Celi (of NIST) confirmed to me on 2014-07-07
that Valgrind <i>does</i> detect Heartbleed if an attacking query is
made against a vulnerable OpenSSL implementation.
He ran OpenSSL version 1.0.1e (released in February 2013),
which is known to be vulnerable to Heartbleed.
In this configuration Valgrind detected an
&#8220;invalid read&#8221; of a region that had been allocated by malloc.
The invalid read occurred as expected
inside the standard C function <tt>memcpy</tt>,
which was called by <tt>tls1_process_heartbeat</tt>
(which is responsible for receiving a heartbeat and processing
a response), which was called by
<tt>ssl3_read_bytes</tt>.
Valgrind could also report that the memory was allocated by the
standard C function <tt>malloc</tt> through OpenSSL <tt>CRYPTO_malloc</tt>,
again, as expected.
In this particular test he sent a message that was known to trigger
the Heartbleed attack.
He notes that to have detected this ahead-of-time with
Valgrind, &#8220;Someone testing the code would likely have to
use a fuzzer to assemble the proper bytes of hex to send to the server.&#8221;
Note that he also
<a href="#asan-confirmed">confirmed that ASan works</a>.


<!--
Time: 	Mon, 7 Jul 2014 15:48:05 +0000   	 
From: 	"Celi, Christopher T." <christopher.celi@nist.gov>
To: 	protocol-testing <protocol-testing@nist.gov>  	 
Subject: 	[Protocol-testing] Valgrind and Heartbleed   	 
Hello all,
I have been working to find a way to detect heartbleed using common testing tools and would like to report that Valgrind, a well-known FLOSS tool used for detecting memory errors in compiled programs, has been able to detect the bug. Running OpenSSL under Valgrind will reveal memory errors such as invalid reads, which we are particularly interested in.
I have built a simple client in Python that sends a malicious heartbeat request encoded in hex to a specified server. Clients like this are readily available online. (The model I used for my client can be found here: https://gist.github.com/ah8r/10632982)
Running a sample OpenSSL server ($ openssl s_server [flags]) using OpenSSL 1.0.1e (which was released in February 2013,) with Valgrind leads to some memory errors that could be further investigated to lead to the bug.
Here is a portion of the output:
==18005== Invalid read of size 8
==18005== at 0x4C2F79E: memcpy@@GLIBC_2.14 (in /usr/lib/valgrind/vgpreload_memcheck-amd64-linux.so)
==18005== by 0x4E6B196: tls1_process_heartbeat (in /home/chris/openssl/lib/libssl.so.1.0.0)
==18005== by 0x4E62F10: ssl3_read_bytes (in /home/chris/openssl/lib/libssl.so.1.0.0)
...
==18005== Address 0x5ae3058 is 0 bytes after a block of size 17,736 alloc'd
==18005== at 0x4C2AB80: malloc (in /usr/lib/valgrind/vgpreload_memcheck-amd64-linux.so)
==18005== by 0x5106C92: CRYPTO_malloc (in /home/chris/openssl/lib/libcrypto.so.1.0.0)
We see the error is an invalid read, makes sense as we are trying to read more information than the buffer should allow. Moving down the error message, the trace first points to "tls1_process_heartbeat," which is the method that is responsible for receiving a heartbeat and processing a response. This method builds the response and is reading more data than it should (as we asked it to do with our malicious heartbeat request.) Later in the error message, the line "Address... is 0 bytes after a block of size 17,736 alloc'd," confirms this. This line tells us that the invalid read mentioned before occurs at this address with >16KB of information leaked. However the trace points us to CRYPTO_malloc, which I believe is how OpenSSL allocates its memory. OpenSSL allocates a large block of memory up front and divides it up as needed during runtime. This could be the reason Valgrind points to this location. Regardless, the error should raise questions in the tls1_process_heartbeat method and hopefully lead a programmer to detect the bug.
As for the practicality of this test, I do know the exact hex to send to the server to get the server to bleed data. Someone testing the code would likely have to use a fuzzer to assemble the proper bytes of hex to send to the server.
If there are any questions about my process please fire away, I'd be more than happy to answer them.
Thanks,
Chris Celi
-->

<p>
Many tools use <i>guard pages</i> to detect reads or writes
that march over or under a buffer.
In these systems, a guard page is added after and/or before the
allocated memory; attempts to access the guard page region is trapped
and specially responded to (e.g., it may lead to a crash).
Often these tools are implemented by intercepting a
few heap memory allocation calls (such as malloc).
Tools that intercept heap allocations and add guard pages
typically do not require source code, which is an advantage,
but they can can only detect heap-based problems.
Also, many guard page systems
have a significant performance overhead in both speed and memory use.
For example, Guard Malloc is report to increase execution time
by a factor of 100 times or more [Takanen2008, page 181]
in addition to a very large memory overhead.
These tools primarily focus on detecting access of unallocated memory
(including use after free), but they can sometimes detect use before
initialization by filling un-initialized memory with unusual values.
Examples of such tools include electric fence,
<a href="http://duma.sourceforge.net/">Detect Unintended Memory Access (DUMA)</a> (a fork of electric fence), guard malloc, and
the OpenBSD malloc; all of these are FLOSS.

<p>
Some system memory allocators, such as the
OpenBSD malloc, have a built-in guard page mechanism.
These would have inhibited or stopped Heartbleed, depending on
how it is implemented.
In particular,
<a href="http://www.openbsd.org/cgi-bin/man.cgi?query=malloc">OpenBSD&#8217;s
malloc implementation supports guard pages</a>.
In OpenBSD, the &#8220;G&#8221; option
causes &#8220;each page size or larger
allocation is followed by a guard page that will cause a
segmentation fault upon any access.&#8221;
This can be combined with the &#8220;P&#8221; option (the default),
which moves allocations within a page
(&#8220;allocations larger than half
a page but smaller than a page are aligned to the end of a page
to catch buffer overruns in more cases.&#8221;)
The OpenBSD mechanism can be enabled for a particular program
or even enabled by default across the whole system, and this
can protect many situations.
The OpenBSD malloc approach is reported to have relatively moderate
overhead and yet &#8220;caught serious bugs in lots of major software&#8221;
[<a href="http://undeadly.org/cgi?action=article&amp;sid=20031017121955">OpenBSD-Journal</a>]
[<a href="http://www.openwall.com/lists/musl/2014/01/07/3">Felker2014</a>].

<p>
There is a weakness in the OpenBSD malloc mechanism:
Even with both G and P enabled, small allocations (half a page or less)
are not immediately followed by a guard page.
I think it would be even better if the OpenBSD guard page mechanism
could insert a guard page immediately after even relatively small allocations,
even though this would probably have a serious speed and memory size impact.
But even as it is, enabling both G and P means that
all allocations larger than half a page are immediately
followed by a guard page (subject to alignment limits),
and that allocations that are a half a page or less will
at most leak half a page.
That can be very significant reduction in leak size compared to the 64KiB
of the original Heartbleed attack, depending on the page size (often 4KiB).

<p>
Memory allocations must be aligned, so guard pages may leak a few bytes
at the end depending on the implementation.
I suspect ASan would be faster than adding guard page on every allocation,
but adding guard pages do not require a recompile in most programs,
so there is an advantage to having it.
Unfortunately, the popular
GNU libc malloc does not include this kind of functionality at all.

<p>
Intel Memory Protection Extensions (Intel MPX) or other
CPU-specific bounds checking mechanisms might help.
MPX adds new registers called bound registers to hold bounds for
pointers, and new instructions to manage and use the bounds.
MPX is to be released as part of the Skylake architecture,
but as of 2014 these CPUs are not available to the public.
It will take longer for them to be widely available, and that does
not necessarily help non-Intel systems
(e.g., smartphones do not usually use an Intel chip).

<p>
There are other tools and approaches.
The point is that many tools can detect memory over-reads,
and using at least one of these tools can make fuzz testing
more effective.


<h3>Fuzz testing in practice</h3>
<p>
In general, when using fuzz testing you should
turn on as many anomaly detectors as you can.
The only detection mechanism used for the first fuzzer
was &#8220;did the unchanged program crash/hang?&#8221; -
and many fuzzers still only do that.
You should at least enable program assertion checks and
create as many assertions as you reasonably can.
You might also do additional checking to ensure that the
intermediate or final state is valid
(for example, sanity-check outputs
and examine what files are produced in what directories).
But for the purposes of Heartbleed-like vulnerabilities,
you should at least turn on invalid memory access detectors like ASan.

<p>
Many of these tools, including ASan and guard page based programs,
require that the program under test
<a href="#allocate-normally">allocate and deallocate memory normally</a>.
In particular, the program must not
combine multiple allocations into one allocation request
(e.g., as is done by a slab allocator or memory slicing implementation).
At the least, the program should
make it trivial to use a normal allocation approach
instead for use in fuzz testing (and test that it works).

<p>
It it true that encryption libraries can create special
issues for fuzzers
[<a href="https://plus.google.com/+JustinUberti/posts/Ah5Gwb9jF4q">Uberti</a>].
But these issues are easily addressed.
As Paul Black has stated to me separately,
&#8220;a tool based on mutated messages should mutate all parts of
the message at all levels: individual bits, before encryption,
after encryption, session creation, the whole handshake, [etc.]&#8221;. 
Or as Apostol Vassilev has stated to me separately,
&#8220;a thorough fuzzer should
exercise forbidden state machine transitions&#8221;.

<p>
The first fuzzers generated truly random data to be sent to a program.
However, other methods for creating data can improve
fuzzing effectiveness.
Most fuzzers can be divided into three categories:
<ul>
<li><i>Fully random</i> fuzzers send truly random data to a program.
Truly random fuzzers are the easiest to get started with, but
they typically only test a small portion of a program that has
complex input structures.
Examples include <a href="http://sourceforge.net/projects/fuzz/">fuzz</a>.
<li><i>Mutation-based</i> aka <i>dumb</i> fuzzers
start with sample input data and then modify those samples, often though
simple transforms like bit-flipping.
Mutation-based fuzzers are the next fastest and cheapest to get started with;
since they start with valid inputs, they can often test
more of a program than a fully random program.
Examples of mutation-based fuzzers include
<a href="http://www.vdalabs.com/tools/efs_gpf.html">General Purpose Fuzzer (GPF)</a>,
<a href="http://sourceforge.net/projects/taof/">The Art of Fuzzing (Taof)</a>,
and
ProxyFuzz.
<li><i>Generation-based</i> aka <i>smart</i> fuzzers
are provided with detailed information about how to
generated the specific protocol being tested.
Generation-based fuzzers tend to be even more thorough,
but they require more effort (since you have to create a definition
of what needs to be generated).
Examples of generation-based fuzzers include SPIKE, Sulley,
and the
<a href="http://www.codenomicon.com/defensics/">Codenomicon Defensics</a>
fuzzer.
</ul>

<p>
There are many other ways to categorize fuzzers, too.
Template-based fuzzers use existing traces
and fuzz parts of the recorded data.
Block-based fuzzers break individual protocol messages down
in static and variable parts and fuzz only the variable part.
Dynamic Generation/Evolution-based fuzzers learn the protocol of the
Target of Evaluation (TOE)
by feeding the TOE with data and interpreting its responses,
e.g. using evolutionary algorithms.
Model-based fuzzers employ a model of the protocol.
The model is executed on- or offline to
generate complex interactions with the TOE.
This enables fuzzing data after a point such as authentication,
an important issue for SSL/TLS.

<p>
Fuzzing can also be combined with traditional tests, again,
so that the fuzzing can go beyond a point like authentication.
For more discussion about these fuzzer variations and the use of
model-based fuzzers, see
[<a href="http://www.spacios.eu/sectest2012/pdfs/SecTestICST_Schieferdecker.pdf">Schieferdecker2012</a>].
Codenomicon also
<a href="http://www.codenomicon.com/products/coverage.shtml">discusses
different fuzzing approaches and their coverage</a>.
Other relevant papers include
&#8220;A Model-based Approach to Security Flaw Detection of
Network Protocol Implementations&#8221;
[<a href="http://www.ieee-icnp.org/2008/papers/Index12.pdf">Hsu2008</a>]
Whitebox-based fuzzers examine the program to improve what to fuzz
(and thus are really hybrid analysis approaches); these can
extend effectiveness, but they require more effort to implement and
simply are not necessary to find vulnerabilities like Heartbleed.
For more information on fuzz testing, see
[Takanen2008] and [Sutton2007].

<p>
A lot of work has been going on to improve the coverage of code
in fuzz testing.
In general, as more code is covered by fuzz testing
(as measured as statements or branches), the more likely that
fuzz testing will detect a vulnerability if present.
Thus, some fuzzers use information about the program being executed
to improve fuzzing capabilities.
Some tools, such as the FLOSS
<a href="https://code.google.com/p/american-fuzzy-lop/">American fuzzy lop</a>,
instrument code to improve code coverage while fuzzing.
Microsoft has had good experience with
constraint-based whitebox fuzz testing, in which they
leverage symbolic execution on binary traces
and constraint solving to construct new inputs to a program
[<a href="http://research.microsoft.com/en-us/um/people/pg/public_psfiles/icse2013.pdf">Bounimova2013</a>]
However, simply sending the data is not enough; a fuzzer would have to
have detected that there was a problem, and out-of-bounds reads
typically do not cause a crash or other easily-detected problem
unless something else has been done.
<!--
However, it unlikely that coverage-based approaches
would have helped find Heartbleed.
Again, since the problem was <i>missing</i> validation code,
looking for unexecuted statements or branches would not find anything amiss.
-->

<p>
It is possible that a mutation-based fuzzer could have found Heartbleed,
once it is coupled with better fault detection,
but a mutation-based fuzzer would probably only find Heartbleed
if the starting test cases included a heartbeat message.
A generation-based fuzzer, once coupled with better fault detection,
would be <i>highly</i> likely to find Heartbleed... but only
if (1) it included rules to generate a heartbeat, <i>and</i>
(2) it fuzzed lengths as well.
For example, the Sulley fuzzing framework automatically computes
block lengths, but by default it does not fuzz the lengths to make
them incompatible with the data being sent.
If you use Sulley, you&#8217;ll probably need to set the
&#8220;block sizers&#8221; to be &#8220;fuzzable=True&#8221;; this creates a more rigorous test
(as is probably needed to detect Heartbleed) but it is not the default.
Thus, Heartbleed shows that we need to be fuzzing lengths and <i>not</i>
assuming that lengths are always being checked properly.

<p>
Oh, I should add a quick terminology note.
People sometimes use the term &#8220;negative testing&#8221;
to make it appear to be a synonym for fuzz testing
[Takanen2008, page xix].
I do not use the term that way.
Instead, I use the term negative testing in a broader sense.
Still, fuzz testing is a useful approach for negative testing,
so much so that I have listed it as a separate category.

<p>
It would be possible to send inputs like a traditional fuzzer,
but examine the outputs more thoroughly.
This approach is discussed later in the section on
<a href="#fuzzer-examine-output">fuzzing with output examination</a>.

<p>
It&#8217;s debatable whether or not fuzzers are more
expensive than negative testing, but here is my reasoning.
One advantage of negative testing is that it is really easy to get started;
presuming you already have a test suite, you can just start adding
negative tests.
More importantly, though, negative tests rapidly give an unambiguous answer
as to what caused the problem, and since they require little computing
power (compared to fuzz testing) developers can easily
re-run a test suite on every patch.
In contrast, fuzz testing often
requires more computing power and interpretation of results;
computing power is cheap, but this factor still slows down
feedback to developers.
The potentially-faster feedback of negative testing
could lead to faster developer detection and fixes.
Today a key cost driver is developer time, not computing time;
a mechanism that best reduces developer time is really helpful
and tends to be less costly.
Also, you can make a negative test suite once for a given protocol;
you can then easily reuse the test suite on every implementation and
every patch of each implementation.
Of course, these are not in conflict; it is better
to do <i>both</i> negative testing and fuzz testing.


<h2 id="compile-standard">Compiling with address checking and standard memory allocator (hybrid analysis)</h2>

<p>
What if you want to use a program right now,
in situations where it&#8217;s really important to counter attackers
from unknown potential vulnerabilities?
It turns out there is at least one way that could have worked.
In addition, it might have provided some early warning
of exploitation (a rather late form of detection, but it is detection).

<p>
One approach is to
use a mechanism that detects (at run-time) attempts to read
past the end of an allocated memory region.
In this approach, you&#8217;re not just changing how tests are run;
the idea is that you actually use this version during operation!
This requires that the program
<a href="#allocate-normally">allocate and deallocate memory normally</a>.
In particular, the program must not
combine multiple allocations into one allocation request
(as is done by a slab allocator or memory slicing implementation).

<p>
There are several mechanisms that could detect such things at run-time.
These are basically a subset of the detection mechanisms for
<a href="#fuzzing-check-standard">fuzzing with address checking and standard memory allocator (dynamic analysis)</a>, with the additional
challenge that speed and memory use are much more important.
Here are few examples:
<ol>
<li>Address sanitizer (ASan).
Note that you have to recompile the program to do this.
As noted above,
ASan is just a flag (<i>-fsanitize=address</i>)
in the LLVM/clang and gcc compilers, so this is relatively easy to do
in most C software.
This has an average overhead of 73% performance, and 2x-4x memory
[Serebryany2012].
This is probably not something you&#8217;d want to do on a smartphone
(few people will want their battery life halved), and
many busy websites will not welcome the overhead either.
But modern computers have far more performance and memory than in the past,
so in some situations this is acceptable...
and this is something you can do immediately to counter unknown attacks.
ASan is especially powerful at detecting a long list of potential
problems, including most invalid buffer accesses
(not just this particular kind).
ASan is not available on all compilers; it would be a good idea
for other C, C++, and Objective-C compilers to add it.
<li>Intel Memory Protection Extensions (Intel MPX).
As of 2014 these CPUs are not available to the public, and this
will not help non-CPU architectures.
<li>Memory allocation guard pages.
Some debugging systems and system memory allocators
make it possible to add unmapped
&#8220;guard pages&#8221; after the
allocated memory that prevent both reading and writing.
These would have inhibited or stopped Heartbleed, depending on
how it is implemented.
For example,
<a href="http://www.openbsd.org/cgi-bin/man.cgi?query=malloc">OpenBSD&#8217;s
malloc implementation supports guard pages</a>.
I think GNU libc and similar runtimes should add
something like the OpenBSD malloc guard page mechanism
so that over-reads can be countered.
</ol>

<p>
I suspect ASan would be faster than adding guard page on every allocation,
but adding guard pages does not usually require a recompile,
so there is an advantage to using it instead.

<p>
This is really a damage <i>reduction</i> approach, instead of an
approach that eliminates the problem.
From a security point of view this approach turns a loss of confidentiality
into a loss of availability.
In many cases, however, this is a good trade-off.
Also, this approach makes the problem visible once the system is under
attack; once a problem is visible it is usually easy to correct.

<p>
This approach can be easily combined with a honeypot or honeynet
(my thanks to Vincent Legoll, who pointed this out to me on 2014-05-05).
Set up these hardened implementations on honeypot/honeynet systems
(systems that should not be used by non-attackers), basically
to detect and trap attackers.
If an attacker tries to break the software, the software would
crash instead, and that could be logged and tracked as especially important.
Forensics could then detect some specific zero-day exploitations.
I think this could also be done by some logging systems combined with
intrusion detection systems; again, if a crash occurs in
a hardened crypto library, log it specially.
This would make it much easier to detect widespread exploitation
of a 0-day attack.
Distributions, core infrastructure organizations, and other organizations
could establish these across the Internet and help protect us all.
This would a relatively late form of detection, but in some cases
it would detect attacks before others were attacked.

<p>
While this approach doesn&#8217;t fully fix the problem, it
does provide a powerful mitigation, and can be used as part of a
larger detection approach.
Some distributions
or organizations might want to use these countermeasures
in specific situations,
or at least make these countermeasures easier to enable.

<p>
Changing the code doesn&#8217;t cost much effort, and recompiling is
usually fairly simple also (when you have the source code).
However, the performance loss would be really significant in many
settings; it&#8217;s like losing part of the hardware performance you paid for.
For example, using ASan you lose around half your (speed) performance.
Thus, I&#8217;m counting this approach as a more expensive solution,
to capture the loss of this hardware.
In many situations the operational impact would be significant;
on smartphones this would reduce speed and battery life, and
on popular servers this could slow response and increase
electrical power costs.
If future CPUs add hardware support for ASan, the speed
impact could be reduced significantly (the ASan paper estimates that
the speed overhead would go from 73% to about 20%).
I would love to see CPU manufacturers explore this.

<h2 id="review-field-required">Focused manual spotcheck requiring validation of every field (static analysis)</h2>
<p>
The vulnerable code <i>was</i> reviewed by a human,
so merely having a single human reviewer was obviously <i>not</i> enough.
<p>
However, a variation <i>would</i> have worked -
requiring the human (manual) review to specifically check every field to
ensure that every field was validated.
Checklists sometimes get a bad name in computer security.
I suspect one reason is that
sometimes checklists are deployed to people who don&#8217;t
know what they&#8217;re doing, who then can&#8217;t use them effectively.
But expert airplane pilots routinely use checklists, even though
they <i>do</i> know what they are doing.
If patches are only accepted after they are reviewed using a checklist,
and the checklist includes &#8220;must show that
every untrusted data field is validated&#8221;,
then it is likely that this vulnerability would have been countered.

<p>
I had originally included this approach as part of the approach
<a href="#thorough-audit"><i>thorough human review / audit</i></a>,
but this is a different and much lower-cost approach.
However, it does require that the reviewer(s) apply it to every 
patch as they come in; it cannot easily help with a large
body of pre-existing code.


<h2 id="fuzzer-examine-output">Fuzzing with output examination (dynamic analysis)</h2>
<p>
Fuzz testing traditionally involves sending lots of input to a program
and looking for grossly incorrect behavior such as crashes.
In fuzzing with output examination,
the fuzzing system also examines the TOE output, e.g.,
to determine if the output is expected, or if it has
various anomalies that suggest vulnerabilities.
To accomplish this, the fuzzing system is
provided additional information about the expected response
(e.g., as required by a specification) or lack thereof,
e.g., some constraints on the expected TOE output.
The TOE response can also be compared to patterns
(typically based on heuristics) that suggest
vulnerable behavior (such as evidence of a cross-site scripting vulnerability).
<p>
This is possible to do with generation-based fuzzers because these
kinds of fuzzers are already provided information
about the correct sequence of interface input (e.g., of a protocol).
This approach simply extends this information to also describe the
expected output.
The description of expected output need not be exact.
It becomes increasingly likely to find existing vulnerabilities by making
the output description increasingly precise, but of course,
more exacting descriptions require much more effort to create.
In some sense this approach stretches fuzz testing back towards traditional
<a href="#negative-testing">thorough negative testing in test cases</a>.
Early fuzz testing gave up the idea of knowing exactly what the expected
output is (to simplify creating test cases), while this approach
re-introduces the idea of examining results
more carefully for correct behavior.
<p>
This is the approach that Codenomicon used to find Heartbleed.
In their approach, they developed an additional mechanism
called &#8220;Safeguard&#8221; inside their Defensics tool.
Safeguard analyzes the TOE responses
to determine if they matched what was expected.
More information about this (at a very high level) can be found in
[<a href="http://www.codenomicon.com/news/news/2014-05-20.shtml">Codenomicon-How</a>],
[<a href="http://www.businessinsider.com/heartbleed-bug-codenomicon-2014-4">Eadicicco</a>],
and
[<a href="http://www.latinpost.com/articles/10440/20140411/heartbleed-bug-discovered.htm">Chandrashekar</a>].
Codenomicon originally added this to just one protocol suite, SSL/TLS, but
based on their success with this approach they are adding this approach
to several other interfaces.
I understand they intend to add Safeguard to
five more interfaces by the end of June 2014, which clearly indicates that
Codenomicon thinks this approach has value.
<p>
Codenomicon (particularly Mikko Varpiola) provided
more information about Safeguard and SSL/TLS in particular.
Safeguard was inspired by examination of an earlier vulnerability,
<a href="https://web.nvd.nist.gov/view/vuln/detail?vulnId=CVE-2012-2388">CVE-2012-2388</a>.
This vulnerability involved signature handling which was tied
to user authentication.
A Codenomicon engineer realized
that this kind of vulnerability could be detected by a fuzzer
if it could detect that certain stages of a protocol
could be incorrectly skipped.
They then began to develop additional checks to examine the TOE output
more rigorously.
<p>
Safeguard (at least for SSL/TLS) implements four kinds of checks:
<ol>
<li><i>Authentication bypass</i>.
This checks if a user with insufficient credentials gets granted access 
to the resources that they&#8217;re not supposed to access. 
This includes guessing the &#8220;right&#8221; commands,
skipping an authentication phase,
or allowing unauthorized access to system / protected resources.
This is the one that started the whole approach.
<li><i>Weak encryption warning</i>.
This checks if a known weak cryptographic algorithm is accepted.
This is especially useful for detecting if a client or server
is able to force or downgrade its partner to a weaker
cryptographic algorithm.
<li><i>Amplification</i>.
This warns if a small amount of data sent to the TOE results
in a very large response by the TOE.
This is an especially important issue for higher-level protocols
(such as DNS) that can be built on connectionless protocols like UDP.
<a href="https://www.us-cert.gov/ncas/alerts/TA14-017A">US-CERT Alert (TA14-017A), UDP-based Amplification Attacks</a>,
notes that
&#8220;certain UDP protocols have been found to have [responses]
much larger than the initial request...
[so] a single packet can generate tens or hundreds of times the bandwidth...
This is called an amplification attack, and when combined with
a reflective DoS attack on a large scale it makes
it relatively easy to conduct DDoS attacks.&#8221;
In the case of Safeguard, at the beginning of a test run they calculate a
a baseline amplification table (BAT) of requests sent vs.
responses received based on known valid protocol interactions.
They then send fuzzed packets, and calculate the
bandwidth amplification factor (BAF) for each interaction 
resulting from the fuzzed message.
This allows them to pinpoint issues leading to attacks,
such as
<a href="http://ics-cert.us-cert.gov/advisories/ICSA-14-051-04">recent
NTP reflection attacks</a> where a 
small UDP messages from spoofed addresses were used to generate a 
large responses from NTP server.
Many UDP-based protocols are inherently vulnerable to amplification,
but some protocols and implementations are more vulnerable than others.

<li><i>Data leakage</i>.
If the amplification check&#8217;s BAF reaches a certain level,
this checks if &#8220;something unusual came back&#8221;
using heuristics such as entropy calculations and string matching.
The goal is to determine if the program is getting memory contents,
if it has triggered a SQL injection, or in some other way has
an output that suggests a vulnerability.
Heartbleed was caught with this data leakage check in Safeguard.
Heartbleed triggered a warning for a rather large BAF, followed by alarm 
because of what was detected in its return data.
</ol>

<p>
Mikko Varpiola told me that these seem to be surprisingly useful
when fuzzing protocols that carry usernames.
These are often processed by a SQL database,
and this kind of fuzzing can help detect
SQL injection vulnerabilities.

<p>
My thanks to the people from Codenomicon for providing information to me
on how Safebuard works in Defensics:
Steve Hayes, Josh Morin, Bob Sturm, and Mikko Varpiola.
Mikko Varpiola, in particular, provided me with a lot of more
detailed information on Safeguard.
Any mistakes in my paraphrasing of their information are my own.

<p>
One advantage of this approach is that you <i>only</i> need to observe
the output of the system.
You do <i>not</i> need source code or the
ability to manipulate the underlying platform, so this can be used
to examine systems like routers as black boxes.
That is really impressive, and is a significant contrast to
<a href="#fuzzing-check-standard">fuzzing with address checking</a>
(which typically requires source code or at least the ability to manipulate
the underlying platform).
<p>
A challenge with fuzzing with output examination is that
someone must create this additional information about the expected output.
Obviously it takes additional time to encode the information about
the expected output (since this in addition to the interface information
that is already necessary to generate input).
Another problem is that determining this information is not easy.
Specifications (such as IETF RFCs) are notorious for under-specifying
what should happen with incorrect or barely-correct input.
It is possible to start with a more rigorous requirement and then
add various exceptions or allow more variations,
but this can take many iterations involving detailed examinations of
TOE output.
It is also possible to weakly specify the results, but the more generous the
specification, the less likely it is to find vulnerabilities.

<p>
I have identified this as somewhat more costly because it requires
significant interface-specific analysis to determine what the output
requirements should be, and then encode them.
However, once this information is encoded it can be reused to test
later versions or alternative implementations of the same interface.


<h2 id="context-configured-static">Context-configured source code weakness analyzers, including annotation systems (static analysis)</h2>

<p>
Traditional source code weakness analyzers
could not find Heartbleed, because they used general-purpose
heuristics that simply didn&#8217;t work well enough in this case,
in part because of the complexity of the code.
It is always best <a href="#simplify-code">simplify the code</a>
where you can, but there is always some minimal complexity based
on what you are trying to accomplish, and real humans are unlikely
to achieve perfect simplicity anyway.
Coverity is developing some new heuristics that they think would
detect Heartbleed
[<a href="http://security.coverity.com/blog/2014/Apr/on-detecting-heartbleed-with-static-analysis.html">Chou2014</a>] ... and good for them!
At least one person has implemented similar heuristics using
clang
[<a href="http://blog.trailofbits.com/2014/04/27/using-static-analysis-and-clang-to-find-heartbleed/">Ruef2014</a>]
Indeed, I expect all source code weakness analyzers to
improve over time, and thus find vulnerabilities that they
didn&#8217;t find before.
But generic heuristics can only go so far at any point in time;
can you go beyond?

<p>
The answer is yes, and I call this a
<i>context-configured</i>
source code weakness analyzer.
The basic idea is that you start with a source code weakness analyzer,
but you then provide far more information about the 
program that you are analyzing.
<p>
This approach requires much more time than just running a
source code weakness analyzer, and this additional information is
typically tied to just one specific tool
(tying you to that tool).
However, if you provide more information about your program, the
source code weakness analyzer can do a much better job.

<p>
Klocwork has shown that this approach definitely works for Heartbleed
[<a href="http://www.klocwork.com/blog/software-security/saving-you-from-heartbleed/">Sarkar2014</a>].

<p id="annotation-systems">
Now let&#8217;s talk about <i>annotation systems</i>.
There are various ways to provide this additional information to
static analysis tools; an annotation system adds this additional
information as part of the program itself.
One common way is to add an annotation system
to the programming language, and then modify the program
to use these annotations.
These annotations can be added by directly changing the code
(using new keywords), added as comments, or added in separate files.
Examples of tools or annotation languages for C include
<a href="http://msdn.microsoft.com/en-us/library/hh916383.aspx">Microsoft&#8217;s SAL</a>,
splint, Deputy, Oink/CQual++, cqual, and
Frama-C <a href="http://frama-c.com/acsl.html">ANSI/ISO C Specification Language (ACSL)</a>.
Static analysis tools can check the information from the annotation system
on every compilation, providing quick feedback once they are used,
and they are not limited to specific input values
(i.e., they are not limited by the problems of dynamic analysis).
You could easily argue that adding this information
(via annotation systems) is really a different technique.
<p>
Seriously using these additional annotations
to counter vulnerabilities often requires
a non-trivial amount of work if you are starting with existing code.
There are also many different incompatible annotation systems for C,
and there are no standards for them, which further impedes their use.
After all, it takes work to add annotations, and those annotations
lock you into to a specific tool.
Microsoft SAL has additional problems; there is no FLOSS implementation
and it is only available on Windows.
I think that annotation systems would be much more widely used
if there was a single widely-accepted <i>standard</i> annotation
notation for each major programming language, including C.
It would be hard to <i>get</i> that kind of agreement for languages
like C when there isn&#8217;t already such a notation.
Peter Gutmann has written a post on some of his experiences
[<a href="https://www.cs.auckland.ac.nz/~pgut001/pubs/sal.html">Gutmann</a>].
<p>
However, annotation systems have many advantages.
Annotation systems can find vulnerabilities that simply
are not countered by switching to a different language.
Also, they are often cheaper than switching to a different language
(because you are simply adding additional information to an existing program).
Of course, these are not in conflict; you can switch languages
<i>and</i> use a code annotation system for the new language.

<h2 id="multi-implementation-coverage">Multi-implementation 100% branch coverage (hybrid analysis)</h2>
<p>
Another approach that would probably have detected Heartbleed
is 100% branch coverage of alternative implementations.
As noted earlier,
<a href="#code-coverage">branch testing cannot detect when
input validation code is <i>missing</i> in a particular program</a>.
Branch coverage <i>can</i>, however, detect
existing untested branches in a <i>different</i> implementation.
Striving for a test suite that gives
full branch coverage of <i>multiple</i> implementations
greatly increases the likelihood that missing validation code and
missing exception handling would be detected.
Stronger test coverage measures, such as
<a href="https://en.wikipedia.org/wiki/Modified_Condition/Decision_Coverage">modified condition/decision coverage (MC/DC)</a>,
would work as well.
<p>
Like all coverage approaches, this is fundamentally a hybrid analysis
technique.
This uses dynamic analysis to run tests... and static analysis to
determine which branches (or related coverage measures) have been
left untested.
<p>
This approach is a somewhat
specialized approach for finding vulnerabilities.
The test suite <i>must</i> be applied across multiple implementations,
all with 100% branch coverage, so it requires multiple implementations
to be used at all.
What&#8217;s more, the more different the implementations, the better.
Also, this approach is probably less capable (by itself)
at finding security vulnerabilities than other approaches.
That&#8217;s because there may be many different inputs that follow the
same path, yet only a small subset of them might trigger a vulnerability.
It also only works if one of the other implementations implements
the particular component under test (in SSL/TLS support for the
heartbeat is optional) and implements the potentially-missing
input validation code.
<p>
I have never seen this specific approach discussed in the literature;
usually people discuss branch coverage of a single implementation
(instead of multiple implementations).
Still, it is fair to note that this approach can not only help improve
quality, but it could also have found this particular vulnerability.
<p>
One trouble: these would not
<i>necessarily</i> counter Heartbleed, because much depends on
the configuration extensions or annotations used and how they are used.
In particular, the output would need to be checked thoroughly enough
to detect that a problem occurred.
On the other hand, they do not depend on hitting exactly the right input;
static analyzers can examine a large number of situations simultaneously.
<p>
Multi-implementation 100% branch coverage is more costly than
<a href="#negative-testing">thorough negative testing</a>,
primarily because if you have a poor test suite
it can take a lot of time to work backwards from a missed branch
to figure out how to trigger it.
Also, missed branches are often specialized error-handling systems
that can be difficult to trigger, or undocumented
&#8220;can&#8217;t happen&#8221; branches used as part of defensive design.
In addition, the test suite has to grow enough to cover <i>multiple</i>
implementations at 100%; many organizations do not even try
to grow a test suite to do 100% branch coverage of a <i>single</i>
implementation, never mind 100% coverage of multiple implementations.


<h2 id="aggressive-assertions">Aggressive run-time assertions (dynamic analysis)</h2>

<p>
Software developers could aggressively insert and enable run-time assertions.
There is speculation that this <i>might</i> have countered Heartbleed,
so I will discuss this possibility here.

<p>
A software developer can assert that various value relationships
or states must be true.
These assertions can then be checked at run-time,
at least while testing the software.
Nearly all languages have a built-in <i>assert</i>
mechanism (or equivalent) that can cause an exception or crash if
a condition is not true at run-time at a specific program location.
Several languages have more advanced built-in assertion mechanisms
for specifying preconditions, postconditions, and invariants
that can be checked at run-time
(examples include
<a href="https://www.eiffel.com/values/design-by-contract/">Eiffel&#8217;s
design-by-contract mechanisms</a> and the
<a href="http://www.ada-auth.org/standards/12rat/html/Rat12-2.html">Ada 2012 contracts</a>).
In some cases the language can optimize some of these assertions away,
leaving the assertions it cannot optimize away at run-time.
Indeed, an annotation system may be partly
implemented statically, and partly implemented dynamically;
see my <a href="annotation-systems">previous comments about
annotation systems</a> for their static application.

<p>
<a href="https://www.cl.cam.ac.uk/research/security/ctsrd/pdfs/201404-eurosys2014-tesla.pdf">Temporally Enhanced System Logic Assertions (TESLA)</a>
is an even more advanced research approach that allows temporal assertions.
You can find further information at the website
<a href="http://www.cl.cam.ac.uk/research/security/ctsrd/tesla/">http://www.cl.cam.ac.uk/research/security/ctsrd/tesla/</a>.
<a href="http://frama-c.com/eacsl.html">Frama-C E-ACSL</a> annotation language
is a subset of ACSL; Frama-C can take E-ACSL annotations
and cause run-time failures if the annotations are violated.
E-ACSL support is in a preliminary state in Frama-C as of May 2014.

<p>
There is no doubt that assertions can be an excellent mechanism
for detecting invalid states, and invalid states can sometimes be an
indicator of a vulnerability.

<p>
However, this approach does have some weaknesses when it comes to
countering Heartbleed.
Neither the original developer nor its reviewer realized that
checking the request packet length value was important;
since the length check was not included, it is unlikely that
the developer would have remembered to add assertions to check for it.
This is also a problem for
<a href="#negative-testing">thorough negative testing</a>, but
negative testing is easily done by a group
separate from those developing functional code,
and it is much easier to ensure that (for example) all data fields
are checked, so I think negative testing would be more likely to
find this specific type of vulnerability.
Thus, while aggressive annotations can be a very useful approach
for countering vulnerabilities, it is somewhat speculative it
would have worked for this particular case.

<p>
Note that aggressive run-time assertions work <i>very</i> well with the
fuzz testing approach described earlier.
Run-time assertions detect very specific problems in program state,
and thus create more situations that a fuzzer can detect.
<a href="http://www.squarefree.com/2014/02/03/fuzzers-love-assertions/">Jesse Ruderman expressed the complementary relationship of fuzzers and
assertions</a> in a wonderfully pithy way:
<blockquote>
<i>Fuzzers make things go wrong.<br>
Assertions make sure we find out.</i>
</blockquote>

<p>
I have placed this approach as a somewhat more expensive option.
For this approach to have detected Heartbleed (without knowing about
it ahead of time) would have required very aggressive use of assertions.
Adding all those assertions would take significant development time
<i>and</i> would typically also impose a significant run-time cost.

<!--
TODO: TESLA

Jon Anderson's work on TESLA (Temporally Enhanced System Logic Assertions)
https://www.cl.cam.ac.uk/research/security/ctsrd/pdfs/201404-eurosys2014-tesla.pdf

"Also, I see I was not careful enough regarding TESLA.  It is not conceived as being formal methods on the fly, but has some sensible formal basis in the automated formation of state machines that act as detectors of undesired events.  There is plenty of room for more formality on the fly, but Jon Anderson has not really been pursuing that yet, but that opens up lots of future research for Jon and others.

-->


<h2 id="safe-language">Safer language (static analysis)</h2>

<p>
The underlying cause of Heartbleed is that the C programming language
(used by OpenSSL) does not
include any built-in detection or countermeasure for improper
restriction of buffers (including buffer over-writes and over-reads).
Improper restriction can often lead to catastrophic failures,
so almost all other programming languages automatically
counter improper restriction (e.g., by resizing data structures or
by raising an exception when the buffer is exceeded).

<p>
If vulnerabilities in a given program can have catastrophic effects,
then those choosing its programming language(s)
should prefer the options that reduce the likelihood of vulnerabilities.
The more catastrophic the effects, the stronger this preference should be.
Most programming languages provide at least some direct
protections against otherwise-dangerous vulnerabilities, such as
improper restriction protection.
Some programming languages also
provide constructs that are less likely to be
misused or are less likely to be incorrectly used.
Ideally, a language would prevent all vulnerabilities.
It is highly unlikely that a general-purpose language could ever
prevent all vulnerabilities, but it is a worthwhile goal for language
designers to strive for.
There is no &#8220;perfectly safe&#8221; programming language;
instead, there is a continuum, with some
languages providing more vulnerability countermeasures than others.

<h3 id="dangerous-languages">Dangerous languages and why people use them</h3>
<p>
The most dangerous widely-used languages for
security-relevant software are C, C++, and Objective-C.
All of these languages provide no built-in restrictions on buffer access,
indeed, it takes a non-trivial effort to <i>avoid</i> problems
like buffer over-reads and over-writes.
Improper restrictions on buffer access
continue to be a widely-used type of vulnerability
that often have catastrophic effects.
Using or switching to almost any other language
(other than C, C++, or Objective-C)
would completely eliminate buffer-related vulnerabilities,
including Heartbleed.
This is <i>especially</i> true for C, because it lacks many of the
higher-level constructs that make it somewhat easier
to avoid buffer-handling problems.
Most languages also prevent memory deallocation errors
that could lead to vulnerabilities
(e.g., automatic garbage collection), and some languages
are designed to counter additional vulnerabilities as well.
One of the reasons there are so many vulnerabilities in modern
systems is the overuse of the C, C++, and Objective-C languages.
In fact, some people have proposed banning the use of these
languages in security-sensitive code.

<p>
However, there are <i>reasons</i> that C, C++, and Objective-C
are widely used.
The <a href="http://www.tiobe.com/index.php/content/paperinfo/tpci/index.html">TIOBE Programming Community index measures programming language popularity</a>,
and as of April 2014 these languages occupy three of the four top slots
(the top 4 languages in order are C, Java, Objective-C, and C++).
These reasons include higher performance (in speed and memory use),
ease of interface, large libraries, platform preference, and familiarity.
Also, switching languages for large existing programs (like OpenSSL)
is usually a big effort.
Let&#8217;s examine a few of these reasons.

<h3 id="performance">Speed and memory performance of alternatives</h3>
<p>
One oft-cited issue is that programs in C, C++, and Objective-C
tend to have noticeably higher speed than programs
written in most other languages.
In addition, most other languages lack the lower-level mechanisms
that are needed if you need to directly interact with hardware
(and this direct interaction can also increase speed).
If you need the speed, and perhaps the direct interaction,
the list of likely languages gets much shorter.
And speed sometimes matters in a world of mobile devices
(which have limited resources)
and massive server farms
(where poor performance would make them
<a href="http://inhabitat.com/facebooks-fancy-new-server-farm-will-be-cooled-naturally-by-arctic-air/facebook-lulea-node-pole3/">further heat
their environment</a>).
The <a href="http://benchmarksgame.alioth.debian.org/u32/which-programs-are-fastest.php">benchmarks game</a>
includes some speed analysis of various programs written in various languages
[<a href="http://benchmarksgame.alioth.debian.org/u32/which-programs-are-fastest.php">BenchmarksGame</a>].
The posting
&#8220;<a href="http://forums.xkcd.com/viewtopic.php?f=11&amp;t=108685">Approximate speed classes of programming languages</a>&#8221; took that data
and grouped languages into different tiers
based on their approximate speed
[<a href="http://forums.xkcd.com/viewtopic.php?f=11&amp;t=108685">Jplus2014</a>].
<p>
No benchmark is perfect, and it is always best to measure performance
for a specific situation.
Still, I prefer measured numbers to big guesses.
If performance is your most important criteria, and you do not want to
write in assembly language, there are other viable options.
Based on those and similar studies, other reasonable languages
when high performance is needed include:
<ul>
<li><a href="http://www.rust-lang.org/">Rust</a>
generally has very good performance.
<li>Fortran.
Fortran implementations often beat everyone else in performance, especially
in its primary niche of numerical calculation.
There are many (mostly older) libraries maintained in Fortran.
However, it is not clear to me that many people would be willing to
switch a lot of code to this granddaddy of all other programming languages.
In particular, to my knowledge even modern Fortrans
do not have standard mechanisms to interface to
lower-level hardware interfaces (for programs that need that).
<li>Ada.
Ada was designed to be used for hard real-time system implementation,
so it is not surprising that it has good performance and
includes mechanisms to access low-level components.
Ada is also specifically designed to counter errors, preferably at compile time.
Even its syntax is specifically designed to counter errors, and
Ada certainly counters buffer over-reads like Heartbleed.
Many people do not like Ada, often because Ada&#8217;s very strict
static type-checking makes it harder to get a program accepted by
an Ada compiler.
However, this strict type-checking is one of its
key mechanisms for compile-time detection of defects;
Ada can detect at compile time or run time many problems that other
programming languages do not detect.
Ada is widely used in some high-assurance areas
like air traffic control, railroad control, and the like.
<a href="http://archive.adaic.com/projects/atwork/boeing.html">Nearly
all of the Boeing 777 code is in Ada</a>.
The CubeSat program recently created an unintentional
real-world test of Ada combined with SPARK.
Twelve university CubeSats were launched.
Only CubeSat out of those twelve are working properly as of June 2014,
the one from Vermont Tech, and it was the only one that
used SPARK Ada for its code.
Project leader Dr. Carl Brandon says, &#8220;The use of SPARK/Ada helped
make our software much more reliable than the others.&#8221;
[AdaCore 2014]
No formal specification method nor design methodology
was used in the development of the CubeSat software
(although SPARK supports them), but the developers did use
the SPARK/Ada information flow analysis and proofs of freedom from
runtime error
[<a href="http://www.cubesatlab.org/doc/brandon-chapin-AdaEurope2013.pdf">Brandon</a>].
Program correctness is not vitally important in many programs,
and in any case Ada is not the solution to every problem.
However, Ada is a useful language to consider
when correctness is vitally important yet
run-time performance cannot be sacrificed.
<li><a href="http://www.ats-lang.org/">ATS</a>.
This is not a well-known or widely-used programming language, but
it did remarkably well in this particular benchmark suite.
I should note that ATS is
no longer listed in more recent performance charts
(I do not know why).
</ul>

<p>
There are many other programming languages, especially if
you&#8217;re willing to give up a little speed
as determined by that benchmark.
If you're willing to accept the overhead of automated garbage collection
systems as a form of memory management, there are many compiled languages
with good performance.
For example,
<a href="http://golang.org/">Go</a> (developed by Google) has
good performance.
(There&#8217;s even been some work on
<a href="http://gophercon.sourcegraph.com/post/83820197495/russ-cox-porting-the-go-compiler-from-c-to-go">converting C to Go automatically</a>,
though currently that work is only focused on translating the compiler,
not C in general.)
Java has reasonable performance on modern JITs, once it gets going,
but there is a non-trivial startup time.
Other languages that look promising by these benchmark metrics include Scala,
Free Pascal, Lisp SBCL (Steel Bank Common Lisp),
Haskell, C# on Mono, F# on Mono, and OCaml
(depending on how you cut off the next tier).
Neither the <a href="http://dlang.org/">D programming language</a>
nor the
<a href="http://nimrod-lang.org/">Nimrod programming language</a>
are listed in that benchmark, but they are also designed for efficiency.


<p>
Of course, if speed is not critical, there are a huge
number of languages available.
<a href="http://gcn.com/articles/2014/04/24/programming-language-security.aspx?s=gcntech_250414">At least one study</a>
suggests that there is no statistical difference in the number of
vulnerabilities in programs written with .NET, Java, ASP, PHP,
Cold Fusion, and Perl.
I often use Python when speed is not important because it has a
clean and easily-understood syntax.
Other languages, such as Ruby and Clojure, have many fans.
Scheme is powerful (and I think the
<a href="http://readable.sourceforge.net/">readable Lisp extensions</a>
solve the readability problems often noted about Lisp-based languages
like Scheme).
All of these other languages are safer than C, C++, or Objective-C, in
the sense that all of them protect against buffer over-reads
(and buffer over-writes)
by default.

<p>
There are just too many programming languages to list,
so I&#8217;ll stop here.
My goal is not to list all alternatives;
my goal is to make it clear that there <i>are</i> alternatives.

<p>
Performance is not just about speed;
memory management approaches can also be important.
This is especially true on mobile devices like smartphones.
C, C++, and Objective-C do not provide automated garbage collectors;
many other languages include them.
Developers are generally more productive (in terms of functionality over time)
if they do not have to think about memory management,
but in some environments that is unrealistic.
Drew Crawford has a lengthy discussion about mobile device development,
where he states that
&#8220;automated garbage collectors work well if you
have at least six times as much memory as needed, but efficiency
can [be] greatly harmed if there is less than four times as much memory.
iOS has formed a culture around doing most things manually and trying
to make the compiler do some of the easy parts.
Android has formed a culture around improving
a garbage collector that they try very hard not to use in practice.
But either way, everybody spends a lot of time
thinking about memory management when they write mobile applications.
There&#8217;s just no substitute for thinking about memory&#8221;
[<a href="http://sealedabstract.com/rants/why-mobile-web-apps-are-slow/">Crawford2013</a>].
Automated garbage collection is deprecated in OS X Mountain
Lion v10.8, and will be removed in a future version of OS X; Automatic
Reference Counting (ARC) is the recommended approach instead
for OS X and iOS
[<a href="https://developer.apple.com/library/mac/releasenotes/ObjectiveC/RN-TransitioningToARC/Introduction/Introduction.html">Apple2013</a>].
Again, there are reasons people choose C, C++, and Objective-C.


<p>
So why are programs in C, C++, and Objective-C often higher-performance
(in speed and memory management) than many alternatives?
The answer is, in part, because the languages are designed to be that way.
In particular,
C is designed to make it possible to write programs that run quickly
and work well with limited resources (e.g., little memory).
The C rationale states that a key principle in C is
&#8220;trust the programmer&#8221;
and that
&#8220;many operations are defined to be how
the target machine&#8217;s hardware does it&#8221;
(which impedes portability but helps performance).
Also,
<a href="https://stackoverflow.com/questions/418914/why-is-c-so-fast-and-why-arent-other-languages-as-fast-or-faster">C&#8217;s performance cost model is transparent</a>,
so a C or C++ developer
can usually estimate the performance aspects of a construct before
using it.
Different programming languages provide different levels of abstraction,
and languages with higher-level
abstractions can sometimes make detailed control more difficult.
Indeed, many developers have difficulties estimating
the performance aspects of programs written in languages that are
significantly higher-level than C.
Humans do not always estimate correctly, of course, but it is often
hard to achieve good performance if it is hard to estimate performance.
Performance transparency is especially important in cryptography,
because developers need to counter timing attacks and
electrical power attacks (my thanks to Markus Armbruster
for pointing out this link between cryptography and performance transparency).
So merely obtaining high performance is not enough; it is sometimes
necessary to ensure that timing or power variances are small,
yet few tools provide these measures.
Cryptographic libraries can be written in other languages, of course, but
other complications can arise depending on what language is used.
Of course, implementations greatly vary in their performance;
heavily-optimized compilers and run-times
can achieve great performance compared to
compilers and run-times that are not as heavily optimized.
<!--
Here's information on opmtimizing Haskell:
http://book.realworldhaskell.org/read/profiling-and-optimization.html
-->

<h3 id="interfacing-languages">Interfacing with other languages</h3>
<p>
Many developers choose C, C++, or Objective-C
to simplify interfacing with other components.
Many useful utilities have C interfaces, and most language
infrastructures can call libraries written in C.
However, many programming language systems have easy ways to
both call C routines and to be called by other systems using C interfaces.
Thus, this isn&#8217;t as important a reason today to choose these languages.

<h3 id="reducing-c-risks">Reducing language risks</h3>
<p>
Developers using C, C++, and Objective-C <i>can</i> reduce their risks in
various ways, such as using less-risky library functions and using
language subsets.
These merely reduce the risk somewhat, not eliminate it;
it is really easy to make a mistake even when using these facilities.
Still, risk reduction can be valuable.

<p>
Creating library functions that reduce the likelihood
of security vulnerabilities, and then using them, can help reduce
the risk of vulnerabilities.
This is especially true if these functions are part of the
standard library for these languages, since these will tend to be
widely-understood, portable, and well-supported.
Here are a few examples for various languages:
<ol>
<li><i>C</i>.
There are many options for C, but all current options have problems;
in many cases the better options are not widely available.
<a href="http://www.informit.com/articles/article.aspx?p=2036582">&#8220;Secure Coding in C and C++: Strings and Buffer Overflows&#8221; by Robert C. Seacord
(Apr 24, 2013)</a>
(an extract from his book &#8220;Secure Coding in C and C++&#8221; Second Edition)
briefly explains various options for safely handling
string buffers in C and C++.
The latest C standard (C11) has added many
functions to the built-in C library,
particularly ones that provide bounded string handling for
fixed-size buffers.
These additions can help in some cases.
In particular, see appendix K (&#8220;bounds-checking interfaces&#8221;),
which adds functions like <tt>gets_s()</tt>, <tt>strcpy_s()</tt>,
<tt>strcat_s()</tt>, and <tt>tmpfile_s()</tt>.
C programs can also use strlcpy/strlcat, simple and safer functions
for fixed-size buffers
that were originally developed by the OpenBSD developers.
Unfortunately, these functions (both appendix K and strlcpy/strlcat)
are still not widely available on all major platforms.
In addition, the C standard still fails to provide any easy-to-use
dynamically-resizing mechanisms, a remarkable failing in 2014.
The 1999 version of C (C99) did add support for variable-length arrays
(where lengths vary at runtime), which was a small step.
However, the later C11 specification
made support for variable length arrays optional (per 6.7.6.2(4)),
making even their use a potential portability problem.
In general,
<a href="http://www.drdobbs.com/managed-string-library-for-c/184402023">the
C standard still fails to provide easy-to-use dynamically-resizing
string/buffer functions</a>.
Some simple functions, like <tt>asprintf()</tt> and
<tt>reallocarray()</tt>, are widely available, but their
omission from the standard means many developers will continue to avoid them
(and insert vulnerabilities instead).
ISO/IEC TR 24731-2 does define new and safer
string-handling functions for C that use dynamic allocation, but it
is not universally avaialble.
Other libraries are also available that provide missing capabilities,
but again they are not standard, and the
C standard does not provide mechanisms to simplify their syntax.
C programs can use the glib (not glibc) library with its <tt>GString</tt> type,
or one of the many other solutions such as
<tt>string_m</tt> and <tt>SafeStr</tt>.
Developers of portable C software often end up using clunky mechanisms
to support dynamically-sized buffers
(e.g., using <tt>realloc()</tt>,
manually tracking and passing the buffer size separately, and also manually
checking every buffer reference to see if it is within bounds).
There should be a standard
way to easily create and use dynamically-resizing buffers in C.
<li><i>C++</i>.  C++ programs can use the class <tt>std::string</tt>
or its cousins
<tt>std::wstring</tt>, <tt>std::u16_string</tt>, and <tt>std::u32_string</tt>.
<li><i>Objective-C</i>.
Objective-C users can use its classes <tt>NSString</tt> and
<tt>NSMutableString</tt>.
</ol>

<p>
You can also use a subset of some language
(including C, C++, and Objective-C) with the goal of increasing
security (e.g., by avoiding dangerous constructs).
However, when designing subsets it is important to measure actual
problems, since otherwise the subset may be
unhelpful or even counter-productive.
Les Hatton has developed
<a href="http://www.leshatton.org/index_SA.html">EC--
(a safer subset of C)</a> based on measurements of actual code
[<a href="http://www.leshatton.org/index_SA.html">Hatton2003</a>].
Les Hatton has also published devastating
critiques of C subsets like MISRA C 1998 and MISRA C 2004,
where he finds that
&#8220;both versions of the MISRA C standard are far too noisy
to be of any real use... [the]
real to false positive ratio is not much better in MISRA C 2004
than it was in MISRA C 1998 and it is unacceptably low in both&#8221;
[<a href="http://www.leshatton.org/index_SA.html">Hatton2005</a>].

<h3 id="language-implications">Security implications of other languages</h3>
<p>
You can write insecure software in any language.
For example, vulnerabilities to SQL injection are another common weakness,
and this can occur in practically every language
(including C, Java, and many others).
However, most languages provide mechanisms
(like prepared statements in a built-in or external library)
that are easy to use and completely avoid common problems like SQL injections.
Yes, you have to be careful to use these mechanisms...
but that is true for C, Java, and most other languages.

<p>
Selecting safer languages does not avoid all problems that can
occur in unsafe languages.
Safer languages are typically implemented on top of
infrastructures and libraries that are themselves written
in unsafe languages like C, C++, or Objective-C.
The run-time libraries of most language implementations today
are written in C, many libraries are written in C,
most applications eventually call down to the C run-time library,
and practically all operating system kernels
in wide use today are written in C.
Still, it is a matter of degree; vulnerabilities specific to
C, C++, or Objective-C can only occur in the
parts written in those languages, so selecting safer languages
can reducing overall risk.
(I knew of this long before, but my thanks to David Ramos at
Stanford University for encouraging me to add this information.)

<p>
Some languages (like C# and Ada) normally counter vulnerabilities
but have escape mechanisms that let you temporarily disable protections
(e.g., both can temporarily disable buffer over-read protection).
But these escape mechanisms are easily found and are isolated in
well-written code; they help reduce the overall risk by
isolating unsafe actions to small, easily-reviewed portions.
In most languages, people have to <i>work</i>
to create buffer over-read vulnerabilities (if they are possible at all).
In contrast, in C, C++, and Objective-C, you have to do
extra work to <i>avoid</i> buffer over-read vulnerabilities.

<p>
Other language traits, such as strong static typing,
also affect vulnerability density or reliability.
One experiment
fuzzed programs in various languages and demonstrated that
&#8220;Languages with strong static typing (Java, Haskell, C++)
caught more errors at compile time than languages
with weak or dynamic type systems
(Ruby, Python, Perl, PHP, and JavaScript).
Somewhat predictably, C fell somewhere in the middle,
confirming a widely-held belief that its type system
is not as strong...
However, C produced a higher number of run-time errors,
which in the end resulted in a rate of incorrect output
similar to that of the other strongly-typed languages...
code written in weakly-typed languages is [also]
more probable to run without a ... crash or an exception...
these two differences result in a higher rate of
wrong output from programs written in languages with weak typing
[than with programs in languages with strong static typing]&#8221;
[<a href="http://www.dmst.aueb.gr/dds/pubs/conf/2012-PLATEAU-Fuzzer/pub/html/fuzzer.html">Spinellis2012</a>].
In this paper I have focused on countering
improper restriction of operations within the bounds of a memory buffer,
since that is the underlying cause of Heartbleed.

<p>
It&#8217;s also true that other languages sometimes have other
additional challenges when using them to create secure software.
For example, I know of no way to securely erase data inside Java.
This is because Java lacks functionality like .NET&#8217;s SecureString;
Java structures (like a String, raw array, or StringBuffer)
may end up copied several times in memory due to how
memory allocation and garbage collector work.
This is not unique to Java; it&#8217;s hard to securely erase data
in <i>many</i> languages.
This is in contrast to C; C developers often get it wrong
(the naive approach allows C compilers to remove the erasure),
but it <i>is</i> possible to securely erase data in C.
However, in Java (and some other languages), it is relatively easy to
solve this by creating a small non-Java module to securely erase a few values;
the rest of the program is still protected
against buffer over-reads and over-writes.
Besides, exploiting additional memory copies requires
a significant amount of access to a program or its environment.
Secure erasures are often a useful damage-reduction measure
(such as the damage caused through Heartbleed).
In contrast, buffer over-reads and over-writes
can sometimes be exploited directly
with far less access, sometimes simply through a network connections.
In general, failing to restrict buffers is far more dangerous than the
other problems that most other languages add.

<p>
It is more difficult to write <i>secure</i> software
in C, C++, and Objective-C.
Most languages have built-in and complete
protections against buffer over-reads and over-writes...
but C, C++, and Objective-C are notable exceptions.
On the other hand, it should be obvious <i>why</i> they are used.

<h3 id="language-selection">Language selection impact</h3>
<p>
Whenever a new security-relevant program is begun, the
programming language should be carefully considered.
Choosing a safer language, where reasonable, can automatically
eliminate entire sets of potential vulnerabilities - including
the buffer over-reads that permitted Heartbleed.
Also, computers are much more powerful than they were historically;
in many cases some performance can be traded away.
What&#8217;s more, when starting to write a new program, using another
programming language is nearly zero-cost.
I believe there are cases where less-safe languages are appropriate,
and rewriting code takes a lot of effort.
However, using almost any language other than C, C++, or Objective-C
will at <i>least</i> restrict operations to be within their boundaries,
and vulnerabilities due to improper restriction
often have extremely large impacts.

<p>
Choosing a safer language at the beginning of a project,
especially a widely-used language with at least one FLOSS implementation,
has very low or no cost.
I have identified safer languages
as a higher-cost approach because <i>switching</i>
a non-trivial program (like OpenSSL) to a different safer language
is a big effort.


<h2 id="sound-static-analyzer"><span id="complete-static-analyzer">Complete static analyzer (static analysis)</span></h2>

<p>
A <i>complete</i> static analyzer,
also sometimes called a <i>sound</i> static analyzer,
is designed to find <i>all</i>
vulnerabilities of a given category.
Creating these kinds of tools is rather challenging for C,
but it is possible.
However, these analysis tools often
have to limit what constructs they can use,
and developers typically have to limit their programs to work within
those constructs and/or provide additional annotations to provide
additional information the tool needs.
For example,
<a href="http://www.astree.ens.fr/">Astrée</a>
is a static program analyzer that aims to prove
the absence of Run Time Errors (RTE) in C programs,
including out-of-bounds array indexing.
However, Astrée requires that the program avoid
dynamic memory allocation and recursion.
Note that the current OpenSSL implementation strongly
depends on dynamic memory allocation.

<p>
Tools that focus on finding everything will often report
issues that are <i>not</i> vulnerabilities, which must then be analyzed
to determine if they are actually vulnerabilities.
However, if it is critical to counter <i>all</i> vulnerabilities,
that trade-off may be worth making.

<h2 id="thorough-audit">Thorough human review / audit (static analysis)</h2>
<p>
A thorough independent human review of software, specifically focused
on ensuring security and finding vulnerabilities,
is typically an excellent way to find vulnerabilities.
These reviews, aka an audit, should presume the software is vulnerable
until shown otherwise.

<p>
Neel Mehta of Google was one of the two discoverers of Heartbleed.
<a href="https://news.ycombinator.com/item?id=7556826">According
to comments on Y Combinator&#8217;s &#8220;Hacker News&#8221;</a>,
Neel Mehta found Heartbleed by auditing code
[<a href="https://news.ycombinator.com/item?id=7556826">DrewHintz</a>].

<p>
The notion that thorough human review is often better than
tools that use heuristics to find vulnerabilities
makes intuitive sense, but more importantly,
experimental data confirms it.
For example, Kupsch and Miller found that a human audit
using their First Principles Vulnerability Assessment (FPVA) approach
was far more comprehensive on a sample program than using
Coverity Prevent and Fortify Source Code Analyzer (SCA)
[<a href="http://research.cs.wisc.edu/mist/papers/ManVsAutoVulnAssessment.pdf">Kupsch2009</a>].
All human reviews can unintentionally fail to identify vulnerabilities,
but such reviews can do quite well.
In the Kupsch experiment, the FPVA human review
found 15 serious vulnerabilities;
Fortify found 6, Coverity found 1, and neither automated tool
found a vulnerability not found by the human review.

<p>
But the downside of human review
should also be obvious: It takes effort and expertise
to do this kind of review, and changes also require review.
Human review is simply not practical to do for all software,
and even when it is, the challenge is finding a way to fund it.

<p>
A minor downside is that human review can only review a specific version,
yet software changes over time.
This, however, is not serious problem.
A good review will not only specifically analyze the software to
see if it is correct, but will also identify systemic changes that
would greatly reduce the likelihood of vulnerabilities
even as the software changes.

<p>
Note that this kind of audit is different than a typical
simple review of a patch before acceptance.
The addition that created the Heartbleed vulnerability
was created by developer who was trying to avoid vulnerabilities, and
it was accepted by another reviewer.
However, patch reviews usually simultaneously review the
functional improvement and look for vulnerabilities, so it is
much easier for them to miss vulnerabilities.
As noted earlier, a
<a href="#review-field-required">human review requiring
validation of every field</a> for each patch
would probably have caught this...
but now that the code exists, merely reviewing new patches will
not be enough.
Trying to review every patch separately at this point is
probably not cost-effective.
Also, patch-by-patch review can miss global problems that might
be otherwise missed.
A separate review, focusing on vulnerabilities across an entire system,
is often more effective.

<p>
In many cases the software should be modified and simplified
before this review/audit takes place, and I think that is true of
OpenSSL especially.
Programs that are excessively complicated are difficult for both
tools and humans to properly evaluate.
OpenSSL uses unnecessarily complex structures, which makes it harder
to both humans and machines to review.

<p>
This kind of review really does happen.
Indeed, around the same time as Heartbleed was announced,
a security review of a key part of TrueCrypt was released
[<a href="https://opencryptoaudit.org/reports">Junestam2014</a>]
[<a href="https://isecpartners.github.io/news/2014/04/14/iSEC-Completes-Truecrypt-Audit.html">Ritter2014</a>].
(<a href="http://www.infoworld.com/d/open-source-software/truecrypt-or-false-would-be-open-source-project-must-clean-its-act-230862?source=rss_infoworld_top_stories_">TrueCrypt&#8217;s odd license is probably not FLOSS</a>, and it
<a href="https://lwn.net/Articles/599930/">abruptly shut down in May 2014</a>
with a
<a href="http://www.reuters.com/article/2014/05/29/us-internet-security-encryption-idUSKBN0E925M20140529">possibility of revival</a>.
It is still an independent audit of
software with publicly-viewable source code.)
More recently, the
<a href="https://threatpost.com/openssl-receives-funding-for-developers-will-undergo-security-audit/106349">Linux Foundation&#8217;s
Core Infrastructure Initiative will fund an audit
of OpenSSL by the Open Crypto Audit Project</a>
(as well as provide enough money to the OpenSSL project
to hire two full-time developers).

<p>
In short, thorough independent human review 
takes significant effort and expertise,
but it can produce great results.

<h2 id="formal-methods">Formal methods (static analysis)</h2>

<p>
But what if you really want to be essentially <i>certain</i> that
a program does exactly what it is supposed to do?
There is a set of approaches called <i>formal methods</i>
that can give far greater confidence than any of the techniques
listed above.
Formal methods involve
the use of
&#8220;mathematically rigorous techniques and tools for the specification, design and verification of software and hardware systems&#8221;
[<a href="http://shemesh.larc.nasa.gov/fm/fm-what.html">Butler</a>].
Given the difficulties of applying formal methods, they are more likely to be
small programs or modules at the moment,
but it is definitely possible to apply formal methods.
Besides, if you really want to have extremely high confidence
that a program does or does not do something,
formal methods are still the <i>only</i> way to achieve that confidence.

<p>
There are various ways to apply formal methods.
Some people only use formal methods to create specifications,
and do nothing more with formal methods
(this is called <i>level 0</i> or <i>formal methods lite</i>).
Some people may go a little further, proving some statements about the
specifications or refining the specification towards a more concrete
model (aka <i>level 1</i>).
Neither of these approaches would have found Heartbleed.
For Heartbleed, formal methods would have to be applied to create proofs
about the code itself (aka <i>level 2</i>), at the
source or executable code level, so that is what I will focus on here.

<p>
In general, proving claims about code itself requires that you
create a formal specification (describing what the code <i>should</i> do),
and then a proof that the code meets that specification.
Just about any specification would have sufficed to find Heartbleed, though.
Trying to read from an out-of-bounds area
(an &#8220;improper restriction&#8221;) is undefined in C,
and in most systems you cannot prove anything if the program
permits an undefined activity.
Thus, trying to prove anything would force you to prove it
could not read beyond... and you would not be able to do it.
(Some systems can sometimes give you counter-examples; a counter-example
would immediately reveal this problem.)
The attraction of formal methods is that you can go far beyond this;
with formal methods you can prove that the program
always does something specific when it runs.

<p>
In practice proofs about programs typically involve systems that
allow annotation of code.
It&#8217;s worth noting that
annotation systems can often be used in a
variety of ways short of formal proof.
For more information, see my previous comments about
<a href="#annotation-systems">annotation systems</a>.

<p>
If you&#8217;re interested in more, especially more information about
the FLOSS tools available that support formal methods, see
<a href="https://dwheeler.com/secure-class/">my presentation
on formal methods from my class on developing secure software</a>.
An interesting suite for formal methods
is Toccata (formerly ProVal),
which combines Frama-C, Why3 (formerly Why), and many
automated and interactive tools.
By combining many different tools it potentially makes it possible
to prove programs correct with far less effort than before.
What&#8217;s more, they can handle a large subset of C (as well as other
languages); most formal methods systems cannot.
<a href="http://www.spark-2014.org/">SPARK 2014</a> is based on Ada
but lets you prove claims about programs, and they have recently
connected it to Toccata.
Recent advances in algorithms (e.g., SAT solvers) and
greatly increased distributed hardware performance are gradually
making these approaches easier to apply (not easy, but easier).

<p>
Examples of formally verified programs other than cryptographic libraries
include seL4 (<a href="https://microkerneldude.wordpress.com/2012/10/02/giving-it-away-part-2-on-microkernels-and-the-national-interes/">a proven implementation of the L4 microkernel interface</a>),
<a href="http://compcert.inria.fr/compcert-C.html">CompCert C</a> (a compiler for a subset of C),
<a href="https://cakeml.org/">cakeML</a> (a compiler for a subset of ML),
<a href="http://www.adacore.com/sparkpro/tokeneer/discovery/">Tokeneer</a>
(an identity system that primarily serves as a complete demonstration
of using Z and SPARK), and
<a href="http://cps-vo.org/node/12230">iFACTS</a>
(part of an air traffic control system using Z and SPARK).

<!--
ACL2 applied to binary x86 code, e.g.:
http://www.cs.utexas.edu/~hunt/research/crash-ut/
-->

<p>
The OpenSSL implementation of SHA-256 has been formally
proved as a full formal machine-checked verification.
The proof was created using the Coq proof assistant and
the Verifiable C program logic (a separation logic for C).
[<a href="http://www.cs.princeton.edu/~appel/papers/verif-sha.pdf">Appel2014</a>]

<h2 id="others-might-work">Other approaches might have worked</h2>

<p>
I have tried to create a complete list, but there may be other approaches
that would have detected the Heartbleed vulnerability ahead-of-time.
People are continuously developing new approaches, of course.
In addition, sometimes it is not clear that a particular approach
would have found Heartbleed ahead-of-time.
<p>
For example, is not at all clear to me that current
<a href="https://en.wikipedia.org/wiki/Concolic_testing">concolic testing</a>
tools would have found the Heartbleed vulnerability.
In at least some cases it seems doubtful.
<a href="https://jburnim.github.io/crest/">CREST</a>, for example,
is an automated test generation tool for C that uses concolic testing.
CREST is FLOSS, though it depends on at least one non-FLOSS component.
However, CREST currently only reasons symbolically about
linear integer arithmetic, so it seems unlikely that it would have
worked with OpenSSL.
If anyone can confirm or disprove that a concolic tester
would have found Heartbleed, please let me know.
<p>
It is possible that the results of the
<a href="http://www.iarpa.gov/Programs/sso/STONESOUP/stonesoup.html">STONESOUP</a>
research program, such as the
<a href="http://www.grammatech.com/research/technologies/peasoup">Preventing Exploits Against Software of Uncertain Provenance (PEASOUP) project</a>,
could have countered Heartbleed.
However, I have not been able to confirm a STONESOUP project that
(1) countered Heartbleed and (2) was ready and available for production use
at the time instead of just research use.
If anyone has more information that they would like to share with me,
especially if they are willing to make it public, please let me know.


<h1 id="preconditions">Preconditions</h1>

<p>
Many of these techniques have important preconditions;
let&#8217;s talk further about each.

<h2 id="simplify-code">Simplify the code</h2>

<p>
Many of the static techniques for countering Heartbleed-like
defects, including manual review, were thwarted because the OpenSSL
code is just too complex.
Code that is security-sensitive needs to be
&#8220;as simple as possible&#8221;.
<p>
Many secure software developers suspect that first using
&#8220;software quality&#8221;
tools to detect especially complicated structures,
and then simplifying those structures, is likely to produce
more secure software.
The idea is that static analysis approaches (both automated tools
and manual human review) tend to have problems with complex code;
using tools to detect some of that complexity, and simplifying the code,
may make the static analysis approaches more effective.
I suspect that they are right, but I have not seen any
rigorous data that supports it.
Thus, this seems like a plausible idea, but I hope that someone
will eventually create and publish some scientific research to
support or refute this hypothesis.
<p>
In any case, simplifying code is more than running software quality tools.
It is a mindset; there should be a continuous effort to simplify
(refactor) the code, because otherwise just adding capabilities will slowly
increase the software complexity.
The code should be refactored over time to make it simple and clear,
not just constantly add new features.
Little things like code formatting matter, since
badly-formatted code is much harder for humans to review.
The goal should be code that is <i>obviously right</i>,
as opposed to code that is so complicated that
<i>I can&#8217;t see any problems</i>.

<p>
Overly-complex code often leads to vulnerabilities.
In 2006 Debian accidentally broke the OpenSSL
pseudo-random number generator by modifying the software to
eliminate a valgrind warning.
However, the person modifying the software did not really
understand it.
That person asked for help, but the very complexity of the OpenSSL code
made it hard for others to realize that the change introduced a vulnerability.
Cox reviews what happened and concludes:
&#8220;Try not to write clever code. Try to write well-organized code.
Inevitably, you will write clever, poorly-organized code.
If someone comes along asking questions about it, use that as a
sign that perhaps the code is probably too clever or not well enough
organized. Rewrite it to be simpler and easier to understand.&#8221;
[<a href="http://research.swtch.com/openssl">Cox2008</a>].

<p>
The LibreSSL developers have taken the OpenSSL code and are specifically
working to simplify the code.
<a href="http://www.openbsd.org/papers/bsdcan14-libressl/mgp00001.html"><i>LibreSSL - An OpenSSL replacement</i> (by Bob Beck)</a> describes
some problems of the OpenSSL codebase
(from the viewpoint of the LibreSSL project fork).
They are reformatting the code to make it much easier to understand.
They are doing many sensible things too, such as
removing code to support long-obsolete VAX VMS systems.
The
<a href="http://opensslrampage.org/">opensslrampage.org</a> website
captures some of the comments by the LibreSSL developers.
Some of their comments are over-the-top (after all, they&#8217;re
trying to justify why people should use their fork instead),
but many of their comments are completely legitimate.
For example, OpenSSL creates portable C code in poor ways
(e.g., by creating a complex nest of <tt>#ifdef</tt> clauses
and a set of functions that have similar names yet different semantics
compared to standard functions).
There are much better ways to create secure yet portable programs, e.g., see
[<a href="http://www.openbsd.org/papers/portability.pdf">Miller2005</a>].
However, the LibreSSL developers are also removing code people care about.
For example, they are also
<a href="http://opensslrampage.org/post/83555615721/the-future-or-lack-thereof-of-libressls-fips-object#disqus_thread">removing support for FIPS 140-2 validation</a>,
which is <i>required</i> for US government use and something many private
companies want.
There is always a conflict between making the code simple and
making the code useful in many circumstances;
the simplest code does nothing!
Still, it is clear that many programs (including OpenSSL) could
be made much simpler than they currently are.


<h2 id="simplify-api">Simplify the Application Program Interface (API)</h2>

<p>
Although it is slightly out-of-scope for this paper, a related
problem is that application program interfaces (APIs)
are often absurdly complex or difficult to use.

<p>
Most cryptographic libraries and data-transport libraries
are absurdly complex.
They often present to developers a &#8220;confusing array of settings and options&#8221;.
As a result, a vast number of applications and higher-level libraries
incorrectly use the cryptographic libraries, resulting in
vulnerable systems.
Most of these problems have been worked out in web browsers, but
they continue to be a problem in all other code.
For more information, see
&#8220;The Most Dangerous Code in the World:
Validating SSL Certificates in Non-Browser Software&#8221;
[<a href="https://crypto.stanford.edu/~dabo/pubs/abstracts/ssl-client-bugs.html">Georgiev2012</a>].

<p>
Although this is technically not a vulnerability in
the SSL/TLS implementation, that is irrelevant.
The cryptographic library is the component that creates the
overly-complex interface, so as a result, it is still the component
at fault.

<p>
A related problem is that underlying libraries and systems that
cryptographic libraries are built on have APIs that are
often overly difficult to use.
The C standard omits important functionality like
<tt>asprintf</tt> and
<a href="http://www.openbsd.org/cgi-bin/man.cgi?query=reallocarray"><tt>reallocarray</tt></a>
(<a href="http://opensslrampage.org/post/87313919966/convert-53-malloc-a-b-to-reallocarray-null-a#disqus_thread">reallocarray is useful!</a>).
As a result, programmers have to work around these omissions,
but their solutions often lead to bugs.
Some of these bugs, unfortunately, lead to vulnerabilities.


<h2 id="allocate-normally">Allocate and deallocate memory normally</h2>

<p>
Secure programs should allocate and deallocate memory normally,
without special program-specific allocation systems or
memory caching systems.
At the very least it should
easy to disable them, and testing should ensure that disabling them works.
Some of the techniques for mitigating the effects of Heartbleed
appear to have been thwarted because of the way that OpenSSL
allocated memory.

<p>
The basic problem was that OpenSSL included an
application-specific caching freelist of unallocated memory.
Its purpose was to speed up allocations
when the same size is repeatedly requested.
By default OpenSSL did allocate memory normally (going through malloc),
However, instead of deallocating when a memory region was no longer in use,
in some cases it put the region into a freelist of unused regions,
making it ready for immediate reuse.
This cached freelist unfortunately subverted some mitigation mechanisms in
some operating systems and C run-times, because
they were not always notified when a memory region was no longer in use.

<p>
As
<a href="http://article.gmane.org/gmane.os.openbsd.misc/211963">Theo de Raadt noted</a>, &#8220;years ago we added exploit mitigations counter measures to libc
malloc and mmap, so that a variety of bugs can be exposed.
Such
memory accesses will cause an immediate crash, or even a core dump...
[once the problem is revealed]
then the bug can be analyzed, and fixed forever.
Some other debugging toolkits get them too.  To a large extent these
come with almost no performance cost.
But around that time OpenSSL adds a wrapper around malloc &amp; free so
that the library will cache memory on [its own]....
OH, because SOME platforms have slow performance, it means even if you
build protective technology into malloc() and free(), it will be ineffective.
On ALL PLATFORMS, because that option is the default,
and Ted&#8217;s tests show you can&#8217;t turn it off
because they haven&#8217;t tested without it in ages.
So then a bug shows up which leaks the content of memory mishandled by
that layer.  If the memory had been properly returned via free, it
would likely have been handed to munmap, and triggered a daemon crash
instead of leaking your keys.&#8221;
[<a href="http://article.gmane.org/gmane.os.openbsd.misc/211963">de Raadt</a>]
[<a href="http://www.tedunangst.com/flak/post/analysis-of-openssl-freelist-reuse">Ted</a>]

<p>
There seems to be a lot of confusion about what exactly went
wrong with OpenSSL&#8217;s memory allocation approach, and
Chris Rohlf has made a number of useful clarifications
[<a href="http://blog.leafsr.com/2014/04/11/my-heart-is-ok-but-my-eyes-are-bleeding/">Rohlf2014</a>].
I think these clarifications are important, because we must first
understand the problem before we can fix it.
In particular, Rohlf points out that OpenSSL
<i>did</i> use the standard malloc() C memory allocation routine
(by default) when it wanted a whole new memory block.
The issue is that once a memory block was allocated,
OpenSSL itself managed that memory further.
Rohlf also (correctly) points out that in most typical
environments this use of a freelist is irrelevant;
a freelist puts different memory allocations together, but
many typical memory allocation systems also
put different memory allocations together.
However, while Rohlf is absolutely correct that typical
memory allocation implementations do the same thing, the point
is the OpenSSL implementation seriously impeded various
mitigation measures.
There is another confusion about the memory allocation system
of OpenSSL, but first we need to go over some basics.

<p>
A common approach is to handle at
least some memory allocations and deallocations specially.
The idea is to cache and reuse some objects or memory regions when
they are no longer in use;
in some cases this approach can significantly improve performance.
More specific examples of this approach
include specially handling a common memory allocation size, and/or
reusing objects or memory regions by holding a cache of unused ones.
There are a number of specific techniques for doing this, including creating an
<a href="https://en.wikipedia.org/wiki/Object_pool">object pool</a>,
as well as creating a <i>slab allocator</i>.
The
<a href="https://developer.gnome.org/glib/2.40/glib-Memory-Slices.html">Glib
library (the basic support for GTK+) includes a mechanism
called a <i>memory slice</i> to improve memory allocation performance</a>.
Many graphical user interfaces (GUIs) and programs
that are not security-sensitive use these approaches.

<p>
It turns out that some of these approaches
can disable some detection tools
like address sanitizer (ASan), electric fence, and valgrind.
This is particularly a problem for fuzz testing;
if these tools are disabled, then fuzz testing
becomes much less effective.
Indeed, fuzz testing may not be able to detect many out-of-range reads
when these approaches are in use.

<p>
Earlier reports about OpenSSL seemed to suggest that OpenSSL
was using an approach that
self-managed a larger region of memory and then subdivided it further.
This is what happens in a slab allocator or memory slicing implementation.
This approach, intended to improve performance,
would foil ASan and guard page systems, and thus,
could inhibit detection of over-reads entirely in those cases.
Older versions of this paper reported that this seemed to be what was
happening.
However, I have since delved more into the OpenSSL code,
and this does <i>not</i> seem to be true in OpenSSL.
That is good news of a sort for Heartbleed.
Still, these kinds of allocation schemes are relatively common,
and I know of no one who has warned anyone about the risks
of these approaches.

<p>
Security-sensitive software should avoid using memory
caching systems, and should never
combine multiple allocations into one underlying allocation request
(as is done by a slab allocator or memory slicing implementation).
At the very least they should provide an easy well-documented mechanism to
disable caching/combining memory allocations
<i>and</i> include tests of that configuration in its
regression test suite (to ensure it is tested).
OpenSSL had a disabling mechanism, but it no longer worked
(because no one tested it), and in any case few people
understood that these memory allocation mechanisms
could <i>disable</i> many capabilities of security analysis tools.

<p>
We also need to modify our educational material so that developers
and tester will know that memory caching systems can seriously
hamper security analysis.
I have already done this in
<a href="https://dwheeler.com/secure-class/">my presentation
materials for my class on developing secure software</a>;
others need to do the same.

<p>
In the longer term, perhaps there should be some standard
interfaces for caching freelists / slab allocators in languages such as C.
If there were standard interfaces then tools could easily
be modified to automatically adjust to them.


<h2 id="standard-license">Use a standard FLOSS license</h2>

<p>
This is speculative on my part, but I believe that
much more code review and many more contributions would occur if 
OpenSSL used a standard widely-used license.
OpenSSL has traditionally used an odd variant license that is incompatible
with the GPL and LGPL
[<a href="https://people.gnome.org/~markmc/openssl-and-the-gpl.html">McLoughlin2004</a>]
[<a href="https://www.gnu.org/licenses/license-list.html#OpenSSL">GNU-Licenses</a>].
<!-- Apache License 1.0 and 4-clause BSD License -->
This is weird, because
<a href="https://dwheeler.com/essays/gpl-compatible.html">the
GPL is the single most common FLOSS license</a>.
This incompatibility is worked around through a license loophole
in many cases, or by making an explicit license exception in the
software that uses OpenSSL.
However, this awkward licensing situation means that many people who
prefer the GPL or LGPL will often not help develop or audit OpenSSL.
Some of those who prefer less-restrictive licenses may also be
less inclined to help, because again, it is not a standard license.
<p>
I do have some evidence that the non-standard licensing is a problem.
A completely separate software package, GnuTLS,
was initially specifically created so that software using a standard
GPL license could easily use SSL/TLS.
The LibreSSL fork of OpenSSL appears to be switching to the 2-clause
BSD license (a more common license) when they write new code,
in comparison to the OpenSSL license.
The OpenSSL developers themselves appear to be aware of this problem;
<a href="https://www.imperialviolet.org/2014/06/20/boringssl.html">Google&#8217;s
&#8220;BoringSSL&#8221; announcement mentions that
&#8220;We have already relicensed some of our prior contributions
to OpenSSL under an ISC license at their request
and completely new code that we write will also be so licensed&#8221;.</a>
The ISC license is a much more common FLOSS license, and
is functionally equivalent to the 2-clause BSD and
MIT licenses.

<p>
On <a href="https://www.openssl.org/blog/blog/2017/03/22/license/"
>March 16, 2017, OpenSSL announced that they were changing the license
of their software to the Apache License version 2.0</a>.
The Apache 2.0 license is far more popular and standard OSS license.
This is compatible with nearly all OSS licenses, including Apache 2.0
(of course), MIT, Revised BSD (aka BSD-new or BSD 3-clause), and
BSD 2-Clause (aka simplified or FreeBSD),
The situation with the LGPL and GPL is more complicated, but is
generally good news.
Almost all LGPL and GPL software is licensed to be "version 2 or later"
or is released using license version 3, and it's known that version 3 of
those licenses are compatible with the Apache 2.0 license.
There are a few programs that are GPL version 2.0 only
(such as the Linux kernel and git), but they're in the minority.
It takes a long time to change a license, but this relicensing
effort by OpenSSL should greatly heal problems from the past.

<p>
It&#8217;s been known, for a long time, that using a widely-used FLOSS license
is important for FLOSS projects.
Bruce Perens noted back in 1999,
&#8220;do not write a new license if it is possible to use one of the ones
listed here. The propagation of many different and incompatible licenses
works to the detriment of Open Source software because fragments of one
program cannot be used in another program with an incompatible license&#8221;
[<a href="http://oreilly.com/catalog/opensources/book/perens.html">Perens1999</a>].
Later on the Open Source Initiative (OSI) created the
<a href="http://opensource.org/proliferation">License Proliferation Project</a>,
noting that many licenses
&#8220;were legally incompatible with other free and open source licenses,
seriously constraining the ways in which developers could innovate by
combining rather than merely extending Open Source software&#8221;.
A key result is that OSI now directly lists, in its
<a href="http://opensource.org/licenses">Open Source Licenses</a> page,
only <i>Popular</i> Licenses, which are
&#8220;popular, widely used, or have strong communities&#8221;.
<p>
Most FLOSS is released under the GPL, LGPL, MIT/X,
Revised BSD (aka BSD-new or BSD 3-clause),
BSD 2-Clause (aka simplified or FreeBSD),
or Apache 2.0 licenses, and I recommend limiting new FLOSS programs
to that list of licenses.
Yes, you could add a few more; the OSI&#8217;s popular license list
(as of 2014-05-01) includes those and a few more.
However, the point here is that
the OpenSSL license is simply <i>not</i> a common license.
Most importantly, it is a non-standard license that is known to be
<i>incompatible</i> with some of the most widely-used licenses.
Where possible, it&#8217;s best to stick to common licenses instead.


<!--
<p>
One of the advantages of open source software is that there
can be many eyes on the code.
But if the code is overly complex, those eyes are
less like to find it.
-->

<!--
Kaminski openssl too complex
-->


<h1 id="reduce-impact">What would reduce the impact of Heartbleed-like vulnerabilities?</h1>

<p>
So what would likely have reduced the impact of Heartbleed-like
vulnerabilities, rather than eliminate it entirely?
After all, when a vulnerability slips through, you would like to
reduce its impact.
Below are a few approaches.

<h2 id="memory-defenses">Enable memory allocator defenses
once a standard memory allocator is used instead</h2>
<p>
Many systems include memory allocation mechanisms (by default or
enable-able) that reduce damage, and some may sometimes detect problems too.
This is no guarantee against problems, but it might reduce the impact.
For example:
<ul>
<li>One common approach is to zero out memory
on allocation and/or deallocation.
This means that if data is revealed,
it&#8217;s less likely to have something interesting.
<li>On 2014-04-29 David Wagner mentioned another interesting alternative:
&#8220;use a special memory allocator that allocates each object at a
random address.  On a 64-bit system, with a 48-bit address space,
this would have placed allocated objects far away from each other,
that the Heartbleed bug (which leaked data up to 64KiB away from one
object) would not have disclosed information about any other object.
This could be incorporated into the standard memory allocator.  Note:
I&#8217;m not advocating this.  I&#8217;m not saying it&#8217;s the best defense.  I&#8217;m just
responding to your call for ideas on what else could have prevented it.&#8221;
<li><a href="http://www.openbsd.org/cgi-bin/man.cgi?query=malloc">OpenBSD&#8217;s malloc implementation supports guard pages</a>, as noted earlier.
In particular, its &#8220;G&#8221 and &#8220;P&#8221; options
can reduce or prevent information leaks.
Unfortunately, many popular malloc implementations
(such as GNU libc malloc) do not include this kind of functionality.
</ul>

<h2 id="overwrite-critical">Overwrite critical information
whenever you&#8217;re done with it</h2>

<p>
Programs should overwrite (destroy) critical information
whenever they are done with it.
Critical information includes passwords and private cryptographic keys.
This reduces the impact of a vulnerability, since once information
is destroyed it cannot be revealed.

<p>
Be sure this is not optimized away; most compilers will eliminate
such overwrites if you don&#8217;t take steps to avoid it.
This is such a common mistake that it has been assigned
a weakness id of
<a href="http://cwe.mitre.org/data/definitions/14.html">CWE-14
(Compiler Removal of Code to Clear Buffers)</a>.
It is specifically identified as
<a href="https://www.securecoding.cert.org/confluence/display/seccode/MSC06-C.+Beware+of+compiler+optimizations">CERT C Coding standard recommendation MSC06-C
(beware of compiler optimizations)</a> and
<a href="https://www.securecoding.cert.org/confluence/display/cplusplus/MSC06-CPP.+Be+aware+of+compiler+optimization+when+dealing+with+sensitive+data">C++ Secure Coding Standard recommendation MSC06-CPP (Be aware of compiler optimization when dealing with sensitive data)</a>.
My own book
<a href="https://dwheeler.com/secure-programs/">Secure Programming for Linux and Unix HOWTO</a> includes a section specifically discussing how to
<a href="https://dwheeler.com/secure-programs/Secure-Programs-HOWTO/protect-secrets.html">Specially Protect Secrets (Passwords and Keys) in User Memory</a>.

<p>
Let&#8217;s see why this is a problem.
A naive programmer who wanted to erase memory in C might choose
the standard C function <tt>memset()</tt>.
However, if <tt>memset()</tt> is used to erase sensitive data,
the program would normally not use that memory again.
Why would it?  That data has been erased!
However, modern compilers will typically notice that the
memory isn&#8217;t being used again, and will
will silently remove the memory erasure code.
After all, if the memory won&#8217;t be used again, it&#8217;s a waste of resources
to erase it.
This is <i>not</i> a compiler error;
compilers are <i>explicitly allowed</i> to do this, and it is even
enshrined in the C and C++ standards.
The problem is that the compilers were given incorrect information.
Software developers need to specifically
tell the compiler that this erasure is <i>not</i> a no-op,
and that therefore this optimization should <i>not</i> be done.

<p>
There are various ways to do this correctly in some languages
(though in some languages it is not possible).
The best way in C is to use the new C11 <tt>memset_s</tt> function,
which does this correctly and is a standard function.
On Microsoft Windows the non-portable <tt>SecureZeroMemory()</tt>
does the trick.
It is also possible to use <tt>memset()</tt>, by providing
additional information to the compiler (e.g., by using <tt>volatile</tt>).
Some other languages provide this capability, e.g.,
the C# <tt>SecureString</tt> class provides this functionality.
At the time of this writing Java fails to directly provide this functionality,
but you can implement it by creating an implementation in C and calling it
from Java.
(Java&#8217;s <tt>StringBuffer</tt> class supports overwrites,
and using it is better than using the Java <tt>String</tt> class for this case,
but using <tt>StringBuffer</tt> can often lead to residual unerased copies
of sensitive data.)
It is impossible to do this with JavaScript today,
though a future extension could add it.
The point is that incorrect erasure is a common problem;
programs that manipulate sensitive data
should make sure they erase that sensitive data as quickly as possible.


<h2 id="pfs">Make perfect forward security (PFS) encryption algorithms
the default</h2>

<p>
A cryptographic system has perfect forward secrecy (PFS),
aka forward secrecy, when it
non-deterministically generates new random public keys for each session.
When PFS is not used, the compromise of a private key typically
enables attackers to decrypt past communications that they have recorded.
When PFS is enabled, messages are not necessarily exposed when
some keys are exposed,
because there is no single secret value used for all messages.

<h2 id="privilege-separation">Use privilege separation to separate
critical cryptographic secrets</h2>
<p>
It can be helpful to separate critical cryptographic secrets
from the rest of the code, so that even if even the rest of the program
is subverted it cannot directly access secrets like private keys.
<p>
This applies the basic security principle that programs
should be provided only most limited privilege necessary to their job.
These approaches can reduce the likelihood or impact of a
critical vulnerability by reducing the amount of software where
a vulnerability would reveal critical information like a key.
However, I am listing it as a risk reduction approach, not as a full
countermeasure.
Somewhere code <i>must</i> have access to privileged data and that
code might be vulnerable.
Here are a few additional notes:
<ul>
<li>David Wagner pointed this out and also explained further,
&#8220;for instance, the RSA private key (and any other long-lived crypto
secrets) could have been moved into a separate process, and only the
code that operates on the private key moved into that process.  Better,
on forthcoming Intel systems,
OpenSSL could adopt Intel SGX (secure enclaves)
to run all crypto computations that touch the private key in
a separate sandboxed execution environment.
This is sometimes known under the
term <i>software cryptographic module</i>,
<i>virtual TPM</i>, <i>virtual HSM</i>, etc.
SGX provides hardware support for
isolating that critical code, but one could of course use other mechanisms
for isolation instead of SGX, like process isolation or sandboxing.
There&#8217;s been plenty of academic work on this general approach of keeping
the crypto code separate, and some of it has even considered OpenSSL
specifically.&#8221;
An introduction to SGX is provided in
[<a href="http://theinvisiblethings.blogspot.com/2013/08/thoughts-on-intels-upcoming-software.html">Rutkowska2013</a>],
and a discussion of privilege separation applied to OpenSSL
is included in
[<a href="http://users.ece.cmu.edu/~dawnsong/papers/privtrans.pdf">Brumley</a>]
(especially section 4.5).
<li>Peter Neumann has pointed out that capability-based hardware,
sandboxing, and fine-grained access controls
(such as the CHERI architecture) also provide strong mechanisms
to limit privileges in ways that could have countered Heartbleed.
<li>A student in my class on developing secure software
suggested that perhaps memory could be isolated per-user on
a server.
That&#8217;s an interesting idea.
</ul>

<h2 id="fix-certificates">Fix the SSL/TLS certificate infrastructure, especially certificate revocation</h2>
<p>
The entire SSL/TLS infrastructure has a large number of serious problems,
including a large number of often-untrustworthy root certificate authorities,
poorly-vetted certificates,
and a badly broken certificate revocation process
[<a href="http://www.theregister.co.uk/2011/04/11/state_of_ssl_analysis/">Goodin2011</a>].
As a related point, Qualsys SSL labs has posted an
<a href="https://www.ssllabs.com/projects/ssl-threat-model/index.html">SSL threat model</a>.
Moxie Marlinspike&#8217;s
<a href="http://www.thoughtcrime.org/blog/ssl-and-the-future-of-authenticity/">SSL And The Future Of Authenticity</a> argues that
&#8220;the current problems with the [SSL/TLS Certificate Authority]
system can be reduced to a single missing property.
I call this property trust agility&#8221;
[<a href="http://www.thoughtcrime.org/blog/ssl-and-the-future-of-authenticity/">Marlinspike2011</a>].
&#8220;<a href="http://queue.acm.org/detail.cfm?id=2673311">Assessing legal and technical solutions to secure HTTPS</a>&#8221; 
outlines systemic vulnerabilities of HTTPS
(from both a legal and technological perspective);
they conclude that vulnerabilities will likely persist for years to come,
due to problems such as perverse incentives
[<a href="http://queue.acm.org/detail.cfm?id=2673311">Arnbak2014</a>].
<p>
For the moment, let&#8217;s specifically focus on
the broken certificate revocation process, which is of special concern
when trying to respond to Heartbleed.
The Heartbleed vulnerability made it possible for attackers to exfiltrate
the private keys for their certificates, which is really bad.
In theory, vulnerable sites can solve private key exposure
by deploying new certificates and revoking the old certificates
(once the underlying cause, like Heartbleed, is fixed).
Unfortunately, today&#8217;s certificate revocation mechanisms 
are fundamentally broken, and
few people (until recently) have paid much attention to the problems.
As a result, today attackers can often to cause web browsers to
accept certificates even after they have been revoked.
There is a critical need for a push to create security standards
in this area that work far better than currently-available mechanisms,
and then implement them by default everywhere.
I personally think that there should be a determined push for
<a href="#x509-ocsp-must-staple">X.509 OCSP must-staple</a>,
but whatever the solution turns out to be, we need one.
<p>
This is a complicated area; I will try to just summarize the issues.
Here are a few certificate revocation mechanisms, and the problems with them:
<ul>
<li><i>Certificate Revocation List (CRL)</i>. A CRL is a big file
that enumerates revoked certificates (including temporarily revoked ones,
which are instead called <i>held</i> certificates).
This was the original approach for revocation.
The idea was that a program would download and check a CRL
before accepting a certificate.
But traditional CRLs have not scaled well to today&#8217;s internet;
today CRLs tend to be large, and updates have to be
frequently downloaded to keep the list current, making them
less and less practical.
People are already moving away from CRLs, for example,
Firefox 24 dropped auto-updating CRLs and the
user interface for importing CRLs.
<li><i>Online Certificate Status Protocol (OCSP)</i>.
In this approach, the program can contact a server to request the
status about a specific certificate.
OCSP is supposed to require less network bandwidth than CRLs,
and was created to enable near real-time status checks.
However, OCSP creates a huge volume and response time requirement for
certificate authorities (CAs), since OCSP requires that CAs
must provide responses to all clients in real time whenever a client
encounters a new site using SSL/TLS.
OCSP also creates a serious privacy vulnerability: it causes clients
to disclose to CAs exactly which sites the clients are contacting.
Also, both OCSP and CRLs have a fundamental problem:
What happens if you cannot get an answer?
In practice, systems that implement OCSP or CRLs do
<i>soft-fail</i> by default, that is, they accept certificates
if they cannot find out otherwise.
Yet soft-fail makes the whole approach mostly useless,
since attackers can often interfere with or strip out these requests.
Hard-fail works fine for me, and many other people report that it works
for them too.
However, switching to hard-fail does have its own serious problems.
This additional check slows down every initial connection to a secured
site, and many users are sensitive to this additional time.
OCSP server failures are common, you have to temporarily disable this
when connecting to captive portals (like a hotel WiFi),
and if hard-fail were used widely then attackers could disable
all of HTTPS by disabling just a few OCSP servers.
An attacker may even be able to use OCSP stapling
to include an OCSP response with the revoked certificate,
foiling the checking
[<a href="https://www.imperialviolet.org/2014/04/19/revchecking.html">Langley2014a</a>]
[<a href="https://www.imperialviolet.org/2014/04/29/revocationagain.html">Langley2014b</a>].
<li><i>OCSP Stapling</i>.
OCSP stapling tries to solve the problems of OCSP
by having certificate holders query the OCSP server, not the final client,
and getting a signed time-stamped response that is only valid for a short
time (e.g., a few days).
When clients later visit a site, they receive from the site the
certificate and this additional &#8220;stapled&#8221; response.
If the stapled response is not received, then the client can fall
back to another approach such as standard OCSP.
This is not as widely deployed as CRLs and OCSP, but
a number of web servers and web browsers do support OCSP stapling.
However, OCSP stapling by itself is still vulnerable to an attacker who
has a compromised/revoked secret key.
The attacker can pose as the target server and
simply not staple any OCSP responses in the TLS handshake.
The client would then need to fall back to direct queries to
the OCSP server, but in many cases the attacker can simply block
or filter the client&#8217;s access to the OCSP server
in the same way as the non-stapled case.
(My thanks to Daniel Kahn Gillmor, who provided corrections
on this point.)
In that case, the normal soft-fail process in browsers
will cause the whole system to fail.
<li><i>CRLSets</i>.
A CRLSet is basically short CRL built into a web browser.
For example, the Chrome web browser developers
compile a daily list of what they consider
&#8220;high-value revocations&#8221;
and use Chrome&#8217;s auto-update mechanism
to push this list to Chrome installations.
Adam Langley, an expert at Google, advocates the CRLSet mechanism
as opposed to CRLs and OCSP
[<a href="https://www.imperialviolet.org/2014/04/19/revchecking.html">Langley2014a</a>].
I agree that a CRLSet can be useful when a web browser should forceably
revoke the certificate of a widely-used site.
But overall I think CRLSets are fundamentally broken.
A CRLSet is basically just a short incomplete blacklist.
As Langley states, a CRLSet
&#8220;is not complete,
nor big enough to cope with large numbers of revocations...
the original hope with CRLSets was that we could get revocations
[categorized] into important and administrative and push only the important
ones... sadly, that mostly hasn&#8217;t happened.&#8221;
Gibson has a valid point when he states,
&#8220;Internet certificate revocation lists
enumerate more than two million revoked and untrustworthy certificates.
Yet Chrome&#8217;s CRLSet presently lists approximately 24 thousand revoked
certificates.
Every other one of the more than two million is implicitly
trusted by Chrome&#8221;
[<a href="https://www.grc.com/revocation/crlsets.htm">Gibson</a>].
The Certificate Authority Security Council (CASC) has come out strongly
against the CRLSet mechanism as the sole certificate revocation mechanism:
&#8220;Heartbleed is a perfect example of why revocation is important even
without identified key compromise.
No one can say for certain that their
server&#8217;s private key was compromised.
Most of the revocations that have occurred
are going on CRLs for &#8220;business reasons&#8221;
(as Adam defines it) and not picked up by CRLSets.
It&#8217;s now clear that CRLSets are simply a blacklist of high-profile
revoked certificates.
Other browsers have similar blacklists, and
these can be effective at times (for example, to indicate revocation
of an intermediate certificate that may be several years old and does
not contain an OCSP pointer). But they&#8217;re not a substitute for OCSP
checking of end-entity certificates...
Even if revocation checking by OCSP
isn&#8217;t 100 percent accurate, it can still protect a high percentage
of users...  Turning off revocation checking
for everyone means that no one is protected.&#8221;
[<a href="https://casecurity.org/2014/05/08/casc-heartbleed-response/">CASC2014</a>].
Just to be clear, I do <i>not</i> think that CRLs or OCSP
are working well.  Not at all.
I also agree that CRLSets can have some utility, especially
when there is a certificate exposure of a specific high-profile site.
However, vulnerabilities like Heartbleed are different.
In Heartbleed, sites cannot be sure if their private keys have
been exfiltrated or not, so they need to revoke them just to be sure...
and since OpenSSL is widely used, this affects a <i>lot</i> of sites.
CRLSets do not even <i>begin</i> to solve Heartbleed-like problems.
<li><i>Must-staple HTTP header</i>.
<a href="https://wiki.mozilla.org/CA:ImprovingRevocation#OCSP_Must-Staple">Firefox is planning to implement a new must-staple HTTP header
as an interim measure</a> until better mechanisms like
<a href="#x509-ocsp-must-staple">X.509 OCSP must-staple</a>
become available.
This simply adds support for a new HTTP header as part of the response
to an HTTPS connection request.
If this new header is supplied by the server and supported by the browser,
the browser would require a stapled OCSP response for that domain and
its subdomains.
Again, however, many attackers can subvert this if this is the
first visit by the client, because the attacker can still forge
the initial connection.
Be careful; some people confuse this <i>must-staple HTTP header</i> approach
with the
<a href="#x509-ocsp-must-staple">X.509 OCSP must-staple</a>
approach, and they are not the same thing.
<li><i>Short-lived certificates</i>.
One solution is to deploy
certificates that only last a short time, e.g., a few days instead of years.
In theory, this should work today;
expiration times are well-tested basic part of certificates.
However, this is probably not scalable.
Many systems presume that
certificates are valid for longer times, and the additional CA work
by CAs might create easy opportunities for mistakes.
Also, many users are used to expired certificates and routinely
ignore their warnings.
Finally, many mechanisms (such as some forms of pinning)
counter other certificate problems by presuming
that certificates rarely change (which is true), so this fix
could create increase the risk of other vulnerabilities.
<li id="x509-ocsp-must-staple"><i>X.509 OCSP must-staple</i>.
In this approach, the certificate has a marker saying that
the client <i>must</i> also receive OCSP stapling, and then
OCSP stapling is always used.
This is an addition to OCSP stapling, but it
prevents the attacker from filtering out the OCSP stapling,
since the client would then know something is wrong.
Note that this is not just stapled OCSP, and this is not a
must-staple HTTP header; people sometimes confuse these related but
different approaches.
Unlike the must-staple HTTP header approach, an attacker cannot easily
filter this out, because the certificate itself states that stapling
is required.
This approach has the same effects as short-lived certificates, but without
some of the problems of them (as noted earlier).
Langley, CASC, and many others recommend OCSP must-staple
[<a href="https://www.imperialviolet.org/2014/04/19/revchecking.html">Langley2014a</a>]
[<a href="https://casecurity.org/2014/05/08/casc-heartbleed-response/">CASC2014</a>].
However, although there is
<a href="https://tools.ietf.org/html/draft-hallambaker-muststaple-00">ongoing</a>
<a href="http://www.ietf.org/mail-archive/web/tls/current/msg10351.html">work</a>
on must-staple
(aka an &#8220;OCSP Stapling Required&#8221; extension to X.509),
there seems to be no formal specification for it and
this extension is not widely implemented.
For this to work we need a formal specification, wide implementation in
servers and browsers, and many signed X.509 certificates that include
this extension.
<li><i>Other approaches</i>.
There are other approaches that try to deal with suspect certificates,
including TACK (by Moxie Marlinspike and Trevor Perrin) and
Mutually Endorsing CA Infrastructure
[<a href="http://arstechnica.com/security/2014/04/how-heartbleed-transformed-https-security-into-the-stuff-of-absurdist-theater/">Goodin2014b</a>].
Again, these are not widely accepted or implemented.
</ul>
<!--
Other possible sources:
https://www.imperialviolet.org/2012/02/05/crlsets.html
One solution is to make
online certificate status protocol (OCSP)
"Must-Staple" the default
[<a href="https://www.grc.com/revocation/commentary.htm">Gibson2014a</a>]
[<a href="https://www.grc.com/revocation.htm">Gibson2014b</a>]
[<a href="https://www.grc.com/revocation/crlsets.htm">Gibson2014c</a>].

Firefox 32 adds public key pinning, just as Chrome already does:
http://monica-at-mozilla.blogspot.com/2014/08/firefox-32-supports-public-key-pinning.html
-->


<h2 id="updates">Make it easy to update software</h2>
<p>
Mistakes <i>will</i> happen; organizations must plan to quickly repair them
when necessary.
Yet many organizations are unprepared for the inevitable discovery of
vulnerabilities in the systems that they sell, provide, or maintain.
<a href="http://www.techweekeurope.co.uk/security/cyberwar/heartbleed-impact-devices-iot-176957">Shodon (an Internet-of-Things search engine)
estimates that 200,000 devices connected to the open Internet
were still vulnerable to Heartbleed
over 18 months after it was publicly announced</a>.

<p>
For example, most users of the older Android version 4.1.4 are
currently vulnerable to Heartbleed.
That&#8217;s because although Google fixed this version of Android quickly,
the phone manufacturers are sometimes very slow at updating the phone,
and then the phone service carriers typically delay updates
for unconscionably long times (or fail to provide them at all).
It may be time to talk about holding manufacturers and carriers
liable for failing to repair phones they&#8217;ve sold in a timely way
when there is a known fix for a security defect.
<p>
Similarly, I think the FIPS 140-2 validation process needs to be changed
to allow for rapid update of libraries when a vulnerability is found.
<a href="http://veridicalsystems.com/blog/immutability-of-fips/">Steve Marquess argues in &#8220;The Immutability of FIPS&#8221; (updated 2014-03-29)</a> that 
&#8220;the single most distinguishing (and IMHO deplorable)
feature of FIPS 140-2 validation is the almost total prohibition
of changes to validated modules.&#8221;

<h2 id="salted-hashes">Store passwords as salted hashes</h2>
<p>
Of course, continue to store passwords as salted hashed values, and
not as cleartext or as a reversible value.
If you are storing passwords as cleartext or a reversible value,
and you do not have a compelling reason,
you should probably be fired.

<h2 id="general">General issues: Secure software education/training and reduce attacker incentives</h2>
<p>
I should at least mention that various general improvements
would likely reduce the number of exploitable vulnerabilities in
general, and thus should be applied.
This includes secure software education/training
(which should reduce the total number of vulnerabilities in software)
and
reducing attacker incentives
(which should reduce the total number of unreported vulnerabilities that
attackers have in their arsenals).
<p>
Many software developers still get little education or training
in how to develop secure software.
Yet nearly all software is either connected directly to a network,
or at least receives data through a network.
Many developers do not know how to design software in a way that
resists attack, what the common weaknesses are, or how to
correctly counter them.
In fact, many developers do not even know the difference between
<i>security software</i> and <i>secure software</i>.
The material is available, indeed,
<a href="https://dwheeler.com/secure-programs/">I give
away a book on how to develop secure software</a>
[<a href="https://dwheeler.com/secure-programs/">Wheeler2004</a>].
But developers have to learn and apply this knowledge.
<p>
We also need to reduce the economic incentives for attack, or at least,
the economic incentives for finding vulnerabilities and selling them
to attackers.
In many cases people will not tell defenders about vulnerabilities
because they can make more money selling information to attackers.
After all, people might legally make over a million dollars
selling information on a single vulnerability.
Bug bounty values simply cannot keep up in the current environment.
We need to investigate ways to reduce these incentives.
For example, perhaps we should criminalize selling
vulnerability information to anyone other than
the supplier or the reporter&#8217;s government.
Basically, treat vulnerability information like organ donation:
intentionally eliminate the economic incentives in a specific area
for a greater social good. 
I think it&#8217;s impossible to prevent a citizen from telling his
country&#8217;s government about a software vulnerability;
a citizen could easily see it as his duty.
I also think no government would forbid buying
such information for itself.
But additional constraints might reduce the number of people
actively looking for vulnerabilities to <i>exploit</i> instead of
to <i>fix</i>.
Obviously there are some people will do illegal things, but some people
will avoid doing illegal things in principle, and others will avoid
illegal activities because they fear getting caught. You don&#8217;t need
to stop all possible cases, just enough to change the economics.
Maybe there&#8217;s a better way. If so, please propose it!
In any case, we need to find a way to make capitalism work
<i>for</i> instead of <i>against</i> security
[<a href="https://dwheeler.com/blog/2013/11/16/#vulnerability-economics">Wheeler2013</a>].

<h2 id="practices">General issues: Apply good development practices</h2>

<p>
Once other developers began examining the development processes used
by OpenSSL before Heartbleed they often did not like what they saw.
Since the time of Heartbleed the
<a href="https://bestpractices.coreinfrastructure.org">CII Best Practices
Badge Project</a> was formed, which identified a set of best practices
for OSS projects and provided a site to help projects determine if they
met those best practices.

<p>
When OpenSSL measured its old processes against this set of best practices,
it only scored a 62% of the passing badge requirements.
"The OpenSSL project attained its CII badge in February 2016, by
enabling TLS for its web site, protecting downloads of OpenSSL with TLS,
publishing processes for reporting vulnerabilities and
contributing code, using static and dynamic analysis before public
releases, and using continuous integration" [Walden 2020].

<p>
While applying best practices do not <i>guarantee</i> that there are
no vulnerabilities, such practices can reduce the likelihood of their
occurance, reduce their impact, and speed response when vulnerabilities
do slip through.

<p>
More generally, the paper [Walden 2020]
(and its
<a href="https://www.youtube.com/watch?v=hQqL4WcAJzQ"
>matching Youtube video</a>)
provides an intriguing analysis of the changes that the OpenSSL project
made in response to Heartbleed that resulted in signficant
improvements to the project.
It particularly notes that
project activity and the CII best practices badge "may be
better indicators of code quality".

<h1 id="applying">Applying these approaches</h1>

<p>
As I&#8217;ve noted earlier, developing secure software requires a
<i>combination</i> of tools and approaches.
When there&#8217;s a failure, as clearly happened here, we need to
learn why it failed and try to do better (based on what we learn).

<p>
This has not been a good time for cryptographic libraries.
These libraries
are vitally important, yet many serious problems have been found in 2014:
<ol>
<li>Heartbleed in OpenSSL, the focus of this paper.
<li>The Apple iOS <i>goto fail</i> vulnerability
(<a href="https://web.nvd.nist.gov/view/vuln/detail?vulnId=CVE-2014-1266">CVE-2014-1266</a>),
which was a simple failure to check for invalid certificates.
This vulnerability could have been easily detected through
a variety of mechanisms including
static dead code detectors (including those provided by
some compiler warning flags), test coverage tools, and/or negative testing
with invalid certificates.
<li>GnuTLS also failed to verify certificates correctly
(<a href="https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2014-0092">CVE-2014-0092</a>)
[<a href="http://arstechnica.com/security/2014/03/critical-crypto-bug-leaves-linux-hundreds-of-apps-open-to-eavesdropping/">Goodin2014a</a>].
Again, there appear to be many techniques that could have
detected this before the code was released, including
negative testing.
<li>OpenSSL CCS injection (CVE-2014-0224), where
<a href="https://www.openssl.org/news/secadv_20140605.txt">a carefully
crafted handshake can cause the use of weak keying material</a>.
<a href="http://ccsinjection.lepidum.co.jp/blog/2014-06-05/CCS-Injection-en/index.html">Masashi Kikuchi has separately described how he found
this vulnerability</a>.
He first thought about how he could prove the correctness of the
implementation using Coq.
He focused on the transitions on CCS (as he knew it is the most
complicated part of the state machine), and then checked if the code
verified the transition conditions.
He found that OpenSSL failed to verify the conditions, and that
revealed a vulnerability.
<li><a href="https://lwn.net/Articles/578375/">A random number generator
(Dual EC DRBG) failed to run at all</a> in OpenSSL.
Remarkably, this error can be detected by just trying to use it,
so even basic functional testing should have caught this.
<li>Microsoft later in 2014
revealed an extremely serious security vulnerability
<a href="http://www.cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2014-6321">CVE-2014-6321</a> (aka MS14-066), and sometimes called WinShock,
which is a remote code execution vulnerability in Windows' SChannel.
SChannel is a security package that provides SSL and TLS on Microsoft
Windows systems.
This vulnerability is much worse than Heartbleed; Heartbleed was only
a remote information disclosure, while this enables
remote code execution (RCE) (the most severe result),
and it also affects both clients and servers.
Reports are that
<a href="http://adi.is/winshock.txt">this code
was written at least 15 years ago</a>, and it was missed even though
Microsoft has been looking for years to find vulnerabilities like this.
</ol>

<p>
<i>The Register</i> ran the article <a href="http://www.theregister.co.uk/2014/11/12/ms_crypto_library_megaflaw/">Annus HORRIBILIS for TLS! ALL the bigguns now officially pwned in 2014</a>.

<p>
Many people depend on cryptographic libraries,
but evaluating them requires specialized expertise.
The US government has established a process to evaluate
cryptographic modules, called FIPS 140-2.
(There&#8217;s a difference between a module and a library; stay tuned.)
I think it is a good idea to have a broad process for evaluating
these items that are vitally important and difficult to evaluate.

<p>
There is an important thing to understand, though: the FIPS 140-2
process does <i>not</i> currently examine cryptographic protocols,
including implementations of SSL/TLS.
Instead, the current FIPS 140-2 process
<i>only</i> evaluates the underlying cryptographic algorithms and schemes
in the <i>cryptographic module</i> it examines.
I did <i>not</i> make that clear in older versions of this paper;
I hope this clarifies things.
As stated in
<a href="http://csrc.nist.gov/groups/STM/cmvp/documents/fips140-2/FIPS1402IG.pdf"><i>Implementation Guidance for FIPS PUB 140-2 and the Cryptographic Module Validation Program</i> (April 25, 2014)</a>
(referenced via <a href="http://csrc.nist.gov/groups/STM/cmvp/standards.html">http://csrc.nist.gov/groups/STM/cmvp/standards.html</a>),
&#8220;the cryptographic modules may implement various protocols known in the
security industry. The examples of such protocols are IKE, TLS, SSH, SRTP,
SNMP and TPM, listed in NIST SP 800-135rev1...  FIPS 140-2 and its Annexes
do not address protocols. Only the cryptographic algorithms (such as,
for example, AES or DSA) and schemes (such as the key agreement schemes
from NIST SP 800-56A or the RSA-based key encapsulation schemes) that
are Approved and allowed may be used in the Approved mode of operations.&#8221;

<p>
In fact, OpenSSL can keep its FIPS 140-2 validation (#1747)
even after it fixes the Heartbleed vulnerability, due to some
odd technical details.
The FIPS 140-2 process
didn&#8217;t evaluate the <i>normal</i> OpenSSL code, but instead only
evaluated a specially-crafted software module called the
<a href="https://www.openssl.org/docs/fips/fipsnotes.html">OpenSSL FIPS Object Module</a>.
The OpenSSL FIPS Object module (aka &#8220;FIPS module&#8221;)
has the same interface, and is derived from the OpenSSL code,
but it is a separate component.
Users who enable &#8220;FIPS mode&#8221; end up
running the FIPS module code instead of the regular OpenSSL code
for certain cryptographic capabilities.
Thus, the OpenSSL FIPS Object module
can maintain its FIPS 140-2 validation,
even after it was changed to fix Heartbleed.
That&#8217;s because the FIPS 140-2 process
does not evaluate protocols like SSL/TLS, and because
the code that must be changed to fix it are not part of the FIPS module.
Note that this is completely different than what happens in other cases.
Normally any change at all to a cryptographic module can lead to loss of
validation, and it can take a long time (say 6-12 months) and significant
money to get a new validation.
This risk of loss creates a perverse incentive:
cryptographic module developers have a strong incentive
to leave problems unfixed.
If this is true, then I&#8217;m sure the OpenSSL developers and FIPS users are
happy... it means that Heartbleed can be rapidly fixed while keeping
FIPS compliance (for those who need it).
I confirmed this understanding with NIST on 2014-05-09
(thank you very much!).


<p>
So by design, the FIPS 140-2 process did <i>not</i> examine the
SSL/TLS implementation, or anything involving cryptographic protocols.
I think it&#8217;s fair to ask, however, if the process <i>should</i>
be looking at cryptographic protocols.
Properly evaluating cryptographic protocols requires the same sort
of specialized knowledge as with other cryptographic code.
At the least, it&#8217;s fairly easy to create and
run a big test suite against a standard
protocol like SSL/TLS
(that&#8217;s because there is a standard external interface).
It&#8217;s also surprising that the validation process
didn&#8217;t discover the failure of the
<a href="https://lwn.net/Articles/578375/">random number generator
(Dual EC DRBG)</a>.
I think the FIPS 140-2 process should at least dynamically test
every supported cryptographic random number generator to ensure
that they are statistically random and that they avoid common mistakes
(e.g., that they are not implementations of inappropriate
algorithms like linear congruential generator or Mersenne twister).
This would cost practically nothing and provide some confidence.
If FIPS 140-2 process is not extended to review cryptographic protocols,
then <i>something</i> should be established to review them.
In general, we need to re-examine FIPS 140-2 to make it faster,
less expensive, <i>and</i> more thorough.
The current process makes it difficult to update libraries, for example,
leading at least one person to conclude that you can be
<a href="http://veridicalsystems.com/blog/secure-or-compliant-pick-one/">secure
or compliant, not both</a>.

<p>
No testing process can find all problems, so expecting perfection
is not reasonable.
Still, there are a variety of newer techniques that might
increase the speed, reduce the cost, or improve the testing thoroughness
when evaluating cryptographic code.
Even more importantly, we all need to learn from the attackers,
and improve current evaluation processes
to counter the vulnerabilities that are currently slipping through.
My goal is not to bash various existing evaluation processes;
evaluation is a hard job.
My goal is to figure out what can be done to improve things,
because cryptography is <i>important</i> in
today&#8217;s world.

<p>
The world needs rigorous and transparent
processes for testing cryptographic protocols
(including their supporting algorithms),
one that allows for rapid updates given security flaws (when they happen)
and openly updates its processes to prevent recurrences.
I could easily see a FLOSS project to create a much more rigorous
test suite that could be used by cryptographic library implementers
and concerned users (including governments and their evaluation processes).

<p>
I should briefly note that in June 2014
<a href="https://www.openssl.org/about/roadmap.html">OpenSSL
released a roadmap</a> on how they will improve things
(<a href="http://arstechnica.com/information-technology/2014/07/openssl-speeds-up-development-to-avoid-being-slow-moving-and-insular/">Ars Technica
commented on the OpenSSL roadmap</a>).
The OpenSSL developers plan to do
some of the things noted in this paper, e.g., they plan to
reduce library API complexity,
make the coding style consistent,
perform code review on all new commits,
audit the current code base, and
&#8220;regularly audit the code using appropriate analysis tools&#8221;.
At this time the roadmap does not commit to fixing the OpenSSL
licensing problems, which is unfortunate.
I think some of the OpenSSL problems stem in part from their weird
incompatible license; why would people want to help a project
when many FLOSS projects would have trouble using the results?


<p id="floss-general">
I suppose I should briefly mention FLOSS in general.
Some people have tried to claim that Heartbleed is somehow
proof that FLOSS cannot create good software.
This doesn&#8217;t make sense;
legions of vulnerabilities have been found in
proprietary software, and
the 2013 Coverity Scan Report found that on average
&#8220;open source code quality surpasses proprietary code quality
in C/C++ projects [as measured by average defect density]&#8221;
[<a href="http://softwareintegrity.coverity.com/register-for-scan-report-2013.html?cs=pr">Coverity2014</a>].
In reality, some FLOSS programs are secure, while others are less so,
just like proprietary software.
FLOSS has some potential advantages for security,
but they are only a potentiality.
You still have to examine particular software to determine if it is
appropriate for your needs... and that includes its security.

<p>
Some people have argued that access to source code does not
help improve security, but Heartbleed is a good disproof of this.
Heartbleed was initially found in <i>exactly</i> the way that
FLOSS advocates say FLOSS security works.
Heartbleed was first found by Google,
who did not write the original code, using
<a href="#thorough-audit">thorough human review / audit</a>
of the publicly-available OpenSSL source code.
It took two years - longer than anyone would like -
but Heartbleed is clear evidence that releasing software as FLOSS
<i>can</i> lead to finding and fixing vulnerabilities.

<p id="linus-law">
It is true that
Eric S. Raymond has claimed the following as &#8220;Linus&#8217; Law&#8221;:
&#8220;given a large enough beta-tester and co-developer base,
almost every problem will be characterized quickly
and the fix will be obvious to someone&#8221; - or less formally -
&#8220;given enough eyeballs, all bugs are shallow&#8221;
[<a href="http://www.catb.org/esr/writings/cathedral-bazaar/">Raymond1999</a>].
But this doesn&#8217;t mean what some people want it to mean.
First, Raymond is actually contrasting different ways of developing FLOSS;
he&#8217;s not talking about FLOSS vs. non-FLOSS.
Second, notice that the more careful phrasing requires a
&#8220;large enough co-developer&#8221; base;
cryptography involves specialized issues that
make it harder to get a larger developer base, and the
<a href="#standard-license">non-standard OpenSSL license</a>
has probably also inhibited the number of co-developers.
Third, note the text
&#8220;almost every problem will be characterized quickly&#8221;;
he never claims that <i>all</i> problems will be found, or
that they will all be characterized quickly.
Finally, as this paper hopefully makes clear, it turns out that most of
the traditional techniques for finding or countering these problems
(including dynamic and static analysis)
didn&#8217;t work, even though a number of tools and approaches
were used to analyze OpenSSL.
Instead, there were <i>systemic</i> problems finding
vulnerabilities like this, because
different people started with similar assumptions.
The good news is that, because this vulnerability and its causes can
be openly discussed, we can identify and fix these systemic problems.
In a larger setting, this paper represents the work of people
to identify systemic problems, and fix them, so that
vulnerabilities like it <i>will</i> be much more likely to
be characterized and fixed quickly.

<p>
This paper focuses on how to respond technically.
However, in many ways
non-technical issues matter more.
<a href="http://www.kalzumeus.com/2014/04/09/what-heartbleed-can-teach-the-oss-community-about-marketing/">Summer Maynard&#8217;s
&#8220;What Heartbleed Can Teach The OSS Community About Marketing&#8221;</a>
shows how the Heartbleed vulnerability was marketed
(primarily because it
&#8220;has a name, a logo, and a dedicated web presence&#8221;).
That marketing was really important; Heartbleed was a bad vulnerability,
but the excellent marketing markedly sped up the response and
greatly reduced the damage.
Projects to help fund critical components like these are
also potentially valuable; the
<a href="http://www.linuxfoundation.org/programs/core-infrastructure-initiative">Core Infrastructure Initiative (housed at the Linux Foundation)</a>
is a multi-million dollar project to fund open source projects
that are in the critical path for core computing functions,
and was inspired by Heartbleed.


<h1 id="exemplars">Exemplars</h1>
<p>
So are there any examples of projects doing well?
After all, it&#8217;s easy to complain, but if no one can do well,
then perhaps it is not possible.
Besides, if there are no exemplars to copy from, it&#8217;s
hard to learn how to do them.
<p>
I asked people to identify examples of <i>robust</i> FLOSS,
either in terms of reliability or security.
Of course, a program can be robust given expected inputs and
insecure (because intelligent attackers can specifically rig
inputs to cause undesired behavior).
Also, even really good systems have occasional problems.
Still, it&#8217;s a great idea to look at exemplars, because
it is much easier to copy approaches once you can
see them in action.
Here are some of the projects that people identified:

<ol>
<li><a href="http://www.openbsd.org/">OpenBSD</a>.
<a href="http://www.openbsd.org/security.html">OpenBSD
aspires to be #1 on security</a>.
They do continuous security auditing by a team of 6-12 people:
&#8220;we are not so much looking for security holes,
as we are looking for basic software bugs,
and if years later someone discovers the problem used to be a
security issue, and we fixed it because it was just a bug, well,
all the better. Flaws have been found in just about every area of the
system. Entire new classes of security problems have been found during
our audit, and often source code which had been audited earlier needs
re-auditing with these new flaws in mind. Code often gets audited multiple
times, and by multiple people with different auditing skills...
Another facet of our security auditing process is its proactiveness.
In most cases we have found that the determination of exploitability is
not an issue. During our ongoing auditing process we find many bugs,
and endeavor to fix them even though exploitability is not proven.&#8221;
They try to create and implement many new techniques for
countering vulnerabilities, including strlcpy()/strlcat(),
guard pages, and randomized malloc().
They also work to ship &#8220;secure by default&#8221;
and practice full disclosure.
<li><a href="http://www.openssh.com/">OpenSSH</a>.
OpenSSH implements the SSH protocols and key connectivity tools.
OpenSSH is developed by the OpenBSD Project using two teams.
One team does strictly OpenBSD-based development (to be as simple as possible),
and the other team takes that version and makes it portable
to run on many operating systems.
<a href="http://www.openssh.com/security.html">OpenSSH is developed using
the OpenBSD security process</a> (since it is part of OpenBSD).
The OpenSSH developers have worked to
reduce OpenSSH&#8217;s attack surface; their approaches include
defensive programming (preventing errors by
inserting additional checks),
avoiding complexity in dependent libraries,
mildly changing the protocol to reduce attack surface,
privilege separation,
and changing the program to maximize the benefit
of attack mitigation measures in the operating system (OS)
[<a href="http://www.openbsd.org/papers/openssh-measures-asiabsdcon2007.pdf">Miller2007</a>].
For more on how OpenSSH implements privilege separation, see
[<a href="https://www.usenix.org/events/sec03/tech/full_papers/provos_et_al/provos_et_al.pdf">Provos2003</a>].
<li><a href="https://www.sqlite.org/">SQLite</a>
(<a href="https://www.sqlite.org/copyright.html">public domain</a>).
<a href="https://www.sqlite.org/testing.html">SQLite uses
very aggressive (dynamic) testing approaches to get high reliability</a>.
The project has over 1000 times as much test code and test scripts
as it does source lines of code (SLOC).
Their approach includes three independently developed test harnesses,
anomaly testing
(e.g., out-of-memory tests, I/O error tests,
crash and power loss tests, and compound failure tests),
fuzz testing (SQL fuzz, malformed database files,
and boundary value tests),
regression testing,
automatic resource leak detection,
100% branch test and MC/DC coverage
(including forced coverage of boundary conditions),
millions of test cases,
extensive use of assert() and run-time checks,
Valgrind analysis,
signed-integer overflow checks,
and developer checklists.
They also compile without warnings with all (normal)
warning flags enabled and use the Clang static analyzer tool.
On 2014-05-10
Peter Gutmann told me,
&#8220;I always use SQLite as my go-to example
of professional-level OSS development,
I went to a talk given by the [developers] a few years back
on how they develop and test it and was mightily impressed.&#8221;
<li><a href="http://www.postfix.org">Postfix</a>
(<a href="http://mirrors-usa.go-parts.com/postfix/source/index.html">IBM Public License</a>).
I note that both
Elaine R. Palmer and Bill Cheswick thought they did an overall
good job on security and reliability.
The
<a href="http://www.360is.com/06-postfix.htm">Postfix approach
for developing secure software</a>
emphasizes using
a very experienced team of just a few security conscious individuals,
writing it from scratch to be secure (and in particular resistant to
buffer overflows), and an architecture that involves
running as a set of daemons each performing a different set of tasks
(facilitating a &#8220;least privilege&#8221; approach
that can be easily contained further using chroot or virtual containers).
Postfix is implemented using a safe subset of C and POSIX, combined with an
<a href="https://en.wikipedia.org/wiki/Postfix_%28software%29#Implementation">abstraction layer that creates safe alternatives</a>.
For example, it has a &#8220;vstring&#8221; primitive to help resist
buffer overflow attacks and a
&#8220;safe open&#8221; primitive to resist race conditions.
<!--
Elaine R Palmer <erpalmer@us.ibm.com> writes:
covering both security and reliability. . .
PostFix see http://www.postfix.org
-->
<li><a href="http://www.catb.org/gpsd/">GPSD</a>, the
Global Positioning System Service Daemon
(BSD-new, aka Revised BSD or 3-clause license).
This uses
<a href="http://www.aosabook.org/en/gpsd.html">extensive regression testing,
rigorous static checking with multiple tools
and an architectural approach that reduces risks
(e.g., they forbid the use of malloc in the core)</a>.
They use a custom framework for an extensive regression testing suite,
including the use of tools like valgrind.
Their static analysis tools include splint, cppcheck, and Coverity;
they report that,
&#8220;we do not know of any program suite larger than GPSD that
is fully splint-annotated, and strongly suspect that none such yet exist&#8221;.
Perhaps most importantly, they design for zero defects.
Eric Raymond states that
&#8220;if it&#8217;s mobile and hosts anything non-Windows,
GPSD is almost certainly running, including
GPS monitoring on all the world&#8217;s smartphones,
a significant minority of marine navigation systems,
driverless autos from the DARPA
Grand Challenge onwards, [and] most robot submarines and aerial drones...
just one CVE and no known exploits in ten years.
Months at a time go by between new defect reports of any kind...
GPSD is notable as the basis for my assertion that conventional good
practice with C can get you very close to never-break. I got fanatical
about regression testing and routinely applying four static analyzers;
it paid off.&#8221;
<!--
<li><a href="https://www.cs.auckland.ac.nz/~pgut001/cryptlib/index.html">cryptlib</a> (it is not clear this is FLOSS, it uses a non-standard license).
Peter Gutmann reports that as of 2014 it
has had zero (known) vulnerabilities in 19 years.
-->
</ol>

<!--
Possibly add:

Tokeneer (formally proven using Z and SPARK)

Ivy (a proof-checker, itself proven with ACL2)

giflib: Gershom Elber's sound architecture on the original MS-DOS code
plus Toshio Kuratomi and myself not screwing up the Unix port produced
code with an exceptionally low defect rate over 25 years of turning
GIFs into pixels on every platform in the known universe. Two CVEs,
no actual known exploits, and fewer than 3 new bug reports a year on
average over that time.
Whatever machine or device you are on right now probably used giflib to build
part of its display.
-->

<p>
This is certainly not a complete list, and
vulnerabilities will probably be found in at least one of them.
Still, it&#8217;s useful to point to projects that are
seriously working to improve security and/or reliability,
and <i>how</i> they are doing it,
so that others can figure out what might be worth imitating.


<h1 id="conclusions">Conclusions and recommendations</h1>

<p>
The OpenSSL developers and reviewers did use
many tools to detect and reduce the number of vulnerabilities. 
However, the Heartbleed vulnerability suggests
that software development projects (both FLOSS and proprietary) can have
systemic problems in the way that they counter vulnerabilities.

<p>
A key lesson to be learned is that the
<a href="#static-not-found">static</a> and
<a href="#dynamic-not-found">dynamic</a>
analysis approaches often used by many projects today
<i>cannot</i> find problems like Heartbleed.
This includes
<a href="#mostly-positive">mostly-positive automated test suites</a>,
<a href="#basic-fuzzers">common fuzz testing approaches</a>, and
<a href="#code-coverage">typical statement or branch code coverage
approaches</a>.
Several source code weakness analyzer developers are improving their tools
to detect vulnerabilities very similar to Heartbleed,
and that is good news.
But it is obvious that this is not enough.

<p>
No tool or technique guarantees to find all possible vulnerabilities.
However, there are several approaches that could have found Heartbleed,
and vulnerabilities like it, before the vulnerable software was released.
Projects that want to create
secure software need to also add at least one of the following approaches
(and preferably several of them):
<ol>
<li><a href="#negative-testing">Thorough negative testing in test cases (dynamic analysis)</a>
<li><a href="#fuzzing-check-standard">Fuzzing with address checking and standard memory allocator (dynamic analysis)</a>
<li><a href="#compile-standard">Compiling with address checking and standard memory allocator (hybrid analysis)</a>
<li><a href="#review-field-required">Focused manual spotcheck requiring validation of every field (static analysis)</a>
<li><a href="#fuzzer-examine-output">Fuzzing with output examination (dynamic analysis)</a>
<li><a href="#context-configured-static">Context-configured source code weakness analyzers, including annotation systems (static analysis)</a>
<li><a href="#multi-implementation-coverage">Multi-implementation 100% branch coverage (hybrid analysis)</a>
<li><a href="#aggressive-assertions">Aggressive run-time assertions (dynamic analysis)</a>
<li><a href="#safe-language">Safer language (static analysis)</a>
<li><a href="#sound-static-analyzer">Complete static analyzer (static analysis)</a>
<li><a href="#thorough-audit">Thorough human review / audit (static analysis)</a>
<li><a href="#formal-methods">Formal methods (static analysis)</a>
</ol>

<p>
For example,
Google found Heartbleed using
<a href="#thorough-audit">thorough human review / audit</a>, while
Codenomicon found it very soon afterwards using
<a href="#fuzzer-examine-output">fuzzing with output examination</a>.

<p>
Projects should make sure that they are
<a href="#preconditions">easier to analyze</a>.
For example, they should
<a href="#simplify-code">simplify their code</a>,
<a href="#simplify-api">simplify their Application Program Interface (API)</a>,
<a href="#allocate-normally">allocate and deallocate memory normally</a>,
and (if FLOSS)
<a href="#standard-license">use a standard FLOSS license</a> that is
compatible with widely-used FLOSS licenses.
It would also be good to create
a single widely-accepted <i>standard</i> annotation
notation for each major programming language, including C, so that
an annotation language would be easier to adopt.
It would be hard to <i>get</i> that kind of agreement for languages
like C (when there isn&#8217;t already such a notation),
but I think more projects would use them if they were standardized.

<p>
There are also many ways to
<a href="#reduce-impact">reduce the impact of Heartbleed-like vulnerabilities</a> that should be considered:
<ol>
<li><a href="#memory-defenses">Enable memory allocator defenses
once a standard memory allocator is used instead</a>.
Some systems, like GNU malloc on Linux, do not really have such
mechanisms, so they would need to be added first.
<li><a href="#overwrite-critical">Overwrite critical information
(like passwords and private cryptographic keys)
whenever you&#8217;re done with it</a>.
<li><a href="#pfs">Make perfect forward security (PFS) encryption algorithms
the default</a>.
<li><a href="#privilege-separation">Use privilege separation to separate
the critical cryptographic secrets from the rest of the code</a>.
<li><a href="#fix-certificates">Fix the SSL/TLS certificate infrastructure,
especially the default certificate revocation process</a>.
This will require concerted effort to get a real solution (such as
<a href="#x509-ocsp-must-staple">X.509 OCSP must-staple</a>)
specified, implemented, and widely deployed;
we need to start now.
<li><a href="#updates">Make it easy to update software</a>.
<li><a href="#salted-hashes">Store passwords as salted hashes</a>.
<li><a href="#general">General issues: Secure software education/training and reduce attacker incentives</a>.
<li><a href=="#practices">General issues: Apply good development practices</a>
</ol>

<p>
Educational materials should be modified to add these points.
In particular, we need to warn developers about the potential security problems
caused by using memory caching systems in C, C++, or Objective-C;
few materials do that today.
Of course, the real problem is that few developers learn
<a href="https://dwheeler.com/secure-programs">how to
develop secure software</a>, even though nearly all
programs are under attack
(because they connect to the Internet or take data from the Internet).

<p>
Thankfully, it's possible to take such lessons to heart (as it were).
The paper [Walden 2020]
(and its
<a href="https://www.youtube.com/watch?v=hQqL4WcAJzQ"
>matching Youtube video</a>)
analyzes the changes that the OpenSSL project
made in response to Heartbleed, and found that
those changes resulted in signficant improvements to the project results.

<p>
Now let&#8217;s talk specifically about SSL/TLS implementations.
In the short term, I think that a FLOSS project should be established
to create a thorough regression test suite for SSL/TLS, built
using a set of techniques:
<ul>
<li>The suite should do
<a href="#negative-testing">thorough negative testing</a>
(creating tests that should cause
failures/rejections instead of successes).
It needs to test options (the heartbeat of Heartbleed was an option),
of course.
<li>It would be best if this test suite were extended over time to reach towards
<a href="#multi-implementation-coverage">100% branch coverage
of <i>multiple</i> implementations</a>,
since measuring coverage across multiple implementations
can help detect missing input validation and error handling.
<li>Combining these traditional negative tests with
<a href="#fuzzing-check-standard">fuzzing with address checking
and standard memory allocator</a>
that can be run as well would be even better - <i>much</i> better.
Fuzz testing can find many security problems in an attack surface, but
traditional fuzz testing is often shallow, and
fuzz testing can have limited effect if they
are naively applied to cryptographic protocols.
Combining intelligent fuzz testing with traditional tests that
&#8220;get further&#8221;
into the protocol would make the whole testing process far more effective
than fuzz testing on its own.
A full test harness should try to detect even subtle state
errors (e.g., test with and without ASan where that applies, and include
a run with all run-time assertions enabled).
</ul>

<p>
Such a test suite need not compete with other test suites, indeed,
having multiple test suites for an important interface can be really valuable.
Test suites like this can be created for any protocol or interface,
especially those with multiple implementations.
However, I think it is important to focus on SSL/TLS.
A number of serious vulnerabilities have recently been found in a
many different SSL/TLS implementations, all of which can be found
through negative testing.
Also, if <i>any</i> cryptographic library is vulnerable, that vulnerability
is likely to expose data that was properly protected by <i>other</i>
implementations.
This test suite could be reused across all SSL/TLS projects, every time
any developer makes any change, eliminating problems
long before they show up on a user&#8217;s machine.
This suite could also be used by potential users and governments.
If used for validation, a supplier could be required to provide
additional tests to be potentially added to the next version
(making the test suite better over time and helping it keep up
with functional additions).
A common test suite
would give us all better confidence in all SSL/TLS libraries
(including OpenSSL, the libreSSL fork of OpenSSL, GnuTLS,
Mozilla NSS, Google&#8217;s BoringSSL, and so on).
It also has funding advantages; if you&#8217;re not sure if you
should support OpenSSL, the LibreSSL fork, the Google BoringSSL,
or anything else, that
does not matter - such a test suite would help everyone.
Individual projects would continue to have their own test suites,
but clearly the current test suites are inadequate.
Relying on any one technique
to detect vulnerabilities is a bad idea, including
creating a common rigorous test suite; we need to do more.
But this seems to me to be a useful start.

<p>
The goal of this paper is not to ridicule the original developers.
Instead, the goal of this paper is to help identify how to improve things,
so that important projects like OpenSSL can prevent
future similar vulnerabilities by changing how they develop and
evaluate their software.
More broadly, projects need to examine the lessons from
Heartbleed (as listed above) and other vulnerabilities,
and then make sure that they effectively counter similar problems.
They also need to examine the
<a href="#exemplars">projects that seem to be doing well</a>,
to see what approaches could be copied.

<p>
If you enjoyed this paper, you might also enjoy the entire
suite of related papers in
my essay suite <a href="learning-from-disaster.html">Learning from Disaster</a>,
which includes
<a href="shellshock.html">my paper on Shellshock</a>
[<a href="shellshock.html">Wheeler2014b</a>],
the <a href="poodle-sslv3.html">POODLE attack on SSLv3</a>, and the
<a href="apple-goto-fail.html">Apple goto fail vulnerability</a>.

<p>
There is no magic bullet.
However, there <i>are</i> important lessons that
need to be learned.
Projects need to aggressively use a suite of approaches
so that vulnerabilities like Heartbleed almost never occur again.



<h1 id="references">References</h1>
<p>
URLs were current as 2014-05-28.
I have formatted this paper so that people can
find the key references from a printed copy.

<p>
[AdaCore 2014]
AdaCore.
&#8220;Spotlighting a GAP Member: Vermont Technical College (US):
Vermont Tech&#8217;s CubeSat is in orbit and sending down photos and data&#8221;.
<i>GNAT Pro Insider</i>.
Spring/Summer 2014.
page 2.

<p>
[<a href="http://cs.gmu.edu/~offutt/softwaretest/">Ammann2008</a>]
Ammann, Paul and Jeff Offutt.
<i>Introduction to Software Testing</i>.
2008.
University Press.
ISBN-13: 978-0521880381.
ISBN-10: 0521880386.
<a href="http://cs.gmu.edu/~offutt/softwaretest/">http://cs.gmu.edu/~offutt/softwaretest/</a>

<p>
[<a href="http://www.grammatech.com/blog/finding-heartbleed-with-codesonar">Anderson2014</a>]
Anderson, Paul.
Finding Heartbleed with CodeSonar.
May 1, 2014.
<a href="http://www.grammatech.com/blog/finding-heartbleed-with-codesonar">http://www.grammatech.com/blog/finding-heartbleed-with-codesonar</a>

<p>
[<a href="http://www.cs.princeton.edu/~appel/papers/verif-sha.pdf">Appel2014</a>]
Appel, Andrew W.
Verification of a Cryptographic Primitive: SHA-256
<a href="http://www.cs.princeton.edu/~appel/papers/verif-sha.pdf">http://www.cs.princeton.edu/~appel/papers/verif-sha.pdf</a>

<p>
[<a href="https://developer.apple.com/library/mac/releasenotes/ObjectiveC/RN-TransitioningToARC/Introduction/Introduction.html">Apple2013</a>]
Apple.
Apple Automatic Reference Counting (ARC). &#8220;Transitioning to ARC Release Notes.&#8221;  August 2013.
<a href="https://developer.apple.com/library/mac/releasenotes/ObjectiveC/RN-TransitioningToARC/Introduction/Introduction.html">https://developer.apple.com/library/mac/releasenotes/ObjectiveC/RN-TransitioningToARC/Introduction/Introduction.html</a>

<p>
[<a href="http://arcanesentiment.blogspot.se/2014/04/a-sound-bug-finder-is-unsound.html">ArcaneSentiment2014</a>]
Arcane Sentiment.
A sound bug finder is an unsound correctness prover.
2014.
<a href="http://arcanesentiment.blogspot.se/2014/04/a-sound-bug-finder-is-unsound.html">http://arcanesentiment.blogspot.se/2014/04/a-sound-bug-finder-is-unsound.html</a>

<p>
[<a href="http://queue.acm.org/detail.cfm?id=2673311">Arnbak2014</a>]
Arnbak, Axel et al.
&#8220;<a href="http://queue.acm.org/detail.cfm?id=2673311">Assessing legal and technical solutions to secure HTTPS</a>&#8221;.
<i>ACM Queue</i>, vol. 12, no. 8.
<a href="http://queue.acm.org/detail.cfm?id=2673311">http://queue.acm.org/detail.cfm?id=2673311</a>

<p>
<a href="https://blog.hboeck.de/archives/868-How-Heartbleed-couldve-been-found.html">[Boeck2015]</a>
Boeck, Hanno.
"How Heartbleed could've been found".
<i>Hanno's blog</i>.
2014-04-07.
<a href="https://blog.hboeck.de/archives/868-How-Heartbleed-couldve-been-found.html">https://blog.hboeck.de/archives/868-How-Heartbleed-couldve-been-found.html</a>

<p>
[<a href="http://samate.nist.gov/docs/NAVSEA-Tools-Paper-2009-03-02.pdf">BAH2009</a>]
Booz Allen Hamilton.  Software Security Assessment Tools Review.  March 2, 2009.
<a href="http://samate.nist.gov/docs/NAVSEA-Tools-Paper-2009-03-02.pdf">http://samate.nist.gov/docs/NAVSEA-Tools-Paper-2009-03-02.pdf</a>

<p>
[<a href="http://benchmarksgame.alioth.debian.org/u32/which-programs-are-fastest.php">BenchmarksGame</a>]
Benchmarks Game.
<a href="http://benchmarksgame.alioth.debian.org/u32/which-programs-are-fastest.php">http://benchmarksgame.alioth.debian.org/u32/which-programs-are-fastest.php</a>

<p>
[<a href="http://cacm.acm.org/magazines/2010/2/69354-a-few-billion-lines-of-code-later/fulltext">Bessey2010</a>]
Bessey, Al, Ken Block, Ben Chelf, Andy Chou, Bryan Fulton, Seth Hallem, Charles Henri-Gros, Asya Kamsky, Scott McPeak, and Dawson Engler.
A Few Billion Lines of Code Later: Using Static Analysis to Find Bugs in the Real World.
Communications of the ACM, Vol. 53 No. 2, Pages 66-75.
2010.
<a href="http://cacm.acm.org/magazines/2010/2/69354-a-few-billion-lines-of-code-later/fulltext">http://cacm.acm.org/magazines/2010/2/69354-a-few-billion-lines-of-code-later/fulltext</a>

<p>
[<a href="http://research.microsoft.com/en-us/um/people/pg/public_psfiles/icse2013.pdf">Bounimova2013</a>]
Bounimova, Ella, Patrice Godefroid, and David Molnar.
&#8220;Billions and Billions of Constraints:
Whitebox Fuzz Testing in Production&#8221;.
2013.
<a href="http://research.microsoft.com/en-us/um/people/pg/public_psfiles/icse2013.pdf">Bounimova2013</a>

<p>
[<a href="http://www.cubesatlab.org/doc/brandon-chapin-AdaEurope2013.pdf">Brandon2013</a>]
Brandon, Carl, and Peter Chapin.
A SPARK/Ada CubeSat Control Program.
Ada-Europe 2013, LNCS 7896, pp. 51-64.
2013.
Springer-Verlag Berlin Heidelberg.
<a href="http://www.cubesatlab.org/doc/brandon-chapin-AdaEurope2013.pdf">http://www.cubesatlab.org/doc/brandon-chapin-AdaEurope2013.pdf</a>


<p>
[<a href="http://users.ece.cmu.edu/~dawnsong/papers/privtrans.pdf">Brumley</a>]
Brumley, David and Dawn Song.
Privtrans: Automatically Partitioning Programs for Privilege Separation.
<a href="http://users.ece.cmu.edu/~dawnsong/papers/privtrans.pdf">http://users.ece.cmu.edu/~dawnsong/papers/privtrans.pdf</a>

<p>
[<a href="http://shemesh.larc.nasa.gov/fm/fm-what.html">Butler</a>]
Butler, Ricky W.
&#8220;What is Formal Methods?&#8221;
<a href="http://shemesh.larc.nasa.gov/fm/fm-what.html">http://shemesh.larc.nasa.gov/fm/fm-what.html</a>

<p>
[<a href="http://blog.existentialize.com/diagnosis-of-the-openssl-heartbleed-bug.html">Cassidy2014</a>]
Cassidy, Sean.
Diagnosis of the OpenSSL Heartbleed Bug.
2014-04-07.
<a href="http://blog.existentialize.com/diagnosis-of-the-openssl-heartbleed-bug.html">http://blog.existentialize.com/diagnosis-of-the-openssl-heartbleed-bug.html</a>

<p>
[<a href="https://casecurity.org/2014/05/08/casc-heartbleed-response/">CASC2014</a>].
Certificate Authority Security Council (CASC).
<i>CASC Heartbleed Response</i>.
2014-05-08.
<a href="https://casecurity.org/2014/05/08/casc-heartbleed-response/">https://casecurity.org/2014/05/08/casc-heartbleed-response/</a>

<p>
[<a href="http://www.latinpost.com/articles/10440/20140411/heartbleed-bug-discovered.htm">Chandrashekar</a>]
Chandrashekar, Keerthi.
&#8220;How the Heartbleed Bug Was Discovered&#8221;.
<i>Latin Post</i>.
2014-04-11.
<a href="http://www.latinpost.com/articles/10440/20140411/heartbleed-bug-discovered.htm">http://www.latinpost.com/articles/10440/20140411/heartbleed-bug-discovered.htm</a>

<p>
[<a href="http://security.coverity.com/blog/2014/Apr/on-detecting-heartbleed-with-static-analysis.html">Chou2014</a>]
Chou, Andy.
&#8220;On Detecting Heartbleed with Static Analysis&#8221;.
April 13, 2014.
<a href="http://security.coverity.com/blog/2014/Apr/on-detecting-heartbleed-with-static-analysis.html">http://security.coverity.com/blog/2014/Apr/on-detecting-heartbleed-with-static-analysis.html</a>

<p>
[<a href="http://www.codenomicon.com/news/news/2014-05-20.shtml">Codenomicon-How</a>]
Codenomicon.
Heartbleed &amp; SafeGuard: How We Found It.
<a href="http://www.codenomicon.com/news/news/2014-05-20.shtml">http://www.codenomicon.com/news/news/2014-05-20.shtml</a>

<p>
[<a href="http://softwareintegrity.coverity.com/register-for-scan-report-2013.html?cs=pr">Coverity2014</a>]
Coverity.
<i>2013 Coverity Scan Report</i>.
2014.
<a href="http://softwareintegrity.coverity.com/register-for-scan-report-2013.html?cs=pr">http://softwareintegrity.coverity.com/register-for-scan-report-2013.html?cs=pr</a>
<!-- http://www.coverity.com/press-releases/coverity-scan-report-finds-open-source-software-quality-outpaces-proprietary-code-for-the-first-time/ -->

<p>
[<a href="http://research.swtch.com/openssl">Cox2008</a>]
Cox, Russ.
Lessons from the Debian/OpenSSL Fiasco.
May 21, 2008.
<a href="http://research.swtch.com/openssl">http://research.swtch.com/openssl</a>

<p>
[<a href="http://sealedabstract.com/rants/why-mobile-web-apps-are-slow/">Crawford2013</a>].
Crawford, Drew.  &#8220;Why mobile web apps are slow.&#8221;  July 2013.
<a href="http://sealedabstract.com/rants/why-mobile-web-apps-are-slow/">http://sealedabstract.com/rants/why-mobile-web-apps-are-slow/</a>

<p>
[<a href="http://article.gmane.org/gmane.os.openbsd.misc/211963">de Raadt</a>]
de Raadt, Theo.
&#8220;Re: FYA: http://heartbleed.com/&#8221;.
<i>Newsgroup gmane.os.openbsd.misc</i>.
<a href="http://article.gmane.org/gmane.os.openbsd.misc/211963">http://article.gmane.org/gmane.os.openbsd.misc/211963</a>

<p>
[<a href="https://news.ycombinator.com/item?id=7556826">DrewHintz</a>]
DrewHintz.
Comment on &#8220;Neel Mehta donates Heartbleed bounty to Freedom of the Press Foundation&#8221;.
<a href="https://news.ycombinator.com/item?id=7556826">https://news.ycombinator.com/item?id=7556826</a>

<p>
[<a href="https://jhalderm.com/pub/papers/heartbleed-imc14.pdf">Durumeric2014</a>]
Durumeric, Zakir, James Kasten, David Adrian, J. Alex Halderman,
Michael Bailey, Frank Li, Nicholas Weaver,
Johanna Amann, Jethro Beekman, Mathias Payer, Vern Paxson.
The Matter of Heartbleed.
IMC 14, November 5-7, 2014, Vancouver, BC, Canada.
<a href="https://jhalderm.com/pub/papers/heartbleed-imc14.pdf">https://jhalderm.com/pub/papers/heartbleed-imc14.pdf</a>
Available via archive.org.


<p>
[<a href="http://www.businessinsider.com/heartbleed-bug-codenomicon-2014-4">Eadicicco</a>]
Eadicicco, Lisa.
&#8220;How A Group Of Engineers Uncovered The Biggest Bug The Internet Has Seen In Years&#8221;.
<i>Business Insider</i>.
2014-04-10.
<a href="http://www.businessinsider.com/heartbleed-bug-codenomicon-2014-4">http://www.businessinsider.com/heartbleed-bug-codenomicon-2014-4</a>

<p>
[<a href="http://www.openwall.com/lists/musl/2014/01/07/3">Felker2014</a>].
Felker, Rich.
&#8220;Re: Re: Removing sbrk and brk&#8221;.
2014-01-07.
<a href="http://www.openwall.com/lists/musl/2014/01/07/3">http://www.openwall.com/lists/musl/2014/01/07/3</a>.

<p>
[<a href="http://threatpost.com/research-finds-no-large-scale-heartbleed-exploit-attempts-before-vulnerability-disclosure/108161">Fisher2014</a>]
Fisher, Dennis.
&#8220;Research Finds No Large Scale Heartbleed Exploit Attempts Before Vulnerability Disclosure&#8221;
<i>Threatpost</i>.
2014-09-09.
<a href="http://threatpost.com/research-finds-no-large-scale-heartbleed-exploit-attempts-before-vulnerability-disclosure/108161">http://threatpost.com/research-finds-no-large-scale-heartbleed-exploit-attempts-before-vulnerability-disclosure/108161</a>



<p>
[<a href="https://crypto.stanford.edu/~dabo/pubs/abstracts/ssl-client-bugs.html">Georgiev2012</a>].
Georgiev, Martin, Subodh Iyengar, Suman Jana, Rishita Anubhai, Dan Boneh, and Vitaly Shmatikov.
&#8220;The most dangerous code in the world: validating SSL certificates in non-browser software&#8221;.
<i>ACM Conference on Computer and Communications Security</i>.
2012.
pp. 38-49.

<p>
[<a href="https://www.grc.com/revocation/crlsets.htm">Gibson</a>]
Gibson, Steve.
An Evaluation of the Effectiveness of Chrome&#8217;s CRLSets.
2014.
<a href="https://www.grc.com/revocation/crlsets.htm">https://www.grc.com/revocation/crlsets.htm</a>

<p>
[<a href="http://research.microsoft.com/en-us/projects/atg/ndss2008.pdf">Godefroid2008</a>]
Godefroid, Patrice, Michael Y. Levin, and David Berkeley.
&#8220;Automated Whitebox Fuzz Testing&#8221;.

<p>
[<a href="http://arstechnica.com/security/2014/03/critical-crypto-bug-leaves-linux-hundreds-of-apps-open-to-eavesdropping/">Goodin2014a</a>]
Goodin, Dan.
March 4, 2014.
&#8220;Critical crypto bug leaves Linux, hundreds of apps open to eavesdropping&#8221;.
Ars Technica.
<a href="http://arstechnica.com/security/2014/03/critical-crypto-bug-leaves-linux-hundreds-of-apps-open-to-eavesdropping/">http://arstechnica.com/security/2014/03/critical-crypto-bug-leaves-linux-hundreds-of-apps-open-to-eavesdropping</a>

<p>
[<a href="http://arstechnica.com/security/2014/04/how-heartbleed-transformed-https-security-into-the-stuff-of-absurdist-theater/">Goodin2014b</a>].
Goodin, Dan.
&#8220;How Heartbleed transformed HTTPS security into the stuff of absurdist theater:
Certificate revocation checking in browsers is &#8220;useless,&#8221; crypto guru warns.&#8221;
Ars Technica.
2014-04-21.
<a href="http://arstechnica.com/security/2014/04/how-heartbleed-transformed-https-security-into-the-stuff-of-absurdist-theater/">http://arstechnica.com/security/2014/04/how-heartbleed-transformed-https-security-into-the-stuff-of-absurdist-theater/</a>

<p>
[<a href="https://www.gnu.org/licenses/license-list.html#OpenSSL">GNU-Licenses</a>]
GNU project.
<i>Various Licenses and Comments about Them</i>.
<a href="https://www.gnu.org/licenses/license-list.html#OpenSSL">https://www.gnu.org/licenses/license-list.html#OpenSSL</a>

<p>
[<a href="http://research.microsoft.com/en-us/projects/atg/ndss2008.pdf">Godefroid2008</a>]
Godefroid, Patrice, Michael Y. Levin, and David Berkeley.
Automated Whitebox Fuzz Testing.
<a href="http://research.microsoft.com/en-us/projects/atg/ndss2008.pdf">http://research.microsoft.com/en-us/projects/atg/ndss2008.pdf</a>

<p>
[<a href="http://www.theregister.co.uk/2011/04/11/state_of_ssl_analysis/">Goodin2011</a>]
Goodin, Dan.
&#8220;How is SSL hopelessly broken? Let us count the ways:
Blunders expose huge cracks in net&#8217;s trust foundation&#8221;
<i>The Register</i>.
2011-04-11.
<a href="http://www.theregister.co.uk/2011/04/11/state_of_ssl_analysis/">http://www.theregister.co.uk/2011/04/11/state_of_ssl_analysis/</a>

<p>
[<a href="https://www.cs.auckland.ac.nz/~pgut001/pubs/sal.html">Gutmann</a>]
Gutmann, Peter.
Experiences with SAL/PREfast
https://www.cs.auckland.ac.nz/~pgut001/pubs/sal.html

<p>
[<a href="http://www.leshatton.org/index_SA.html">Hatton2003</a>].
Hatton, Les.
&#8220;EC--, a measurement based safer subset of ISO C suitable for embedded system development.&#8221;
2003.
Information and Software Technology, 47 (3) (2005), p. 181-187.
<a href="http://www.leshatton.org/index_SA.html">http://www.leshatton.org/index_SA.html</a>

<p>
[<a href="http://www.leshatton.org/index_SA.html">Hatton2005</a>]
Hatton, Les.
<i>Language subsetting in an industrial context: a
comparison of MISRA C 1998 and MISRA C 2004</i>.
November 20, 2005.
Information and Software Technology 49 (5), p. 475-482, May 2007.
<a href="http://www.leshatton.org/index_SA.html">http://www.leshatton.org/index_SA.html</a>

<p>
[<a href="http://heartbleed.com/">Heartbleed.com</a>]
<a href="http://heartbleed.com/">Heartbleed.com</a> web site.

<p>
[<a href="http://infoscience.epfl.ch/record/153107/files/ESSCAT-report">Hofer2010</a>]
Hofer, Thomas.
Evaluating Static Source Code Analysis Tools
2010.
<a href="http://infoscience.epfl.ch/record/153107/files/ESSCAT-report">http://infoscience.epfl.ch/record/153107/files/ESSCAT-report</a>

<p>
[<a href="http://www.ieee-icnp.org/2008/papers/Index12.pdf">Hsu2008</a>]
Hsu, Yating, Guoqiang Shu, and David Lee.
&#8220;A Model-based Approach to Security Flaw Detection of
Network Protocol Implementations&#8221;
<a href="http://www.ieee-icnp.org/2008/papers/Index12.pdf">http://www.ieee-icnp.org/2008/papers/Index12.pdf</a>

<p>
[<a href="http://forums.xkcd.com/viewtopic.php?f=11&amp;t=108685">Jplus2014</a>]
Jplus.
2014.
Approximate speed classes of programming languages.
XKCD Forums.
<a href="http://forums.xkcd.com/viewtopic.php?f=11&amp;t=108685">http://forums.xkcd.com/viewtopic.php?f=11&amp;t=108685</a>

<p>
[<a href="https://opencryptoaudit.org/reports">Junestam2014</a>]
Junestam, Andreas and Nicolas Guigo (iSECpartners).
<i>Open Crypto Audit Project: TrueCrypt: Security Assessment</i>.
Prepared for the Open Crypto Audit Project.
2014.
<a href="https://opencryptoaudit.org/reports">https://opencryptoaudit.org/reports</a>

<p>
[<a href="http://research.cs.wisc.edu/mist/papers/ManVsAutoVulnAssessment.pdf">Kupsch2009</a>]
Kupsch, J. A. and Barton P. Miller.
&#8220;Manual vs. automated vulnerability assessment: A case study&#8221;.
<i>The First International Workshop on Managing Insider Security Threats</i>, West Lafayette.
2009.
<a href="http://research.cs.wisc.edu/mist/papers/ManVsAutoVulnAssessment.pdf">http://research.cs.wisc.edu/mist/papers/ManVsAutoVulnAssessment.pdf</a>

<p>
[<a href="https://continuousassurance.org/swamp/SWAMP-Heartbleed-White-aper-22Apr2014.pdf">Kupsch2014-April</a>]
Kupsch, James A., and Barton P. Miller.
&#8220;Why do Software Assurance Tools Have Problems Finding Bugs
Like Heartbleed?&#8221;
<a href="https://continuousassurance.org/swamp/SWAMP-Heartbleed-White-aper-22Apr2014.pdf">https://continuousassurance.org/swamp/SWAMP-Heartbleed-White-aper-22Apr2014.pdf</a>

<p>
[<a href="https://continuousassurance.org/swamp/SWAMP-Heartbleed.pdf">Kupsch2014-May</a>]
Kupsch, James A., and Barton P. Miller.
&#8220;Why do Software Assurance Tools Have Problems Finding Bugs
Like Heartbleed?&#8221;
<a href="https://continuousassurance.org/swamp/SWAMP-Heartbleed.pdf">https://continuousassurance.org/swamp/SWAMP-Heartbleed.pdf</a>

<p>
[<a href="https://www.imperialviolet.org/2014/04/19/revchecking.html">Langley2014a</a>]
Langley, Adam.
No, don&#8217;t enable revocation checking.
2014-04-19.
<a href="https://www.imperialviolet.org/2014/04/19/revchecking.html">https://www.imperialviolet.org/2014/04/19/revchecking.html</a>


<p>
[<a href="https://www.imperialviolet.org/2014/04/29/revocationagain.html">Langley2014b</a>]
Langley, Adam.
Revocation still doesn&#8217;t work.
2014-04-29.
<a href="https://www.imperialviolet.org/2014/04/29/revocationagain.html">https://www.imperialviolet.org/2014/04/29/revocationagain.html</a>

<p>
<a href="http://lcamtuf.blogspot.com/2014/11/pulling-jpegs-out-of-thin-air.html">[lcamtuf2014]</a>
lcamtuf (Michal Zalewski).
Pulling JPEGs out of thin air.
<a href="http://lcamtuf.blogspot.com/2014/11/pulling-jpegs-out-of-thin-air.html">http://lcamtuf.blogspot.com/2014/11/pulling-jpegs-out-of-thin-air.html</a>

<p>
[<a href="http://ddosattackprotection.org/blog/heartbleed-bug/">Mailheau</a>]
Mailheau, Rita.
&#8220;Getting to the Heart of the Heartbleed Bug&#8221;.
2014-05-27.
<a href="http://ddosattackprotection.org/blog/heartbleed-bug/">http://ddosattackprotection.org/blog/heartbleed-bug/</a>

<p>
[<a href="https://people.gnome.org/~markmc/openssl-and-the-gpl.html">McLoughlin2004</a>]
Mark McLoughlin, Mark.
June 22, 2004.
<a href="https://people.gnome.org/~markmc/openssl-and-the-gpl.html">https://people.gnome.org/~markmc/openssl-and-the-gpl.html</a>

<p>
[<a href="http://www.thoughtcrime.org/blog/ssl-and-the-future-of-authenticity/">Marlinspike2011</a>]
<i>SSL And The Future Of Authenticity</i>.
April 11, 2011.
<a href="http://www.thoughtcrime.org/blog/ssl-and-the-future-of-authenticity/">http://www.thoughtcrime.org/blog/ssl-and-the-future-of-authenticity</a>

<p>
[<a href="http://www.openbsd.org/papers/portability.pdf">Miller2005</a>]
Miller, Damien.
&#8220;Secure Portability&#8221;.
<i>AUUG 2005</i>.
October 2005.
<a href="http://www.openbsd.org/papers/portability.pdf">http://www.openbsd.org/papers/portability.pdf</a> or
<a href="http://www.mindrot.org/~djm/auug2005/">http://www.mindrot.org/~djm/auug2005/</a>

<p>
[<a href="http://www.openbsd.org/papers/openssh-measures-asiabsdcon2007.pdf">Miller2007</a>]
Miller, Damien.
&#8220;Security measures in OpenSSH&#8221;.
<i>Proceedings of the AsiaBSDCon 2007, Usenix</i>.
March 2007.
<a href="http://www.openbsd.org/papers/openssh-measures-asiabsdcon2007.pdf">http://www.openbsd.org/papers/openssh-measures-asiabsdcon2007.pdf</a>

<p>
[<a href="http://samate.nist.gov/index.php/Tool_Survey.html">NIST</a>]
NIST (SAMATE project).
<i>Tool Survey</i>.
<a href="http://samate.nist.gov/index.php/Tool_Survey.html">http://samate.nist.gov/index.php/Tool_Survey.html</a>

<p>
[<a href="http://samate.nist.gov/SATE5OckhamCriteria.html">NIST-Sound</a>]
NIST.
NIST SAMATE SATE V Ockham Sound Analysis Criteria.
<a href="http://samate.nist.gov/SATE5OckhamCriteria.html">http://samate.nist.gov/SATE5OckhamCriteria.html</a>

<p>
[<a href="http://undeadly.org/cgi?action=article&amp;sid=20031017121955">OpenBSD-Journal</a>]
Various authors (thread).
&#8220;malloc() Guard Pages&#8221;.
<i>OpenBSD Journal</i>.
2003.
See, e.g., &#8220;At least 11 bugs were found so far.&#8221; by David Krause.

<p>
[<a href="http://oreilly.com/catalog/opensources/book/perens.html">Perens1999</a>]
Perens, Bruce.
&#8220;The Open Source Definition&#8221;
<i>Open Sources: Voices from the Open Source Revolution</i>
1st Edition. January 1999.
1-56592-582-3.
<a href="http://oreilly.com/catalog/opensources/book/perens.html">http://oreilly.com/catalog/opensources/book/perens.html</a>

<p>
[<a href="https://www.usenix.org/events/sec03/tech/full_papers/provos_et_al/provos_et_al.pdf">Provos2003</a>]
Provos, Niels, Markus Friedl, and Peter Honeyman.
&#8220;Preventing privilege escalation&#8221;.
<i>Proceedings of the 12th USENIX Security Symposium</i>.
Pages 231-242. August 2003.
<a href="https://www.usenix.org/events/sec03/tech/full_papers/provos_et_al/provos_et_al.pdf">https://www.usenix.org/events/sec03/tech/full_papers/provos_et_al/provos_et_al.pdf</a>

<p>
[<a href="http://www.csoonline.com/article/2466726/data-protection/heartbleed-to-blame-for-community-health-systems-breach.html">Ragan2014</a>]
Ragan, Steve.
&#8220;Heartbleed to blame for Community Health Systems breach&#8221;.
CSO.
Aug 19, 2014 5:44 PM.
[<a href="http://www.csoonline.com/article/2466726/data-protection/heartbleed-to-blame-for-community-health-systems-breach.html">http://www.csoonline.com/article/2466726/data-protection/heartbleed-to-blame-for-community-health-systems-breach.html</a>.





<p>
[<a href="http://www.catb.org/esr/writings/cathedral-bazaar/">Raymond1999</a>]
Raymond, Eric.
&#8220;The Cathedral and the Bazaar&#8221;.
<i>The Cathedral and the Bazaar</i>.
<a href="http://www.catb.org/esr/writings/cathedral-bazaar/">http://www.catb.org/esr/writings/cathedral-bazaar/</a> -
note that Linus&#8217; law is at
<a href="http://www.catb.org/esr/writings/cathedral-bazaar/cathedral-bazaar/ar01s04.html">http://www.catb.org/esr/writings/cathedral-bazaar/cathedral-bazaar/ar01s04.html</a>

<p>
[<a href="https://isecpartners.github.io/news/2014/04/14/iSEC-Completes-Truecrypt-Audit.html">Ritter2014</a>]
Ritter, Tom.
iSEC Completes TrueCrypt Audit: Is TrueCrypt Audited Yet? Yes, in part!
April 24, 2014.
<a href="https://isecpartners.github.io/news/2014/04/14/iSEC-Completes-Truecrypt-Audit.html">https://isecpartners.github.io/news/2014/04/14/iSEC-Completes-Truecrypt-Audit.html</a>

<p>
[<a href="http://blog.leafsr.com/2014/04/11/my-heart-is-ok-but-my-eyes-are-bleeding/">Rohlf2014</a>]
Rohlf, Chris.
2014-04-11.
<i>My heart is ok, but my eyes are bleeding</i>.
Leaf Security Research.
<a href="http://blog.leafsr.com/2014/04/11/my-heart-is-ok-but-my-eyes-are-bleeding/">http://blog.leafsr.com/2014/04/11/my-heart-is-ok-but-my-eyes-are-bleeding/</a>

<p>
[<a href="http://blog.trailofbits.com/2014/04/27/using-static-analysis-and-clang-to-find-heartbleed/">Ruef2014</a>]
Ruef, Andrew.
Using Static Analysis And Clang To Find Heartbleed.
April 27, 2014.
<a href="http://blog.trailofbits.com/2014/04/27/using-static-analysis-and-clang-to-find-heartbleed/">http://blog.trailofbits.com/2014/04/27/using-static-analysis-and-clang-to-find-heartbleed/</a>

<p>
[<a href="http://theinvisiblethings.blogspot.com/2013/08/thoughts-on-intels-upcoming-software.html">Rutkowska2013</a>]
Rutkowska, Joanna.
Thoughts on Intel&#8217;s upcoming Software Guard Extensions (Part 1).
August 30, 2013.
<a href="http://theinvisiblethings.blogspot.com/2013/08/thoughts-on-intels-upcoming-software.html">http://theinvisiblethings.blogspot.com/2013/08/thoughts-on-intels-upcoming-software.html</a>

<p>
[<a href="http://www.klocwork.com/blog/software-security/saving-you-from-heartbleed/">Sarkar2014</a>]
Sarkar, Roy.
&#8220;Saving you from Heartbleed&#8221;.
April 16, 2014.
<a href="http://www.klocwork.com/blog/software-security/saving-you-from-heartbleed/">http://www.klocwork.com/blog/software-security/saving-you-from-heartbleed/</a>

<p>
[<a href="http://www.spacios.eu/sectest2012/pdfs/SecTestICST_Schieferdecker.pdf">Schieferdecker2012</a>]
Schieferdecker, Ina, Jürgen Großmann, and Martin Schneider.
<i>Model-Based Fuzzing for Security Testing</i>.
2012-04-21.
<a href="http://www.spacios.eu/sectest2012/pdfs/SecTestICST_Schieferdecker.pdf">http://www.spacios.eu/sectest2012/pdfs/SecTestICST_Schieferdecker.pdf</a>

<p>
[<a href="https://www.schneier.com/blog/archives/2014/04/heartbleed.html">Schneier2014</a>]
Schneier, Bruce.
Heartbleed.
2014-04-09.
<a href="https://www.schneier.com/blog/archives/2014/04/heartbleed.html">https://www.schneier.com/blog/archives/2014/04/heartbleed.html</a>


<p>
[<a href="https://www.usenix.org/system/files/conference/atc12/atc12-final39.pdf">Serebryany2012</a>]
Serebryany, Konstantin, Derek Bruening, Alexander Potapenko, and Dmitry Vyukov.
&#8220;Address Sanitizer: A Fast Address Sanity Checker&#8221;.
USENIX ATC 2012.
<a href="https://www.usenix.org/system/files/conference/atc12/atc12-final39.pdf">https://www.usenix.org/system/files/conference/atc12/atc12-final39.pdf</a>

<p>
[<a href="http://qspace.library.queensu.ca/handle/1974/1359">Shahriar2008</a>]
Shahriar, Hossain.
<i>Mutation-based testing of buffer overflows,
SQL injections, and format string bugs</i>.
2008.
<a href="http://qspace.library.queensu.ca/handle/1974/1359">http://qspace.library.queensu.ca/handle/1974/1359</a>

<p>
[<a href="http://www.dmst.aueb.gr/dds/pubs/conf/2012-PLATEAU-Fuzzer/pub/html/fuzzer.html">Spinellis2012</a>]
Spinellis, Diomidis, Vassilios Karakoidas, and Panagiotis Louridas.
&#8220;Comparative language fuzz testing: Programming languages vs. fat fingers.&#8221;
<i>PLATEAU 2012: 4th Annual International Workshop on Evaluation and Usability of Programming Languages and Tools - Systems, Programming, Languages and Applications: Software for Humanity (SPLASH 2012).</i>
ACM, October 2012.
(doi:10.1145/2414721.2414727)
<a href="http://www.dmst.aueb.gr/dds/pubs/conf/2012-PLATEAU-Fuzzer/pub/html/fuzzer.html">http://www.dmst.aueb.gr/dds/pubs/conf/2012-PLATEAU-Fuzzer/pub/html/fuzzer.html</a>

<p>
[Sutton2007]
Sutton, Michael, Adam Greene, and Pedram Amini.
<i>Fuzzing: Brute Force Vulnerability Discovery</i>.
2007.
Addison-Wesley.
ISBN 0-321-44611-9.

<p>
[Takanen2008]
Takanen, Ari, Jared D. Demott, and Charles Miller.
<i>Fuzzing for Software Security Testing and Quality Assurance</i>.
2008.
Norwood, MA: Artech House.
ISBN-13 978-1-69693-214-2.

<p>
[<a href="http://www.tedunangst.com/flak/post/analysis-of-openssl-freelist-reuse">Ted</a>]
Ted.
Analysis of openssl freelist reuse.
<a href="http://www.tedunangst.com/flak/post/analysis-of-openssl-freelist-reuse">http://www.tedunangst.com/flak/post/analysis-of-openssl-freelist-reuse</a>

<p>
[<a href="https://www.trustedsec.com/august-2014/chs-hacked-heartbleed-exclusive-trustedsec/">TrustedSec</a>]
<i>CHS Hacked via Heartbleed Vulnerability</i>.
August 19, 2014.
<a href="https://www.trustedsec.com/august-2014/chs-hacked-heartbleed-exclusive-trustedsec/">https://www.trustedsec.com/august-2014/chs-hacked-heartbleed-exclusive-trustedsec/</a>



<p>
[<a href="https://plus.google.com/+JustinUberti/posts/Ah5Gwb9jF4q">Uberti</a>]
Uberti, Justin.
Untitled blog post.
<a href="https://plus.google.com/+JustinUberti/posts/Ah5Gwb9jF4q">https://plus.google.com/+JustinUberti/posts/Ah5Gwb9jF4q</a>

<p>
[<a href="https://arxiv.org/abs/2005.14242">Walden 2020</a>]
Walden, James,
"The Impact of a Major Security Event on an Open Source Project:
The Case of OpenSSL",
17th International Conference on Mining Software Repositories,
2020-05-28,
The video presentation
<a href="https://www.youtube.com/watch?v=hQqL4WcAJzQ"
>"The Impact of a Major Security Event on an Open Source Project:
The Case of OpenSSL"</a> is also available.

<p>
[<a href="https://dwheeler.com/secure-programs/">Wheeler2004</a>]
Wheeler, David A.
<i>Secure Programming for Linux and Unix HOWTO</i>.
2004.
<a href="https://dwheeler.com/secure-programs/">https://dwheeler.com/secure-programs/</a>

<p>
[<a href="https://dwheeler.com/blog/2013/11/16/#vulnerability-economics">Wheeler2013</a>]
Wheeler, David A.
<i>Vulnerability bidding wars and vulnerability economics</i>.
2013-11-16.
<a href="https://dwheeler.com/blog/2013/11/16/#vulnerability-economics">https://dwheeler.com/blog/2013/11/16/#vulnerability-economics</a>

<p>
[<a href="http://www.acq.osd.mil/se/initiatives/init_pp-sse.html">Wheeler2014a</a>]
Wheeler, David A., and Rama S. Moorthy.
&#8220;State-of-the-Art Resources (SOAR) for Software Vulnerability Detection, Test, and Evaluation,&#8221; Institute for Defense Analyses Report P-5061, July 2014.
<a href="http://www.acq.osd.mil/se/docs/P-5061-software-soar-mobility-Final-Full-Doc-20140716.pdf">Main report</a> and <a href="http://www.acq.osd.mil/se/docs/P-5061-AppendixE-soar-sw-matrix-v9-mobility.xlsx">Appendix E (Software State-of-the-Art Resources (SOAR) Matrix)</a>.
<a href="http://www.acq.osd.mil/se/initiatives/init_pp-sse.html">http://www.acq.osd.mil/se/initiatives/init_pp-sse.html</a>.
Alternate location (with embedded appendix E):
<a href="https://www.ida.org/~/media/Corporate/Files/Publications/IDA_Documents/ITSD/2014/P-5061.ashx">https://www.ida.org/~/media/Corporate/Files/Publications/IDA_Documents/ITSD/2014/P-5061.ashx</a>

[<a href="shellshock.html">Wheeler2014b</a>].
Wheeler, David A.
<a href="https://dwheeler.com/essays/shellshock.html">https://dwheeler.com/essays/shellshock.html</a>.

<p>
[<a href="https://www.whitehouse.gov/blog/2014/04/28/heartbleed-understanding-when-we-disclose-cyber-vulnerabilities">[WhiteHouse2014]</a>]
White House.  2014.
&#8220;Heartbleed: Understanding When We Disclose Cyber Vulnerabilities&#8221;
April 28, 2014 at 3:00 PM ET.
<!-- by Michael Daniel -->

<p>
[<a href="http://xkcd.com/1354/">XKCD</a>]
Munroe, Randall.
&#8220;Heartbleed Explanation&#8221;.
<i>XKCD</i>.
<a href="http://xkcd.com/1354/">http://xkcd.com/1354/</a>

<p>
<hr>
<p>
My sincere thanks to the many people who interacted with me to
provide useful feedback on either this paper or on issues relating to it.
Alphabetically by last name these include
Markus Armbruster,
Paul E. Black (NIST),
Bill Cheswick,
Christopher T. Celi (NIST),
Michael J Cooper (NIST),
Andy Chou,
Mark Cornwell,
Daniel Kahn Gillmor,
Peter Gutmann,
Steve Hayes (Codenomicon),
Walt Houser,
Josh Morin (Codenomicon),
Vincent Legoll,
Doug Montgomery (NIST),
Bart P. Miller (University of Wisconsin),
Peter Neumann,
Stephen Nightingale (NIST),
Vadim Okun (NIST),
Elaine R. Palmer,
David Ramos,
Eric S. Raymond,
Chris Rohlf,
Bob Sturm (Codenomicon),
Mikko Varpiola (Codenomicon),
Apostol Vassilev (NIST),
David Wagner,
and
Jim Zemlin (Executive Director of The Linux Foundation).
Walt Houser, in particular, sent me many careful edits and critiques
that I am very thankful for.
There are probably others; I thank them too.

<p>
A brief summary of this paper was published in <i>IEEE Computer</i>.
The citation is:
<a href="http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=6879758">Wheeler, David.
&#8220;Preventing Heartbleed&#8221;.
<i>IEEE Computer</i>.
Volume 47, Issue 8.
August 2014.
pp. 80-83.</a>

<p>
<a href="https://www.ida.org/en/SAC/SACResearchDivisions/ITSD/ITSD_Ideas_Home/IDA_Ideas_Wheeler_Heartbleed2014.aspx">You can see a video of me
presenting a summary of this paper</a>.

<p>
<hr>
<p>
If you enjoyed this paper, you might also enjoy the entire
suite of related papers in
my essay suite <a href="learning-from-disaster.html">Learning from Disaster</a>.
Feel free to see my home page at
<a href="https://dwheeler.com">https://dwheeler.com</a>.
You may also want to look at my paper
<a href="https://dwheeler.com/oss_fs_why.html">Why OSS/FS? Look at
the Numbers!</a> and my book on
<a href="https://dwheeler.com/secure-programs">how to develop
secure programs</a>.

<p>
As always, these are my personal opinions, and not necessarily the opinions
of my employer, government, or
<a href="https://dwheeler.com/wiggles.html">guinea pig</a>.
As usual with my personal writing,
I use logical quotation, as do
<a href="http://www.catb.org/jargon/html/writing-style.html">most hackers</a>
and
<a href="https://en.wikipedia.org/wiki/MOS:LQ#LQ">Wikipedia</a>
(<a href="https://en.wikipedia.org/wiki/Wikipedia:Logical_quotation_on_Wikipedia">here is more on the advantages of logical quotation</a>).
This paper uses the
<a href="http://physics.nist.gov/cuu/Units/binary.html">standard prefixes for
binary multiples</a>; 1 KiB is exactly 2^10 bytes,
while 1 KB is exactly 10^3 bytes.

<p>
(C) Copyright 2014-2024 <a href="https://dwheeler.com">David A. Wheeler</a>.

</body>
</html>






<!--

Below are random notes that might be pulled into the paper someday.



<p>
[<a href="https://www.grc.com/revocation/commentary.htm">Gibson2014a</a>]
Gibson, Steve.

<p>
[<a href="https://www.grc.com/revocation.htm">Gibson2014b</a>]
Gibson, Steve.
Security Certificate Revocation Awareness Test.
<a href="https://www.grc.com/revocation.htm">https://www.grc.com/revocation.htm</a>

<p>
[<a href="https://www.grc.com/revocation/crlsets.htm">Gibson2014c</a>]
Gibson, Steve.
An Evaluation of the Effectiveness of Chrome's CRLSets.
<a href="https://www.grc.com/revocation/crlsets.htm">https://www.grc.com/revocation/crlsets.htm</a>



It was
"independently discovered by a team of security engineers
(Riku, Antti and Matti) at Codenomicon and Neel Mehta of Google Security,
who first reported it to the OpenSSL team. Codenomicon team found
Heartbleed bug while improving the SafeGuard feature in Codenomicon's
Defensics security testing tools and reported this bug to the NCSC-FI
for vulnerability coordination and reporting to OpenSSL team."
[<a href="http://heartbleed.com/">Heartbleed.com</a>].




http://www.linuxfoundation.org/news-media/announcements/2014/04/amazon-web-services-cisco-dell-facebook-fujitsu-google-ibm-intel
"Core infrastructure initiative"

http://www.securityweek.com/tech-titans-launch-core-infrastructure-initiative-secure-key-open-source-components
Technology Giants Aim to Secure Critical Open Source Projects Through ‘Core Infrastructure Initiative’ via The Linux Foundation
Industry heavyweights including Microsoft, Google, Intel, and Cisco are banding together to support and fund open source projects that make up critical elements of global information infrastructure.
The new Core Infrastructure Initiative brings technology companies together to identify and fund open source projects that are widely used in core computing and Internet functions, The Linux Foundation announced today. Formed primarily as the industry's response to the Heartbleed crisis, the OpenSSL library will be the initiative's first project. Other open source projects will follow. 


(http://www.codenomicon.com/news/newsletter/archive/2014-01.html#2
Codenomicon - no details)



LibreSSL fork:
http://arstechnica.com/information-technology/2014/04/openssl-code-beyond-repair-claims-creator-of-libressl-fork/
http://www.tedunangst.com/flak/post/origins-of-libressl
LibreSSL appears to be slowly switching to a standard license instead of OpenSSL nonsense:
http://anoncvs.estpak.ee/cgi-bin/cgit/openbsd-src/commit/lib/libssl?id=374808f4e9c9684f7bcb1762258c4cc9b14706a4



Dual independent discovery increased worry

http://llvm.org/devmtg/2013-11/slides/Serebryany-ASAN.pdf
New features in
AddressSanitizer
LLVM developer meeting
Nov 7, 2013
Alexey Samsonov, Kostya Serebryany


http://www.aosabook.org/en/gpsd.html
GPSD
Eric Raymond

"GPSD is a suite of tools for managing collections of GPS devices and other sensors related to navigation and precision timekeeping, including marine AIS (Automatic Identification System) radios and digital compasses...
GPSD has historically had an exceptionally low defect rate, as measured both by auditing tools such as splint, valgrind, and Coverity and by the incidence of bug reports on its tracker and elsewhere. This did not come about by accident; the project has been very aggressive about incorporating technology for automated testing, and that effort has paid off handsomely...
In 2010, GPSD won the first Good Code Grant from the Alliance for Code Excellence.
...
7.9 Designing for Zero Defects
...
The GPSD project developers believe that the only acceptable policy
is to design for zero defects. Software complexity being what it is,
we have not quite achieved this—but for a project GPSD's size and age
and complexity we come very close.
Our strategy for doing this is a combination of architecture and coding
policies that aim to exclude the possibility of defects in shipped code.
...
One important policy is this: the gpsd daemon never uses
dynamic storage allocation—no malloc or calloc, and
no calls to any functions or libraries that require it.
At a stroke this banishes the single most notorious defect attractor
in C coding. We have no memory leaks and no double-malloc or double-free
bugs, and we never will...

A useful side effect of this policy is that it increases the effectiveness
of static code checkers such as splint, cppcheck, and Coverity. This
feeds into another major policy choice; we make extremely heavy use of
both these code-auditing tools and a custom framework for regression
testing. (We do not know of any program suite larger than GPSD that is
fully splint-annotated, and strongly suspect that none such yet exist.)

The highly modular architecture of GPSD aids us here as well. The module
boundaries serve as cut points where we can rig test harnesses, and we
have very systematically done so. Our normal regression test checks
everything from the floating-point behavior of the host hardware up
through JSON parsing to correct reporting behavior on over seventy
different sensor logs.


"How SQLite is tested"
https://www.sqlite.org/testing.html

GnuTLS - uses dangerous functions in a widespread way
http://www.openldap.org/lists/openldap-devel/200802/msg00072.html

http://www.zdnet.com/gnutls-big-internal-bugs-few-real-world-problems-7000027041/

LibreSSL won't support FIPS:
http://opensslrampage.org/post/83555615721/the-future-or-lack-thereof-of-libressls-fips-object#disqus_thread
"The future (or lack thereof) of LibreSSL’s FIPS Object Module"

http://opensslrampage.org/
LibreSSL comments

See these commetns:
http://www.reddit.com/r/programming/comments/24ipso/how_to_prevent_the_next_heartbleed/


(I use passwords because they are an easy example, and sadly
even today these kinds of problems can happen.
In 2014
<a href="http://arstechnica.com/gaming/2014/04/microsoft-plugs-xbox-one-security-hole-discovered-by-five-year-old/">a five-year-old boy
discovered that he could log in to arbitrary XBox One accounts
by using many spaces as the secondary password</a>.)

http://peachfuzzer.com/resources/
"Peach is a SmartFuzzer that is capable of performing both generation and mutation based fuzzing."
"Peach fuzzer" has a community version (MIT license) and "commercial" and
"enterprise" licenses.  It depends on .NET, but does run on Mono.
XML input format - some may consider its input format clunky.

https://code.google.com/p/fuzzdb/
aggregate of known attack patterns, but higher-level than SSL/TLS

"Flayer" was released by Google and is an OSS fuzzer.
It has found vulnerabilities in OpenSSL, OpenSSH, and others.
http://www.zdnet.com/blog/security/google-ships-open-source-security-fuzzer/517
https://code.google.com/p/flayer/

“We have used a number of patches on top of OpenSSL for many years. Some of them have been accepted into the main OpenSSL repository, but many of them don’t mesh with OpenSSL’s guarantee of API and ABI stability and many of them are a little too experimental… So we’re switching models to one where we import changes from OpenSSL rather than rebasing on top of them. The result of that https://boringssl.googlesource.com/boringssl/ will start to appear in the Chromium repository soon and, over time, we hope to use it in Android and internally too.  There are no guarantees of API or ABI stability with this code: we are not aiming to replace OpenSSL as an open-source project. We will still be sending them bug fixes when we find them and we will be importing changes from upstream. Also, we will still be funding the Core Infrastructure Initiative and the OpenBSD Foundation. But we’ll also be more able to import changes from LibreSSL and they are welcome to take changes from us. We have already relicensed some of our prior contributions to OpenSSL under an ISC license at their request and completely new code that we write will also be so licensed.”

https://www.imperialviolet.org/2014/06/20/boringssl.html 


-->

